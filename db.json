{"meta":{"version":1,"warehouse":"2.2.0"},"models":{"Asset":[{"_id":"source/img/about_time.jpg","path":"img/about_time.jpg","modified":0,"renderable":0},{"_id":"source/img/agg-pro.jpg","path":"img/agg-pro.jpg","modified":0,"renderable":0},{"_id":"source/img/check-error-event.png","path":"img/check-error-event.png","modified":0,"renderable":0},{"_id":"source/img/email_1.png","path":"img/email_1.png","modified":0,"renderable":0},{"_id":"source/img/email_2.png","path":"img/email_2.png","modified":0,"renderable":0},{"_id":"source/img/google.jpg","path":"img/google.jpg","modified":0,"renderable":0},{"_id":"source/img/judge_code.jpg","path":"img/judge_code.jpg","modified":0,"renderable":0},{"_id":"source/img/openfalcon-web .png","path":"img/openfalcon-web .png","modified":0,"renderable":0},{"_id":"source/img/programmer.png","path":"img/programmer.png","modified":0,"renderable":0},{"_id":"source/img/prometheus1.png","path":"img/prometheus1.png","modified":0,"renderable":0},{"_id":"source/img/promethues-alarm.png","path":"img/promethues-alarm.png","modified":0,"renderable":0},{"_id":"source/img/rule_file.png","path":"img/rule_file.png","modified":0,"renderable":0},{"_id":"source/img/sensu-support-platforms.png","path":"img/sensu-support-platforms.png","modified":0,"renderable":0},{"_id":"source/img/uchiwa.png","path":"img/uchiwa.png","modified":0,"renderable":0},{"_id":"themes/yilia/source/main.266c1c.css","path":"main.266c1c.css","modified":0,"renderable":1},{"_id":"themes/yilia/source/slider.096dc6.js","path":"slider.096dc6.js","modified":0,"renderable":1},{"_id":"source/img/sensu-diagram.gif","path":"img/sensu-diagram.gif","modified":0,"renderable":0},{"_id":"source/img/sensu组成部分.png","path":"img/sensu组成部分.png","modified":0,"renderable":0},{"_id":"source/img/tick.png","path":"img/tick.png","modified":0,"renderable":0},{"_id":"themes/yilia/source/main.266c1c.js","path":"main.266c1c.js","modified":0,"renderable":1},{"_id":"themes/yilia/source/mobile.906508.js","path":"mobile.906508.js","modified":0,"renderable":1},{"_id":"source/img/bmw.jpg","path":"img/bmw.jpg","modified":0,"renderable":0},{"_id":"source/img/alipay.png","path":"img/alipay.png","modified":0,"renderable":0},{"_id":"themes/yilia/source/fonts/iconfont.16acc2.ttf","path":"fonts/iconfont.16acc2.ttf","modified":0,"renderable":1},{"_id":"themes/yilia/source/fonts/default-skin.b257fa.svg","path":"fonts/default-skin.b257fa.svg","modified":0,"renderable":1},{"_id":"themes/yilia/source/fonts/iconfont.45d7ee.svg","path":"fonts/iconfont.45d7ee.svg","modified":0,"renderable":1},{"_id":"themes/yilia/source/fonts/iconfont.b322fa.eot","path":"fonts/iconfont.b322fa.eot","modified":0,"renderable":1},{"_id":"themes/yilia/source/fonts/iconfont.8c627f.woff","path":"fonts/iconfont.8c627f.woff","modified":0,"renderable":1},{"_id":"themes/yilia/source/fonts/tooltip.4004ff.svg","path":"fonts/tooltip.4004ff.svg","modified":0,"renderable":1},{"_id":"themes/yilia/source/img/default-skin.png","path":"img/default-skin.png","modified":0,"renderable":1},{"_id":"themes/yilia/source/img/preloader.gif","path":"img/preloader.gif","modified":0,"renderable":1},{"_id":"themes/yilia/source/img/scrollbar_arrow.png","path":"img/scrollbar_arrow.png","modified":0,"renderable":1},{"_id":"source/img/weixinpay.png","path":"img/weixinpay.png","modified":0,"renderable":0},{"_id":"source/img/openfalcon-func.png","path":"img/openfalcon-func.png","modified":0,"renderable":0},{"_id":"source/img/jslon.jpg","path":"img/jslon.jpg","modified":0,"renderable":0},{"_id":"source/img/docker-swarm-architecture.png","path":"img/docker-swarm-architecture.png","modified":0,"renderable":0},{"_id":"source/img/docker-swarm-replicated-vs-global.png","path":"img/docker-swarm-replicated-vs-global.png","modified":0,"renderable":0},{"_id":"source/img/docker-swarm-logo.png","path":"img/docker-swarm-logo.png","modified":0,"renderable":0},{"_id":"source/img/ingress-routing-mesh.png","path":"img/ingress-routing-mesh.png","modified":0,"renderable":0},{"_id":"source/img/intelnal-lb.jpg","path":"img/intelnal-lb.jpg","modified":0,"renderable":0},{"_id":"source/img/external-routing-mesh.png","path":"img/external-routing-mesh.png","modified":0,"renderable":0},{"_id":"source/img/overlay-gwbridge.jpg","path":"img/overlay-gwbridge.jpg","modified":0,"renderable":0},{"_id":"source/img/overlay-vxlan.png","path":"img/overlay-vxlan.png","modified":0,"renderable":0},{"_id":"source/img/service-dns.png","path":"img/service-dns.png","modified":0,"renderable":0},{"_id":"source/img/swarm-node.png","path":"img/swarm-node.png","modified":0,"renderable":0},{"_id":"source/img/swarm-service-lifecycle.png","path":"img/swarm-service-lifecycle.png","modified":0,"renderable":0},{"_id":"source/img/services-diagram.png","path":"img/services-diagram.png","modified":0,"renderable":0},{"_id":"source/img/overlay.jpg","path":"img/overlay.jpg","modified":0,"renderable":0},{"_id":"source/img/swarm-architecture-1.jpg","path":"img/swarm-architecture-1.jpg","modified":0,"renderable":0}],"Cache":[{"_id":"themes/yilia/.editorconfig","hash":"daaa8757fac18f8735fadd0a37a42c06f421ca14","modified":1524802263658},{"_id":"themes/yilia/.babelrc","hash":"db600d40e93e6d8023737a65d58d3be7370e5e30","modified":1524802263655},{"_id":"themes/yilia/.eslintignore","hash":"ed9d8911ca08c3dd5072c48dd0be4d06f8897730","modified":1524802263663},{"_id":"themes/yilia/.eslintrc.js","hash":"303d25adf02ad65720e537a16a4a137d14bb755f","modified":1524802263665},{"_id":"themes/yilia/README.md","hash":"86757b00d393bd4956a252d92a469f11f2ae8914","modified":1524802263668},{"_id":"themes/yilia/_config.yml","hash":"18857cc465a26ef8b36dbdc2090669a4271778c8","modified":1524811746006},{"_id":"themes/yilia/package.json","hash":"ee6aa61f1cb89fd549e3e087c0232207a9c9ee30","modified":1524802263802},{"_id":"themes/yilia/webpack.config.js","hash":"da7657347109ddb4ab8602b219778117254677fe","modified":1524802264015},{"_id":"source/_drafts/my-secret.md","hash":"c8af738a7cfd9579ec83f89f53decedea107c23e","modified":1524802263329},{"_id":"source/_posts/Google-Full.md","hash":"6d536fd8c51be7beb42e7d38a26a32e7b1bf64d2","modified":1524802263329},{"_id":"source/_posts/be-yourself.md","hash":"073f7e8137b76a9f38d62828a6f5d7cc6fd821d8","modified":1524802263330},{"_id":"source/_posts/clean-code.md","hash":"ab847209d21f114f3acec5e1a26247074006920c","modified":1524802263330},{"_id":"source/_posts/compared.md","hash":"ed53474ede604a8b557cd0153975529b3f18c25d","modified":1526560751808},{"_id":"source/_posts/elasticsearch-monitor.md","hash":"a6b2f43dabf6356ed4e5e78e8484094bfa838f45","modified":1524802263334},{"_id":"source/_posts/git-github.md","hash":"4c3e1ba9327d199b0d7d59385a589a934f623e19","modified":1524802263341},{"_id":"source/_posts/hello-world.md","hash":"6886d51c210b73581b259618daf975e35236cf9b","modified":1524802263345},{"_id":"source/_posts/joel-on-software-1-1.md","hash":"42337890346bf6ebfdb8c6a7f8e66684b3c6f793","modified":1524802263348},{"_id":"source/_posts/joel-on-software-1-2.md","hash":"6acbe4d91d9c31d9b8d3a4206c9ca69cb25d975c","modified":1524802263352},{"_id":"source/_posts/lamp-build.md","hash":"a66d79b8be9ba5bd7d7b178b719f35efb3f466c8","modified":1524802263356},{"_id":"source/_posts/more-joel-on-software.md","hash":"46ec2714d7d2e1572efa101b5b7f79d7e759519c","modified":1524802263361},{"_id":"source/_posts/open-falcon.md","hash":"79e128175189a0973da76ef1018439581a75b63d","modified":1540023343513},{"_id":"source/_posts/phoenix.md","hash":"935919c33f5bdd40440248ac2af8bb54ac8f364d","modified":1540023343513},{"_id":"source/_posts/programmer-thought.md","hash":"b5040cc0c27693ba2c43d87636af0d552441170d","modified":1524802263364},{"_id":"source/_posts/prometheus.md","hash":"9c94f6b8f40eaecec62f57bb4cb9b73f4c71cbd2","modified":1540023343514},{"_id":"source/_posts/python-build.md","hash":"6b45d9d502f3acec3f5347d53e9d2688b1be0b98","modified":1524802263367},{"_id":"source/_posts/sensu.md","hash":"56b5300f58232abd172cddbbac75f8c81b0adc6b","modified":1540023343515},{"_id":"source/_posts/tick.md","hash":"714fc2333f79231fe30ecfc5e7a6ae18dd09e9be","modified":1540023343515},{"_id":"source/_posts/wisdom-in-stocks.md","hash":"965b71e0ebbccb6b50a3116d5f4f82513d8c4872","modified":1524802263370},{"_id":"source/_posts/zookeeper-monitor.md","hash":"3061fb28c900aa6f1e309f80dee01f92934c667b","modified":1524802263373},{"_id":"source/img/about_time.jpg","hash":"89a86b255771bdac646343dc9c6c4a3075c01fa0","modified":1524802263378},{"_id":"source/img/agg-pro.jpg","hash":"5bffb7f623c6071595eac344cd13a3def22b2872","modified":1515419127131},{"_id":"source/img/check-error-event.png","hash":"3d70d6143e367c1c76d0fd9e98ea2a62c679c4f1","modified":1517390842467},{"_id":"source/img/email_1.png","hash":"5911e8d70ef39013be19dab6530bbfd1c0edfe3f","modified":1521794359342},{"_id":"source/img/email_2.png","hash":"41c649aa93a87649a9d8014696240a91b1c6a94f","modified":1521795790776},{"_id":"source/img/google.jpg","hash":"877f97f1dac3ec6bfb30fce3546c368831d1a0de","modified":1524802263389},{"_id":"source/img/jalon.jpg","hash":"dab191c8c6f5b0204a4af1f015c036c5c9d72ec3","modified":1524802263393},{"_id":"source/img/judge_code.jpg","hash":"224ebbf4949bd936a376dc3405bade46e1afcf08","modified":1524802263398},{"_id":"source/img/openfalcon-web .png","hash":"40757cd8437bb6caaaa2138f43a1e1a070fccec4","modified":1518165079852},{"_id":"source/img/programmer.png","hash":"f3decc389c703855f5da4a82baa045957ed910cc","modified":1524802263400},{"_id":"source/img/prometheus1.png","hash":"351f3f29d1d13b67335c0107eefa9dcdd723d733","modified":1520410489270},{"_id":"source/img/promethues-alarm.png","hash":"e6d3153926c7d365f6984dfda4583b582690ca92","modified":1521687624256},{"_id":"source/img/rule_file.png","hash":"cdeb8556bd66978cc37916afc14bcd827ce42837","modified":1521687964063},{"_id":"source/img/sensu-support-platforms.png","hash":"c8791a792b51731ebcb7dcd5e2cf28263de738ed","modified":1516590737146},{"_id":"source/img/uchiwa.png","hash":"97684cfb9ae228068dd9d363bc520f1a1b9edcb8","modified":1516617089229},{"_id":"themes/yilia/languages/default.yml","hash":"f26a34a7983d4bc17c65c7f0f14da598e62ce66d","modified":1524802263673},{"_id":"themes/yilia/languages/fr.yml","hash":"b4be1c1592a72012e48df2b3ec41cc9685573e50","modified":1524802263676},{"_id":"themes/yilia/languages/nl.yml","hash":"3d82ec703d0b3287739d7cb4750a715ae83bfcb3","modified":1524802263678},{"_id":"themes/yilia/languages/no.yml","hash":"ddf2035e920a5ecb9076138c184257d9f51896a7","modified":1524802263681},{"_id":"themes/yilia/languages/ru.yml","hash":"2a476b4c6e04900914c81378941640ac5d58a1f0","modified":1524802263683},{"_id":"themes/yilia/languages/zh-CN.yml","hash":"b057f389c6713010f97d461e48ec959b0b6f3b44","modified":1524802263685},{"_id":"themes/yilia/languages/zh-tw.yml","hash":"f5f0ca88185da7a8457760d84bf221781473bd7c","modified":1524802263687},{"_id":"themes/yilia/layout/archive.ejs","hash":"2703b07cc8ac64ae46d1d263f4653013c7e1666b","modified":1524802263780},{"_id":"themes/yilia/layout/category.ejs","hash":"765426a9c8236828dc34759e604cc2c52292835a","modified":1524802263783},{"_id":"themes/yilia/layout/index.ejs","hash":"ec498c6c0606acde997ce195dad97b267418d980","modified":1524802263786},{"_id":"themes/yilia/layout/layout.ejs","hash":"b471ab706d48e0be3f783eab1c94bf5878ef5a94","modified":1524802263789},{"_id":"themes/yilia/layout/page.ejs","hash":"7d80e4e36b14d30a7cd2ac1f61376d9ebf264e8b","modified":1524802263793},{"_id":"themes/yilia/layout/post.ejs","hash":"7d80e4e36b14d30a7cd2ac1f61376d9ebf264e8b","modified":1524802263796},{"_id":"themes/yilia/layout/tag.ejs","hash":"eaa7b4ccb2ca7befb90142e4e68995fb1ea68b2e","modified":1524802263799},{"_id":"themes/yilia/source/main.266c1c.css","hash":"6b9cfabb81f021081a93da5a069674e9be910194","modified":1524802263997},{"_id":"themes/yilia/source/slider.096dc6.js","hash":"25e34d09ead8cabd34d777997c4b1f073918f6cf","modified":1524802264013},{"_id":"themes/yilia/source-src/css.ejs","hash":"cf7eab48d626433120d1ef9697f719a359817018","modified":1524802263808},{"_id":"themes/yilia/source-src/script.ejs","hash":"28abac2426761d7e715b38aadd86ce6549c8ae77","modified":1524802263967},{"_id":"source/img/sensu-diagram.gif","hash":"87a902f0a8274a48ae380b6f82ae8beb25032fd4","modified":1516348821686},{"_id":"source/img/sensu组成部分.png","hash":"e6d81e948584afddbe40ea3f53830fed494994ba","modified":1516347636657},{"_id":"source/img/tick.png","hash":"e0284ecfcca97173f6df1e864f4015776c2209ef","modified":1522658249739},{"_id":"themes/yilia/layout/_partial/toc.ejs","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1524802263768},{"_id":"themes/yilia/source/main.266c1c.js","hash":"42f38b932426a0bd24a41343a75f8517ca00e8eb","modified":1524802264000},{"_id":"themes/yilia/source/mobile.906508.js","hash":"60329066ec16a2f264b438b94c9d1cc44a551f88","modified":1524802264009},{"_id":"source/img/bmw.jpg","hash":"ddfef63ef878c15f0bc0ab0847d5aeab7c0af43c","modified":1524802263386},{"_id":"source/img/alipay.png","hash":"e6ba6d37773c24a784f877962e2563ea5fbcf2a9","modified":1524810473622},{"_id":"themes/yilia/layout/_partial/after-footer.ejs","hash":"c70f367f54064a441e574c913f5e0ea121d0f899","modified":1524802263692},{"_id":"themes/yilia/layout/_partial/archive-post.ejs","hash":"edc0154b30a4127acda10297bec6aacf754b4ac4","modified":1524802263695},{"_id":"themes/yilia/layout/_partial/archive.ejs","hash":"a4eacc2bc1278095a0ef99f904b0634c78f980eb","modified":1524802263697},{"_id":"themes/yilia/layout/_partial/aside.ejs","hash":"751e5deab5365348be5243688b419c82d337ab9a","modified":1524802263702},{"_id":"themes/yilia/layout/_partial/article.ejs","hash":"dff6e1f3b5e82495ec776baa24d9e6bbaad883df","modified":1524802263699},{"_id":"themes/yilia/layout/_partial/baidu-analytics.ejs","hash":"155327c23607f69989b58845f24d842a54e504b8","modified":1524802263705},{"_id":"themes/yilia/layout/_partial/footer.ejs","hash":"871f81cacd5d41cb2eb001cd56254217a857dc2f","modified":1524802263710},{"_id":"themes/yilia/layout/_partial/css.ejs","hash":"9bfcbd9e71401b6da6b2bbbe61e97625ca247b7a","modified":1524802263707},{"_id":"themes/yilia/layout/_partial/google-analytics.ejs","hash":"1ccc627d7697e68fddc367c73ac09920457e5b35","modified":1524802263712},{"_id":"themes/yilia/layout/_partial/head.ejs","hash":"12ca7d8dba56bc767b9309dda9526dcbaffc1614","modified":1524802263715},{"_id":"themes/yilia/layout/_partial/header.ejs","hash":"b69855e07b65117769adc515cb64b803932068c9","modified":1524802263717},{"_id":"themes/yilia/layout/_partial/left-col.ejs","hash":"e2b3f2b3631ef211a4d98d11f0da2d285340f10e","modified":1524802263719},{"_id":"themes/yilia/layout/_partial/mathjax.ejs","hash":"11550a418921d330e6553be0569a94ab5a217967","modified":1524802263723},{"_id":"themes/yilia/layout/_partial/mobile-nav.ejs","hash":"ccec1fc70f021cb50ac85b524e7949878ab93a18","modified":1524802263729},{"_id":"themes/yilia/layout/_partial/tools.ejs","hash":"0ffcb251b79e8a920c9b4cb6bb7a96a808816165","modified":1524802263771},{"_id":"themes/yilia/layout/_partial/viewer.ejs","hash":"cc1c39903aed0a0601d104238d2bbd13ad2a36f3","modified":1524802263777},{"_id":"themes/yilia/source/fonts/iconfont.16acc2.ttf","hash":"f342ac8bf4d937f42a7d6a0032ad267ab47eb7f2","modified":1524802263976},{"_id":"themes/yilia/source/fonts/default-skin.b257fa.svg","hash":"2ac727c9e092331d35cce95af209ccfac6d4c7c7","modified":1524802263972},{"_id":"themes/yilia/source/fonts/iconfont.45d7ee.svg","hash":"75767c904d483d9b93469afb6b92bb6bdface639","modified":1524802263979},{"_id":"themes/yilia/source/fonts/iconfont.b322fa.eot","hash":"bc8c5e88f4994a852041b4d83f126d9c4d419b4a","modified":1524802263983},{"_id":"themes/yilia/source/fonts/iconfont.8c627f.woff","hash":"aa9672cb097f7fd73ae5a03bcd3d9d726935bc0a","modified":1524802263981},{"_id":"themes/yilia/source/fonts/tooltip.4004ff.svg","hash":"397fe4b1093bf9b62457dac48aa15dac06b54a3c","modified":1524802263985},{"_id":"themes/yilia/source/img/default-skin.png","hash":"ed95a8e40a2c3478c5915376acb8e5f33677f24d","modified":1524802263988},{"_id":"themes/yilia/source/img/preloader.gif","hash":"6342367c93c82da1b9c620e97c84a389cc43d96d","modified":1524802263991},{"_id":"themes/yilia/source/img/scrollbar_arrow.png","hash":"d64a33c4ddfbdb89deeb6f4e3d36eb84dc4777c0","modified":1524802263995},{"_id":"themes/yilia/source-src/css/_core.scss","hash":"29ba600e98ed55f7af4ade8038272c84cba21188","modified":1524802263812},{"_id":"themes/yilia/source-src/css/_function.scss","hash":"ce227b6f5a9af194fd5d455200630f32c05e151f","modified":1524802263815},{"_id":"themes/yilia/source-src/css/archive.scss","hash":"d6a7dd88404b383b5b94e4c7ec675a410c41f3cc","modified":1524802263818},{"_id":"themes/yilia/source-src/css/article-inner.scss","hash":"f7388f5c11370ef462f7cb913d8f72edf24ecaf9","modified":1524802263822},{"_id":"themes/yilia/source-src/css/article-nav.scss","hash":"8f82fe898ba1c1bd00c24a7d8270feddc7eba3bc","modified":1524802263828},{"_id":"themes/yilia/source-src/css/article-main.scss","hash":"1577a2336b3ad122f49f60dff2bc1a97d4e7b18b","modified":1524802263826},{"_id":"themes/yilia/source-src/css/article.scss","hash":"55d082fec4c6bb341725567acaa29ce37d50320a","modified":1524802263831},{"_id":"themes/yilia/source-src/css/aside.scss","hash":"07244c188f58ecfb90bb7c047b8cde977f1dc4b4","modified":1524802263834},{"_id":"themes/yilia/source-src/css/comment.scss","hash":"2d1c70bb606c0d87e4f68ec2e600e08b27f32b99","modified":1524802263836},{"_id":"themes/yilia/source-src/css/fonts.scss","hash":"96d7eb1d42c06fdcccb8ef969f6ecd30c3194903","modified":1524802263854},{"_id":"themes/yilia/source-src/css/footer.scss","hash":"7ca837a4cc34db1c35f01baec85eb10ccc64ea86","modified":1524802263868},{"_id":"themes/yilia/source-src/css/global.scss","hash":"b4cb4f45a55d4250cd9056f76dab2a3c0dabcec4","modified":1524802263871},{"_id":"themes/yilia/source-src/css/grid.scss","hash":"f53ea8270752b5919ec5d79224d22af91f2eda12","modified":1524802263874},{"_id":"themes/yilia/source-src/css/highlight.scss","hash":"40e5aa5056dc0b3b9f51c5b387370b612e265d4e","modified":1524802263876},{"_id":"themes/yilia/source-src/css/left.scss","hash":"80dac621e43581a254d0152d5df901e4d0b01c09","modified":1524802263888},{"_id":"themes/yilia/source-src/css/main.scss","hash":"9eba1fcf4805256697528fcf3b767cf6dd8d0591","modified":1524802263890},{"_id":"themes/yilia/source-src/css/mobile-slider.scss","hash":"19f10fd2f0c3377aa4b165b3c2291ecf86dd9351","modified":1524802263893},{"_id":"themes/yilia/source-src/css/mobile.scss","hash":"d995dcd483a250fe61b426158afb61bf8923a927","modified":1524802263896},{"_id":"themes/yilia/source-src/css/page.scss","hash":"244c4d75c375978ff9edb74acc68825e63c6b235","modified":1524802263898},{"_id":"themes/yilia/source-src/css/reward.scss","hash":"a557a9ed244c82b8b71e9da9de3339d92783499f","modified":1524802263900},{"_id":"themes/yilia/source-src/css/scroll.scss","hash":"2495f7e4e3b055735c531f944b5f40a118a351ec","modified":1524802263903},{"_id":"themes/yilia/source-src/css/share.scss","hash":"9d6f6884f40c191882e56a1e1e1192400944a515","modified":1524802263906},{"_id":"themes/yilia/source-src/css/social.scss","hash":"a10a038a1dac8953cb4ffc7e04272eff9fac54e4","modified":1524802263910},{"_id":"themes/yilia/source-src/css/tags-cloud.scss","hash":"399744e98e7c67939ed9b23c2670d8baad044eda","modified":1524802263913},{"_id":"themes/yilia/source-src/css/tags.scss","hash":"915c93edd67c5326695cc7dc84b14c5f154dbcc8","modified":1524802263916},{"_id":"themes/yilia/source-src/css/tools.scss","hash":"6932c642bf8191768d7090982a91c8c1f1c4ed1e","modified":1524802263918},{"_id":"themes/yilia/source-src/css/tooltip.scss","hash":"b81cedbe31accca82e597801186911a7b5e6841c","modified":1524802263921},{"_id":"themes/yilia/source-src/js/Q.js","hash":"e56d9710afa79b31ca6b9fbd845f6d1895f5214b","modified":1524802263928},{"_id":"themes/yilia/source-src/js/anm.js","hash":"d18f6276a352b871390a4112d479b9e58b8cdbbe","modified":1524802263932},{"_id":"themes/yilia/source-src/js/aside.js","hash":"5e4c3c3d61f1e1ce2f09688d3aff25fadc851fff","modified":1524802263936},{"_id":"themes/yilia/source-src/js/browser.js","hash":"4dc04845cf27f350922b63f1813a9c82e6e33b05","modified":1524802263938},{"_id":"themes/yilia/source-src/js/fix.js","hash":"d12df875d3b587354ce59fb7c431ecece53560e3","modified":1524802263944},{"_id":"themes/yilia/source-src/js/main.js","hash":"fe98bf90ce61658fe16ae057f8b6a512a845af3b","modified":1524802263945},{"_id":"themes/yilia/source-src/js/mobile.js","hash":"461c08ffcbc724d74ec7e0ff38e171eefe0f89fd","modified":1524802263949},{"_id":"themes/yilia/source-src/js/report.js","hash":"57680f9a23bd0a1eaafd64ae08cc33e20627ab15","modified":1524802263952},{"_id":"themes/yilia/source-src/js/share.js","hash":"d4ccff8266c37363b3904226f5d035b7db882c61","modified":1524802263955},{"_id":"themes/yilia/source-src/js/slider.js","hash":"707842efee006e3ea9b6765d7460f4ef4f08e41f","modified":1524802263959},{"_id":"themes/yilia/source-src/js/util.js","hash":"3bcdeb95072b85600874424e6929e3e22cfddaa0","modified":1524802263961},{"_id":"themes/yilia/source-src/js/viewer.js","hash":"3e0fd4479a40ddbd1571c6c953df7e23637b61f5","modified":1524802263965},{"_id":"source/img/weixinpay.png","hash":"42703319b16c43ffc5a0084f2b8b3f7526ee83b0","modified":1524810492442},{"_id":"themes/yilia/layout/_partial/script.ejs","hash":"c71c3e704e844df3676f4a1b82d9cd8286f0b06b","modified":1524802263766},{"_id":"source/img/openfalcon-func.png","hash":"6e06aee431cd8f2d1fcf52fecd0ed4f51f19264f","modified":1518059388135},{"_id":"themes/yilia/layout/_partial/post/category.ejs","hash":"f75b236818b6c0ec0e5e6c12a517825d6230d756","modified":1524802263733},{"_id":"themes/yilia/layout/_partial/post/changyan.ejs","hash":"cc384aeaed9ffde92efdf192c26db4da3fe5858f","modified":1524802263736},{"_id":"themes/yilia/layout/_partial/post/date.ejs","hash":"aae96de18d48cd3b9b7bf6fed0100e15b53cca97","modified":1524802263739},{"_id":"themes/yilia/layout/_partial/post/duoshuo.ejs","hash":"f6b4c4eaafb5ac386273354b5f64a26139b7a3b0","modified":1524802263744},{"_id":"themes/yilia/layout/_partial/post/nav.ejs","hash":"b6a97043f9ec37e571aacacfedcda1d4d75e3c7c","modified":1524802263748},{"_id":"themes/yilia/layout/_partial/post/share.ejs","hash":"06a2dd18ac9a43fbc9a59c61e6f795f9326e9927","modified":1524802263752},{"_id":"themes/yilia/layout/_partial/post/tag.ejs","hash":"2c4e4ca36c9bb4318506c38aca7127f1f44d827f","modified":1524802263755},{"_id":"themes/yilia/layout/_partial/post/title.ejs","hash":"d4a460a35e2112d0c7414fd5e19b3a16093f1caf","modified":1524802263759},{"_id":"themes/yilia/layout/_partial/post/wangyiyun.ejs","hash":"fb022502c741b4a26bad6b2ad37245c10ede3f1a","modified":1524802263762},{"_id":"themes/yilia/source-src/css/core/_animation.scss","hash":"1834c3ed8560716e63bb3a50be94cac87fbbeaf3","modified":1524802263840},{"_id":"themes/yilia/source-src/css/core/_media-queries.scss","hash":"262ffcd88775080b7f511db37f58d2bcb1b2bfc7","modified":1524802263842},{"_id":"themes/yilia/source-src/css/core/_mixin.scss","hash":"91db061c9c17628291a005e5bd4936cf9d35a6c4","modified":1524802263845},{"_id":"themes/yilia/source-src/css/core/_reset.scss","hash":"398a49913b4a47d928103562b1ce94520be4026a","modified":1524802263848},{"_id":"themes/yilia/source-src/css/core/_variables.scss","hash":"6e75bdaa46de83094ba0873099c6e7d656a22453","modified":1524802263851},{"_id":"themes/yilia/source-src/css/fonts/iconfont.eot","hash":"bc8c5e88f4994a852041b4d83f126d9c4d419b4a","modified":1524802263858},{"_id":"themes/yilia/source-src/css/fonts/iconfont.svg","hash":"75767c904d483d9b93469afb6b92bb6bdface639","modified":1524802263861},{"_id":"themes/yilia/source-src/css/fonts/iconfont.ttf","hash":"f342ac8bf4d937f42a7d6a0032ad267ab47eb7f2","modified":1524802263864},{"_id":"themes/yilia/source-src/css/fonts/iconfont.woff","hash":"aa9672cb097f7fd73ae5a03bcd3d9d726935bc0a","modified":1524802263866},{"_id":"themes/yilia/source-src/css/img/checkered-pattern.png","hash":"049262fa0886989d750637b264bed34ab51c23c8","modified":1524802263880},{"_id":"themes/yilia/source-src/css/img/scrollbar_arrow.png","hash":"d64a33c4ddfbdb89deeb6f4e3d36eb84dc4777c0","modified":1524802263883},{"_id":"themes/yilia/source-src/css/img/tooltip.svg","hash":"397fe4b1093bf9b62457dac48aa15dac06b54a3c","modified":1524802263885},{"_id":"public/content.json","hash":"dd749c79d69aaa526bec0458d761814d2290f3fe","modified":1540024663166},{"_id":"public/2018/05/16/compared/index.html","hash":"30e3ea788a6dcce3ac08638e7250bbfc974dccdd","modified":1540024665538},{"_id":"public/2018/04/27/open-falcon/index.html","hash":"09e17f44a53a1de203d1523e1291d8c9076751bb","modified":1540024665867},{"_id":"public/2018/03/27/sensu/index.html","hash":"380f7c220675769ad1d9204bea416d526cd18a32","modified":1540024665867},{"_id":"public/2018/03/25/prometheus/index.html","hash":"b30feabad77ec179520cc88dfebfc0eaa6cef566","modified":1540024665875},{"_id":"public/2018/02/27/tick/index.html","hash":"e2ee368b076df8e68f4520e6ca99f47dbc845d72","modified":1540024665875},{"_id":"public/2017/08/20/zookeeper-monitor/index.html","hash":"05270831a8df6830f6a9634bcbe6ccc085fa4330","modified":1540024665876},{"_id":"public/2017/08/17/elasticsearch-monitor/index.html","hash":"fc0f27c611d19b88c5eee94f33f367abcdf2186b","modified":1540024665876},{"_id":"public/2017/03/20/lamp-build/index.html","hash":"bcaa64d760ecac12a423b511493b5568e5489def","modified":1540024665877},{"_id":"public/2017/02/20/more-joel-on-software/index.html","hash":"6377ddd00062004ebda2ffef3060ead93e9f5c8c","modified":1540024665877},{"_id":"public/2016/08/19/python-build/index.html","hash":"09d1a377b94dbc3bb167622530c0a1928d3f3cc4","modified":1540024665877},{"_id":"public/2016/06/21/joel-on-software-1-2/index.html","hash":"4dde7782f05082d285b8e9e7083181248f7ad7fd","modified":1540024665879},{"_id":"public/2015/08/19/Google-Full/index.html","hash":"bcaca5e118f2598214024b25cda148ea8299b1fc","modified":1540024665877},{"_id":"public/2015/06/20/joel-on-software-1-1/index.html","hash":"0ef399755409a6fdcaf9b2378cd5bc3a8edcb9ce","modified":1540024665878},{"_id":"public/2015/06/12/wisdom-in-stocks/index.html","hash":"633435d72f9d39c069c87030aeeb67c45712badb","modified":1540024665878},{"_id":"public/2015/05/20/git-github/index.html","hash":"e5bf5946a7debfd0e915e8b0c39d0cb7085853a8","modified":1540024665878},{"_id":"public/2015/04/20/programmer-thought/index.html","hash":"3f96caad160c7d7415a7ab6046966b12891bd899","modified":1540024665878},{"_id":"public/2014/08/20/clean-code/index.html","hash":"622fba0367fd12a484670b1890d5777f6363b7ea","modified":1540024665879},{"_id":"public/2014/08/17/be-yourself/index.html","hash":"74f00528abe11e4d952dc385941c3d86ff7af267","modified":1540024665879},{"_id":"public/2013/06/15/hello-world/index.html","hash":"5da33e48f7be52054de2929b3f3c16ba9c741718","modified":1540024665879},{"_id":"public/archives/index.html","hash":"d748e058595ffeb7dd248e1352d8fcc80316836c","modified":1540024665882},{"_id":"public/archives/page/2/index.html","hash":"8394a92899936f5880a835ebb36ba87b45fd50c5","modified":1540024665886},{"_id":"public/archives/2013/index.html","hash":"634b6a06067f31c567528ce1e32dea8586c3ef99","modified":1540024665882},{"_id":"public/archives/2013/06/index.html","hash":"61b531e4f31196a84d75b40af46c64c050487e82","modified":1540024665883},{"_id":"public/archives/2014/index.html","hash":"ba9c733cf3fa6d3188d4f0e5bfb71a9ebc4ee24c","modified":1540024665883},{"_id":"public/archives/2014/08/index.html","hash":"99dac3dcd91b7b37e2f057a57fb6b5e10b77e17a","modified":1540024665883},{"_id":"public/archives/2015/index.html","hash":"d86515ff5972eaa956a514b45c01804557d85f89","modified":1540024665883},{"_id":"public/archives/2015/04/index.html","hash":"be18b0984251a44548639a14bfdd98b35a062e75","modified":1540024665883},{"_id":"public/archives/2015/05/index.html","hash":"54c3f9fb8437770aad52a800dec1bf253b45042a","modified":1540024665883},{"_id":"public/archives/2015/06/index.html","hash":"25b572c4864d4737c5436b95c6b4116adef22151","modified":1540024665883},{"_id":"public/archives/2015/08/index.html","hash":"563b2aac235fb1a7213dc5cffecd2bed6ea218aa","modified":1540024665884},{"_id":"public/archives/2016/index.html","hash":"7e8eee51d84914ae37304c1ead5f4d000c35780d","modified":1540024665884},{"_id":"public/archives/2016/06/index.html","hash":"626ab758f1c66fa9c3a0c1b3e3c7c17b9122df85","modified":1540024665884},{"_id":"public/archives/2016/08/index.html","hash":"00deaaac621c446c3f6c46d0c9e654b8428d0c26","modified":1540024665885},{"_id":"public/archives/2017/index.html","hash":"d3773ea684aa921be94c0ab6f15e6cda6ca8bae9","modified":1540024665884},{"_id":"public/archives/2017/02/index.html","hash":"79aee57280d89894ff0ca7e24f10c7bbccd2599d","modified":1540024665884},{"_id":"public/archives/2017/03/index.html","hash":"02e7cc4c2d4e580fa837867c1def4c2e87a39974","modified":1540024665884},{"_id":"public/archives/2017/08/index.html","hash":"ffa67f7d4d469241142d8feab37eb0f9d0e9216c","modified":1540024665884},{"_id":"public/archives/2018/index.html","hash":"3a74a29bd970f45b0619789d36a0711ab71f7b37","modified":1540024665885},{"_id":"public/archives/2018/02/index.html","hash":"d449be837fa312210e0d0c2fff345e912fd0df35","modified":1540024665885},{"_id":"public/archives/2018/03/index.html","hash":"5adf7ca5771fe47c26bc0f3100dd97a2f73115de","modified":1540024665885},{"_id":"public/archives/2018/04/index.html","hash":"e95cf63f1ede5e0168409496cdf07adf6ea0864e","modified":1540024665885},{"_id":"public/archives/2018/05/index.html","hash":"7758b3b628516f7976e80c086aeac020e8c2a474","modified":1540024665885},{"_id":"public/index.html","hash":"10e4547455cc23e9b07fc0c3e978efc89a8a3d81","modified":1540024665879},{"_id":"public/page/2/index.html","hash":"fe88627e47dd9a388c8f423c4eda729202cbcc1e","modified":1540024665880},{"_id":"public/categories/技术/index.html","hash":"442cbd04b37183d0f9e92619249a61c86e7ecb03","modified":1540024665879},{"_id":"public/tags/随笔/index.html","hash":"23633257c2fb9b9592ca6d903d31895fe27191eb","modified":1540024665880},{"_id":"public/tags/Google/index.html","hash":"6738a99f1b9e434563ee0088489dd4dd929f0d8f","modified":1540024665880},{"_id":"public/tags/Elasticsearch/index.html","hash":"eae5fe22ed3f2fc23683db9c5cb9410d53910788","modified":1540024665880},{"_id":"public/tags/Monitor/index.html","hash":"e1f00935dac0ca85f95add758af0370dc7ea51c7","modified":1540024665880},{"_id":"public/tags/Git/index.html","hash":"02e88281a91b4ff26920b008f0ba6a09c4e9ce42","modified":1540024665880},{"_id":"public/tags/GitHub/index.html","hash":"215c92e536b2cf1ab09730961079ca2a2352a8e3","modified":1540024665881},{"_id":"public/tags/Hexo/index.html","hash":"04800b9cc49997310b86a63570f4c4e324209193","modified":1540024665880},{"_id":"public/tags/LAMP/index.html","hash":"62dfef86ad0864acd65f2bc682348001426f456b","modified":1540024665881},{"_id":"public/tags/Open-falcon/index.html","hash":"1ceea0ac5b7faf90ed6a55dc3047c061c8d5421d","modified":1540024665881},{"_id":"public/tags/Go/index.html","hash":"10fd83f492e5ea44b6b3ac41c6fe6ad643a23a44","modified":1540024665881},{"_id":"public/tags/Prometheus/index.html","hash":"6695ab7dd3b30eee92fbf122d2f81dd32eae9ae7","modified":1540024665881},{"_id":"public/tags/Python/index.html","hash":"631767ee2248a01fac16aad35e7562b738e1fa9d","modified":1540024665881},{"_id":"public/tags/TICK/index.html","hash":"48aa65a719fc8f06c31dee1005c1e139f27f9b1a","modified":1540024665881},{"_id":"public/tags/InfluxDB/index.html","hash":"0dea87427d805497f5da0ea4b665409259c9c04f","modified":1540024665882},{"_id":"public/tags/Telegraf/index.html","hash":"cad731a25e579ad45a079fb64766c53542960b86","modified":1540024665882},{"_id":"public/tags/Sensu/index.html","hash":"f8fa7dd8ea6d11733d07384576a265003b8a50b2","modified":1540024665882},{"_id":"public/tags/Ruby/index.html","hash":"51e9385e2651cabd764b145e58bee6b2ace6870d","modified":1540024665882},{"_id":"public/tags/Zookeeper/index.html","hash":"a4b47b765b65a8a7831da39f00769d6d72f1a438","modified":1540024665882},{"_id":"public/2018/05/17/phoenix/index.html","hash":"986e36a73d473c2f68acc408e4df3957328855b0","modified":1540024665537},{"_id":"source/img/jslon.jpg","hash":"a1dbcbb3ad080f833fbda5d645667c3ed5443217","modified":1526562981998},{"_id":"source/_posts/docker-swarm-scheduler.md","hash":"2650d4d5e153198e782d7db203cee95319ea9448","modified":1540024755379},{"_id":"source/_posts/docker-swarm-manager-ha.md","hash":"bf42b9e66a742648c05819352d7bc2f14ced6d01","modified":1540024457341},{"_id":"source/img/docker-swarm-architecture.png","hash":"7e654da5c1a3d47690250e9c9df631d6ed82a55a","modified":1533555201945},{"_id":"source/img/docker-swarm-replicated-vs-global.png","hash":"35efd4d64283c0e84edc613755d2893967a74fef","modified":1533563101090},{"_id":"source/img/docker-swarm-logo.png","hash":"7904e49ae5b26f55577fe7345d3d8334bfc835e2","modified":1533718579004},{"_id":"source/img/ingress-routing-mesh.png","hash":"6a67b8b4070347f264b0572de94c0e20c8d0ae40","modified":1533883724318},{"_id":"source/img/intelnal-lb.jpg","hash":"e4e832d878fd8ae25fd8c1538eec00c93f87f2a1","modified":1533561559750},{"_id":"source/img/external-routing-mesh.png","hash":"509a0f33f7efcaa0f983c4781e0ac4fc7cce4e85","modified":1533562043769},{"_id":"source/img/overlay-gwbridge.jpg","hash":"0b5e1a5f607a2178f7c322896f5f69295124d85b","modified":1532961608000},{"_id":"source/img/overlay-vxlan.png","hash":"0f11c0922818efa00acaf27f5c90a8c5f88d5e49","modified":1534939528871},{"_id":"source/img/service-dns.png","hash":"0edfa55db2ad7358a786a01ed169d96717c10de1","modified":1533561141706},{"_id":"source/img/swarm-node.png","hash":"2134a3e98efa39bcb054e74471a93cc5fdb70eab","modified":1533548394523},{"_id":"source/img/swarm-service-lifecycle.png","hash":"11c9ca9676f39718212dd6eb25f285f75a8fd7b0","modified":1533555916260},{"_id":"source/img/services-diagram.png","hash":"45c2188715db770ebdbe4d1a19d27a00d8fa125e","modified":1533548447507},{"_id":"source/img/overlay.jpg","hash":"35d2918bffd97a5bddd2c97f040a6786a7e42e0b","modified":1533559826710},{"_id":"source/img/swarm-architecture-1.jpg","hash":"b54a178c4ccc9bd683da99833564148d210cda9c","modified":1533558939592},{"_id":"public/2018/10/20/docker-swarm-scheduler/index.html","hash":"6328890e9e9ba2088cd26f41220351f856533022","modified":1540024665898},{"_id":"public/2018/10/19/docker-swarm-manager-ha/index.html","hash":"c98849c32bc364aa993eda5513bcd54fcb39bcc5","modified":1540024665898},{"_id":"public/categories/技术/page/2/index.html","hash":"c48cc0acc2df648d1adead34c0fe242e95ba2cf7","modified":1540024665899},{"_id":"public/page/3/index.html","hash":"036037b48775eb8792f5808cf53aa83cff92c086","modified":1540024665899},{"_id":"public/tags/Docker-Swarm/index.html","hash":"4a3d0240b1665e63ebc2a9b378c2663527d6f78c","modified":1540024665899},{"_id":"public/tags/HA/index.html","hash":"3aaad2d0a488152048f6006e21de36f86d3168a8","modified":1540024665899},{"_id":"public/tags/Service/index.html","hash":"5c9ff1aa1c742b9b4fd456bd2f84439efc2bd511","modified":1540024665899},{"_id":"public/archives/page/3/index.html","hash":"2233fbafad3281453483b8d0b044ed29fb590bd6","modified":1540024665899},{"_id":"public/archives/2018/10/index.html","hash":"b291adb5bd3c40210231eddb5384901dd2576cd6","modified":1540024665899},{"_id":"public/img/docker-swarm-architecture.png","hash":"7e654da5c1a3d47690250e9c9df631d6ed82a55a","modified":1540024665899},{"_id":"public/img/docker-swarm-replicated-vs-global.png","hash":"35efd4d64283c0e84edc613755d2893967a74fef","modified":1540024665900},{"_id":"public/img/docker-swarm-logo.png","hash":"7904e49ae5b26f55577fe7345d3d8334bfc835e2","modified":1540024665900},{"_id":"public/img/ingress-routing-mesh.png","hash":"6a67b8b4070347f264b0572de94c0e20c8d0ae40","modified":1540024665900},{"_id":"public/img/intelnal-lb.jpg","hash":"e4e832d878fd8ae25fd8c1538eec00c93f87f2a1","modified":1540024665900},{"_id":"public/img/external-routing-mesh.png","hash":"509a0f33f7efcaa0f983c4781e0ac4fc7cce4e85","modified":1540024665941},{"_id":"public/img/overlay-gwbridge.jpg","hash":"0b5e1a5f607a2178f7c322896f5f69295124d85b","modified":1540024665941},{"_id":"public/img/overlay-vxlan.png","hash":"0f11c0922818efa00acaf27f5c90a8c5f88d5e49","modified":1540024665941},{"_id":"public/img/service-dns.png","hash":"0edfa55db2ad7358a786a01ed169d96717c10de1","modified":1540024665941},{"_id":"public/img/swarm-node.png","hash":"2134a3e98efa39bcb054e74471a93cc5fdb70eab","modified":1540024665941},{"_id":"public/img/swarm-service-lifecycle.png","hash":"11c9ca9676f39718212dd6eb25f285f75a8fd7b0","modified":1540024665941},{"_id":"public/img/services-diagram.png","hash":"45c2188715db770ebdbe4d1a19d27a00d8fa125e","modified":1540024665942},{"_id":"public/img/overlay.jpg","hash":"35d2918bffd97a5bddd2c97f040a6786a7e42e0b","modified":1540024665970},{"_id":"public/img/swarm-architecture-1.jpg","hash":"b54a178c4ccc9bd683da99833564148d210cda9c","modified":1540024666010}],"Category":[{"name":"技术","_id":"cjhajczoc0007e4rw4adiezct"}],"Data":[],"Page":[],"Post":[{"title":"my-secret","_content":"","source":"_drafts/my-secret.md","raw":"---\ntitle: my-secret\ntags: 随笔\n---\n","slug":"my-secret","published":0,"date":"2018-04-27T04:11:03.329Z","updated":"2018-04-27T04:11:03.329Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjhajcznm0000e4rwcj6fy8zu","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"做最好的自己","date":"2014-08-17T15:34:28.000Z","_content":"\n# 无情岁月增中减，有味诗书淡后甜\n>一月你还没有出现；二月你睡在了隔壁；三月下起了大雨；四月遍地蔷薇；五月我们对面坐着，犹如梦中，就这样六月到了；六月里青草盛开，处处芬芳；七月，悲喜交加，麦浪翻滚连同草地，直到天涯；八月就是八月，八月我守口如瓶，八月我是瓶中的水，你是青天白云；九月和十月，是两双眼睛，装满了大海，你在海上。我在海下；十一月尚未到来，透着它的窗口，我望见了十二月，十二月大雪弥漫。\n你看，这无情的时光却似处处有情，处处藏情，这样你那朝五晚九、没日没夜的努力学习，似乎也晴朗明媚了不少吧！\n\n# 静坐常思己过，闲谈莫论人非\n>能受苦乃为志士，肯吃亏不是痴人，敬君子方显有德，怕小人不算无能，如得意不宜重往，凡做事应有余步。持黄金为珍贵，知安乐方值千金，事临头三思为妙，怒上心忍让最高。\n\n>朝金缨在《格言联壁》中有云：“静坐常思已过，闲谈莫论人非”。上联讲严于律己，下联讲宽厚待人。综上所述，读诵经典，思惟经义，把经典里面的教诲落实在生活与工作中，凡人亦会变得优雅、宁静、志远、真诚、清净、平等、正觉、慈悲、看破、放下、自在、随缘、念佛。\n\n\n![Alt text](/img/bmw.jpg \"宝马\")","source":"_posts/be-yourself.md","raw":"---\ntitle: 做最好的自己\ndate: 2014-08-17 23:34:28\ntags: \n- 随笔 \n---\n\n# 无情岁月增中减，有味诗书淡后甜\n>一月你还没有出现；二月你睡在了隔壁；三月下起了大雨；四月遍地蔷薇；五月我们对面坐着，犹如梦中，就这样六月到了；六月里青草盛开，处处芬芳；七月，悲喜交加，麦浪翻滚连同草地，直到天涯；八月就是八月，八月我守口如瓶，八月我是瓶中的水，你是青天白云；九月和十月，是两双眼睛，装满了大海，你在海上。我在海下；十一月尚未到来，透着它的窗口，我望见了十二月，十二月大雪弥漫。\n你看，这无情的时光却似处处有情，处处藏情，这样你那朝五晚九、没日没夜的努力学习，似乎也晴朗明媚了不少吧！\n\n# 静坐常思己过，闲谈莫论人非\n>能受苦乃为志士，肯吃亏不是痴人，敬君子方显有德，怕小人不算无能，如得意不宜重往，凡做事应有余步。持黄金为珍贵，知安乐方值千金，事临头三思为妙，怒上心忍让最高。\n\n>朝金缨在《格言联壁》中有云：“静坐常思已过，闲谈莫论人非”。上联讲严于律己，下联讲宽厚待人。综上所述，读诵经典，思惟经义，把经典里面的教诲落实在生活与工作中，凡人亦会变得优雅、宁静、志远、真诚、清净、平等、正觉、慈悲、看破、放下、自在、随缘、念佛。\n\n\n![Alt text](/img/bmw.jpg \"宝马\")","slug":"be-yourself","published":1,"updated":"2018-04-27T04:11:03.330Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjhajcznv0001e4rwxsxtyr3d","content":"<h1 id=\"无情岁月增中减，有味诗书淡后甜\"><a href=\"#无情岁月增中减，有味诗书淡后甜\" class=\"headerlink\" title=\"无情岁月增中减，有味诗书淡后甜\"></a>无情岁月增中减，有味诗书淡后甜</h1><blockquote>\n<p>一月你还没有出现；二月你睡在了隔壁；三月下起了大雨；四月遍地蔷薇；五月我们对面坐着，犹如梦中，就这样六月到了；六月里青草盛开，处处芬芳；七月，悲喜交加，麦浪翻滚连同草地，直到天涯；八月就是八月，八月我守口如瓶，八月我是瓶中的水，你是青天白云；九月和十月，是两双眼睛，装满了大海，你在海上。我在海下；十一月尚未到来，透着它的窗口，我望见了十二月，十二月大雪弥漫。<br>你看，这无情的时光却似处处有情，处处藏情，这样你那朝五晚九、没日没夜的努力学习，似乎也晴朗明媚了不少吧！</p>\n</blockquote>\n<h1 id=\"静坐常思己过，闲谈莫论人非\"><a href=\"#静坐常思己过，闲谈莫论人非\" class=\"headerlink\" title=\"静坐常思己过，闲谈莫论人非\"></a>静坐常思己过，闲谈莫论人非</h1><blockquote>\n<p>能受苦乃为志士，肯吃亏不是痴人，敬君子方显有德，怕小人不算无能，如得意不宜重往，凡做事应有余步。持黄金为珍贵，知安乐方值千金，事临头三思为妙，怒上心忍让最高。</p>\n<p>朝金缨在《格言联壁》中有云：“静坐常思已过，闲谈莫论人非”。上联讲严于律己，下联讲宽厚待人。综上所述，读诵经典，思惟经义，把经典里面的教诲落实在生活与工作中，凡人亦会变得优雅、宁静、志远、真诚、清净、平等、正觉、慈悲、看破、放下、自在、随缘、念佛。</p>\n</blockquote>\n<p><img src=\"/img/bmw.jpg\" alt=\"Alt text\" title=\"宝马\"></p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"无情岁月增中减，有味诗书淡后甜\"><a href=\"#无情岁月增中减，有味诗书淡后甜\" class=\"headerlink\" title=\"无情岁月增中减，有味诗书淡后甜\"></a>无情岁月增中减，有味诗书淡后甜</h1><blockquote>\n<p>一月你还没有出现；二月你睡在了隔壁；三月下起了大雨；四月遍地蔷薇；五月我们对面坐着，犹如梦中，就这样六月到了；六月里青草盛开，处处芬芳；七月，悲喜交加，麦浪翻滚连同草地，直到天涯；八月就是八月，八月我守口如瓶，八月我是瓶中的水，你是青天白云；九月和十月，是两双眼睛，装满了大海，你在海上。我在海下；十一月尚未到来，透着它的窗口，我望见了十二月，十二月大雪弥漫。<br>你看，这无情的时光却似处处有情，处处藏情，这样你那朝五晚九、没日没夜的努力学习，似乎也晴朗明媚了不少吧！</p>\n</blockquote>\n<h1 id=\"静坐常思己过，闲谈莫论人非\"><a href=\"#静坐常思己过，闲谈莫论人非\" class=\"headerlink\" title=\"静坐常思己过，闲谈莫论人非\"></a>静坐常思己过，闲谈莫论人非</h1><blockquote>\n<p>能受苦乃为志士，肯吃亏不是痴人，敬君子方显有德，怕小人不算无能，如得意不宜重往，凡做事应有余步。持黄金为珍贵，知安乐方值千金，事临头三思为妙，怒上心忍让最高。</p>\n<p>朝金缨在《格言联壁》中有云：“静坐常思已过，闲谈莫论人非”。上联讲严于律己，下联讲宽厚待人。综上所述，读诵经典，思惟经义，把经典里面的教诲落实在生活与工作中，凡人亦会变得优雅、宁静、志远、真诚、清净、平等、正觉、慈悲、看破、放下、自在、随缘、念佛。</p>\n</blockquote>\n<p><img src=\"/img/bmw.jpg\" alt=\"Alt text\" title=\"宝马\"></p>\n"},{"title":"Google 使用技巧","date":"2015-08-19T14:53:53.000Z","_content":"![Google Img](/img/google.jpg)\n# 下载离线包 \n通过该链接下载离线下载最新的chrome官方离线安装包\n[Download Url](https://www.google.com/intl/zh-CN/chrome/browser/desktop/index.html?standalone=1 \"离线下载\")  \n\n\n# 常用插件\n>Adblock Plus  ：广告过滤\nAppspector ： Detect web applications and javascript libraries run on browsing website.\ncrxMouse Chrome Gestures ：鼠标手势\nHumble New Tab Page ：书签管理、最常访问、最近关闭等\nIE Tab：IE内核打开网页\nSwitchyOmega ：代理工具 \nLastPass： Free Password Manager 密码管理器\nOctotree ：Code tree for GitHub and GitLab\nOneTab ：节省高达95％的内存，并减轻标签页混乱现象\nAxure RP Extension for Chrome ： Axure RP 浏览器插件\n\n# 我喜欢的主题\n>炭黑+銀色金屬\n\n<!-- more -->\n\n# APPs\n>Postman：http测试工具  \nTypeClub：打字练习\n\n# google 使用技巧\n>link:URL = 列出到链接到目标URL的网页清单.\nrelated:URL = 列出于目标URL地址有关的网页.\nsite:Domain Name Registration and Web Hosting 搜索区域仅限于目标网站.\nallinurl:WORDS = 只显示在URL地址里有搜索结果的页面.\ninurl:WORD = 跟allinurl类似,但是只在URL中搜索第一个词.\nallintitle:WORD = 搜索网页标题.\nintitle:WORD = 跟allintitle类似,但是只在标题里搜索第一个词.\ncache:URL = 将显示关于URL的Google缓存(中国不可用).\ninfo:URL = 将显示一个包含了这些元素的页面:类似结果的链接,反向链接,还有包括了这个URL的页面.在搜索框里直接输入URL会起到同样的效果.\nfiletype:SOMEFILETYPE = 指定文件类型.\n-filetype:SOMEFILETYPE = 剔除指定文件类型.\nsite:Welcome to SomeSite.Net ! “+Welcome to SomeSite.Net !” = 显示该站点有多少网页被google收录\nallintext: = 搜索文本,但不包括网页标题和链接\nallinlinks: = 搜索链接, 不包括文本和标题\nWordA OR WordB = 搜索包含两关键词之一的页面\n“Word” OR “Phrase” = 精确的要求搜索单词或者句子\nWordA -WordB = 包含单词A但是不包含单词B\nWordA +WordB = 都包含\n~WORD = 寻找此单词和它的同义词\n~WORD-WORD = 只搜索同义词,不要原词\n","source":"_posts/Google-Full.md","raw":"---\ntitle: Google 使用技巧\ndate: 2015-08-19 22:53:53\ntags: Google \n---\n![Google Img](/img/google.jpg)\n# 下载离线包 \n通过该链接下载离线下载最新的chrome官方离线安装包\n[Download Url](https://www.google.com/intl/zh-CN/chrome/browser/desktop/index.html?standalone=1 \"离线下载\")  \n\n\n# 常用插件\n>Adblock Plus  ：广告过滤\nAppspector ： Detect web applications and javascript libraries run on browsing website.\ncrxMouse Chrome Gestures ：鼠标手势\nHumble New Tab Page ：书签管理、最常访问、最近关闭等\nIE Tab：IE内核打开网页\nSwitchyOmega ：代理工具 \nLastPass： Free Password Manager 密码管理器\nOctotree ：Code tree for GitHub and GitLab\nOneTab ：节省高达95％的内存，并减轻标签页混乱现象\nAxure RP Extension for Chrome ： Axure RP 浏览器插件\n\n# 我喜欢的主题\n>炭黑+銀色金屬\n\n<!-- more -->\n\n# APPs\n>Postman：http测试工具  \nTypeClub：打字练习\n\n# google 使用技巧\n>link:URL = 列出到链接到目标URL的网页清单.\nrelated:URL = 列出于目标URL地址有关的网页.\nsite:Domain Name Registration and Web Hosting 搜索区域仅限于目标网站.\nallinurl:WORDS = 只显示在URL地址里有搜索结果的页面.\ninurl:WORD = 跟allinurl类似,但是只在URL中搜索第一个词.\nallintitle:WORD = 搜索网页标题.\nintitle:WORD = 跟allintitle类似,但是只在标题里搜索第一个词.\ncache:URL = 将显示关于URL的Google缓存(中国不可用).\ninfo:URL = 将显示一个包含了这些元素的页面:类似结果的链接,反向链接,还有包括了这个URL的页面.在搜索框里直接输入URL会起到同样的效果.\nfiletype:SOMEFILETYPE = 指定文件类型.\n-filetype:SOMEFILETYPE = 剔除指定文件类型.\nsite:Welcome to SomeSite.Net ! “+Welcome to SomeSite.Net !” = 显示该站点有多少网页被google收录\nallintext: = 搜索文本,但不包括网页标题和链接\nallinlinks: = 搜索链接, 不包括文本和标题\nWordA OR WordB = 搜索包含两关键词之一的页面\n“Word” OR “Phrase” = 精确的要求搜索单词或者句子\nWordA -WordB = 包含单词A但是不包含单词B\nWordA +WordB = 都包含\n~WORD = 寻找此单词和它的同义词\n~WORD-WORD = 只搜索同义词,不要原词\n","slug":"Google-Full","published":1,"updated":"2018-04-27T04:11:03.329Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjhajczo30003e4rw5oibrebo","content":"<p><img src=\"/img/google.jpg\" alt=\"Google Img\"></p>\n<h1 id=\"下载离线包\"><a href=\"#下载离线包\" class=\"headerlink\" title=\"下载离线包\"></a>下载离线包</h1><p>通过该链接下载离线下载最新的chrome官方离线安装包<br><a href=\"https://www.google.com/intl/zh-CN/chrome/browser/desktop/index.html?standalone=1\" title=\"离线下载\" target=\"_blank\" rel=\"external\">Download Url</a>  </p>\n<h1 id=\"常用插件\"><a href=\"#常用插件\" class=\"headerlink\" title=\"常用插件\"></a>常用插件</h1><blockquote>\n<p>Adblock Plus  ：广告过滤<br>Appspector ： Detect web applications and javascript libraries run on browsing website.<br>crxMouse Chrome Gestures ：鼠标手势<br>Humble New Tab Page ：书签管理、最常访问、最近关闭等<br>IE Tab：IE内核打开网页<br>SwitchyOmega ：代理工具<br>LastPass： Free Password Manager 密码管理器<br>Octotree ：Code tree for GitHub and GitLab<br>OneTab ：节省高达95％的内存，并减轻标签页混乱现象<br>Axure RP Extension for Chrome ： Axure RP 浏览器插件</p>\n</blockquote>\n<h1 id=\"我喜欢的主题\"><a href=\"#我喜欢的主题\" class=\"headerlink\" title=\"我喜欢的主题\"></a>我喜欢的主题</h1><blockquote>\n<p>炭黑+銀色金屬</p>\n</blockquote>\n<a id=\"more\"></a>\n<h1 id=\"APPs\"><a href=\"#APPs\" class=\"headerlink\" title=\"APPs\"></a>APPs</h1><blockquote>\n<p>Postman：http测试工具<br>TypeClub：打字练习</p>\n</blockquote>\n<h1 id=\"google-使用技巧\"><a href=\"#google-使用技巧\" class=\"headerlink\" title=\"google 使用技巧\"></a>google 使用技巧</h1><blockquote>\n<p>link:URL = 列出到链接到目标URL的网页清单.<br>related:URL = 列出于目标URL地址有关的网页.<br>site:Domain Name Registration and Web Hosting 搜索区域仅限于目标网站.<br>allinurl:WORDS = 只显示在URL地址里有搜索结果的页面.<br>inurl:WORD = 跟allinurl类似,但是只在URL中搜索第一个词.<br>allintitle:WORD = 搜索网页标题.<br>intitle:WORD = 跟allintitle类似,但是只在标题里搜索第一个词.<br>cache:URL = 将显示关于URL的Google缓存(中国不可用).<br>info:URL = 将显示一个包含了这些元素的页面:类似结果的链接,反向链接,还有包括了这个URL的页面.在搜索框里直接输入URL会起到同样的效果.<br>filetype:SOMEFILETYPE = 指定文件类型.<br>-filetype:SOMEFILETYPE = 剔除指定文件类型.<br>site:Welcome to SomeSite.Net ! “+Welcome to SomeSite.Net !” = 显示该站点有多少网页被google收录<br>allintext: = 搜索文本,但不包括网页标题和链接<br>allinlinks: = 搜索链接, 不包括文本和标题<br>WordA OR WordB = 搜索包含两关键词之一的页面<br>“Word” OR “Phrase” = 精确的要求搜索单词或者句子<br>WordA -WordB = 包含单词A但是不包含单词B<br>WordA +WordB = 都包含<br>~WORD = 寻找此单词和它的同义词<br>~WORD-WORD = 只搜索同义词,不要原词</p>\n</blockquote>\n","site":{"data":{}},"excerpt":"<p><img src=\"/img/google.jpg\" alt=\"Google Img\"></p>\n<h1 id=\"下载离线包\"><a href=\"#下载离线包\" class=\"headerlink\" title=\"下载离线包\"></a>下载离线包</h1><p>通过该链接下载离线下载最新的chrome官方离线安装包<br><a href=\"https://www.google.com/intl/zh-CN/chrome/browser/desktop/index.html?standalone=1\" title=\"离线下载\" target=\"_blank\" rel=\"external\">Download Url</a>  </p>\n<h1 id=\"常用插件\"><a href=\"#常用插件\" class=\"headerlink\" title=\"常用插件\"></a>常用插件</h1><blockquote>\n<p>Adblock Plus  ：广告过滤<br>Appspector ： Detect web applications and javascript libraries run on browsing website.<br>crxMouse Chrome Gestures ：鼠标手势<br>Humble New Tab Page ：书签管理、最常访问、最近关闭等<br>IE Tab：IE内核打开网页<br>SwitchyOmega ：代理工具<br>LastPass： Free Password Manager 密码管理器<br>Octotree ：Code tree for GitHub and GitLab<br>OneTab ：节省高达95％的内存，并减轻标签页混乱现象<br>Axure RP Extension for Chrome ： Axure RP 浏览器插件</p>\n</blockquote>\n<h1 id=\"我喜欢的主题\"><a href=\"#我喜欢的主题\" class=\"headerlink\" title=\"我喜欢的主题\"></a>我喜欢的主题</h1><blockquote>\n<p>炭黑+銀色金屬</p>\n</blockquote>","more":"<h1 id=\"APPs\"><a href=\"#APPs\" class=\"headerlink\" title=\"APPs\"></a>APPs</h1><blockquote>\n<p>Postman：http测试工具<br>TypeClub：打字练习</p>\n</blockquote>\n<h1 id=\"google-使用技巧\"><a href=\"#google-使用技巧\" class=\"headerlink\" title=\"google 使用技巧\"></a>google 使用技巧</h1><blockquote>\n<p>link:URL = 列出到链接到目标URL的网页清单.<br>related:URL = 列出于目标URL地址有关的网页.<br>site:Domain Name Registration and Web Hosting 搜索区域仅限于目标网站.<br>allinurl:WORDS = 只显示在URL地址里有搜索结果的页面.<br>inurl:WORD = 跟allinurl类似,但是只在URL中搜索第一个词.<br>allintitle:WORD = 搜索网页标题.<br>intitle:WORD = 跟allintitle类似,但是只在标题里搜索第一个词.<br>cache:URL = 将显示关于URL的Google缓存(中国不可用).<br>info:URL = 将显示一个包含了这些元素的页面:类似结果的链接,反向链接,还有包括了这个URL的页面.在搜索框里直接输入URL会起到同样的效果.<br>filetype:SOMEFILETYPE = 指定文件类型.<br>-filetype:SOMEFILETYPE = 剔除指定文件类型.<br>site:Welcome to SomeSite.Net ! “+Welcome to SomeSite.Net !” = 显示该站点有多少网页被google收录<br>allintext: = 搜索文本,但不包括网页标题和链接<br>allinlinks: = 搜索链接, 不包括文本和标题<br>WordA OR WordB = 搜索包含两关键词之一的页面<br>“Word” OR “Phrase” = 精确的要求搜索单词或者句子<br>WordA -WordB = 包含单词A但是不包含单词B<br>WordA +WordB = 都包含<br>~WORD = 寻找此单词和它的同义词<br>~WORD-WORD = 只搜索同义词,不要原词</p>\n</blockquote>"},{"title":"开源监控体系对比","date":"2018-05-16T15:10:07.000Z","_content":"\n## Prometheus、TICK、Sensu、Openfalcon 对比\n\n**目的**\n\n- 研究开源监控系统架构，完善现有的后台采集结构\n- 利用开源监控体系中的采集插件，节省KM研究、开发新监控对象（各类分布式组件）的时间\n- 使用开源监控系统为独立的一套监控系统，作为端到端等分布式应用场景的监控系统\n- 与现有网管后台结合，将开源监控系统作为一类采集数据源\n\n\n\n OpenTSDB、Graphite 偏向数据存储，并无完整的监控体系（采集、存储、告警、展示）。\n\n## 对比表格\n\n以下列出几个重要的关注点\n\n|                              |               Prometheus               |     TICK     |        Sensu         |      Openfalcon      |\n| :--------------------------: | :------------------------------------: | :----------: | :------------------: | :------------------: |\n|             开发语言             |                   Go                   |      Go      |         ruby         |          Go          |\n| <a href=\"#数据收集方式\">数据收集方式</a> |            Pull/Pushgateway            |     Push     |         Push         |         Push         |\n|            采集客户端             |               各种Exporter               |   telegraf   | sensu-client<br>各种脚本 | Falcon-agent<br>各种脚本 |\n|   <a href=\"#数据模型\">数据模型</a>   |                key+tag                 |   key+tag    |         json         |       key+tag        |\n|              存储              | 本地存储 <br>远程数据库(influxDB可读写/OpenTSDB只写) |   influxDB   |        redis         |    mysql/opentsdb    |\n|          agent部署方式           |                   人工                   |      人工      |          人工          |          人工          |\n|            任务更新策略            |                   无                    |      无       |          无           | dashbord可关联主机和pluins |\n|   <a href=\"#数据查询\">数据查询</a>   |                HTTP API                | SQL/HTTP API |  HTTP API/redis-cli  |         sql          |\n\n<!-- more -->\n\n## 数据收集方式\n\nPrometheus 与众不同的采用Pull方式，由服务端主动从被监控机器上面抓取数据。Prometheus 也提供Pushgateway的方式，支持其他数据源主动推送数据到Pushgateway，Pushgateway只做数据缓存，之后仍然是等待服务端抓取。Exporter作为被监控对象暴露指标的客户端，运行在被监控对象上面，并且在Prometheus 服务端配置作为Job，需配置抓取地址端口和间隔。针对各种监控对象，可部署各种Exporter。\n\nPull和Push的主要区别在于Pull方式下，监控系统可以方便的增加删除被监控对象，但是需要被监控对象主动开放端口，这对防火墙/NAT方式下的被监控对象是特殊的要求。而Push模式下，每个采集客户端需要知道服务端的信息，才能上报数据。相比之下，网管后台使用Rabbitmq隔离了服务端的信息，但是仍然需要客户端配置Rabbitmq信息。\n\nTICK使用telegraf作为数据采集端，由telegraf 根据input配置主动采集并发送到output配置的后端，一般是InfluxDB数据库。telegraf还能对采集数据做聚合过滤等处理。telegraf是Go开发的二进制可执行对象，各种对象的采集均打包在一起，通过指定input插件类型，可以指定采集某类指标。相比之下，网管后台使用Java+KM的方式，由Java负责进程调度，KM负责指定采集对象。\n\nSensu 也是采用Push 方式，Sensu由Client发送采集数据到transport，服务端再去transport 读取数据，transport一般使用Rabbitmq。这点跟网管有点类似。Client利用插件采集数据，是ruby脚本，需要额外安装。安装后会在/opt/sensu/embedded/bin/ 路径下下载安装rubu解释器和check插件。process-checks 是进程检测插件，cpu-checks 是cpu检测指标插件。采集配置较为繁琐。\n\nOpenfalcon 也是采用Push 方式，由Client发送采集数据到transport，与Sensu  不同的是，transport 会做一些数据规整，检查之后，转发到多个后端系统去处理。transfer目前支持的业务后端，有三种，judge、graph、opentsdb。judge是告警判定组件，graph作为高性能数据存储、归档、查询组件，opentsdb主要用作数据存储服务。falcon-agent本身仅支持主机类的指标采集，对于其他业务系统、开源软件，需独立的使用采集脚本，将数据按照规定格式，发送到falcon-agent 所在的监听端口。最终再由falcon-agent 转发到服务端。\n\n\n\n综上，其实介绍了四种主要的采集逻辑。以两种不同对象的采集作为示例说明\n\n- 主机类指标CPU\n- 开源软件Redis\n\n<a href=\"#对比表格\">返回目录</a>\n\n## 数据模型\n\n架构的分层隔离，依赖于各层之间接口的规范。对于采集数据而言，Pull/Push 体现的是传输方式，而数据格式，则是数据组织的规范。以下是各个系统的数据格式。\n\n- openfalcon  采用和OpenTSDB相似的数据格式：metric、endpoint加多组key value tags\n\n```\n{\n    metric: load.1min,\n    endpoint: open-falcon-host,\n    tags: srv=falcon,idc=aws-sgp,group=az1,\n    value: 1.5,\n    timestamp: `date +%s`,\n    counterType: GAUGE,\n    step: 60\n}\n```\n\n其中，metric是监控指标名称，endpoint是监控实体，tags是监控数据的属性标签，counterType是Open-Falcon定义的数据类型(取值为GAUGE、COUNTER)，step为监控数据的上报周期，value和timestamp是有效的监控数据。\n\n- prometheus    <metric name>{<label name>=<label value>, ...}\n\n  prometheus 有四种类型的指标：counter、gauge、[summary、histogram](https://prometheus.io/docs/practices/histograms/) ，[示例](http://192.168.7.176:9090/metrics)\n\n```\n# HELP http_requests_total Total number of HTTP requests made.\n# TYPE http_requests_total counter\nhttp_requests_total{code=\"200\",handler=\"alerts\",method=\"get\"} 2\nhttp_requests_total{code=\"200\",handler=\"config\",method=\"get\"} 1\nhttp_requests_total{code=\"200\",handler=\"flags\",method=\"get\"} 2\n\n# HELP go_goroutines Number of goroutines that currently exist.\n# TYPE go_goroutines gauge\ngo_goroutines 235\n\n# HELP go_gc_duration_seconds A summary of the GC invocation durations.\n# TYPE go_gc_duration_seconds summary\ngo_gc_duration_seconds{quantile=\"0\"} 4.5975e-05\ngo_gc_duration_seconds{quantile=\"0.25\"} 8.3846e-05\ngo_gc_duration_seconds{quantile=\"0.5\"} 0.000100729\ngo_gc_duration_seconds{quantile=\"0.75\"} 0.000123377\ngo_gc_duration_seconds{quantile=\"1\"} 0.026837959\ngo_gc_duration_seconds_sum 1.990353198\ngo_gc_duration_seconds_count 10956\n\n# HELP prometheus_tsdb_compaction_chunk_range Final time range of chunks on their first compaction\n# TYPE prometheus_tsdb_compaction_chunk_range histogram\nprometheus_tsdb_compaction_chunk_range_bucket{le=\"100\"} 336\nprometheus_tsdb_compaction_chunk_range_bucket{le=\"400\"} 336\nprometheus_tsdb_compaction_chunk_range_bucket{le=\"1600\"} 336\nprometheus_tsdb_compaction_chunk_range_bucket{le=\"6400\"} 336\nprometheus_tsdb_compaction_chunk_range_bucket{le=\"25600\"} 2336\nprometheus_tsdb_compaction_chunk_range_bucket{le=\"102400\"} 269009\nprometheus_tsdb_compaction_chunk_range_bucket{le=\"409600\"} 271929\nprometheus_tsdb_compaction_chunk_range_bucket{le=\"1.6384e+06\"} 4.2524554e+07\nprometheus_tsdb_compaction_chunk_range_bucket{le=\"6.5536e+06\"} 4.3107242e+07\nprometheus_tsdb_compaction_chunk_range_bucket{le=\"2.62144e+07\"} 4.3107877e+07\nprometheus_tsdb_compaction_chunk_range_bucket{le=\"+Inf\"} 4.3107877e+07\nprometheus_tsdb_compaction_chunk_range_sum 4.4190173791382e+13\nprometheus_tsdb_compaction_chunk_range_count 4.3107877e+07\n```\n\n不同类型的指标，有不同的查询方法，需根据指标含义选择合适的查询方法\n\nhistogram_quantile(0.9, rate(prometheus_tsdb_compaction_chunk_range_bucket[10m]))\n\n- Telegraf 支持多种输入输出格式，重点讲解InfluxDB Line Protocol，另外两种是json/Graphite\n\n```\nweather,location=us-midwest temperature=82 1465839830100400200\n  |    -------------------- --------------  |\n  |             |             |             |\n  |             |             |             |\n+-----------+--------+-+---------+-+---------+\n|measurement|,tag_set| |field_set| |timestamp|\n+-----------+--------+-+---------+-+---------+\n\n> cpu,cpu=cpu0,host=0336dcb23579 usage_system=0,usage_idle=100,usage_nice=0,usage_iowait=0,usage_irq=0,usage_guest=0,usage_user=0,usage_softirq=0,usage_steal=0,usage_guest_nice=0 1515338025000000000\n> cpu,cpu=cpu-total,host=0336dcb23579    usage_guest_nice=0,usage_idle=100,usage_nice=0,usage_irq=0,usage_steal=0,usage_guest=0,usage_user=0,usage_system=0,usage_iowait=0,usage_softirq=0 1515338025000000000\n```\n\ncpu：measurements ，类似oracle的表名  \n\ncpu=cpu-total,host=0336dcb23579 ： cpu、host为tag，索引；    内部以，分割 \n\nusage_guest_nice=0,usage_idle=100,usage_nice=0... : usage_guest_nice等为field，即被采集的指标字段。\n\n***field在measurements是不可或缺的，tag可无， tag与field用空格隔开，field放置在后面***\n\n1515338025000000000： time 时间戳\n\n这里顺便提下 influx metric -> graphite 的转换格式\n\n```\ntemplate = \"host.tags.measurement.field\"\n```\n\n```\ncpu,cpu=cpu-total,dc=us-east-1,host=tars usage_idle=98.09,usage_user=0.89 1455320660004257758\n=>\ntars.cpu-total.us-east-1.cpu.usage_user 0.89 1455320690\ntars.cpu-total.us-east-1.cpu.usage_idle 98.09 1455320690\n```\n\n- Sensu 指标数据，实际上是check的返回结果。应该是json字符串\n\n```shell\n{\n    \"client\": \"tick-client\",\n    \"check\": {\n      \"type\": \"metric\",\n      \"command\": \"metrics-cpu.rb\",\n      \"subscribers\": [\n        \"production\"\n      ],\n      \"interval\": 10,\n      \"handler\": \"tcp_socket\",\n      \"standalone\": true,\n      \"name\": \"cpu_metrics\",\n      \"issued\": 1525270655,\n      \"executed\": 1525270655,\n      \"duration\": 0.074,\n      \"output\": \"tick.cpu.total.user 738228 1525270655\\n...\",\n      \"status\": 0,\n      \"history\": [\n        0,\n\t\t...\n      ]\n    }\n}\n\n```\n\noutput 就是chcek返回结果。由不同的指标采集插件返回，默认的返回格式是Graphite 格式。\n\n```\nsensu.cpu.user 0.50 1515534170\nsensu.cpu.nice 0.00 1515534170\nsensu.cpu.system 0.00 1515534170\nsensu.cpu.idle 99.50 1515534170\nsensu.cpu.iowait 0.00 1515534170\nsensu.cpu.irq 0.00 1515534170\nsensu.cpu.softirq 0.00 1515534170\nsensu.cpu.steal 0.00 1515534170\nsensu.cpu.guest 0.00 1515534170\n```\n\n\n\n<a href=\"#对比表格\">返回目录</a>\n\n## 数据查询\n\n- Prometheus   HTTP API接口查询，查询结果类型：\"resultType\": \"matrix\" | \"vector\" | \"scalar\" | \"string\"\n\n  matrix ： 返回某个时间段内的指标数据集合\n\n  vector：返回单个时间点的指标数据\n\n  ```\n  # Instant queries\n  curl 'http://192.168.7.176:9090/api/v1/query?query=elasticsearch_node_stats_up&time=1525313513.452&_=1525313461038' -s | python -m json.tool\n\n  # Range queries\n  curl 'http://192.168.7.176:9090/api/v1/query_range?query=prometheus_tsdb_compaction_chunk_range_bucket&start=1525307102.216&end=1525307302.216&step=28&_=1525313461049' -s|python -m json.tool\n\n  ```\n\n  ​\n\n- TICK  HTTP API查询/登陆数据库查询sql\n\n  ```\n  curl -G \"http://192.168.7.176:8086/query?pretty=true\" --data-urlencode \"db=mydb\" \\\n  --data-urlencode \"q=SELECT * FROM cpu WHERE host='server01' AND time < now() - 1d\"\n\n  ##==>\n  SELECT * FROM cpu WHERE host='server01' AND time < now() - 1d\n  ```\n\n  ​\n\n- Sensu   HTTP API查询/登陆redis查询sql\n\n  ```\n  curl -s http://127.0.0.1:4567/clients | jq .\n  curl -s http://localhost:4567/events | jq .\n  curl -s http://localhost:4567/results | jq .\n  curl -s http://localhost:4567/checks | jq .\n\n  ###===>\n  redis-cli\n  keys * \n  get result:tick-client:cpu_metrics\n  ```\n\n  ​\n\n- Openfalcon \n\n  从mysql数据库中取数\n\n  ​\n\n<a href=\"#对比表格\">返回目录</a>\n\n## 网管对接\n\n1. 跟云资源采集类似，将上述监控系统作为一个数据采集中心。开发一个转储接口程序，从监控系统中（数据库或者REST API）读取配置、性能数据等，转换数据格式，存入网管系统。\n\n   优点：可以方便对接第三方开源系统，利用其成熟的监控指标和稳定的监控体系。对原有的网管系统也不需要过多的改造，相当于改造重心在转储接口程序。\n\n   ​ 缺点：需要额外维护多套监控系统以及对应的转换程序；对该转换程序尽可能做得通用，因为可以同时对接多套监控系统；对于指标的定义、配置，仍然需要沿用原来的方法，即还要开发KM、配置表单，这部分是无法避免的。\n\n\n\n2. 仅复用监控系统的采集客户端，类似现在的Agent对接开源组件的模式。例如prometheus，可以复用其exporter。即prometheus exporter独立运行并暴露数据至指定监听端口，Agent开发http接口定时扫描读取该端口暴露的指标即可，然后由KM负责解析转换，最终存入网管系统。\n\n   优点：仅仅用到第三方开源系统的采集客户端，抓取我们想要的数据。对原有的网管系统也不需要过多的改造，仅需要新增KM，对这类采集结果做格式解析处理。exporter暴露的数据指标相对统一，处理起来也比较方便。\n\n   缺点：需要额外维护exporter，且需要开通exporter监听端口的访问权限；exporter的定义配置如何通知给网管系统；对于指标的定义、配置，仍然需要沿用原来的方法，即还要开发KM、配置表单，这部分是无法避免的。\n\n\n\n3. 复用监控系统的采集客户端，同时改造网管系统部分功能。例如telegraf，telegraf支持输出结果到influxdb,或者生成本地json文件。如果是本地Json文件，则与prometheus类似，由KM解析即可。如果是influxdb，则需开发程序从influxdb读取解析数据(相当于另一个数据源)。\n\n   优点：仅仅用到第三方开源系统的采集客户端，抓取我们想要的数据。对原有的网管系统也不需要过多的改造，仅需要新增KM或者influxdb读取程序，对这类采集结果做格式解析处理。telegraf暴露的数据指标相对统一，处理起来也比较方便。\n\n   缺点：需要额外维护telegraf进程；telegraf的定义配置如何通知给网管系统；对于指标的定义、配置，仍然需要沿用原来的方法，即还要KM、配置表单，这部分是无法避免的。\n\n<a href=\"#对比表格\">返回目录</a>\n\n\n\n## 支持的采集对象\n\n- Prometheus\n\nhttps://prometheus.io/docs/instrumenting/exporters/\n\n\n\n- Telegraf\n\nhttps://github.com/influxdata/telegraf/tree/master/plugins/inputs\n\njmx方式采集：kafka\n\nhttps://github.com/influxdata/telegraf/tree/master/plugins/inputs/jolokia2\n\nhttps://docs.confluent.io/current/kafka/monitoring.html\n\n\n\n- Sensu\n\nhttps://github.com/sensu-plugins\n\n\n\n- Openfalcon\n\nhttps://book.open-falcon.org/zh_0_2/usage/\n\n[https://github.com/iambocai/falcon-monit-scripts](https://github.com/iambocai/falcon-monit-scripts)\n\n\n\n- Datadog \n\nhttps://app.datadoghq.com/account/settings#integrations\n\n\n\n## 结论\n\n采用第二种方案，结合开源组件的客户端和采集脚本，有Agent负责调度，解析结果并返回。\n\n被监控对象：ES /kakfa\n\n实现的客户端：telegraf/openfaclon   客户端\n\n目的：从前台配置开始，最终采集入库\n\n\n\njolokia2 采集分为两种模式，一种是jvm代理，由java应用（kafka）启动时修改参数引入jolokia2 包```-javaagent:/usr/hdp/2.6.1.0-129/kafka/libs/jolokia-jvm-1.5.0-agent.jar=port=8778,host=0.0.0.0```\n\n第二种是Proxy模式，由于第一种应用的限制，proxy模式不需要在应用端修改\n\n","source":"_posts/compared.md","raw":"---\ntitle: 开源监控体系对比\ndate: 2018-05-16 23:10:07\ncategories: 技术\n---\n\n## Prometheus、TICK、Sensu、Openfalcon 对比\n\n**目的**\n\n- 研究开源监控系统架构，完善现有的后台采集结构\n- 利用开源监控体系中的采集插件，节省KM研究、开发新监控对象（各类分布式组件）的时间\n- 使用开源监控系统为独立的一套监控系统，作为端到端等分布式应用场景的监控系统\n- 与现有网管后台结合，将开源监控系统作为一类采集数据源\n\n\n\n OpenTSDB、Graphite 偏向数据存储，并无完整的监控体系（采集、存储、告警、展示）。\n\n## 对比表格\n\n以下列出几个重要的关注点\n\n|                              |               Prometheus               |     TICK     |        Sensu         |      Openfalcon      |\n| :--------------------------: | :------------------------------------: | :----------: | :------------------: | :------------------: |\n|             开发语言             |                   Go                   |      Go      |         ruby         |          Go          |\n| <a href=\"#数据收集方式\">数据收集方式</a> |            Pull/Pushgateway            |     Push     |         Push         |         Push         |\n|            采集客户端             |               各种Exporter               |   telegraf   | sensu-client<br>各种脚本 | Falcon-agent<br>各种脚本 |\n|   <a href=\"#数据模型\">数据模型</a>   |                key+tag                 |   key+tag    |         json         |       key+tag        |\n|              存储              | 本地存储 <br>远程数据库(influxDB可读写/OpenTSDB只写) |   influxDB   |        redis         |    mysql/opentsdb    |\n|          agent部署方式           |                   人工                   |      人工      |          人工          |          人工          |\n|            任务更新策略            |                   无                    |      无       |          无           | dashbord可关联主机和pluins |\n|   <a href=\"#数据查询\">数据查询</a>   |                HTTP API                | SQL/HTTP API |  HTTP API/redis-cli  |         sql          |\n\n<!-- more -->\n\n## 数据收集方式\n\nPrometheus 与众不同的采用Pull方式，由服务端主动从被监控机器上面抓取数据。Prometheus 也提供Pushgateway的方式，支持其他数据源主动推送数据到Pushgateway，Pushgateway只做数据缓存，之后仍然是等待服务端抓取。Exporter作为被监控对象暴露指标的客户端，运行在被监控对象上面，并且在Prometheus 服务端配置作为Job，需配置抓取地址端口和间隔。针对各种监控对象，可部署各种Exporter。\n\nPull和Push的主要区别在于Pull方式下，监控系统可以方便的增加删除被监控对象，但是需要被监控对象主动开放端口，这对防火墙/NAT方式下的被监控对象是特殊的要求。而Push模式下，每个采集客户端需要知道服务端的信息，才能上报数据。相比之下，网管后台使用Rabbitmq隔离了服务端的信息，但是仍然需要客户端配置Rabbitmq信息。\n\nTICK使用telegraf作为数据采集端，由telegraf 根据input配置主动采集并发送到output配置的后端，一般是InfluxDB数据库。telegraf还能对采集数据做聚合过滤等处理。telegraf是Go开发的二进制可执行对象，各种对象的采集均打包在一起，通过指定input插件类型，可以指定采集某类指标。相比之下，网管后台使用Java+KM的方式，由Java负责进程调度，KM负责指定采集对象。\n\nSensu 也是采用Push 方式，Sensu由Client发送采集数据到transport，服务端再去transport 读取数据，transport一般使用Rabbitmq。这点跟网管有点类似。Client利用插件采集数据，是ruby脚本，需要额外安装。安装后会在/opt/sensu/embedded/bin/ 路径下下载安装rubu解释器和check插件。process-checks 是进程检测插件，cpu-checks 是cpu检测指标插件。采集配置较为繁琐。\n\nOpenfalcon 也是采用Push 方式，由Client发送采集数据到transport，与Sensu  不同的是，transport 会做一些数据规整，检查之后，转发到多个后端系统去处理。transfer目前支持的业务后端，有三种，judge、graph、opentsdb。judge是告警判定组件，graph作为高性能数据存储、归档、查询组件，opentsdb主要用作数据存储服务。falcon-agent本身仅支持主机类的指标采集，对于其他业务系统、开源软件，需独立的使用采集脚本，将数据按照规定格式，发送到falcon-agent 所在的监听端口。最终再由falcon-agent 转发到服务端。\n\n\n\n综上，其实介绍了四种主要的采集逻辑。以两种不同对象的采集作为示例说明\n\n- 主机类指标CPU\n- 开源软件Redis\n\n<a href=\"#对比表格\">返回目录</a>\n\n## 数据模型\n\n架构的分层隔离，依赖于各层之间接口的规范。对于采集数据而言，Pull/Push 体现的是传输方式，而数据格式，则是数据组织的规范。以下是各个系统的数据格式。\n\n- openfalcon  采用和OpenTSDB相似的数据格式：metric、endpoint加多组key value tags\n\n```\n{\n    metric: load.1min,\n    endpoint: open-falcon-host,\n    tags: srv=falcon,idc=aws-sgp,group=az1,\n    value: 1.5,\n    timestamp: `date +%s`,\n    counterType: GAUGE,\n    step: 60\n}\n```\n\n其中，metric是监控指标名称，endpoint是监控实体，tags是监控数据的属性标签，counterType是Open-Falcon定义的数据类型(取值为GAUGE、COUNTER)，step为监控数据的上报周期，value和timestamp是有效的监控数据。\n\n- prometheus    <metric name>{<label name>=<label value>, ...}\n\n  prometheus 有四种类型的指标：counter、gauge、[summary、histogram](https://prometheus.io/docs/practices/histograms/) ，[示例](http://192.168.7.176:9090/metrics)\n\n```\n# HELP http_requests_total Total number of HTTP requests made.\n# TYPE http_requests_total counter\nhttp_requests_total{code=\"200\",handler=\"alerts\",method=\"get\"} 2\nhttp_requests_total{code=\"200\",handler=\"config\",method=\"get\"} 1\nhttp_requests_total{code=\"200\",handler=\"flags\",method=\"get\"} 2\n\n# HELP go_goroutines Number of goroutines that currently exist.\n# TYPE go_goroutines gauge\ngo_goroutines 235\n\n# HELP go_gc_duration_seconds A summary of the GC invocation durations.\n# TYPE go_gc_duration_seconds summary\ngo_gc_duration_seconds{quantile=\"0\"} 4.5975e-05\ngo_gc_duration_seconds{quantile=\"0.25\"} 8.3846e-05\ngo_gc_duration_seconds{quantile=\"0.5\"} 0.000100729\ngo_gc_duration_seconds{quantile=\"0.75\"} 0.000123377\ngo_gc_duration_seconds{quantile=\"1\"} 0.026837959\ngo_gc_duration_seconds_sum 1.990353198\ngo_gc_duration_seconds_count 10956\n\n# HELP prometheus_tsdb_compaction_chunk_range Final time range of chunks on their first compaction\n# TYPE prometheus_tsdb_compaction_chunk_range histogram\nprometheus_tsdb_compaction_chunk_range_bucket{le=\"100\"} 336\nprometheus_tsdb_compaction_chunk_range_bucket{le=\"400\"} 336\nprometheus_tsdb_compaction_chunk_range_bucket{le=\"1600\"} 336\nprometheus_tsdb_compaction_chunk_range_bucket{le=\"6400\"} 336\nprometheus_tsdb_compaction_chunk_range_bucket{le=\"25600\"} 2336\nprometheus_tsdb_compaction_chunk_range_bucket{le=\"102400\"} 269009\nprometheus_tsdb_compaction_chunk_range_bucket{le=\"409600\"} 271929\nprometheus_tsdb_compaction_chunk_range_bucket{le=\"1.6384e+06\"} 4.2524554e+07\nprometheus_tsdb_compaction_chunk_range_bucket{le=\"6.5536e+06\"} 4.3107242e+07\nprometheus_tsdb_compaction_chunk_range_bucket{le=\"2.62144e+07\"} 4.3107877e+07\nprometheus_tsdb_compaction_chunk_range_bucket{le=\"+Inf\"} 4.3107877e+07\nprometheus_tsdb_compaction_chunk_range_sum 4.4190173791382e+13\nprometheus_tsdb_compaction_chunk_range_count 4.3107877e+07\n```\n\n不同类型的指标，有不同的查询方法，需根据指标含义选择合适的查询方法\n\nhistogram_quantile(0.9, rate(prometheus_tsdb_compaction_chunk_range_bucket[10m]))\n\n- Telegraf 支持多种输入输出格式，重点讲解InfluxDB Line Protocol，另外两种是json/Graphite\n\n```\nweather,location=us-midwest temperature=82 1465839830100400200\n  |    -------------------- --------------  |\n  |             |             |             |\n  |             |             |             |\n+-----------+--------+-+---------+-+---------+\n|measurement|,tag_set| |field_set| |timestamp|\n+-----------+--------+-+---------+-+---------+\n\n> cpu,cpu=cpu0,host=0336dcb23579 usage_system=0,usage_idle=100,usage_nice=0,usage_iowait=0,usage_irq=0,usage_guest=0,usage_user=0,usage_softirq=0,usage_steal=0,usage_guest_nice=0 1515338025000000000\n> cpu,cpu=cpu-total,host=0336dcb23579    usage_guest_nice=0,usage_idle=100,usage_nice=0,usage_irq=0,usage_steal=0,usage_guest=0,usage_user=0,usage_system=0,usage_iowait=0,usage_softirq=0 1515338025000000000\n```\n\ncpu：measurements ，类似oracle的表名  \n\ncpu=cpu-total,host=0336dcb23579 ： cpu、host为tag，索引；    内部以，分割 \n\nusage_guest_nice=0,usage_idle=100,usage_nice=0... : usage_guest_nice等为field，即被采集的指标字段。\n\n***field在measurements是不可或缺的，tag可无， tag与field用空格隔开，field放置在后面***\n\n1515338025000000000： time 时间戳\n\n这里顺便提下 influx metric -> graphite 的转换格式\n\n```\ntemplate = \"host.tags.measurement.field\"\n```\n\n```\ncpu,cpu=cpu-total,dc=us-east-1,host=tars usage_idle=98.09,usage_user=0.89 1455320660004257758\n=>\ntars.cpu-total.us-east-1.cpu.usage_user 0.89 1455320690\ntars.cpu-total.us-east-1.cpu.usage_idle 98.09 1455320690\n```\n\n- Sensu 指标数据，实际上是check的返回结果。应该是json字符串\n\n```shell\n{\n    \"client\": \"tick-client\",\n    \"check\": {\n      \"type\": \"metric\",\n      \"command\": \"metrics-cpu.rb\",\n      \"subscribers\": [\n        \"production\"\n      ],\n      \"interval\": 10,\n      \"handler\": \"tcp_socket\",\n      \"standalone\": true,\n      \"name\": \"cpu_metrics\",\n      \"issued\": 1525270655,\n      \"executed\": 1525270655,\n      \"duration\": 0.074,\n      \"output\": \"tick.cpu.total.user 738228 1525270655\\n...\",\n      \"status\": 0,\n      \"history\": [\n        0,\n\t\t...\n      ]\n    }\n}\n\n```\n\noutput 就是chcek返回结果。由不同的指标采集插件返回，默认的返回格式是Graphite 格式。\n\n```\nsensu.cpu.user 0.50 1515534170\nsensu.cpu.nice 0.00 1515534170\nsensu.cpu.system 0.00 1515534170\nsensu.cpu.idle 99.50 1515534170\nsensu.cpu.iowait 0.00 1515534170\nsensu.cpu.irq 0.00 1515534170\nsensu.cpu.softirq 0.00 1515534170\nsensu.cpu.steal 0.00 1515534170\nsensu.cpu.guest 0.00 1515534170\n```\n\n\n\n<a href=\"#对比表格\">返回目录</a>\n\n## 数据查询\n\n- Prometheus   HTTP API接口查询，查询结果类型：\"resultType\": \"matrix\" | \"vector\" | \"scalar\" | \"string\"\n\n  matrix ： 返回某个时间段内的指标数据集合\n\n  vector：返回单个时间点的指标数据\n\n  ```\n  # Instant queries\n  curl 'http://192.168.7.176:9090/api/v1/query?query=elasticsearch_node_stats_up&time=1525313513.452&_=1525313461038' -s | python -m json.tool\n\n  # Range queries\n  curl 'http://192.168.7.176:9090/api/v1/query_range?query=prometheus_tsdb_compaction_chunk_range_bucket&start=1525307102.216&end=1525307302.216&step=28&_=1525313461049' -s|python -m json.tool\n\n  ```\n\n  ​\n\n- TICK  HTTP API查询/登陆数据库查询sql\n\n  ```\n  curl -G \"http://192.168.7.176:8086/query?pretty=true\" --data-urlencode \"db=mydb\" \\\n  --data-urlencode \"q=SELECT * FROM cpu WHERE host='server01' AND time < now() - 1d\"\n\n  ##==>\n  SELECT * FROM cpu WHERE host='server01' AND time < now() - 1d\n  ```\n\n  ​\n\n- Sensu   HTTP API查询/登陆redis查询sql\n\n  ```\n  curl -s http://127.0.0.1:4567/clients | jq .\n  curl -s http://localhost:4567/events | jq .\n  curl -s http://localhost:4567/results | jq .\n  curl -s http://localhost:4567/checks | jq .\n\n  ###===>\n  redis-cli\n  keys * \n  get result:tick-client:cpu_metrics\n  ```\n\n  ​\n\n- Openfalcon \n\n  从mysql数据库中取数\n\n  ​\n\n<a href=\"#对比表格\">返回目录</a>\n\n## 网管对接\n\n1. 跟云资源采集类似，将上述监控系统作为一个数据采集中心。开发一个转储接口程序，从监控系统中（数据库或者REST API）读取配置、性能数据等，转换数据格式，存入网管系统。\n\n   优点：可以方便对接第三方开源系统，利用其成熟的监控指标和稳定的监控体系。对原有的网管系统也不需要过多的改造，相当于改造重心在转储接口程序。\n\n   ​ 缺点：需要额外维护多套监控系统以及对应的转换程序；对该转换程序尽可能做得通用，因为可以同时对接多套监控系统；对于指标的定义、配置，仍然需要沿用原来的方法，即还要开发KM、配置表单，这部分是无法避免的。\n\n\n\n2. 仅复用监控系统的采集客户端，类似现在的Agent对接开源组件的模式。例如prometheus，可以复用其exporter。即prometheus exporter独立运行并暴露数据至指定监听端口，Agent开发http接口定时扫描读取该端口暴露的指标即可，然后由KM负责解析转换，最终存入网管系统。\n\n   优点：仅仅用到第三方开源系统的采集客户端，抓取我们想要的数据。对原有的网管系统也不需要过多的改造，仅需要新增KM，对这类采集结果做格式解析处理。exporter暴露的数据指标相对统一，处理起来也比较方便。\n\n   缺点：需要额外维护exporter，且需要开通exporter监听端口的访问权限；exporter的定义配置如何通知给网管系统；对于指标的定义、配置，仍然需要沿用原来的方法，即还要开发KM、配置表单，这部分是无法避免的。\n\n\n\n3. 复用监控系统的采集客户端，同时改造网管系统部分功能。例如telegraf，telegraf支持输出结果到influxdb,或者生成本地json文件。如果是本地Json文件，则与prometheus类似，由KM解析即可。如果是influxdb，则需开发程序从influxdb读取解析数据(相当于另一个数据源)。\n\n   优点：仅仅用到第三方开源系统的采集客户端，抓取我们想要的数据。对原有的网管系统也不需要过多的改造，仅需要新增KM或者influxdb读取程序，对这类采集结果做格式解析处理。telegraf暴露的数据指标相对统一，处理起来也比较方便。\n\n   缺点：需要额外维护telegraf进程；telegraf的定义配置如何通知给网管系统；对于指标的定义、配置，仍然需要沿用原来的方法，即还要KM、配置表单，这部分是无法避免的。\n\n<a href=\"#对比表格\">返回目录</a>\n\n\n\n## 支持的采集对象\n\n- Prometheus\n\nhttps://prometheus.io/docs/instrumenting/exporters/\n\n\n\n- Telegraf\n\nhttps://github.com/influxdata/telegraf/tree/master/plugins/inputs\n\njmx方式采集：kafka\n\nhttps://github.com/influxdata/telegraf/tree/master/plugins/inputs/jolokia2\n\nhttps://docs.confluent.io/current/kafka/monitoring.html\n\n\n\n- Sensu\n\nhttps://github.com/sensu-plugins\n\n\n\n- Openfalcon\n\nhttps://book.open-falcon.org/zh_0_2/usage/\n\n[https://github.com/iambocai/falcon-monit-scripts](https://github.com/iambocai/falcon-monit-scripts)\n\n\n\n- Datadog \n\nhttps://app.datadoghq.com/account/settings#integrations\n\n\n\n## 结论\n\n采用第二种方案，结合开源组件的客户端和采集脚本，有Agent负责调度，解析结果并返回。\n\n被监控对象：ES /kakfa\n\n实现的客户端：telegraf/openfaclon   客户端\n\n目的：从前台配置开始，最终采集入库\n\n\n\njolokia2 采集分为两种模式，一种是jvm代理，由java应用（kafka）启动时修改参数引入jolokia2 包```-javaagent:/usr/hdp/2.6.1.0-129/kafka/libs/jolokia-jvm-1.5.0-agent.jar=port=8778,host=0.0.0.0```\n\n第二种是Proxy模式，由于第一种应用的限制，proxy模式不需要在应用端修改\n\n","slug":"compared","published":1,"updated":"2018-05-17T12:39:11.808Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjhajczo60004e4rwozbl2dwd","content":"<h2 id=\"Prometheus、TICK、Sensu、Openfalcon-对比\"><a href=\"#Prometheus、TICK、Sensu、Openfalcon-对比\" class=\"headerlink\" title=\"Prometheus、TICK、Sensu、Openfalcon 对比\"></a>Prometheus、TICK、Sensu、Openfalcon 对比</h2><p><strong>目的</strong></p>\n<ul>\n<li>研究开源监控系统架构，完善现有的后台采集结构</li>\n<li>利用开源监控体系中的采集插件，节省KM研究、开发新监控对象（各类分布式组件）的时间</li>\n<li>使用开源监控系统为独立的一套监控系统，作为端到端等分布式应用场景的监控系统</li>\n<li>与现有网管后台结合，将开源监控系统作为一类采集数据源</li>\n</ul>\n<p> OpenTSDB、Graphite 偏向数据存储，并无完整的监控体系（采集、存储、告警、展示）。</p>\n<h2 id=\"对比表格\"><a href=\"#对比表格\" class=\"headerlink\" title=\"对比表格\"></a>对比表格</h2><p>以下列出几个重要的关注点</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\"></th>\n<th style=\"text-align:center\">Prometheus</th>\n<th style=\"text-align:center\">TICK</th>\n<th style=\"text-align:center\">Sensu</th>\n<th style=\"text-align:center\">Openfalcon</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">开发语言</td>\n<td style=\"text-align:center\">Go</td>\n<td style=\"text-align:center\">Go</td>\n<td style=\"text-align:center\">ruby</td>\n<td style=\"text-align:center\">Go</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><a href=\"#数据收集方式\">数据收集方式</a></td>\n<td style=\"text-align:center\">Pull/Pushgateway</td>\n<td style=\"text-align:center\">Push</td>\n<td style=\"text-align:center\">Push</td>\n<td style=\"text-align:center\">Push</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">采集客户端</td>\n<td style=\"text-align:center\">各种Exporter</td>\n<td style=\"text-align:center\">telegraf</td>\n<td style=\"text-align:center\">sensu-client<br>各种脚本</td>\n<td style=\"text-align:center\">Falcon-agent<br>各种脚本</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><a href=\"#数据模型\">数据模型</a></td>\n<td style=\"text-align:center\">key+tag</td>\n<td style=\"text-align:center\">key+tag</td>\n<td style=\"text-align:center\">json</td>\n<td style=\"text-align:center\">key+tag</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">存储</td>\n<td style=\"text-align:center\">本地存储 <br>远程数据库(influxDB可读写/OpenTSDB只写)</td>\n<td style=\"text-align:center\">influxDB</td>\n<td style=\"text-align:center\">redis</td>\n<td style=\"text-align:center\">mysql/opentsdb</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">agent部署方式</td>\n<td style=\"text-align:center\">人工</td>\n<td style=\"text-align:center\">人工</td>\n<td style=\"text-align:center\">人工</td>\n<td style=\"text-align:center\">人工</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">任务更新策略</td>\n<td style=\"text-align:center\">无</td>\n<td style=\"text-align:center\">无</td>\n<td style=\"text-align:center\">无</td>\n<td style=\"text-align:center\">dashbord可关联主机和pluins</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><a href=\"#数据查询\">数据查询</a></td>\n<td style=\"text-align:center\">HTTP API</td>\n<td style=\"text-align:center\">SQL/HTTP API</td>\n<td style=\"text-align:center\">HTTP API/redis-cli</td>\n<td style=\"text-align:center\">sql</td>\n</tr>\n</tbody>\n</table>\n<a id=\"more\"></a>\n<h2 id=\"数据收集方式\"><a href=\"#数据收集方式\" class=\"headerlink\" title=\"数据收集方式\"></a>数据收集方式</h2><p>Prometheus 与众不同的采用Pull方式，由服务端主动从被监控机器上面抓取数据。Prometheus 也提供Pushgateway的方式，支持其他数据源主动推送数据到Pushgateway，Pushgateway只做数据缓存，之后仍然是等待服务端抓取。Exporter作为被监控对象暴露指标的客户端，运行在被监控对象上面，并且在Prometheus 服务端配置作为Job，需配置抓取地址端口和间隔。针对各种监控对象，可部署各种Exporter。</p>\n<p>Pull和Push的主要区别在于Pull方式下，监控系统可以方便的增加删除被监控对象，但是需要被监控对象主动开放端口，这对防火墙/NAT方式下的被监控对象是特殊的要求。而Push模式下，每个采集客户端需要知道服务端的信息，才能上报数据。相比之下，网管后台使用Rabbitmq隔离了服务端的信息，但是仍然需要客户端配置Rabbitmq信息。</p>\n<p>TICK使用telegraf作为数据采集端，由telegraf 根据input配置主动采集并发送到output配置的后端，一般是InfluxDB数据库。telegraf还能对采集数据做聚合过滤等处理。telegraf是Go开发的二进制可执行对象，各种对象的采集均打包在一起，通过指定input插件类型，可以指定采集某类指标。相比之下，网管后台使用Java+KM的方式，由Java负责进程调度，KM负责指定采集对象。</p>\n<p>Sensu 也是采用Push 方式，Sensu由Client发送采集数据到transport，服务端再去transport 读取数据，transport一般使用Rabbitmq。这点跟网管有点类似。Client利用插件采集数据，是ruby脚本，需要额外安装。安装后会在/opt/sensu/embedded/bin/ 路径下下载安装rubu解释器和check插件。process-checks 是进程检测插件，cpu-checks 是cpu检测指标插件。采集配置较为繁琐。</p>\n<p>Openfalcon 也是采用Push 方式，由Client发送采集数据到transport，与Sensu  不同的是，transport 会做一些数据规整，检查之后，转发到多个后端系统去处理。transfer目前支持的业务后端，有三种，judge、graph、opentsdb。judge是告警判定组件，graph作为高性能数据存储、归档、查询组件，opentsdb主要用作数据存储服务。falcon-agent本身仅支持主机类的指标采集，对于其他业务系统、开源软件，需独立的使用采集脚本，将数据按照规定格式，发送到falcon-agent 所在的监听端口。最终再由falcon-agent 转发到服务端。</p>\n<p>综上，其实介绍了四种主要的采集逻辑。以两种不同对象的采集作为示例说明</p>\n<ul>\n<li>主机类指标CPU</li>\n<li>开源软件Redis</li>\n</ul>\n<p><a href=\"#对比表格\">返回目录</a></p>\n<h2 id=\"数据模型\"><a href=\"#数据模型\" class=\"headerlink\" title=\"数据模型\"></a>数据模型</h2><p>架构的分层隔离，依赖于各层之间接口的规范。对于采集数据而言，Pull/Push 体现的是传输方式，而数据格式，则是数据组织的规范。以下是各个系统的数据格式。</p>\n<ul>\n<li>openfalcon  采用和OpenTSDB相似的数据格式：metric、endpoint加多组key value tags</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div></pre></td><td class=\"code\"><pre><div class=\"line\">&#123;</div><div class=\"line\">    metric: load.1min,</div><div class=\"line\">    endpoint: open-falcon-host,</div><div class=\"line\">    tags: srv=falcon,idc=aws-sgp,group=az1,</div><div class=\"line\">    value: 1.5,</div><div class=\"line\">    timestamp: `date +%s`,</div><div class=\"line\">    counterType: GAUGE,</div><div class=\"line\">    step: 60</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>其中，metric是监控指标名称，endpoint是监控实体，tags是监控数据的属性标签，counterType是Open-Falcon定义的数据类型(取值为GAUGE、COUNTER)，step为监控数据的上报周期，value和timestamp是有效的监控数据。</p>\n<ul>\n<li><p>prometheus    <metric name=\"\">{<label name=\"\">=<label value=\"\">, …}</label></label></metric></p>\n<p>prometheus 有四种类型的指标：counter、gauge、<a href=\"https://prometheus.io/docs/practices/histograms/\" target=\"_blank\" rel=\"external\">summary、histogram</a> ，<a href=\"http://192.168.7.176:9090/metrics\" target=\"_blank\" rel=\"external\">示例</a></p>\n</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div></pre></td><td class=\"code\"><pre><div class=\"line\"># HELP http_requests_total Total number of HTTP requests made.</div><div class=\"line\"># TYPE http_requests_total counter</div><div class=\"line\">http_requests_total&#123;code=&quot;200&quot;,handler=&quot;alerts&quot;,method=&quot;get&quot;&#125; 2</div><div class=\"line\">http_requests_total&#123;code=&quot;200&quot;,handler=&quot;config&quot;,method=&quot;get&quot;&#125; 1</div><div class=\"line\">http_requests_total&#123;code=&quot;200&quot;,handler=&quot;flags&quot;,method=&quot;get&quot;&#125; 2</div><div class=\"line\"></div><div class=\"line\"># HELP go_goroutines Number of goroutines that currently exist.</div><div class=\"line\"># TYPE go_goroutines gauge</div><div class=\"line\">go_goroutines 235</div><div class=\"line\"></div><div class=\"line\"># HELP go_gc_duration_seconds A summary of the GC invocation durations.</div><div class=\"line\"># TYPE go_gc_duration_seconds summary</div><div class=\"line\">go_gc_duration_seconds&#123;quantile=&quot;0&quot;&#125; 4.5975e-05</div><div class=\"line\">go_gc_duration_seconds&#123;quantile=&quot;0.25&quot;&#125; 8.3846e-05</div><div class=\"line\">go_gc_duration_seconds&#123;quantile=&quot;0.5&quot;&#125; 0.000100729</div><div class=\"line\">go_gc_duration_seconds&#123;quantile=&quot;0.75&quot;&#125; 0.000123377</div><div class=\"line\">go_gc_duration_seconds&#123;quantile=&quot;1&quot;&#125; 0.026837959</div><div class=\"line\">go_gc_duration_seconds_sum 1.990353198</div><div class=\"line\">go_gc_duration_seconds_count 10956</div><div class=\"line\"></div><div class=\"line\"># HELP prometheus_tsdb_compaction_chunk_range Final time range of chunks on their first compaction</div><div class=\"line\"># TYPE prometheus_tsdb_compaction_chunk_range histogram</div><div class=\"line\">prometheus_tsdb_compaction_chunk_range_bucket&#123;le=&quot;100&quot;&#125; 336</div><div class=\"line\">prometheus_tsdb_compaction_chunk_range_bucket&#123;le=&quot;400&quot;&#125; 336</div><div class=\"line\">prometheus_tsdb_compaction_chunk_range_bucket&#123;le=&quot;1600&quot;&#125; 336</div><div class=\"line\">prometheus_tsdb_compaction_chunk_range_bucket&#123;le=&quot;6400&quot;&#125; 336</div><div class=\"line\">prometheus_tsdb_compaction_chunk_range_bucket&#123;le=&quot;25600&quot;&#125; 2336</div><div class=\"line\">prometheus_tsdb_compaction_chunk_range_bucket&#123;le=&quot;102400&quot;&#125; 269009</div><div class=\"line\">prometheus_tsdb_compaction_chunk_range_bucket&#123;le=&quot;409600&quot;&#125; 271929</div><div class=\"line\">prometheus_tsdb_compaction_chunk_range_bucket&#123;le=&quot;1.6384e+06&quot;&#125; 4.2524554e+07</div><div class=\"line\">prometheus_tsdb_compaction_chunk_range_bucket&#123;le=&quot;6.5536e+06&quot;&#125; 4.3107242e+07</div><div class=\"line\">prometheus_tsdb_compaction_chunk_range_bucket&#123;le=&quot;2.62144e+07&quot;&#125; 4.3107877e+07</div><div class=\"line\">prometheus_tsdb_compaction_chunk_range_bucket&#123;le=&quot;+Inf&quot;&#125; 4.3107877e+07</div><div class=\"line\">prometheus_tsdb_compaction_chunk_range_sum 4.4190173791382e+13</div><div class=\"line\">prometheus_tsdb_compaction_chunk_range_count 4.3107877e+07</div></pre></td></tr></table></figure>\n<p>不同类型的指标，有不同的查询方法，需根据指标含义选择合适的查询方法</p>\n<p>histogram_quantile(0.9, rate(prometheus_tsdb_compaction_chunk_range_bucket[10m]))</p>\n<ul>\n<li>Telegraf 支持多种输入输出格式，重点讲解InfluxDB Line Protocol，另外两种是json/Graphite</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div></pre></td><td class=\"code\"><pre><div class=\"line\">weather,location=us-midwest temperature=82 1465839830100400200</div><div class=\"line\">  |    -------------------- --------------  |</div><div class=\"line\">  |             |             |             |</div><div class=\"line\">  |             |             |             |</div><div class=\"line\">+-----------+--------+-+---------+-+---------+</div><div class=\"line\">|measurement|,tag_set| |field_set| |timestamp|</div><div class=\"line\">+-----------+--------+-+---------+-+---------+</div><div class=\"line\"></div><div class=\"line\">&gt; cpu,cpu=cpu0,host=0336dcb23579 usage_system=0,usage_idle=100,usage_nice=0,usage_iowait=0,usage_irq=0,usage_guest=0,usage_user=0,usage_softirq=0,usage_steal=0,usage_guest_nice=0 1515338025000000000</div><div class=\"line\">&gt; cpu,cpu=cpu-total,host=0336dcb23579    usage_guest_nice=0,usage_idle=100,usage_nice=0,usage_irq=0,usage_steal=0,usage_guest=0,usage_user=0,usage_system=0,usage_iowait=0,usage_softirq=0 1515338025000000000</div></pre></td></tr></table></figure>\n<p>cpu：measurements ，类似oracle的表名  </p>\n<p>cpu=cpu-total,host=0336dcb23579 ： cpu、host为tag，索引；    内部以，分割 </p>\n<p>usage_guest_nice=0,usage_idle=100,usage_nice=0… : usage_guest_nice等为field，即被采集的指标字段。</p>\n<p><strong><em>field在measurements是不可或缺的，tag可无， tag与field用空格隔开，field放置在后面</em></strong></p>\n<p>1515338025000000000： time 时间戳</p>\n<p>这里顺便提下 influx metric -&gt; graphite 的转换格式</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">template = &quot;host.tags.measurement.field&quot;</div></pre></td></tr></table></figure>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\">cpu,cpu=cpu-total,dc=us-east-1,host=tars usage_idle=98.09,usage_user=0.89 1455320660004257758</div><div class=\"line\">=&gt;</div><div class=\"line\">tars.cpu-total.us-east-1.cpu.usage_user 0.89 1455320690</div><div class=\"line\">tars.cpu-total.us-east-1.cpu.usage_idle 98.09 1455320690</div></pre></td></tr></table></figure>\n<ul>\n<li>Sensu 指标数据，实际上是check的返回结果。应该是json字符串</li>\n</ul>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div></pre></td><td class=\"code\"><pre><div class=\"line\">&#123;</div><div class=\"line\">    \"client\": \"tick-client\",</div><div class=\"line\">    \"check\": &#123;</div><div class=\"line\">      \"type\": \"metric\",</div><div class=\"line\">      \"command\": \"metrics-cpu.rb\",</div><div class=\"line\">      \"subscribers\": [</div><div class=\"line\">        \"production\"</div><div class=\"line\">      ],</div><div class=\"line\">      \"interval\": 10,</div><div class=\"line\">      \"handler\": \"tcp_socket\",</div><div class=\"line\">      \"standalone\": true,</div><div class=\"line\">      \"name\": \"cpu_metrics\",</div><div class=\"line\">      \"issued\": 1525270655,</div><div class=\"line\">      \"executed\": 1525270655,</div><div class=\"line\">      \"duration\": 0.074,</div><div class=\"line\">      \"output\": \"tick.cpu.total.user 738228 1525270655\\n...\",</div><div class=\"line\">      \"status\": 0,</div><div class=\"line\">      \"history\": [</div><div class=\"line\">        0,</div><div class=\"line\">\t\t...</div><div class=\"line\">      ]</div><div class=\"line\">    &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>output 就是chcek返回结果。由不同的指标采集插件返回，默认的返回格式是Graphite 格式。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div></pre></td><td class=\"code\"><pre><div class=\"line\">sensu.cpu.user 0.50 1515534170</div><div class=\"line\">sensu.cpu.nice 0.00 1515534170</div><div class=\"line\">sensu.cpu.system 0.00 1515534170</div><div class=\"line\">sensu.cpu.idle 99.50 1515534170</div><div class=\"line\">sensu.cpu.iowait 0.00 1515534170</div><div class=\"line\">sensu.cpu.irq 0.00 1515534170</div><div class=\"line\">sensu.cpu.softirq 0.00 1515534170</div><div class=\"line\">sensu.cpu.steal 0.00 1515534170</div><div class=\"line\">sensu.cpu.guest 0.00 1515534170</div></pre></td></tr></table></figure>\n<p><a href=\"#对比表格\">返回目录</a></p>\n<h2 id=\"数据查询\"><a href=\"#数据查询\" class=\"headerlink\" title=\"数据查询\"></a>数据查询</h2><ul>\n<li><p>Prometheus   HTTP API接口查询，查询结果类型：”resultType”: “matrix” | “vector” | “scalar” | “string”</p>\n<p>matrix ： 返回某个时间段内的指标数据集合</p>\n<p>vector：返回单个时间点的指标数据</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\"># Instant queries</div><div class=\"line\">curl &apos;http://192.168.7.176:9090/api/v1/query?query=elasticsearch_node_stats_up&amp;time=1525313513.452&amp;_=1525313461038&apos; -s | python -m json.tool</div><div class=\"line\"></div><div class=\"line\"># Range queries</div><div class=\"line\">curl &apos;http://192.168.7.176:9090/api/v1/query_range?query=prometheus_tsdb_compaction_chunk_range_bucket&amp;start=1525307102.216&amp;end=1525307302.216&amp;step=28&amp;_=1525313461049&apos; -s|python -m json.tool</div></pre></td></tr></table></figure>\n<p>​</p>\n</li>\n<li><p>TICK  HTTP API查询/登陆数据库查询sql</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\">curl -G &quot;http://192.168.7.176:8086/query?pretty=true&quot; --data-urlencode &quot;db=mydb&quot; \\</div><div class=\"line\">--data-urlencode &quot;q=SELECT * FROM cpu WHERE host=&apos;server01&apos; AND time &lt; now() - 1d&quot;</div><div class=\"line\"></div><div class=\"line\">##==&gt;</div><div class=\"line\">SELECT * FROM cpu WHERE host=&apos;server01&apos; AND time &lt; now() - 1d</div></pre></td></tr></table></figure>\n<p>​</p>\n</li>\n<li><p>Sensu   HTTP API查询/登陆redis查询sql</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div></pre></td><td class=\"code\"><pre><div class=\"line\">curl -s http://127.0.0.1:4567/clients | jq .</div><div class=\"line\">curl -s http://localhost:4567/events | jq .</div><div class=\"line\">curl -s http://localhost:4567/results | jq .</div><div class=\"line\">curl -s http://localhost:4567/checks | jq .</div><div class=\"line\"></div><div class=\"line\">###===&gt;</div><div class=\"line\">redis-cli</div><div class=\"line\">keys * </div><div class=\"line\">get result:tick-client:cpu_metrics</div></pre></td></tr></table></figure>\n<p>​</p>\n</li>\n<li><p>Openfalcon </p>\n<p>从mysql数据库中取数</p>\n<p>​</p>\n</li>\n</ul>\n<p><a href=\"#对比表格\">返回目录</a></p>\n<h2 id=\"网管对接\"><a href=\"#网管对接\" class=\"headerlink\" title=\"网管对接\"></a>网管对接</h2><ol>\n<li><p>跟云资源采集类似，将上述监控系统作为一个数据采集中心。开发一个转储接口程序，从监控系统中（数据库或者REST API）读取配置、性能数据等，转换数据格式，存入网管系统。</p>\n<p>优点：可以方便对接第三方开源系统，利用其成熟的监控指标和稳定的监控体系。对原有的网管系统也不需要过多的改造，相当于改造重心在转储接口程序。</p>\n<p>​ 缺点：需要额外维护多套监控系统以及对应的转换程序；对该转换程序尽可能做得通用，因为可以同时对接多套监控系统；对于指标的定义、配置，仍然需要沿用原来的方法，即还要开发KM、配置表单，这部分是无法避免的。</p>\n</li>\n</ol>\n<ol>\n<li><p>仅复用监控系统的采集客户端，类似现在的Agent对接开源组件的模式。例如prometheus，可以复用其exporter。即prometheus exporter独立运行并暴露数据至指定监听端口，Agent开发http接口定时扫描读取该端口暴露的指标即可，然后由KM负责解析转换，最终存入网管系统。</p>\n<p>优点：仅仅用到第三方开源系统的采集客户端，抓取我们想要的数据。对原有的网管系统也不需要过多的改造，仅需要新增KM，对这类采集结果做格式解析处理。exporter暴露的数据指标相对统一，处理起来也比较方便。</p>\n<p>缺点：需要额外维护exporter，且需要开通exporter监听端口的访问权限；exporter的定义配置如何通知给网管系统；对于指标的定义、配置，仍然需要沿用原来的方法，即还要开发KM、配置表单，这部分是无法避免的。</p>\n</li>\n</ol>\n<ol>\n<li><p>复用监控系统的采集客户端，同时改造网管系统部分功能。例如telegraf，telegraf支持输出结果到influxdb,或者生成本地json文件。如果是本地Json文件，则与prometheus类似，由KM解析即可。如果是influxdb，则需开发程序从influxdb读取解析数据(相当于另一个数据源)。</p>\n<p>优点：仅仅用到第三方开源系统的采集客户端，抓取我们想要的数据。对原有的网管系统也不需要过多的改造，仅需要新增KM或者influxdb读取程序，对这类采集结果做格式解析处理。telegraf暴露的数据指标相对统一，处理起来也比较方便。</p>\n<p>缺点：需要额外维护telegraf进程；telegraf的定义配置如何通知给网管系统；对于指标的定义、配置，仍然需要沿用原来的方法，即还要KM、配置表单，这部分是无法避免的。</p>\n</li>\n</ol>\n<p><a href=\"#对比表格\">返回目录</a></p>\n<h2 id=\"支持的采集对象\"><a href=\"#支持的采集对象\" class=\"headerlink\" title=\"支持的采集对象\"></a>支持的采集对象</h2><ul>\n<li>Prometheus</li>\n</ul>\n<p><a href=\"https://prometheus.io/docs/instrumenting/exporters/\" target=\"_blank\" rel=\"external\">https://prometheus.io/docs/instrumenting/exporters/</a></p>\n<ul>\n<li>Telegraf</li>\n</ul>\n<p><a href=\"https://github.com/influxdata/telegraf/tree/master/plugins/inputs\" target=\"_blank\" rel=\"external\">https://github.com/influxdata/telegraf/tree/master/plugins/inputs</a></p>\n<p>jmx方式采集：kafka</p>\n<p><a href=\"https://github.com/influxdata/telegraf/tree/master/plugins/inputs/jolokia2\" target=\"_blank\" rel=\"external\">https://github.com/influxdata/telegraf/tree/master/plugins/inputs/jolokia2</a></p>\n<p><a href=\"https://docs.confluent.io/current/kafka/monitoring.html\" target=\"_blank\" rel=\"external\">https://docs.confluent.io/current/kafka/monitoring.html</a></p>\n<ul>\n<li>Sensu</li>\n</ul>\n<p><a href=\"https://github.com/sensu-plugins\" target=\"_blank\" rel=\"external\">https://github.com/sensu-plugins</a></p>\n<ul>\n<li>Openfalcon</li>\n</ul>\n<p><a href=\"https://book.open-falcon.org/zh_0_2/usage/\" target=\"_blank\" rel=\"external\">https://book.open-falcon.org/zh_0_2/usage/</a></p>\n<p><a href=\"https://github.com/iambocai/falcon-monit-scripts\" target=\"_blank\" rel=\"external\">https://github.com/iambocai/falcon-monit-scripts</a></p>\n<ul>\n<li>Datadog </li>\n</ul>\n<p><a href=\"https://app.datadoghq.com/account/settings#integrations\" target=\"_blank\" rel=\"external\">https://app.datadoghq.com/account/settings#integrations</a></p>\n<h2 id=\"结论\"><a href=\"#结论\" class=\"headerlink\" title=\"结论\"></a>结论</h2><p>采用第二种方案，结合开源组件的客户端和采集脚本，有Agent负责调度，解析结果并返回。</p>\n<p>被监控对象：ES /kakfa</p>\n<p>实现的客户端：telegraf/openfaclon   客户端</p>\n<p>目的：从前台配置开始，最终采集入库</p>\n<p>jolokia2 采集分为两种模式，一种是jvm代理，由java应用（kafka）启动时修改参数引入jolokia2 包<code>-javaagent:/usr/hdp/2.6.1.0-129/kafka/libs/jolokia-jvm-1.5.0-agent.jar=port=8778,host=0.0.0.0</code></p>\n<p>第二种是Proxy模式，由于第一种应用的限制，proxy模式不需要在应用端修改</p>\n","site":{"data":{}},"excerpt":"<h2 id=\"Prometheus、TICK、Sensu、Openfalcon-对比\"><a href=\"#Prometheus、TICK、Sensu、Openfalcon-对比\" class=\"headerlink\" title=\"Prometheus、TICK、Sensu、Openfalcon 对比\"></a>Prometheus、TICK、Sensu、Openfalcon 对比</h2><p><strong>目的</strong></p>\n<ul>\n<li>研究开源监控系统架构，完善现有的后台采集结构</li>\n<li>利用开源监控体系中的采集插件，节省KM研究、开发新监控对象（各类分布式组件）的时间</li>\n<li>使用开源监控系统为独立的一套监控系统，作为端到端等分布式应用场景的监控系统</li>\n<li>与现有网管后台结合，将开源监控系统作为一类采集数据源</li>\n</ul>\n<p> OpenTSDB、Graphite 偏向数据存储，并无完整的监控体系（采集、存储、告警、展示）。</p>\n<h2 id=\"对比表格\"><a href=\"#对比表格\" class=\"headerlink\" title=\"对比表格\"></a>对比表格</h2><p>以下列出几个重要的关注点</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\"></th>\n<th style=\"text-align:center\">Prometheus</th>\n<th style=\"text-align:center\">TICK</th>\n<th style=\"text-align:center\">Sensu</th>\n<th style=\"text-align:center\">Openfalcon</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">开发语言</td>\n<td style=\"text-align:center\">Go</td>\n<td style=\"text-align:center\">Go</td>\n<td style=\"text-align:center\">ruby</td>\n<td style=\"text-align:center\">Go</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><a href=\"#数据收集方式\">数据收集方式</a></td>\n<td style=\"text-align:center\">Pull/Pushgateway</td>\n<td style=\"text-align:center\">Push</td>\n<td style=\"text-align:center\">Push</td>\n<td style=\"text-align:center\">Push</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">采集客户端</td>\n<td style=\"text-align:center\">各种Exporter</td>\n<td style=\"text-align:center\">telegraf</td>\n<td style=\"text-align:center\">sensu-client<br>各种脚本</td>\n<td style=\"text-align:center\">Falcon-agent<br>各种脚本</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><a href=\"#数据模型\">数据模型</a></td>\n<td style=\"text-align:center\">key+tag</td>\n<td style=\"text-align:center\">key+tag</td>\n<td style=\"text-align:center\">json</td>\n<td style=\"text-align:center\">key+tag</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">存储</td>\n<td style=\"text-align:center\">本地存储 <br>远程数据库(influxDB可读写/OpenTSDB只写)</td>\n<td style=\"text-align:center\">influxDB</td>\n<td style=\"text-align:center\">redis</td>\n<td style=\"text-align:center\">mysql/opentsdb</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">agent部署方式</td>\n<td style=\"text-align:center\">人工</td>\n<td style=\"text-align:center\">人工</td>\n<td style=\"text-align:center\">人工</td>\n<td style=\"text-align:center\">人工</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">任务更新策略</td>\n<td style=\"text-align:center\">无</td>\n<td style=\"text-align:center\">无</td>\n<td style=\"text-align:center\">无</td>\n<td style=\"text-align:center\">dashbord可关联主机和pluins</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><a href=\"#数据查询\">数据查询</a></td>\n<td style=\"text-align:center\">HTTP API</td>\n<td style=\"text-align:center\">SQL/HTTP API</td>\n<td style=\"text-align:center\">HTTP API/redis-cli</td>\n<td style=\"text-align:center\">sql</td>\n</tr>\n</tbody>\n</table>","more":"<h2 id=\"数据收集方式\"><a href=\"#数据收集方式\" class=\"headerlink\" title=\"数据收集方式\"></a>数据收集方式</h2><p>Prometheus 与众不同的采用Pull方式，由服务端主动从被监控机器上面抓取数据。Prometheus 也提供Pushgateway的方式，支持其他数据源主动推送数据到Pushgateway，Pushgateway只做数据缓存，之后仍然是等待服务端抓取。Exporter作为被监控对象暴露指标的客户端，运行在被监控对象上面，并且在Prometheus 服务端配置作为Job，需配置抓取地址端口和间隔。针对各种监控对象，可部署各种Exporter。</p>\n<p>Pull和Push的主要区别在于Pull方式下，监控系统可以方便的增加删除被监控对象，但是需要被监控对象主动开放端口，这对防火墙/NAT方式下的被监控对象是特殊的要求。而Push模式下，每个采集客户端需要知道服务端的信息，才能上报数据。相比之下，网管后台使用Rabbitmq隔离了服务端的信息，但是仍然需要客户端配置Rabbitmq信息。</p>\n<p>TICK使用telegraf作为数据采集端，由telegraf 根据input配置主动采集并发送到output配置的后端，一般是InfluxDB数据库。telegraf还能对采集数据做聚合过滤等处理。telegraf是Go开发的二进制可执行对象，各种对象的采集均打包在一起，通过指定input插件类型，可以指定采集某类指标。相比之下，网管后台使用Java+KM的方式，由Java负责进程调度，KM负责指定采集对象。</p>\n<p>Sensu 也是采用Push 方式，Sensu由Client发送采集数据到transport，服务端再去transport 读取数据，transport一般使用Rabbitmq。这点跟网管有点类似。Client利用插件采集数据，是ruby脚本，需要额外安装。安装后会在/opt/sensu/embedded/bin/ 路径下下载安装rubu解释器和check插件。process-checks 是进程检测插件，cpu-checks 是cpu检测指标插件。采集配置较为繁琐。</p>\n<p>Openfalcon 也是采用Push 方式，由Client发送采集数据到transport，与Sensu  不同的是，transport 会做一些数据规整，检查之后，转发到多个后端系统去处理。transfer目前支持的业务后端，有三种，judge、graph、opentsdb。judge是告警判定组件，graph作为高性能数据存储、归档、查询组件，opentsdb主要用作数据存储服务。falcon-agent本身仅支持主机类的指标采集，对于其他业务系统、开源软件，需独立的使用采集脚本，将数据按照规定格式，发送到falcon-agent 所在的监听端口。最终再由falcon-agent 转发到服务端。</p>\n<p>综上，其实介绍了四种主要的采集逻辑。以两种不同对象的采集作为示例说明</p>\n<ul>\n<li>主机类指标CPU</li>\n<li>开源软件Redis</li>\n</ul>\n<p><a href=\"#对比表格\">返回目录</a></p>\n<h2 id=\"数据模型\"><a href=\"#数据模型\" class=\"headerlink\" title=\"数据模型\"></a>数据模型</h2><p>架构的分层隔离，依赖于各层之间接口的规范。对于采集数据而言，Pull/Push 体现的是传输方式，而数据格式，则是数据组织的规范。以下是各个系统的数据格式。</p>\n<ul>\n<li>openfalcon  采用和OpenTSDB相似的数据格式：metric、endpoint加多组key value tags</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div></pre></td><td class=\"code\"><pre><div class=\"line\">&#123;</div><div class=\"line\">    metric: load.1min,</div><div class=\"line\">    endpoint: open-falcon-host,</div><div class=\"line\">    tags: srv=falcon,idc=aws-sgp,group=az1,</div><div class=\"line\">    value: 1.5,</div><div class=\"line\">    timestamp: `date +%s`,</div><div class=\"line\">    counterType: GAUGE,</div><div class=\"line\">    step: 60</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>其中，metric是监控指标名称，endpoint是监控实体，tags是监控数据的属性标签，counterType是Open-Falcon定义的数据类型(取值为GAUGE、COUNTER)，step为监控数据的上报周期，value和timestamp是有效的监控数据。</p>\n<ul>\n<li><p>prometheus    <metric name=\"\">{<label name=\"\">=<label value=\"\">, …}</label></label></metric></p>\n<p>prometheus 有四种类型的指标：counter、gauge、<a href=\"https://prometheus.io/docs/practices/histograms/\" target=\"_blank\" rel=\"external\">summary、histogram</a> ，<a href=\"http://192.168.7.176:9090/metrics\" target=\"_blank\" rel=\"external\">示例</a></p>\n</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div></pre></td><td class=\"code\"><pre><div class=\"line\"># HELP http_requests_total Total number of HTTP requests made.</div><div class=\"line\"># TYPE http_requests_total counter</div><div class=\"line\">http_requests_total&#123;code=&quot;200&quot;,handler=&quot;alerts&quot;,method=&quot;get&quot;&#125; 2</div><div class=\"line\">http_requests_total&#123;code=&quot;200&quot;,handler=&quot;config&quot;,method=&quot;get&quot;&#125; 1</div><div class=\"line\">http_requests_total&#123;code=&quot;200&quot;,handler=&quot;flags&quot;,method=&quot;get&quot;&#125; 2</div><div class=\"line\"></div><div class=\"line\"># HELP go_goroutines Number of goroutines that currently exist.</div><div class=\"line\"># TYPE go_goroutines gauge</div><div class=\"line\">go_goroutines 235</div><div class=\"line\"></div><div class=\"line\"># HELP go_gc_duration_seconds A summary of the GC invocation durations.</div><div class=\"line\"># TYPE go_gc_duration_seconds summary</div><div class=\"line\">go_gc_duration_seconds&#123;quantile=&quot;0&quot;&#125; 4.5975e-05</div><div class=\"line\">go_gc_duration_seconds&#123;quantile=&quot;0.25&quot;&#125; 8.3846e-05</div><div class=\"line\">go_gc_duration_seconds&#123;quantile=&quot;0.5&quot;&#125; 0.000100729</div><div class=\"line\">go_gc_duration_seconds&#123;quantile=&quot;0.75&quot;&#125; 0.000123377</div><div class=\"line\">go_gc_duration_seconds&#123;quantile=&quot;1&quot;&#125; 0.026837959</div><div class=\"line\">go_gc_duration_seconds_sum 1.990353198</div><div class=\"line\">go_gc_duration_seconds_count 10956</div><div class=\"line\"></div><div class=\"line\"># HELP prometheus_tsdb_compaction_chunk_range Final time range of chunks on their first compaction</div><div class=\"line\"># TYPE prometheus_tsdb_compaction_chunk_range histogram</div><div class=\"line\">prometheus_tsdb_compaction_chunk_range_bucket&#123;le=&quot;100&quot;&#125; 336</div><div class=\"line\">prometheus_tsdb_compaction_chunk_range_bucket&#123;le=&quot;400&quot;&#125; 336</div><div class=\"line\">prometheus_tsdb_compaction_chunk_range_bucket&#123;le=&quot;1600&quot;&#125; 336</div><div class=\"line\">prometheus_tsdb_compaction_chunk_range_bucket&#123;le=&quot;6400&quot;&#125; 336</div><div class=\"line\">prometheus_tsdb_compaction_chunk_range_bucket&#123;le=&quot;25600&quot;&#125; 2336</div><div class=\"line\">prometheus_tsdb_compaction_chunk_range_bucket&#123;le=&quot;102400&quot;&#125; 269009</div><div class=\"line\">prometheus_tsdb_compaction_chunk_range_bucket&#123;le=&quot;409600&quot;&#125; 271929</div><div class=\"line\">prometheus_tsdb_compaction_chunk_range_bucket&#123;le=&quot;1.6384e+06&quot;&#125; 4.2524554e+07</div><div class=\"line\">prometheus_tsdb_compaction_chunk_range_bucket&#123;le=&quot;6.5536e+06&quot;&#125; 4.3107242e+07</div><div class=\"line\">prometheus_tsdb_compaction_chunk_range_bucket&#123;le=&quot;2.62144e+07&quot;&#125; 4.3107877e+07</div><div class=\"line\">prometheus_tsdb_compaction_chunk_range_bucket&#123;le=&quot;+Inf&quot;&#125; 4.3107877e+07</div><div class=\"line\">prometheus_tsdb_compaction_chunk_range_sum 4.4190173791382e+13</div><div class=\"line\">prometheus_tsdb_compaction_chunk_range_count 4.3107877e+07</div></pre></td></tr></table></figure>\n<p>不同类型的指标，有不同的查询方法，需根据指标含义选择合适的查询方法</p>\n<p>histogram_quantile(0.9, rate(prometheus_tsdb_compaction_chunk_range_bucket[10m]))</p>\n<ul>\n<li>Telegraf 支持多种输入输出格式，重点讲解InfluxDB Line Protocol，另外两种是json/Graphite</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div></pre></td><td class=\"code\"><pre><div class=\"line\">weather,location=us-midwest temperature=82 1465839830100400200</div><div class=\"line\">  |    -------------------- --------------  |</div><div class=\"line\">  |             |             |             |</div><div class=\"line\">  |             |             |             |</div><div class=\"line\">+-----------+--------+-+---------+-+---------+</div><div class=\"line\">|measurement|,tag_set| |field_set| |timestamp|</div><div class=\"line\">+-----------+--------+-+---------+-+---------+</div><div class=\"line\"></div><div class=\"line\">&gt; cpu,cpu=cpu0,host=0336dcb23579 usage_system=0,usage_idle=100,usage_nice=0,usage_iowait=0,usage_irq=0,usage_guest=0,usage_user=0,usage_softirq=0,usage_steal=0,usage_guest_nice=0 1515338025000000000</div><div class=\"line\">&gt; cpu,cpu=cpu-total,host=0336dcb23579    usage_guest_nice=0,usage_idle=100,usage_nice=0,usage_irq=0,usage_steal=0,usage_guest=0,usage_user=0,usage_system=0,usage_iowait=0,usage_softirq=0 1515338025000000000</div></pre></td></tr></table></figure>\n<p>cpu：measurements ，类似oracle的表名  </p>\n<p>cpu=cpu-total,host=0336dcb23579 ： cpu、host为tag，索引；    内部以，分割 </p>\n<p>usage_guest_nice=0,usage_idle=100,usage_nice=0… : usage_guest_nice等为field，即被采集的指标字段。</p>\n<p><strong><em>field在measurements是不可或缺的，tag可无， tag与field用空格隔开，field放置在后面</em></strong></p>\n<p>1515338025000000000： time 时间戳</p>\n<p>这里顺便提下 influx metric -&gt; graphite 的转换格式</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">template = &quot;host.tags.measurement.field&quot;</div></pre></td></tr></table></figure>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\">cpu,cpu=cpu-total,dc=us-east-1,host=tars usage_idle=98.09,usage_user=0.89 1455320660004257758</div><div class=\"line\">=&gt;</div><div class=\"line\">tars.cpu-total.us-east-1.cpu.usage_user 0.89 1455320690</div><div class=\"line\">tars.cpu-total.us-east-1.cpu.usage_idle 98.09 1455320690</div></pre></td></tr></table></figure>\n<ul>\n<li>Sensu 指标数据，实际上是check的返回结果。应该是json字符串</li>\n</ul>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div></pre></td><td class=\"code\"><pre><div class=\"line\">&#123;</div><div class=\"line\">    \"client\": \"tick-client\",</div><div class=\"line\">    \"check\": &#123;</div><div class=\"line\">      \"type\": \"metric\",</div><div class=\"line\">      \"command\": \"metrics-cpu.rb\",</div><div class=\"line\">      \"subscribers\": [</div><div class=\"line\">        \"production\"</div><div class=\"line\">      ],</div><div class=\"line\">      \"interval\": 10,</div><div class=\"line\">      \"handler\": \"tcp_socket\",</div><div class=\"line\">      \"standalone\": true,</div><div class=\"line\">      \"name\": \"cpu_metrics\",</div><div class=\"line\">      \"issued\": 1525270655,</div><div class=\"line\">      \"executed\": 1525270655,</div><div class=\"line\">      \"duration\": 0.074,</div><div class=\"line\">      \"output\": \"tick.cpu.total.user 738228 1525270655\\n...\",</div><div class=\"line\">      \"status\": 0,</div><div class=\"line\">      \"history\": [</div><div class=\"line\">        0,</div><div class=\"line\">\t\t...</div><div class=\"line\">      ]</div><div class=\"line\">    &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>output 就是chcek返回结果。由不同的指标采集插件返回，默认的返回格式是Graphite 格式。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div></pre></td><td class=\"code\"><pre><div class=\"line\">sensu.cpu.user 0.50 1515534170</div><div class=\"line\">sensu.cpu.nice 0.00 1515534170</div><div class=\"line\">sensu.cpu.system 0.00 1515534170</div><div class=\"line\">sensu.cpu.idle 99.50 1515534170</div><div class=\"line\">sensu.cpu.iowait 0.00 1515534170</div><div class=\"line\">sensu.cpu.irq 0.00 1515534170</div><div class=\"line\">sensu.cpu.softirq 0.00 1515534170</div><div class=\"line\">sensu.cpu.steal 0.00 1515534170</div><div class=\"line\">sensu.cpu.guest 0.00 1515534170</div></pre></td></tr></table></figure>\n<p><a href=\"#对比表格\">返回目录</a></p>\n<h2 id=\"数据查询\"><a href=\"#数据查询\" class=\"headerlink\" title=\"数据查询\"></a>数据查询</h2><ul>\n<li><p>Prometheus   HTTP API接口查询，查询结果类型：”resultType”: “matrix” | “vector” | “scalar” | “string”</p>\n<p>matrix ： 返回某个时间段内的指标数据集合</p>\n<p>vector：返回单个时间点的指标数据</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\"># Instant queries</div><div class=\"line\">curl &apos;http://192.168.7.176:9090/api/v1/query?query=elasticsearch_node_stats_up&amp;time=1525313513.452&amp;_=1525313461038&apos; -s | python -m json.tool</div><div class=\"line\"></div><div class=\"line\"># Range queries</div><div class=\"line\">curl &apos;http://192.168.7.176:9090/api/v1/query_range?query=prometheus_tsdb_compaction_chunk_range_bucket&amp;start=1525307102.216&amp;end=1525307302.216&amp;step=28&amp;_=1525313461049&apos; -s|python -m json.tool</div></pre></td></tr></table></figure>\n<p>​</p>\n</li>\n<li><p>TICK  HTTP API查询/登陆数据库查询sql</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\">curl -G &quot;http://192.168.7.176:8086/query?pretty=true&quot; --data-urlencode &quot;db=mydb&quot; \\</div><div class=\"line\">--data-urlencode &quot;q=SELECT * FROM cpu WHERE host=&apos;server01&apos; AND time &lt; now() - 1d&quot;</div><div class=\"line\"></div><div class=\"line\">##==&gt;</div><div class=\"line\">SELECT * FROM cpu WHERE host=&apos;server01&apos; AND time &lt; now() - 1d</div></pre></td></tr></table></figure>\n<p>​</p>\n</li>\n<li><p>Sensu   HTTP API查询/登陆redis查询sql</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div></pre></td><td class=\"code\"><pre><div class=\"line\">curl -s http://127.0.0.1:4567/clients | jq .</div><div class=\"line\">curl -s http://localhost:4567/events | jq .</div><div class=\"line\">curl -s http://localhost:4567/results | jq .</div><div class=\"line\">curl -s http://localhost:4567/checks | jq .</div><div class=\"line\"></div><div class=\"line\">###===&gt;</div><div class=\"line\">redis-cli</div><div class=\"line\">keys * </div><div class=\"line\">get result:tick-client:cpu_metrics</div></pre></td></tr></table></figure>\n<p>​</p>\n</li>\n<li><p>Openfalcon </p>\n<p>从mysql数据库中取数</p>\n<p>​</p>\n</li>\n</ul>\n<p><a href=\"#对比表格\">返回目录</a></p>\n<h2 id=\"网管对接\"><a href=\"#网管对接\" class=\"headerlink\" title=\"网管对接\"></a>网管对接</h2><ol>\n<li><p>跟云资源采集类似，将上述监控系统作为一个数据采集中心。开发一个转储接口程序，从监控系统中（数据库或者REST API）读取配置、性能数据等，转换数据格式，存入网管系统。</p>\n<p>优点：可以方便对接第三方开源系统，利用其成熟的监控指标和稳定的监控体系。对原有的网管系统也不需要过多的改造，相当于改造重心在转储接口程序。</p>\n<p>​ 缺点：需要额外维护多套监控系统以及对应的转换程序；对该转换程序尽可能做得通用，因为可以同时对接多套监控系统；对于指标的定义、配置，仍然需要沿用原来的方法，即还要开发KM、配置表单，这部分是无法避免的。</p>\n</li>\n</ol>\n<ol>\n<li><p>仅复用监控系统的采集客户端，类似现在的Agent对接开源组件的模式。例如prometheus，可以复用其exporter。即prometheus exporter独立运行并暴露数据至指定监听端口，Agent开发http接口定时扫描读取该端口暴露的指标即可，然后由KM负责解析转换，最终存入网管系统。</p>\n<p>优点：仅仅用到第三方开源系统的采集客户端，抓取我们想要的数据。对原有的网管系统也不需要过多的改造，仅需要新增KM，对这类采集结果做格式解析处理。exporter暴露的数据指标相对统一，处理起来也比较方便。</p>\n<p>缺点：需要额外维护exporter，且需要开通exporter监听端口的访问权限；exporter的定义配置如何通知给网管系统；对于指标的定义、配置，仍然需要沿用原来的方法，即还要开发KM、配置表单，这部分是无法避免的。</p>\n</li>\n</ol>\n<ol>\n<li><p>复用监控系统的采集客户端，同时改造网管系统部分功能。例如telegraf，telegraf支持输出结果到influxdb,或者生成本地json文件。如果是本地Json文件，则与prometheus类似，由KM解析即可。如果是influxdb，则需开发程序从influxdb读取解析数据(相当于另一个数据源)。</p>\n<p>优点：仅仅用到第三方开源系统的采集客户端，抓取我们想要的数据。对原有的网管系统也不需要过多的改造，仅需要新增KM或者influxdb读取程序，对这类采集结果做格式解析处理。telegraf暴露的数据指标相对统一，处理起来也比较方便。</p>\n<p>缺点：需要额外维护telegraf进程；telegraf的定义配置如何通知给网管系统；对于指标的定义、配置，仍然需要沿用原来的方法，即还要KM、配置表单，这部分是无法避免的。</p>\n</li>\n</ol>\n<p><a href=\"#对比表格\">返回目录</a></p>\n<h2 id=\"支持的采集对象\"><a href=\"#支持的采集对象\" class=\"headerlink\" title=\"支持的采集对象\"></a>支持的采集对象</h2><ul>\n<li>Prometheus</li>\n</ul>\n<p><a href=\"https://prometheus.io/docs/instrumenting/exporters/\" target=\"_blank\" rel=\"external\">https://prometheus.io/docs/instrumenting/exporters/</a></p>\n<ul>\n<li>Telegraf</li>\n</ul>\n<p><a href=\"https://github.com/influxdata/telegraf/tree/master/plugins/inputs\" target=\"_blank\" rel=\"external\">https://github.com/influxdata/telegraf/tree/master/plugins/inputs</a></p>\n<p>jmx方式采集：kafka</p>\n<p><a href=\"https://github.com/influxdata/telegraf/tree/master/plugins/inputs/jolokia2\" target=\"_blank\" rel=\"external\">https://github.com/influxdata/telegraf/tree/master/plugins/inputs/jolokia2</a></p>\n<p><a href=\"https://docs.confluent.io/current/kafka/monitoring.html\" target=\"_blank\" rel=\"external\">https://docs.confluent.io/current/kafka/monitoring.html</a></p>\n<ul>\n<li>Sensu</li>\n</ul>\n<p><a href=\"https://github.com/sensu-plugins\" target=\"_blank\" rel=\"external\">https://github.com/sensu-plugins</a></p>\n<ul>\n<li>Openfalcon</li>\n</ul>\n<p><a href=\"https://book.open-falcon.org/zh_0_2/usage/\" target=\"_blank\" rel=\"external\">https://book.open-falcon.org/zh_0_2/usage/</a></p>\n<p><a href=\"https://github.com/iambocai/falcon-monit-scripts\" target=\"_blank\" rel=\"external\">https://github.com/iambocai/falcon-monit-scripts</a></p>\n<ul>\n<li>Datadog </li>\n</ul>\n<p><a href=\"https://app.datadoghq.com/account/settings#integrations\" target=\"_blank\" rel=\"external\">https://app.datadoghq.com/account/settings#integrations</a></p>\n<h2 id=\"结论\"><a href=\"#结论\" class=\"headerlink\" title=\"结论\"></a>结论</h2><p>采用第二种方案，结合开源组件的客户端和采集脚本，有Agent负责调度，解析结果并返回。</p>\n<p>被监控对象：ES /kakfa</p>\n<p>实现的客户端：telegraf/openfaclon   客户端</p>\n<p>目的：从前台配置开始，最终采集入库</p>\n<p>jolokia2 采集分为两种模式，一种是jvm代理，由java应用（kafka）启动时修改参数引入jolokia2 包<code>-javaagent:/usr/hdp/2.6.1.0-129/kafka/libs/jolokia-jvm-1.5.0-agent.jar=port=8778,host=0.0.0.0</code></p>\n<p>第二种是Proxy模式，由于第一种应用的限制，proxy模式不需要在应用端修改</p>"},{"title":"代码整洁之道","date":"2014-08-20T12:54:30.000Z","_content":"\n这是一本关于如何写好代码，如何当好一名优秀的程序员的书籍。在本书中，作者引用IT牛人的话，阐述了什么是简洁的代码，然后提出了自己的看法。正如作者一直强调的，光看书是远远不够的，要勤于练习，不断改正，才能有所收获。下面就让我们来一探究竟。\n\n# 整洁的代码\n在这章里面，作者探讨了整洁的代码的重要性，以及何为简洁的代码。\n# 有意义的命名\n>名副其实，而且一旦发现有更好的名字，立即换掉旧的。\n避免误导\n有意义的区分,Variable一词永远不应当出现在变量名中，table一词也不应当出现在表名中。\n使用读得出来的名称\n使用可搜索的名称\n避免使用编码\n类名：类名和对象名应该是名词或者名词短语，而不应该是动词。\n方法名：方法名应当是动词或者动词短语。    \n每个概念对应一个词，不要使用双关语。\n不要添加没用的语境：不要添加重复相同的前缀。只要短名称足够清除，就要比长名称好。对于Address类的实体来说，accountAddress和customerAddress都是不错的对象名称，但是用在类名上就不好了。Address是个好类名，如果需要与MAC地址，端口地址，WEB地址做区别，我会考虑使用PostalAddress、MAC和URI。这样的名称更为精确，而精确正是命名的要点。\n\n<!-- more -->\n# 函数 \n通过函数名，可以读出来它是干什么的\n短小，只做一件事情，每个函数一个抽象层次：如果函数只是做了该函数名下同一抽象层上的步骤，则函数还是只是做了一件事。编写函数，毕竟是为了把大一些的概念（换言之，函数的名称）拆分成另一抽象层级的一系列处理步骤。\n避免重复。\n如何写出好的函数：刚开始写函数时，有太多的缩进和嵌套循环，有过长的参数列表，名称是随意取的，也会有重复的代码。不过会配上一套测试单元，覆盖每行丑陋的代码。然后再打磨这些代码，分解函数，修改名称，消除重复，缩短和重新安置方法。有时，还拆散类，保持测试通过。\n# 注释\n少用注释，有些日志注释交给版本管理工具\n\n# 代码格式\n变量声明，应尽可能的靠近其使用位置。\n实体变量应该在类的顶部声明。\n相关函数，应该放在一起。\n探索了各种缩进和空格的使用。最重要的是良好的书写代码的习惯和风格，并且保持一致。记住，代码被阅读的时间远远超过写的时间。所以，写好每一段代码。就像讲好每一个故事一样。\n\n# 对象和数据结构\n对象把数据隐藏于抽象之后，暴露操作数据的函数。数据结构暴露其数据，没有提供有意义的函数。过程式代码（使用数据结构的代码）便于在不改动既有数据结构的前提下添加新的函数，而面向对象代码便于在不改动既有函数的前提下添加新类。反过来讲也说得通，过程式代码难以添加新数据结构，因为必须修改所有函数，面向对象的代码难以修改函数，因为必须修改所有类。\n# 错误处理\n将错误处理隔离看待，独立于主要的程序逻辑，就能写出强固而简洁的代码。\n# 边界\n# 单元测试\n# 类\n类应当短小：应该能够用大概25个单词描述一个类，而且不能用“if”,\"and\",\"or\",\"but\"等词汇。\n单一职权原则：只有一个修改的理由\n内聚：类应该只有少量的实体变量。类中的每个方法都应该操作一个或者多个这种变量。\n\n# 系统：将构造与使用分开的方法之一就是将全部构造过程搬迁至Main模块中，设计系统的其余部分是，假设所有的对象都已正确构造和设置。\n工厂\n控制反转\nAOP   \n# 迭进    \n软件项目的主要成本在于长期维护。\n# 并发编程\n对象是过程的抽象，线程是调度的抽象。\n第一要诀还是遵守单一权责原则，了解并发问题的可能原因，学习类库，了解基本算法，学习如何找到必须锁定的区域并锁定之。\n# 逐步改进\n对一个命令行参数解析程序的案例研究\n要编写整洁的代码，必须先写肮脏代码，然后再清理它。\n\n# Junit内幕\n# 重构SerialDate\n# 味道与启发\n\n\n\n","source":"_posts/clean-code.md","raw":"---\ntitle: 代码整洁之道\ndate: 2014-08-20 20:54:30\ntags: 随笔\n---\n\n这是一本关于如何写好代码，如何当好一名优秀的程序员的书籍。在本书中，作者引用IT牛人的话，阐述了什么是简洁的代码，然后提出了自己的看法。正如作者一直强调的，光看书是远远不够的，要勤于练习，不断改正，才能有所收获。下面就让我们来一探究竟。\n\n# 整洁的代码\n在这章里面，作者探讨了整洁的代码的重要性，以及何为简洁的代码。\n# 有意义的命名\n>名副其实，而且一旦发现有更好的名字，立即换掉旧的。\n避免误导\n有意义的区分,Variable一词永远不应当出现在变量名中，table一词也不应当出现在表名中。\n使用读得出来的名称\n使用可搜索的名称\n避免使用编码\n类名：类名和对象名应该是名词或者名词短语，而不应该是动词。\n方法名：方法名应当是动词或者动词短语。    \n每个概念对应一个词，不要使用双关语。\n不要添加没用的语境：不要添加重复相同的前缀。只要短名称足够清除，就要比长名称好。对于Address类的实体来说，accountAddress和customerAddress都是不错的对象名称，但是用在类名上就不好了。Address是个好类名，如果需要与MAC地址，端口地址，WEB地址做区别，我会考虑使用PostalAddress、MAC和URI。这样的名称更为精确，而精确正是命名的要点。\n\n<!-- more -->\n# 函数 \n通过函数名，可以读出来它是干什么的\n短小，只做一件事情，每个函数一个抽象层次：如果函数只是做了该函数名下同一抽象层上的步骤，则函数还是只是做了一件事。编写函数，毕竟是为了把大一些的概念（换言之，函数的名称）拆分成另一抽象层级的一系列处理步骤。\n避免重复。\n如何写出好的函数：刚开始写函数时，有太多的缩进和嵌套循环，有过长的参数列表，名称是随意取的，也会有重复的代码。不过会配上一套测试单元，覆盖每行丑陋的代码。然后再打磨这些代码，分解函数，修改名称，消除重复，缩短和重新安置方法。有时，还拆散类，保持测试通过。\n# 注释\n少用注释，有些日志注释交给版本管理工具\n\n# 代码格式\n变量声明，应尽可能的靠近其使用位置。\n实体变量应该在类的顶部声明。\n相关函数，应该放在一起。\n探索了各种缩进和空格的使用。最重要的是良好的书写代码的习惯和风格，并且保持一致。记住，代码被阅读的时间远远超过写的时间。所以，写好每一段代码。就像讲好每一个故事一样。\n\n# 对象和数据结构\n对象把数据隐藏于抽象之后，暴露操作数据的函数。数据结构暴露其数据，没有提供有意义的函数。过程式代码（使用数据结构的代码）便于在不改动既有数据结构的前提下添加新的函数，而面向对象代码便于在不改动既有函数的前提下添加新类。反过来讲也说得通，过程式代码难以添加新数据结构，因为必须修改所有函数，面向对象的代码难以修改函数，因为必须修改所有类。\n# 错误处理\n将错误处理隔离看待，独立于主要的程序逻辑，就能写出强固而简洁的代码。\n# 边界\n# 单元测试\n# 类\n类应当短小：应该能够用大概25个单词描述一个类，而且不能用“if”,\"and\",\"or\",\"but\"等词汇。\n单一职权原则：只有一个修改的理由\n内聚：类应该只有少量的实体变量。类中的每个方法都应该操作一个或者多个这种变量。\n\n# 系统：将构造与使用分开的方法之一就是将全部构造过程搬迁至Main模块中，设计系统的其余部分是，假设所有的对象都已正确构造和设置。\n工厂\n控制反转\nAOP   \n# 迭进    \n软件项目的主要成本在于长期维护。\n# 并发编程\n对象是过程的抽象，线程是调度的抽象。\n第一要诀还是遵守单一权责原则，了解并发问题的可能原因，学习类库，了解基本算法，学习如何找到必须锁定的区域并锁定之。\n# 逐步改进\n对一个命令行参数解析程序的案例研究\n要编写整洁的代码，必须先写肮脏代码，然后再清理它。\n\n# Junit内幕\n# 重构SerialDate\n# 味道与启发\n\n\n\n","slug":"clean-code","published":1,"updated":"2018-04-27T04:11:03.330Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjhajczo80005e4rw9po13kw9","content":"<p>这是一本关于如何写好代码，如何当好一名优秀的程序员的书籍。在本书中，作者引用IT牛人的话，阐述了什么是简洁的代码，然后提出了自己的看法。正如作者一直强调的，光看书是远远不够的，要勤于练习，不断改正，才能有所收获。下面就让我们来一探究竟。</p>\n<h1 id=\"整洁的代码\"><a href=\"#整洁的代码\" class=\"headerlink\" title=\"整洁的代码\"></a>整洁的代码</h1><p>在这章里面，作者探讨了整洁的代码的重要性，以及何为简洁的代码。</p>\n<h1 id=\"有意义的命名\"><a href=\"#有意义的命名\" class=\"headerlink\" title=\"有意义的命名\"></a>有意义的命名</h1><blockquote>\n<p>名副其实，而且一旦发现有更好的名字，立即换掉旧的。<br>避免误导<br>有意义的区分,Variable一词永远不应当出现在变量名中，table一词也不应当出现在表名中。<br>使用读得出来的名称<br>使用可搜索的名称<br>避免使用编码<br>类名：类名和对象名应该是名词或者名词短语，而不应该是动词。<br>方法名：方法名应当是动词或者动词短语。<br>每个概念对应一个词，不要使用双关语。<br>不要添加没用的语境：不要添加重复相同的前缀。只要短名称足够清除，就要比长名称好。对于Address类的实体来说，accountAddress和customerAddress都是不错的对象名称，但是用在类名上就不好了。Address是个好类名，如果需要与MAC地址，端口地址，WEB地址做区别，我会考虑使用PostalAddress、MAC和URI。这样的名称更为精确，而精确正是命名的要点。</p>\n</blockquote>\n<a id=\"more\"></a>\n<h1 id=\"函数\"><a href=\"#函数\" class=\"headerlink\" title=\"函数\"></a>函数</h1><p>通过函数名，可以读出来它是干什么的<br>短小，只做一件事情，每个函数一个抽象层次：如果函数只是做了该函数名下同一抽象层上的步骤，则函数还是只是做了一件事。编写函数，毕竟是为了把大一些的概念（换言之，函数的名称）拆分成另一抽象层级的一系列处理步骤。<br>避免重复。<br>如何写出好的函数：刚开始写函数时，有太多的缩进和嵌套循环，有过长的参数列表，名称是随意取的，也会有重复的代码。不过会配上一套测试单元，覆盖每行丑陋的代码。然后再打磨这些代码，分解函数，修改名称，消除重复，缩短和重新安置方法。有时，还拆散类，保持测试通过。</p>\n<h1 id=\"注释\"><a href=\"#注释\" class=\"headerlink\" title=\"注释\"></a>注释</h1><p>少用注释，有些日志注释交给版本管理工具</p>\n<h1 id=\"代码格式\"><a href=\"#代码格式\" class=\"headerlink\" title=\"代码格式\"></a>代码格式</h1><p>变量声明，应尽可能的靠近其使用位置。<br>实体变量应该在类的顶部声明。<br>相关函数，应该放在一起。<br>探索了各种缩进和空格的使用。最重要的是良好的书写代码的习惯和风格，并且保持一致。记住，代码被阅读的时间远远超过写的时间。所以，写好每一段代码。就像讲好每一个故事一样。</p>\n<h1 id=\"对象和数据结构\"><a href=\"#对象和数据结构\" class=\"headerlink\" title=\"对象和数据结构\"></a>对象和数据结构</h1><p>对象把数据隐藏于抽象之后，暴露操作数据的函数。数据结构暴露其数据，没有提供有意义的函数。过程式代码（使用数据结构的代码）便于在不改动既有数据结构的前提下添加新的函数，而面向对象代码便于在不改动既有函数的前提下添加新类。反过来讲也说得通，过程式代码难以添加新数据结构，因为必须修改所有函数，面向对象的代码难以修改函数，因为必须修改所有类。</p>\n<h1 id=\"错误处理\"><a href=\"#错误处理\" class=\"headerlink\" title=\"错误处理\"></a>错误处理</h1><p>将错误处理隔离看待，独立于主要的程序逻辑，就能写出强固而简洁的代码。</p>\n<h1 id=\"边界\"><a href=\"#边界\" class=\"headerlink\" title=\"边界\"></a>边界</h1><h1 id=\"单元测试\"><a href=\"#单元测试\" class=\"headerlink\" title=\"单元测试\"></a>单元测试</h1><h1 id=\"类\"><a href=\"#类\" class=\"headerlink\" title=\"类\"></a>类</h1><p>类应当短小：应该能够用大概25个单词描述一个类，而且不能用“if”,”and”,”or”,”but”等词汇。<br>单一职权原则：只有一个修改的理由<br>内聚：类应该只有少量的实体变量。类中的每个方法都应该操作一个或者多个这种变量。</p>\n<h1 id=\"系统：将构造与使用分开的方法之一就是将全部构造过程搬迁至Main模块中，设计系统的其余部分是，假设所有的对象都已正确构造和设置。\"><a href=\"#系统：将构造与使用分开的方法之一就是将全部构造过程搬迁至Main模块中，设计系统的其余部分是，假设所有的对象都已正确构造和设置。\" class=\"headerlink\" title=\"系统：将构造与使用分开的方法之一就是将全部构造过程搬迁至Main模块中，设计系统的其余部分是，假设所有的对象都已正确构造和设置。\"></a>系统：将构造与使用分开的方法之一就是将全部构造过程搬迁至Main模块中，设计系统的其余部分是，假设所有的对象都已正确构造和设置。</h1><p>工厂<br>控制反转<br>AOP   </p>\n<h1 id=\"迭进\"><a href=\"#迭进\" class=\"headerlink\" title=\"迭进\"></a>迭进</h1><p>软件项目的主要成本在于长期维护。</p>\n<h1 id=\"并发编程\"><a href=\"#并发编程\" class=\"headerlink\" title=\"并发编程\"></a>并发编程</h1><p>对象是过程的抽象，线程是调度的抽象。<br>第一要诀还是遵守单一权责原则，了解并发问题的可能原因，学习类库，了解基本算法，学习如何找到必须锁定的区域并锁定之。</p>\n<h1 id=\"逐步改进\"><a href=\"#逐步改进\" class=\"headerlink\" title=\"逐步改进\"></a>逐步改进</h1><p>对一个命令行参数解析程序的案例研究<br>要编写整洁的代码，必须先写肮脏代码，然后再清理它。</p>\n<h1 id=\"Junit内幕\"><a href=\"#Junit内幕\" class=\"headerlink\" title=\"Junit内幕\"></a>Junit内幕</h1><h1 id=\"重构SerialDate\"><a href=\"#重构SerialDate\" class=\"headerlink\" title=\"重构SerialDate\"></a>重构SerialDate</h1><h1 id=\"味道与启发\"><a href=\"#味道与启发\" class=\"headerlink\" title=\"味道与启发\"></a>味道与启发</h1>","site":{"data":{}},"excerpt":"<p>这是一本关于如何写好代码，如何当好一名优秀的程序员的书籍。在本书中，作者引用IT牛人的话，阐述了什么是简洁的代码，然后提出了自己的看法。正如作者一直强调的，光看书是远远不够的，要勤于练习，不断改正，才能有所收获。下面就让我们来一探究竟。</p>\n<h1 id=\"整洁的代码\"><a href=\"#整洁的代码\" class=\"headerlink\" title=\"整洁的代码\"></a>整洁的代码</h1><p>在这章里面，作者探讨了整洁的代码的重要性，以及何为简洁的代码。</p>\n<h1 id=\"有意义的命名\"><a href=\"#有意义的命名\" class=\"headerlink\" title=\"有意义的命名\"></a>有意义的命名</h1><blockquote>\n<p>名副其实，而且一旦发现有更好的名字，立即换掉旧的。<br>避免误导<br>有意义的区分,Variable一词永远不应当出现在变量名中，table一词也不应当出现在表名中。<br>使用读得出来的名称<br>使用可搜索的名称<br>避免使用编码<br>类名：类名和对象名应该是名词或者名词短语，而不应该是动词。<br>方法名：方法名应当是动词或者动词短语。<br>每个概念对应一个词，不要使用双关语。<br>不要添加没用的语境：不要添加重复相同的前缀。只要短名称足够清除，就要比长名称好。对于Address类的实体来说，accountAddress和customerAddress都是不错的对象名称，但是用在类名上就不好了。Address是个好类名，如果需要与MAC地址，端口地址，WEB地址做区别，我会考虑使用PostalAddress、MAC和URI。这样的名称更为精确，而精确正是命名的要点。</p>\n</blockquote>","more":"<h1 id=\"函数\"><a href=\"#函数\" class=\"headerlink\" title=\"函数\"></a>函数</h1><p>通过函数名，可以读出来它是干什么的<br>短小，只做一件事情，每个函数一个抽象层次：如果函数只是做了该函数名下同一抽象层上的步骤，则函数还是只是做了一件事。编写函数，毕竟是为了把大一些的概念（换言之，函数的名称）拆分成另一抽象层级的一系列处理步骤。<br>避免重复。<br>如何写出好的函数：刚开始写函数时，有太多的缩进和嵌套循环，有过长的参数列表，名称是随意取的，也会有重复的代码。不过会配上一套测试单元，覆盖每行丑陋的代码。然后再打磨这些代码，分解函数，修改名称，消除重复，缩短和重新安置方法。有时，还拆散类，保持测试通过。</p>\n<h1 id=\"注释\"><a href=\"#注释\" class=\"headerlink\" title=\"注释\"></a>注释</h1><p>少用注释，有些日志注释交给版本管理工具</p>\n<h1 id=\"代码格式\"><a href=\"#代码格式\" class=\"headerlink\" title=\"代码格式\"></a>代码格式</h1><p>变量声明，应尽可能的靠近其使用位置。<br>实体变量应该在类的顶部声明。<br>相关函数，应该放在一起。<br>探索了各种缩进和空格的使用。最重要的是良好的书写代码的习惯和风格，并且保持一致。记住，代码被阅读的时间远远超过写的时间。所以，写好每一段代码。就像讲好每一个故事一样。</p>\n<h1 id=\"对象和数据结构\"><a href=\"#对象和数据结构\" class=\"headerlink\" title=\"对象和数据结构\"></a>对象和数据结构</h1><p>对象把数据隐藏于抽象之后，暴露操作数据的函数。数据结构暴露其数据，没有提供有意义的函数。过程式代码（使用数据结构的代码）便于在不改动既有数据结构的前提下添加新的函数，而面向对象代码便于在不改动既有函数的前提下添加新类。反过来讲也说得通，过程式代码难以添加新数据结构，因为必须修改所有函数，面向对象的代码难以修改函数，因为必须修改所有类。</p>\n<h1 id=\"错误处理\"><a href=\"#错误处理\" class=\"headerlink\" title=\"错误处理\"></a>错误处理</h1><p>将错误处理隔离看待，独立于主要的程序逻辑，就能写出强固而简洁的代码。</p>\n<h1 id=\"边界\"><a href=\"#边界\" class=\"headerlink\" title=\"边界\"></a>边界</h1><h1 id=\"单元测试\"><a href=\"#单元测试\" class=\"headerlink\" title=\"单元测试\"></a>单元测试</h1><h1 id=\"类\"><a href=\"#类\" class=\"headerlink\" title=\"类\"></a>类</h1><p>类应当短小：应该能够用大概25个单词描述一个类，而且不能用“if”,”and”,”or”,”but”等词汇。<br>单一职权原则：只有一个修改的理由<br>内聚：类应该只有少量的实体变量。类中的每个方法都应该操作一个或者多个这种变量。</p>\n<h1 id=\"系统：将构造与使用分开的方法之一就是将全部构造过程搬迁至Main模块中，设计系统的其余部分是，假设所有的对象都已正确构造和设置。\"><a href=\"#系统：将构造与使用分开的方法之一就是将全部构造过程搬迁至Main模块中，设计系统的其余部分是，假设所有的对象都已正确构造和设置。\" class=\"headerlink\" title=\"系统：将构造与使用分开的方法之一就是将全部构造过程搬迁至Main模块中，设计系统的其余部分是，假设所有的对象都已正确构造和设置。\"></a>系统：将构造与使用分开的方法之一就是将全部构造过程搬迁至Main模块中，设计系统的其余部分是，假设所有的对象都已正确构造和设置。</h1><p>工厂<br>控制反转<br>AOP   </p>\n<h1 id=\"迭进\"><a href=\"#迭进\" class=\"headerlink\" title=\"迭进\"></a>迭进</h1><p>软件项目的主要成本在于长期维护。</p>\n<h1 id=\"并发编程\"><a href=\"#并发编程\" class=\"headerlink\" title=\"并发编程\"></a>并发编程</h1><p>对象是过程的抽象，线程是调度的抽象。<br>第一要诀还是遵守单一权责原则，了解并发问题的可能原因，学习类库，了解基本算法，学习如何找到必须锁定的区域并锁定之。</p>\n<h1 id=\"逐步改进\"><a href=\"#逐步改进\" class=\"headerlink\" title=\"逐步改进\"></a>逐步改进</h1><p>对一个命令行参数解析程序的案例研究<br>要编写整洁的代码，必须先写肮脏代码，然后再清理它。</p>\n<h1 id=\"Junit内幕\"><a href=\"#Junit内幕\" class=\"headerlink\" title=\"Junit内幕\"></a>Junit内幕</h1><h1 id=\"重构SerialDate\"><a href=\"#重构SerialDate\" class=\"headerlink\" title=\"重构SerialDate\"></a>重构SerialDate</h1><h1 id=\"味道与启发\"><a href=\"#味道与启发\" class=\"headerlink\" title=\"味道与启发\"></a>味道与启发</h1>"},{"title":"Elasticsearch 监控","date":"2017-08-17T12:30:45.000Z","_content":"ElasticSearch是一个基于Lucene的搜索服务器。它提供了一个分布式多用户能力的全文搜索引擎，基于RESTful web接口。Elasticsearch是用Java开发的，并作为Apache许可条款下的开放源码发布，是当前流行的企业级搜索引擎。设计用于云计算中，能够达到实时搜索，稳定，可靠，快速，安装使用方便。\n\n# Elasticsearch集群状态监控\n集群状态\n`curl -XGET http://192.168.17.201:9200/_cluster/health?pretty 2>/dev/null|grep \"status\"|awk -F: '{print $2}'|awk -F \\\" '{print $2}'`\n\n<!-- more -->\n\n# Elasticsearch集群节点数目\n`curl -XGET http://192.168.17.201:9200/_cluster/health?pretty 2>/dev/null|grep \"number_of_nodes\"|awk -F: '{print $2}'|awk -F\\, '{print $1}'`\n","source":"_posts/elasticsearch-monitor.md","raw":"---\ntitle: Elasticsearch 监控\ndate: 2017-08-17 20:30:45\ntags: \n - Elasticsearch\n - Monitor \ncategory: 技术 \n---\nElasticSearch是一个基于Lucene的搜索服务器。它提供了一个分布式多用户能力的全文搜索引擎，基于RESTful web接口。Elasticsearch是用Java开发的，并作为Apache许可条款下的开放源码发布，是当前流行的企业级搜索引擎。设计用于云计算中，能够达到实时搜索，稳定，可靠，快速，安装使用方便。\n\n# Elasticsearch集群状态监控\n集群状态\n`curl -XGET http://192.168.17.201:9200/_cluster/health?pretty 2>/dev/null|grep \"status\"|awk -F: '{print $2}'|awk -F \\\" '{print $2}'`\n\n<!-- more -->\n\n# Elasticsearch集群节点数目\n`curl -XGET http://192.168.17.201:9200/_cluster/health?pretty 2>/dev/null|grep \"number_of_nodes\"|awk -F: '{print $2}'|awk -F\\, '{print $1}'`\n","slug":"elasticsearch-monitor","published":1,"updated":"2018-04-27T04:11:03.334Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjhajczod0009e4rwm9mn90tf","content":"<p>ElasticSearch是一个基于Lucene的搜索服务器。它提供了一个分布式多用户能力的全文搜索引擎，基于RESTful web接口。Elasticsearch是用Java开发的，并作为Apache许可条款下的开放源码发布，是当前流行的企业级搜索引擎。设计用于云计算中，能够达到实时搜索，稳定，可靠，快速，安装使用方便。</p>\n<h1 id=\"Elasticsearch集群状态监控\"><a href=\"#Elasticsearch集群状态监控\" class=\"headerlink\" title=\"Elasticsearch集群状态监控\"></a>Elasticsearch集群状态监控</h1><p>集群状态<br><code>curl -XGET http://192.168.17.201:9200/_cluster/health?pretty 2&gt;/dev/null|grep &quot;status&quot;|awk -F: &#39;{print $2}&#39;|awk -F \\&quot; &#39;{print $2}&#39;</code></p>\n<a id=\"more\"></a>\n<h1 id=\"Elasticsearch集群节点数目\"><a href=\"#Elasticsearch集群节点数目\" class=\"headerlink\" title=\"Elasticsearch集群节点数目\"></a>Elasticsearch集群节点数目</h1><p><code>curl -XGET http://192.168.17.201:9200/_cluster/health?pretty 2&gt;/dev/null|grep &quot;number_of_nodes&quot;|awk -F: &#39;{print $2}&#39;|awk -F\\, &#39;{print $1}&#39;</code></p>\n","site":{"data":{}},"excerpt":"<p>ElasticSearch是一个基于Lucene的搜索服务器。它提供了一个分布式多用户能力的全文搜索引擎，基于RESTful web接口。Elasticsearch是用Java开发的，并作为Apache许可条款下的开放源码发布，是当前流行的企业级搜索引擎。设计用于云计算中，能够达到实时搜索，稳定，可靠，快速，安装使用方便。</p>\n<h1 id=\"Elasticsearch集群状态监控\"><a href=\"#Elasticsearch集群状态监控\" class=\"headerlink\" title=\"Elasticsearch集群状态监控\"></a>Elasticsearch集群状态监控</h1><p>集群状态<br><code>curl -XGET http://192.168.17.201:9200/_cluster/health?pretty 2&gt;/dev/null|grep &quot;status&quot;|awk -F: &#39;{print $2}&#39;|awk -F \\&quot; &#39;{print $2}&#39;</code></p>","more":"<h1 id=\"Elasticsearch集群节点数目\"><a href=\"#Elasticsearch集群节点数目\" class=\"headerlink\" title=\"Elasticsearch集群节点数目\"></a>Elasticsearch集群节点数目</h1><p><code>curl -XGET http://192.168.17.201:9200/_cluster/health?pretty 2&gt;/dev/null|grep &quot;number_of_nodes&quot;|awk -F: &#39;{print $2}&#39;|awk -F\\, &#39;{print $1}&#39;</code></p>"},{"title":"Git/Github","date":"2015-05-20T12:07:50.000Z","_content":"\n# Git \n* win7+   \n在wind7下搭建git环境。需下载windows 版本的git工具。见附件。选择完整安装。安装目录 D:\\Program Files (x86)\\Git\\\n并将该路径添加至PATH环境。之后在cmd 环境下就可以使用了。后续如何使用git 就参考git 使用手册了，跟linux环境下的使用方法类似。\n在windows 下生成id_rsa ，免密码连接github。使用 进入D:\\Program Files (x86)\\Git\\git-bash.exe 命令，进入控制台窗口后，输入\n`ssh-keygen -t rsa -b 4096 -C \"zhoujinl@126.com\"`\n然后一直按回车默认，直至完成。\n最后可以在C:\\Users\\zhoujl\\.ssh 目录下生成ssh 密钥和公钥。 \n然后到.ssh目录中，将生成的id_rsa.pub里面的内容，复制到github网站中的 SSH KEYs 设置中。\ngit 全局设置\n>git config --global user.name zhoujinl\ngit config --global user.email zhoujinl@126.com  \n\n在公司内部，有时需要通过设置代理才能连接上外网。\n设置代理\n`git config --global http.proxy http://user:pwd@server.com:port`\n取消代理\n`git config --global (or --system or --local)  --unset  http.proxy `\n* Unix  \n这里主要介绍linux版本。其余的类linux应该类似。以CentOs环境为例，通过yum命令即可快速安装。\n`yum install git`\n\n<!-- more -->\n\n# GitHub\nGitHub是一个以git作为版本控制器的代码托管社区。我们可以在上面创建项目，然后通过git来进行开发维护和协作。具体的使用方法，可参考网站：http://www.worldhello.net/gotgithub/index.html 有详细介绍。\nPS:在设置完user.name和user.email之后，通过命令生成id_ras.pub，\n`ssh-keygen -t rsa -b 4096 -C \"zhoujinl@126.com\"`\n并且上传到github 官网中。然后通过ssh 方式，从github上面clone项目到本地。由于已经将id_ras.pub上传到到github中，因此可以免密码登陆下载。\n验证是否创建成功，如出现以下结果则说明ssh免密码设置成功。\n>$  ssh -T git@github.com\nHi zhoujinl! You've successfully authenticated, but GitHub does not provide shell access.\n\n然后即可从github上面clone项目下来\n`git clone git@github.com:zhoujinl/zhoujinl.github.io.git`\n注：Are you sure you want to continue connecting (yes/no)?  会要求你是否确认连接，输入yes\n\n另外也可以使用https的方式。从github上克隆项目下来，并且push的时候，要求每次都输入用户名密码\n`git clone https://github.com/zhoujinl/zhoujinl.github.io.git`\n\n\n# Git使用技巧：\n>git pull <远程主机名> <远程分支名>:<本地分支名>\n比如，取回origin主机的next分支，与本地的master分支合并，需要写成下面这样。\ngit pull origin next:master\ngit push <远程主机名> <本地分支名>:<远程分支名>\n本地的master分支推送到origin主机的master分支。如果后者不存在，则会被新建\ngit push origin master\n注意，分支推送顺序的写法是<来源地>:<目的地>，所以git pull是<远程分支>:<本地分支>，而git push是<本地分支>:<远程分支>。\n\n* Git撤销操作：\n    HEAD 最近一个提交\n    HEAD^ 上一次\n1. 处于add之后，未commit之前  \ngit reset HAED <file>  \n2. commit之后，还未push之前  \n>a.先看日志 git log ,找到要回退之前的版本的commit-id\nλ git log\ncommit 992de5dde4f29aa35eb642b8fd55089838572c2d   ###此次错误提交的版本\nAuthor: zhoujl <zhoujl@ffcs.cn>\nDate:   Wed Aug 9 16:40:59 2017 +0800\nLocal change\ncommit 931c4a4e7678b760faa6849e16c849d7c679ac9c       ##上一个版本，应该使用该id\nAuthor: arganzheng <arganzheng@gmail.com>\nDate:   Thu Jun 18 20:21:51 2015 +0800\nsmall modify\nb.执行撤销操作\n`git reset --hard 931c4a4e7678b760faa6849e16c849d7c679ac9c `    \n**注意：**工 作区和暂存区的内容都会被重置到指定提交的时候，如果不加--hard则只移动HEAD的指针，不影响工作区和暂存区的内容。\n3. push之后，如何回退：--待确认\n\t在步骤2的基础上，执行   git push origin HEAD --force\n","source":"_posts/git-github.md","raw":"---\ntitle: Git/Github\ndate: 2015-05-20 20:07:50\ntags: \n - Git\n - GitHub \ncategory: 技术 \n---\n\n# Git \n* win7+   \n在wind7下搭建git环境。需下载windows 版本的git工具。见附件。选择完整安装。安装目录 D:\\Program Files (x86)\\Git\\\n并将该路径添加至PATH环境。之后在cmd 环境下就可以使用了。后续如何使用git 就参考git 使用手册了，跟linux环境下的使用方法类似。\n在windows 下生成id_rsa ，免密码连接github。使用 进入D:\\Program Files (x86)\\Git\\git-bash.exe 命令，进入控制台窗口后，输入\n`ssh-keygen -t rsa -b 4096 -C \"zhoujinl@126.com\"`\n然后一直按回车默认，直至完成。\n最后可以在C:\\Users\\zhoujl\\.ssh 目录下生成ssh 密钥和公钥。 \n然后到.ssh目录中，将生成的id_rsa.pub里面的内容，复制到github网站中的 SSH KEYs 设置中。\ngit 全局设置\n>git config --global user.name zhoujinl\ngit config --global user.email zhoujinl@126.com  \n\n在公司内部，有时需要通过设置代理才能连接上外网。\n设置代理\n`git config --global http.proxy http://user:pwd@server.com:port`\n取消代理\n`git config --global (or --system or --local)  --unset  http.proxy `\n* Unix  \n这里主要介绍linux版本。其余的类linux应该类似。以CentOs环境为例，通过yum命令即可快速安装。\n`yum install git`\n\n<!-- more -->\n\n# GitHub\nGitHub是一个以git作为版本控制器的代码托管社区。我们可以在上面创建项目，然后通过git来进行开发维护和协作。具体的使用方法，可参考网站：http://www.worldhello.net/gotgithub/index.html 有详细介绍。\nPS:在设置完user.name和user.email之后，通过命令生成id_ras.pub，\n`ssh-keygen -t rsa -b 4096 -C \"zhoujinl@126.com\"`\n并且上传到github 官网中。然后通过ssh 方式，从github上面clone项目到本地。由于已经将id_ras.pub上传到到github中，因此可以免密码登陆下载。\n验证是否创建成功，如出现以下结果则说明ssh免密码设置成功。\n>$  ssh -T git@github.com\nHi zhoujinl! You've successfully authenticated, but GitHub does not provide shell access.\n\n然后即可从github上面clone项目下来\n`git clone git@github.com:zhoujinl/zhoujinl.github.io.git`\n注：Are you sure you want to continue connecting (yes/no)?  会要求你是否确认连接，输入yes\n\n另外也可以使用https的方式。从github上克隆项目下来，并且push的时候，要求每次都输入用户名密码\n`git clone https://github.com/zhoujinl/zhoujinl.github.io.git`\n\n\n# Git使用技巧：\n>git pull <远程主机名> <远程分支名>:<本地分支名>\n比如，取回origin主机的next分支，与本地的master分支合并，需要写成下面这样。\ngit pull origin next:master\ngit push <远程主机名> <本地分支名>:<远程分支名>\n本地的master分支推送到origin主机的master分支。如果后者不存在，则会被新建\ngit push origin master\n注意，分支推送顺序的写法是<来源地>:<目的地>，所以git pull是<远程分支>:<本地分支>，而git push是<本地分支>:<远程分支>。\n\n* Git撤销操作：\n    HEAD 最近一个提交\n    HEAD^ 上一次\n1. 处于add之后，未commit之前  \ngit reset HAED <file>  \n2. commit之后，还未push之前  \n>a.先看日志 git log ,找到要回退之前的版本的commit-id\nλ git log\ncommit 992de5dde4f29aa35eb642b8fd55089838572c2d   ###此次错误提交的版本\nAuthor: zhoujl <zhoujl@ffcs.cn>\nDate:   Wed Aug 9 16:40:59 2017 +0800\nLocal change\ncommit 931c4a4e7678b760faa6849e16c849d7c679ac9c       ##上一个版本，应该使用该id\nAuthor: arganzheng <arganzheng@gmail.com>\nDate:   Thu Jun 18 20:21:51 2015 +0800\nsmall modify\nb.执行撤销操作\n`git reset --hard 931c4a4e7678b760faa6849e16c849d7c679ac9c `    \n**注意：**工 作区和暂存区的内容都会被重置到指定提交的时候，如果不加--hard则只移动HEAD的指针，不影响工作区和暂存区的内容。\n3. push之后，如何回退：--待确认\n\t在步骤2的基础上，执行   git push origin HEAD --force\n","slug":"git-github","published":1,"updated":"2018-04-27T04:11:03.341Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjhajczof000be4rww9peqwrt","content":"<h1 id=\"Git\"><a href=\"#Git\" class=\"headerlink\" title=\"Git\"></a>Git</h1><ul>\n<li>win7+<br>在wind7下搭建git环境。需下载windows 版本的git工具。见附件。选择完整安装。安装目录 D:\\Program Files (x86)\\Git\\<br>并将该路径添加至PATH环境。之后在cmd 环境下就可以使用了。后续如何使用git 就参考git 使用手册了，跟linux环境下的使用方法类似。<br>在windows 下生成id_rsa ，免密码连接github。使用 进入D:\\Program Files (x86)\\Git\\git-bash.exe 命令，进入控制台窗口后，输入<br><code>ssh-keygen -t rsa -b 4096 -C &quot;zhoujinl@126.com&quot;</code><br>然后一直按回车默认，直至完成。<br>最后可以在C:\\Users\\zhoujl.ssh 目录下生成ssh 密钥和公钥。<br>然后到.ssh目录中，将生成的id_rsa.pub里面的内容，复制到github网站中的 SSH KEYs 设置中。<br>git 全局设置<blockquote>\n<p>git config –global user.name zhoujinl<br>git config –global user.email zhoujinl@126.com  </p>\n</blockquote>\n</li>\n</ul>\n<p>在公司内部，有时需要通过设置代理才能连接上外网。<br>设置代理<br><code>git config --global http.proxy http://user:pwd@server.com:port</code><br>取消代理<br><code>git config --global (or --system or --local)  --unset  http.proxy</code></p>\n<ul>\n<li>Unix<br>这里主要介绍linux版本。其余的类linux应该类似。以CentOs环境为例，通过yum命令即可快速安装。<br><code>yum install git</code></li>\n</ul>\n<a id=\"more\"></a>\n<h1 id=\"GitHub\"><a href=\"#GitHub\" class=\"headerlink\" title=\"GitHub\"></a>GitHub</h1><p>GitHub是一个以git作为版本控制器的代码托管社区。我们可以在上面创建项目，然后通过git来进行开发维护和协作。具体的使用方法，可参考网站：<a href=\"http://www.worldhello.net/gotgithub/index.html\" target=\"_blank\" rel=\"external\">http://www.worldhello.net/gotgithub/index.html</a> 有详细介绍。<br>PS:在设置完user.name和user.email之后，通过命令生成id_ras.pub，<br><code>ssh-keygen -t rsa -b 4096 -C &quot;zhoujinl@126.com&quot;</code><br>并且上传到github 官网中。然后通过ssh 方式，从github上面clone项目到本地。由于已经将id_ras.pub上传到到github中，因此可以免密码登陆下载。<br>验证是否创建成功，如出现以下结果则说明ssh免密码设置成功。</p>\n<blockquote>\n<p>$  ssh -T git@github.com<br>Hi zhoujinl! You’ve successfully authenticated, but GitHub does not provide shell access.</p>\n</blockquote>\n<p>然后即可从github上面clone项目下来<br><code>git clone git@github.com:zhoujinl/zhoujinl.github.io.git</code><br>注：Are you sure you want to continue connecting (yes/no)?  会要求你是否确认连接，输入yes</p>\n<p>另外也可以使用https的方式。从github上克隆项目下来，并且push的时候，要求每次都输入用户名密码<br><code>git clone https://github.com/zhoujinl/zhoujinl.github.io.git</code></p>\n<h1 id=\"Git使用技巧：\"><a href=\"#Git使用技巧：\" class=\"headerlink\" title=\"Git使用技巧：\"></a>Git使用技巧：</h1><blockquote>\n<p>git pull &lt;远程主机名&gt; &lt;远程分支名&gt;:&lt;本地分支名&gt;<br>比如，取回origin主机的next分支，与本地的master分支合并，需要写成下面这样。<br>git pull origin next:master<br>git push &lt;远程主机名&gt; &lt;本地分支名&gt;:&lt;远程分支名&gt;<br>本地的master分支推送到origin主机的master分支。如果后者不存在，则会被新建<br>git push origin master<br>注意，分支推送顺序的写法是&lt;来源地&gt;:&lt;目的地&gt;，所以git pull是&lt;远程分支&gt;:&lt;本地分支&gt;，而git push是&lt;本地分支&gt;:&lt;远程分支&gt;。</p>\n</blockquote>\n<ul>\n<li>Git撤销操作：<br>  HEAD 最近一个提交<br>  HEAD^ 上一次</li>\n</ul>\n<ol>\n<li>处于add之后，未commit之前<br>git reset HAED <file>  </file></li>\n<li>commit之后，还未push之前  <blockquote>\n<p>a.先看日志 git log ,找到要回退之前的版本的commit-id<br>λ git log<br>commit 992de5dde4f29aa35eb642b8fd55089838572c2d   ###此次错误提交的版本<br>Author: zhoujl <a href=\"&#109;&#x61;&#x69;&#x6c;&#x74;&#111;&#58;&#122;&#x68;&#x6f;&#x75;&#x6a;&#x6c;&#64;&#102;&#102;&#x63;&#115;&#x2e;&#x63;&#x6e;\">&#122;&#x68;&#x6f;&#x75;&#x6a;&#x6c;&#64;&#102;&#102;&#x63;&#115;&#x2e;&#x63;&#x6e;</a><br>Date:   Wed Aug 9 16:40:59 2017 +0800<br>Local change<br>commit 931c4a4e7678b760faa6849e16c849d7c679ac9c       ##上一个版本，应该使用该id<br>Author: arganzheng <a href=\"&#x6d;&#x61;&#105;&#108;&#x74;&#111;&#x3a;&#97;&#x72;&#x67;&#97;&#x6e;&#122;&#x68;&#x65;&#110;&#x67;&#x40;&#103;&#109;&#x61;&#105;&#108;&#x2e;&#99;&#x6f;&#x6d;\">&#97;&#x72;&#x67;&#97;&#x6e;&#122;&#x68;&#x65;&#110;&#x67;&#x40;&#103;&#109;&#x61;&#105;&#108;&#x2e;&#99;&#x6f;&#x6d;</a><br>Date:   Thu Jun 18 20:21:51 2015 +0800<br>small modify<br>b.执行撤销操作<br><code>git reset --hard 931c4a4e7678b760faa6849e16c849d7c679ac9c</code><br><strong>注意：</strong>工 作区和暂存区的内容都会被重置到指定提交的时候，如果不加–hard则只移动HEAD的指针，不影响工作区和暂存区的内容。</p>\n</blockquote>\n</li>\n<li>push之后，如何回退：–待确认<br> 在步骤2的基础上，执行   git push origin HEAD –force</li>\n</ol>\n","site":{"data":{}},"excerpt":"<h1 id=\"Git\"><a href=\"#Git\" class=\"headerlink\" title=\"Git\"></a>Git</h1><ul>\n<li>win7+<br>在wind7下搭建git环境。需下载windows 版本的git工具。见附件。选择完整安装。安装目录 D:\\Program Files (x86)\\Git\\<br>并将该路径添加至PATH环境。之后在cmd 环境下就可以使用了。后续如何使用git 就参考git 使用手册了，跟linux环境下的使用方法类似。<br>在windows 下生成id_rsa ，免密码连接github。使用 进入D:\\Program Files (x86)\\Git\\git-bash.exe 命令，进入控制台窗口后，输入<br><code>ssh-keygen -t rsa -b 4096 -C &quot;zhoujinl@126.com&quot;</code><br>然后一直按回车默认，直至完成。<br>最后可以在C:\\Users\\zhoujl.ssh 目录下生成ssh 密钥和公钥。<br>然后到.ssh目录中，将生成的id_rsa.pub里面的内容，复制到github网站中的 SSH KEYs 设置中。<br>git 全局设置<blockquote>\n<p>git config –global user.name zhoujinl<br>git config –global user.email zhoujinl@126.com  </p>\n</blockquote>\n</li>\n</ul>\n<p>在公司内部，有时需要通过设置代理才能连接上外网。<br>设置代理<br><code>git config --global http.proxy http://user:pwd@server.com:port</code><br>取消代理<br><code>git config --global (or --system or --local)  --unset  http.proxy</code></p>\n<ul>\n<li>Unix<br>这里主要介绍linux版本。其余的类linux应该类似。以CentOs环境为例，通过yum命令即可快速安装。<br><code>yum install git</code></li>\n</ul>","more":"<h1 id=\"GitHub\"><a href=\"#GitHub\" class=\"headerlink\" title=\"GitHub\"></a>GitHub</h1><p>GitHub是一个以git作为版本控制器的代码托管社区。我们可以在上面创建项目，然后通过git来进行开发维护和协作。具体的使用方法，可参考网站：<a href=\"http://www.worldhello.net/gotgithub/index.html\" target=\"_blank\" rel=\"external\">http://www.worldhello.net/gotgithub/index.html</a> 有详细介绍。<br>PS:在设置完user.name和user.email之后，通过命令生成id_ras.pub，<br><code>ssh-keygen -t rsa -b 4096 -C &quot;zhoujinl@126.com&quot;</code><br>并且上传到github 官网中。然后通过ssh 方式，从github上面clone项目到本地。由于已经将id_ras.pub上传到到github中，因此可以免密码登陆下载。<br>验证是否创建成功，如出现以下结果则说明ssh免密码设置成功。</p>\n<blockquote>\n<p>$  ssh -T git@github.com<br>Hi zhoujinl! You’ve successfully authenticated, but GitHub does not provide shell access.</p>\n</blockquote>\n<p>然后即可从github上面clone项目下来<br><code>git clone git@github.com:zhoujinl/zhoujinl.github.io.git</code><br>注：Are you sure you want to continue connecting (yes/no)?  会要求你是否确认连接，输入yes</p>\n<p>另外也可以使用https的方式。从github上克隆项目下来，并且push的时候，要求每次都输入用户名密码<br><code>git clone https://github.com/zhoujinl/zhoujinl.github.io.git</code></p>\n<h1 id=\"Git使用技巧：\"><a href=\"#Git使用技巧：\" class=\"headerlink\" title=\"Git使用技巧：\"></a>Git使用技巧：</h1><blockquote>\n<p>git pull &lt;远程主机名&gt; &lt;远程分支名&gt;:&lt;本地分支名&gt;<br>比如，取回origin主机的next分支，与本地的master分支合并，需要写成下面这样。<br>git pull origin next:master<br>git push &lt;远程主机名&gt; &lt;本地分支名&gt;:&lt;远程分支名&gt;<br>本地的master分支推送到origin主机的master分支。如果后者不存在，则会被新建<br>git push origin master<br>注意，分支推送顺序的写法是&lt;来源地&gt;:&lt;目的地&gt;，所以git pull是&lt;远程分支&gt;:&lt;本地分支&gt;，而git push是&lt;本地分支&gt;:&lt;远程分支&gt;。</p>\n</blockquote>\n<ul>\n<li>Git撤销操作：<br>  HEAD 最近一个提交<br>  HEAD^ 上一次</li>\n</ul>\n<ol>\n<li>处于add之后，未commit之前<br>git reset HAED <file>  </file></li>\n<li>commit之后，还未push之前  <blockquote>\n<p>a.先看日志 git log ,找到要回退之前的版本的commit-id<br>λ git log<br>commit 992de5dde4f29aa35eb642b8fd55089838572c2d   ###此次错误提交的版本<br>Author: zhoujl <a href=\"&#109;&#x61;&#x69;&#x6c;&#x74;&#111;&#58;&#122;&#x68;&#x6f;&#x75;&#x6a;&#x6c;&#64;&#102;&#102;&#x63;&#115;&#x2e;&#x63;&#x6e;\">&#122;&#x68;&#x6f;&#x75;&#x6a;&#x6c;&#64;&#102;&#102;&#x63;&#115;&#x2e;&#x63;&#x6e;</a><br>Date:   Wed Aug 9 16:40:59 2017 +0800<br>Local change<br>commit 931c4a4e7678b760faa6849e16c849d7c679ac9c       ##上一个版本，应该使用该id<br>Author: arganzheng <a href=\"&#x6d;&#x61;&#105;&#108;&#x74;&#111;&#x3a;&#97;&#x72;&#x67;&#97;&#x6e;&#122;&#x68;&#x65;&#110;&#x67;&#x40;&#103;&#109;&#x61;&#105;&#108;&#x2e;&#99;&#x6f;&#x6d;\">&#97;&#x72;&#x67;&#97;&#x6e;&#122;&#x68;&#x65;&#110;&#x67;&#x40;&#103;&#109;&#x61;&#105;&#108;&#x2e;&#99;&#x6f;&#x6d;</a><br>Date:   Thu Jun 18 20:21:51 2015 +0800<br>small modify<br>b.执行撤销操作<br><code>git reset --hard 931c4a4e7678b760faa6849e16c849d7c679ac9c</code><br><strong>注意：</strong>工 作区和暂存区的内容都会被重置到指定提交的时候，如果不加–hard则只移动HEAD的指针，不影响工作区和暂存区的内容。</p>\n</blockquote>\n</li>\n<li>push之后，如何回退：–待确认<br> 在步骤2的基础上，执行   git push origin HEAD –force</li>\n</ol>"},{"title":"Hello World","date":"2013-06-15T14:10:30.000Z","_content":"Welcome to [Hexo](https://hexo.io/)! This is your very first post. Check [documentation](https://hexo.io/docs/) for more info. If you get any problems when using Hexo, you can find the answer in [troubleshooting](https://hexo.io/docs/troubleshooting.html) or you can ask me on [GitHub](https://github.com/hexojs/hexo/issues).\n\n## Quick Start\n\n### Create a new post\n\n``` bash\n$ hexo new \"My New Post\"\n```\n\nMore info: [Writing](https://hexo.io/docs/writing.html)\n\n### Run server\n\n``` bash\n$ hexo server\n```\n\nMore info: [Server](https://hexo.io/docs/server.html)\n\n### Generate static files\n\n``` bash\n$ hexo generate\n```\n\nMore info: [Generating](https://hexo.io/docs/generating.html)\n\n### Deploy to remote sites\n\n``` bash\n$ hexo deploy\n```\n\nMore info: [Deployment](https://hexo.io/docs/deployment.html)\n","source":"_posts/hello-world.md","raw":"---\ntitle: Hello World\ndate: 2013-06-15 22:10:30\ntag: Hexo\n---\nWelcome to [Hexo](https://hexo.io/)! This is your very first post. Check [documentation](https://hexo.io/docs/) for more info. If you get any problems when using Hexo, you can find the answer in [troubleshooting](https://hexo.io/docs/troubleshooting.html) or you can ask me on [GitHub](https://github.com/hexojs/hexo/issues).\n\n## Quick Start\n\n### Create a new post\n\n``` bash\n$ hexo new \"My New Post\"\n```\n\nMore info: [Writing](https://hexo.io/docs/writing.html)\n\n### Run server\n\n``` bash\n$ hexo server\n```\n\nMore info: [Server](https://hexo.io/docs/server.html)\n\n### Generate static files\n\n``` bash\n$ hexo generate\n```\n\nMore info: [Generating](https://hexo.io/docs/generating.html)\n\n### Deploy to remote sites\n\n``` bash\n$ hexo deploy\n```\n\nMore info: [Deployment](https://hexo.io/docs/deployment.html)\n","slug":"hello-world","published":1,"updated":"2018-04-27T04:11:03.345Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjhajczoj000ee4rwob7rym5n","content":"<p>Welcome to <a href=\"https://hexo.io/\" target=\"_blank\" rel=\"external\">Hexo</a>! This is your very first post. Check <a href=\"https://hexo.io/docs/\" target=\"_blank\" rel=\"external\">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href=\"https://hexo.io/docs/troubleshooting.html\" target=\"_blank\" rel=\"external\">troubleshooting</a> or you can ask me on <a href=\"https://github.com/hexojs/hexo/issues\" target=\"_blank\" rel=\"external\">GitHub</a>.</p>\n<h2 id=\"Quick-Start\"><a href=\"#Quick-Start\" class=\"headerlink\" title=\"Quick Start\"></a>Quick Start</h2><h3 id=\"Create-a-new-post\"><a href=\"#Create-a-new-post\" class=\"headerlink\" title=\"Create a new post\"></a>Create a new post</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ hexo new <span class=\"string\">\"My New Post\"</span></div></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/writing.html\" target=\"_blank\" rel=\"external\">Writing</a></p>\n<h3 id=\"Run-server\"><a href=\"#Run-server\" class=\"headerlink\" title=\"Run server\"></a>Run server</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ hexo server</div></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/server.html\" target=\"_blank\" rel=\"external\">Server</a></p>\n<h3 id=\"Generate-static-files\"><a href=\"#Generate-static-files\" class=\"headerlink\" title=\"Generate static files\"></a>Generate static files</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ hexo generate</div></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/generating.html\" target=\"_blank\" rel=\"external\">Generating</a></p>\n<h3 id=\"Deploy-to-remote-sites\"><a href=\"#Deploy-to-remote-sites\" class=\"headerlink\" title=\"Deploy to remote sites\"></a>Deploy to remote sites</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ hexo deploy</div></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/deployment.html\" target=\"_blank\" rel=\"external\">Deployment</a></p>\n","site":{"data":{}},"excerpt":"","more":"<p>Welcome to <a href=\"https://hexo.io/\" target=\"_blank\" rel=\"external\">Hexo</a>! This is your very first post. Check <a href=\"https://hexo.io/docs/\" target=\"_blank\" rel=\"external\">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href=\"https://hexo.io/docs/troubleshooting.html\" target=\"_blank\" rel=\"external\">troubleshooting</a> or you can ask me on <a href=\"https://github.com/hexojs/hexo/issues\" target=\"_blank\" rel=\"external\">GitHub</a>.</p>\n<h2 id=\"Quick-Start\"><a href=\"#Quick-Start\" class=\"headerlink\" title=\"Quick Start\"></a>Quick Start</h2><h3 id=\"Create-a-new-post\"><a href=\"#Create-a-new-post\" class=\"headerlink\" title=\"Create a new post\"></a>Create a new post</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ hexo new <span class=\"string\">\"My New Post\"</span></div></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/writing.html\" target=\"_blank\" rel=\"external\">Writing</a></p>\n<h3 id=\"Run-server\"><a href=\"#Run-server\" class=\"headerlink\" title=\"Run server\"></a>Run server</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ hexo server</div></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/server.html\" target=\"_blank\" rel=\"external\">Server</a></p>\n<h3 id=\"Generate-static-files\"><a href=\"#Generate-static-files\" class=\"headerlink\" title=\"Generate static files\"></a>Generate static files</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ hexo generate</div></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/generating.html\" target=\"_blank\" rel=\"external\">Generating</a></p>\n<h3 id=\"Deploy-to-remote-sites\"><a href=\"#Deploy-to-remote-sites\" class=\"headerlink\" title=\"Deploy to remote sites\"></a>Deploy to remote sites</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ hexo deploy</div></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/deployment.html\" target=\"_blank\" rel=\"external\">Deployment</a></p>\n"},{"title":"软件随想录-卷1（上）","date":"2015-06-20T03:18:18.000Z","_content":"joel Spolsky著  杨帆译   \n[Gitbook 电子书](https://wizardforcel.gitbooks.io/joel-on-software/content/index.html  \"软件随想录\")\n\n# 软件开发的五个世界\n了解你的世界 \n>1.盒装(Shrinkwrap)软件（产品软件）\n2.内部用的软件\n3.嵌入式软件\n4.游戏软件\n5.用后即丢的软件\n\n# 二元文化差异   \n| 文化类型  | 核心区别 | 区别举例1   | 区别举例1   | 区别举例1   |\n| :-------: | :----: | :---: |  :---: |  :---: |\n| unix文化 | 重视为程序员提供有用的程序 | 重视命令行 | 一项程序结束如果没有产生任何输出信息，就说明程序执行正确 | 文档应该写的简洁而完整，读者可以推导出未写出的结论并且信任自己的推导，一件事很少会讲两遍.|\n| windows文化    | 重视对非程序员提供有用的程序 | 重视图形化界面  | 程序执行后如果没有任何提示你就搞不清楚是因为出错了而没有输出还是没出错只是不输出。| 了解一般使用者一般不读文档，因此一件事情会多次提醒 |\n\n<!-- more -->\n\n# 千万不要做的事情\n读代码比写代码还要难\n>不要重写代码，不要重写代码，不要重写代码  \n因为旧代码里面，包含的某个你看似无用丑陋的代码，可能是前人花很长时间改正过来的BUG。\n请在保证单元测试无误的情况下，重构代码，而不是想着重头开始做一套。\n\n# 如何编写技术文档\n>1.要幽默，最简单的搞笑方法就是在没有必要写得具体的时候写得具体一点\n2.像编写用大脑执行的代码一样编写文档\n3.写的尽可能的简单\n4.重读并修改几遍\n5.尽量不要套用模版\n\n# Unicode和字符集知识 \nUnicode实际上只是一个规范，它规定每个字符独一无二的码点。\n>在Unicode中，一个字母映射为一个理论概念：码点。至于如何在内存或者硬盘中表示码点，\n就是另一件事情了（注:不同编码方式有不同的存储形式）。字母A（U+0645）是一个柏拉图的理想型（platnonic  ideal）。它漂浮在天上，有些抽象。\n而字符集编码呢，表示码点的是所谓的编码。例如常见的UTF-8,UTF-16等,都是Unicode家族的编码方式。  \n举例：字符串hello的Unicode表示为 U+0048 U+0065 U+006C U+006C U+006F,那么它用UTF-8编码的存储形式为 48 65 6C 6C 6F (注意！) 这和它在ASCII、ANSI以及\n其他的所有的OEM字符集中的表示都完全一致，而其他语言（如中文、日文等），则需要几个字节来存储一个码点。\n\n# 乔尔测试\n> 1.你们使用版本控制系统吗？\n2.有一键部署吗？\n3.有每日构建吗？\n4.有Bug数据库吗？\n5.你们是否在写新代码前保证Bug都修复完了？\n6.有最新的开发计划吗？\n7.有需求文档吗？\n8.程序员们有安静的工作环境吗？\n9.你们使用能买到的最好的工具吗？\n10.有测试团队吗？\n11.应聘者在面试时写代码吗？\n12.你们进行目标用户随机可用性测试吗？\n\n\n\n# 轻松掌握软件开发进度\n>1.使用excel\n2.保持简单\n3.每个功能点应该分成几个子任务\n4.只有程序员才能准确的预估时间\n5.细分任务，每个任务以小时为单位。\n6.记录原始和当前的时间估计，训练自己的时间估计准度。\n7.每天更新实际用时。\n8.把休假考虑在内\n9.把调试代码的时间也考虑在内\n10.把集成时间考虑在内\n11.预留缓冲时间\n12.不要压缩程序员的预估时间\n13.缩减功能以减少开发时间\n\n# 干扰射击  \n>微软频繁推出的历代数据库连接技术（ODBC,RDO,DAO,ADO,OLEDB,ADO.NET...），目的火力压制，迫使竞争者们用尽全部精力来跟上未然的节拍，移植现有的功能，从而没有时间研发新功能。因此必须抓紧时间，把开发的主动权掌握在自己的手里，二是必须每天进步。\n\n# 修复bug\n* 尽可能收集bug的所有信息\n* 衡量bug的成本与收益\n* 算出修复所有bug的价值  \n   \n# 错误报告\n* 建立bug数据库(小项目可以用记事本)\n* 每个好的错误报告都要包括以下三个步骤：\n1.重现的步骤\n2.期望看到的  \n3.实际看到的\n好的测试员会把重现的步骤缩减到最短。只有一开始测出问题的人才能关闭这个bug。\n要解决错误可以有很多种办法，解决理由有：不予修正，暂缓，无法重现，重复的问题，设计限制。\n","source":"_posts/joel-on-software-1-1.md","raw":"---\ntitle: 软件随想录-卷1（上） \ndate: 2015-06-20 11:18:18\ntags: 随笔\n---\njoel Spolsky著  杨帆译   \n[Gitbook 电子书](https://wizardforcel.gitbooks.io/joel-on-software/content/index.html  \"软件随想录\")\n\n# 软件开发的五个世界\n了解你的世界 \n>1.盒装(Shrinkwrap)软件（产品软件）\n2.内部用的软件\n3.嵌入式软件\n4.游戏软件\n5.用后即丢的软件\n\n# 二元文化差异   \n| 文化类型  | 核心区别 | 区别举例1   | 区别举例1   | 区别举例1   |\n| :-------: | :----: | :---: |  :---: |  :---: |\n| unix文化 | 重视为程序员提供有用的程序 | 重视命令行 | 一项程序结束如果没有产生任何输出信息，就说明程序执行正确 | 文档应该写的简洁而完整，读者可以推导出未写出的结论并且信任自己的推导，一件事很少会讲两遍.|\n| windows文化    | 重视对非程序员提供有用的程序 | 重视图形化界面  | 程序执行后如果没有任何提示你就搞不清楚是因为出错了而没有输出还是没出错只是不输出。| 了解一般使用者一般不读文档，因此一件事情会多次提醒 |\n\n<!-- more -->\n\n# 千万不要做的事情\n读代码比写代码还要难\n>不要重写代码，不要重写代码，不要重写代码  \n因为旧代码里面，包含的某个你看似无用丑陋的代码，可能是前人花很长时间改正过来的BUG。\n请在保证单元测试无误的情况下，重构代码，而不是想着重头开始做一套。\n\n# 如何编写技术文档\n>1.要幽默，最简单的搞笑方法就是在没有必要写得具体的时候写得具体一点\n2.像编写用大脑执行的代码一样编写文档\n3.写的尽可能的简单\n4.重读并修改几遍\n5.尽量不要套用模版\n\n# Unicode和字符集知识 \nUnicode实际上只是一个规范，它规定每个字符独一无二的码点。\n>在Unicode中，一个字母映射为一个理论概念：码点。至于如何在内存或者硬盘中表示码点，\n就是另一件事情了（注:不同编码方式有不同的存储形式）。字母A（U+0645）是一个柏拉图的理想型（platnonic  ideal）。它漂浮在天上，有些抽象。\n而字符集编码呢，表示码点的是所谓的编码。例如常见的UTF-8,UTF-16等,都是Unicode家族的编码方式。  \n举例：字符串hello的Unicode表示为 U+0048 U+0065 U+006C U+006C U+006F,那么它用UTF-8编码的存储形式为 48 65 6C 6C 6F (注意！) 这和它在ASCII、ANSI以及\n其他的所有的OEM字符集中的表示都完全一致，而其他语言（如中文、日文等），则需要几个字节来存储一个码点。\n\n# 乔尔测试\n> 1.你们使用版本控制系统吗？\n2.有一键部署吗？\n3.有每日构建吗？\n4.有Bug数据库吗？\n5.你们是否在写新代码前保证Bug都修复完了？\n6.有最新的开发计划吗？\n7.有需求文档吗？\n8.程序员们有安静的工作环境吗？\n9.你们使用能买到的最好的工具吗？\n10.有测试团队吗？\n11.应聘者在面试时写代码吗？\n12.你们进行目标用户随机可用性测试吗？\n\n\n\n# 轻松掌握软件开发进度\n>1.使用excel\n2.保持简单\n3.每个功能点应该分成几个子任务\n4.只有程序员才能准确的预估时间\n5.细分任务，每个任务以小时为单位。\n6.记录原始和当前的时间估计，训练自己的时间估计准度。\n7.每天更新实际用时。\n8.把休假考虑在内\n9.把调试代码的时间也考虑在内\n10.把集成时间考虑在内\n11.预留缓冲时间\n12.不要压缩程序员的预估时间\n13.缩减功能以减少开发时间\n\n# 干扰射击  \n>微软频繁推出的历代数据库连接技术（ODBC,RDO,DAO,ADO,OLEDB,ADO.NET...），目的火力压制，迫使竞争者们用尽全部精力来跟上未然的节拍，移植现有的功能，从而没有时间研发新功能。因此必须抓紧时间，把开发的主动权掌握在自己的手里，二是必须每天进步。\n\n# 修复bug\n* 尽可能收集bug的所有信息\n* 衡量bug的成本与收益\n* 算出修复所有bug的价值  \n   \n# 错误报告\n* 建立bug数据库(小项目可以用记事本)\n* 每个好的错误报告都要包括以下三个步骤：\n1.重现的步骤\n2.期望看到的  \n3.实际看到的\n好的测试员会把重现的步骤缩减到最短。只有一开始测出问题的人才能关闭这个bug。\n要解决错误可以有很多种办法，解决理由有：不予修正，暂缓，无法重现，重复的问题，设计限制。\n","slug":"joel-on-software-1-1","published":1,"updated":"2018-04-27T04:11:03.348Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjhajczon000ge4rw29ef8d1k","content":"<p>joel Spolsky著  杨帆译<br><a href=\"https://wizardforcel.gitbooks.io/joel-on-software/content/index.html\" title=\"软件随想录\" target=\"_blank\" rel=\"external\">Gitbook 电子书</a></p>\n<h1 id=\"软件开发的五个世界\"><a href=\"#软件开发的五个世界\" class=\"headerlink\" title=\"软件开发的五个世界\"></a>软件开发的五个世界</h1><p>了解你的世界 </p>\n<blockquote>\n<p>1.盒装(Shrinkwrap)软件（产品软件）<br>2.内部用的软件<br>3.嵌入式软件<br>4.游戏软件<br>5.用后即丢的软件</p>\n</blockquote>\n<h1 id=\"二元文化差异\"><a href=\"#二元文化差异\" class=\"headerlink\" title=\"二元文化差异\"></a>二元文化差异</h1><table>\n<thead>\n<tr>\n<th style=\"text-align:center\">文化类型</th>\n<th style=\"text-align:center\">核心区别</th>\n<th style=\"text-align:center\">区别举例1</th>\n<th style=\"text-align:center\">区别举例1</th>\n<th style=\"text-align:center\">区别举例1</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">unix文化</td>\n<td style=\"text-align:center\">重视为程序员提供有用的程序</td>\n<td style=\"text-align:center\">重视命令行</td>\n<td style=\"text-align:center\">一项程序结束如果没有产生任何输出信息，就说明程序执行正确</td>\n<td style=\"text-align:center\">文档应该写的简洁而完整，读者可以推导出未写出的结论并且信任自己的推导，一件事很少会讲两遍.</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">windows文化</td>\n<td style=\"text-align:center\">重视对非程序员提供有用的程序</td>\n<td style=\"text-align:center\">重视图形化界面</td>\n<td style=\"text-align:center\">程序执行后如果没有任何提示你就搞不清楚是因为出错了而没有输出还是没出错只是不输出。</td>\n<td style=\"text-align:center\">了解一般使用者一般不读文档，因此一件事情会多次提醒</td>\n</tr>\n</tbody>\n</table>\n<a id=\"more\"></a>\n<h1 id=\"千万不要做的事情\"><a href=\"#千万不要做的事情\" class=\"headerlink\" title=\"千万不要做的事情\"></a>千万不要做的事情</h1><p>读代码比写代码还要难</p>\n<blockquote>\n<p>不要重写代码，不要重写代码，不要重写代码<br>因为旧代码里面，包含的某个你看似无用丑陋的代码，可能是前人花很长时间改正过来的BUG。<br>请在保证单元测试无误的情况下，重构代码，而不是想着重头开始做一套。</p>\n</blockquote>\n<h1 id=\"如何编写技术文档\"><a href=\"#如何编写技术文档\" class=\"headerlink\" title=\"如何编写技术文档\"></a>如何编写技术文档</h1><blockquote>\n<p>1.要幽默，最简单的搞笑方法就是在没有必要写得具体的时候写得具体一点<br>2.像编写用大脑执行的代码一样编写文档<br>3.写的尽可能的简单<br>4.重读并修改几遍<br>5.尽量不要套用模版</p>\n</blockquote>\n<h1 id=\"Unicode和字符集知识\"><a href=\"#Unicode和字符集知识\" class=\"headerlink\" title=\"Unicode和字符集知识\"></a>Unicode和字符集知识</h1><p>Unicode实际上只是一个规范，它规定每个字符独一无二的码点。</p>\n<blockquote>\n<p>在Unicode中，一个字母映射为一个理论概念：码点。至于如何在内存或者硬盘中表示码点，<br>就是另一件事情了（注:不同编码方式有不同的存储形式）。字母A（U+0645）是一个柏拉图的理想型（platnonic  ideal）。它漂浮在天上，有些抽象。<br>而字符集编码呢，表示码点的是所谓的编码。例如常见的UTF-8,UTF-16等,都是Unicode家族的编码方式。<br>举例：字符串hello的Unicode表示为 U+0048 U+0065 U+006C U+006C U+006F,那么它用UTF-8编码的存储形式为 48 65 6C 6C 6F (注意！) 这和它在ASCII、ANSI以及<br>其他的所有的OEM字符集中的表示都完全一致，而其他语言（如中文、日文等），则需要几个字节来存储一个码点。</p>\n</blockquote>\n<h1 id=\"乔尔测试\"><a href=\"#乔尔测试\" class=\"headerlink\" title=\"乔尔测试\"></a>乔尔测试</h1><blockquote>\n<p>1.你们使用版本控制系统吗？<br>2.有一键部署吗？<br>3.有每日构建吗？<br>4.有Bug数据库吗？<br>5.你们是否在写新代码前保证Bug都修复完了？<br>6.有最新的开发计划吗？<br>7.有需求文档吗？<br>8.程序员们有安静的工作环境吗？<br>9.你们使用能买到的最好的工具吗？<br>10.有测试团队吗？<br>11.应聘者在面试时写代码吗？<br>12.你们进行目标用户随机可用性测试吗？</p>\n</blockquote>\n<h1 id=\"轻松掌握软件开发进度\"><a href=\"#轻松掌握软件开发进度\" class=\"headerlink\" title=\"轻松掌握软件开发进度\"></a>轻松掌握软件开发进度</h1><blockquote>\n<p>1.使用excel<br>2.保持简单<br>3.每个功能点应该分成几个子任务<br>4.只有程序员才能准确的预估时间<br>5.细分任务，每个任务以小时为单位。<br>6.记录原始和当前的时间估计，训练自己的时间估计准度。<br>7.每天更新实际用时。<br>8.把休假考虑在内<br>9.把调试代码的时间也考虑在内<br>10.把集成时间考虑在内<br>11.预留缓冲时间<br>12.不要压缩程序员的预估时间<br>13.缩减功能以减少开发时间</p>\n</blockquote>\n<h1 id=\"干扰射击\"><a href=\"#干扰射击\" class=\"headerlink\" title=\"干扰射击\"></a>干扰射击</h1><blockquote>\n<p>微软频繁推出的历代数据库连接技术（ODBC,RDO,DAO,ADO,OLEDB,ADO.NET…），目的火力压制，迫使竞争者们用尽全部精力来跟上未然的节拍，移植现有的功能，从而没有时间研发新功能。因此必须抓紧时间，把开发的主动权掌握在自己的手里，二是必须每天进步。</p>\n</blockquote>\n<h1 id=\"修复bug\"><a href=\"#修复bug\" class=\"headerlink\" title=\"修复bug\"></a>修复bug</h1><ul>\n<li>尽可能收集bug的所有信息</li>\n<li>衡量bug的成本与收益</li>\n<li>算出修复所有bug的价值  </li>\n</ul>\n<h1 id=\"错误报告\"><a href=\"#错误报告\" class=\"headerlink\" title=\"错误报告\"></a>错误报告</h1><ul>\n<li>建立bug数据库(小项目可以用记事本)</li>\n<li>每个好的错误报告都要包括以下三个步骤：<br>1.重现的步骤<br>2.期望看到的<br>3.实际看到的<br>好的测试员会把重现的步骤缩减到最短。只有一开始测出问题的人才能关闭这个bug。<br>要解决错误可以有很多种办法，解决理由有：不予修正，暂缓，无法重现，重复的问题，设计限制。</li>\n</ul>\n","site":{"data":{}},"excerpt":"<p>joel Spolsky著  杨帆译<br><a href=\"https://wizardforcel.gitbooks.io/joel-on-software/content/index.html\" title=\"软件随想录\" target=\"_blank\" rel=\"external\">Gitbook 电子书</a></p>\n<h1 id=\"软件开发的五个世界\"><a href=\"#软件开发的五个世界\" class=\"headerlink\" title=\"软件开发的五个世界\"></a>软件开发的五个世界</h1><p>了解你的世界 </p>\n<blockquote>\n<p>1.盒装(Shrinkwrap)软件（产品软件）<br>2.内部用的软件<br>3.嵌入式软件<br>4.游戏软件<br>5.用后即丢的软件</p>\n</blockquote>\n<h1 id=\"二元文化差异\"><a href=\"#二元文化差异\" class=\"headerlink\" title=\"二元文化差异\"></a>二元文化差异</h1><table>\n<thead>\n<tr>\n<th style=\"text-align:center\">文化类型</th>\n<th style=\"text-align:center\">核心区别</th>\n<th style=\"text-align:center\">区别举例1</th>\n<th style=\"text-align:center\">区别举例1</th>\n<th style=\"text-align:center\">区别举例1</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">unix文化</td>\n<td style=\"text-align:center\">重视为程序员提供有用的程序</td>\n<td style=\"text-align:center\">重视命令行</td>\n<td style=\"text-align:center\">一项程序结束如果没有产生任何输出信息，就说明程序执行正确</td>\n<td style=\"text-align:center\">文档应该写的简洁而完整，读者可以推导出未写出的结论并且信任自己的推导，一件事很少会讲两遍.</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">windows文化</td>\n<td style=\"text-align:center\">重视对非程序员提供有用的程序</td>\n<td style=\"text-align:center\">重视图形化界面</td>\n<td style=\"text-align:center\">程序执行后如果没有任何提示你就搞不清楚是因为出错了而没有输出还是没出错只是不输出。</td>\n<td style=\"text-align:center\">了解一般使用者一般不读文档，因此一件事情会多次提醒</td>\n</tr>\n</tbody>\n</table>","more":"<h1 id=\"千万不要做的事情\"><a href=\"#千万不要做的事情\" class=\"headerlink\" title=\"千万不要做的事情\"></a>千万不要做的事情</h1><p>读代码比写代码还要难</p>\n<blockquote>\n<p>不要重写代码，不要重写代码，不要重写代码<br>因为旧代码里面，包含的某个你看似无用丑陋的代码，可能是前人花很长时间改正过来的BUG。<br>请在保证单元测试无误的情况下，重构代码，而不是想着重头开始做一套。</p>\n</blockquote>\n<h1 id=\"如何编写技术文档\"><a href=\"#如何编写技术文档\" class=\"headerlink\" title=\"如何编写技术文档\"></a>如何编写技术文档</h1><blockquote>\n<p>1.要幽默，最简单的搞笑方法就是在没有必要写得具体的时候写得具体一点<br>2.像编写用大脑执行的代码一样编写文档<br>3.写的尽可能的简单<br>4.重读并修改几遍<br>5.尽量不要套用模版</p>\n</blockquote>\n<h1 id=\"Unicode和字符集知识\"><a href=\"#Unicode和字符集知识\" class=\"headerlink\" title=\"Unicode和字符集知识\"></a>Unicode和字符集知识</h1><p>Unicode实际上只是一个规范，它规定每个字符独一无二的码点。</p>\n<blockquote>\n<p>在Unicode中，一个字母映射为一个理论概念：码点。至于如何在内存或者硬盘中表示码点，<br>就是另一件事情了（注:不同编码方式有不同的存储形式）。字母A（U+0645）是一个柏拉图的理想型（platnonic  ideal）。它漂浮在天上，有些抽象。<br>而字符集编码呢，表示码点的是所谓的编码。例如常见的UTF-8,UTF-16等,都是Unicode家族的编码方式。<br>举例：字符串hello的Unicode表示为 U+0048 U+0065 U+006C U+006C U+006F,那么它用UTF-8编码的存储形式为 48 65 6C 6C 6F (注意！) 这和它在ASCII、ANSI以及<br>其他的所有的OEM字符集中的表示都完全一致，而其他语言（如中文、日文等），则需要几个字节来存储一个码点。</p>\n</blockquote>\n<h1 id=\"乔尔测试\"><a href=\"#乔尔测试\" class=\"headerlink\" title=\"乔尔测试\"></a>乔尔测试</h1><blockquote>\n<p>1.你们使用版本控制系统吗？<br>2.有一键部署吗？<br>3.有每日构建吗？<br>4.有Bug数据库吗？<br>5.你们是否在写新代码前保证Bug都修复完了？<br>6.有最新的开发计划吗？<br>7.有需求文档吗？<br>8.程序员们有安静的工作环境吗？<br>9.你们使用能买到的最好的工具吗？<br>10.有测试团队吗？<br>11.应聘者在面试时写代码吗？<br>12.你们进行目标用户随机可用性测试吗？</p>\n</blockquote>\n<h1 id=\"轻松掌握软件开发进度\"><a href=\"#轻松掌握软件开发进度\" class=\"headerlink\" title=\"轻松掌握软件开发进度\"></a>轻松掌握软件开发进度</h1><blockquote>\n<p>1.使用excel<br>2.保持简单<br>3.每个功能点应该分成几个子任务<br>4.只有程序员才能准确的预估时间<br>5.细分任务，每个任务以小时为单位。<br>6.记录原始和当前的时间估计，训练自己的时间估计准度。<br>7.每天更新实际用时。<br>8.把休假考虑在内<br>9.把调试代码的时间也考虑在内<br>10.把集成时间考虑在内<br>11.预留缓冲时间<br>12.不要压缩程序员的预估时间<br>13.缩减功能以减少开发时间</p>\n</blockquote>\n<h1 id=\"干扰射击\"><a href=\"#干扰射击\" class=\"headerlink\" title=\"干扰射击\"></a>干扰射击</h1><blockquote>\n<p>微软频繁推出的历代数据库连接技术（ODBC,RDO,DAO,ADO,OLEDB,ADO.NET…），目的火力压制，迫使竞争者们用尽全部精力来跟上未然的节拍，移植现有的功能，从而没有时间研发新功能。因此必须抓紧时间，把开发的主动权掌握在自己的手里，二是必须每天进步。</p>\n</blockquote>\n<h1 id=\"修复bug\"><a href=\"#修复bug\" class=\"headerlink\" title=\"修复bug\"></a>修复bug</h1><ul>\n<li>尽可能收集bug的所有信息</li>\n<li>衡量bug的成本与收益</li>\n<li>算出修复所有bug的价值  </li>\n</ul>\n<h1 id=\"错误报告\"><a href=\"#错误报告\" class=\"headerlink\" title=\"错误报告\"></a>错误报告</h1><ul>\n<li>建立bug数据库(小项目可以用记事本)</li>\n<li>每个好的错误报告都要包括以下三个步骤：<br>1.重现的步骤<br>2.期望看到的<br>3.实际看到的<br>好的测试员会把重现的步骤缩减到最短。只有一开始测出问题的人才能关闭这个bug。<br>要解决错误可以有很多种办法，解决理由有：不予修正，暂缓，无法重现，重复的问题，设计限制。</li>\n</ul>"},{"title":"软件随想录-卷1（下）","date":"2016-06-21T03:18:18.000Z","_content":"joel Spolsky著  杨帆译   \n[Gitbook 电子书](https://wizardforcel.gitbooks.io/joel-on-software/content/index.html  \"软件随想录\")\n\n# 开发人员的管理 \n* 面试游击指南\n面试官有时候会故意刁难你，看你能否坚持自己的正确的观点。好的面试者会坚持自己的观点，然后试图说服面试官。\n* 重金激励害多利少\n原因是绩效考核的不正确，负面评价对士气伤害很大，而正面评价对士气的激励也不如想象的那么好(会让他们觉得自己是为了拿到好成绩才好好工作的，就像巴普洛夫的狗)。\n大多数人都认为自己把事情做得很好(即使不是)。评价机制的不合理：比如某人是团队的粘合剂，总是能够在士气低落的时候激励大家，某人总喜欢研究新技术，别人有问题总要靠他解决，但是他写的代码量很少，这两种人可能得到的评价很低，但是不可否认他们的作用很大。\n绩效考评会使团队产生间隙。\n\n<!-- more -->\n\n# 冰山理论 \n程序员和非技术人员的思考语言不一样。客户不知道他们要什么，别再期望客户知道他们要什么。原型图上漂亮的接口只占10%的工作，而真正的90%的程序设计都是看不到的。\n>推论1：把使用接口的画面展示给非程序人员看时，如果这个接口很不好，对方会以为你整个程序也是很不好的。\n推论2：相反，如果这个接口很漂亮，对方会以为这个程序几乎已经完工。\n推论3：展示时唯一重要的就是外观。一定要让它美得冒泡\n推论4：**当公司规定不会编程的管理人员或客户要对项目\"签字验收\"的时候,提供多个版本的图形设计方案，供他们选择。微调页面的排版方式、视觉样式、字体等，把产品标识\n改大或改小一点，给他们一些决定无关痛痒事情的权力,让他们觉得自己的意见举足轻重。**\n推论5：做产品演示的时候，唯一起作用的就是产品截图，一定要让截图100%完美。\n\n如何绕过冰山：任何在幻灯片展示的产品都只是*一堆像素*而已。如果可以的话，在demo的用户界面中，把那些为完成的部分画成未完成的燕子。比如在工具栏图标\n对应的功能完成之前，把它画成潦草的手绘风格。在构建网络服务的时候，可以先不把为完成的功能展示在网站首页。这样随着开发的进展，人们就能看到首页的\n功能按键从3项逐渐增加到20项。**更重要的是**，要掌控人们对于开发进度的预期。用Excel文件的形式提供一份详细的开发进度规划表。\n\n# 漏洞抽象定律\n>可靠的tcp建立在不可靠的ip协议之上。   \n所有重大的抽象机制在某种程序上都是有漏洞的，所以我们有一天遇到了这些漏洞，不得不去学习它的底层实现。 \n比如二维数组的访问(内存分页)，sql语句查询的快慢(逻辑上等义的sql语句查询起来速度完全不一样)，c++的字符串处理。\n\n# 凡事没有看上去那么简单\n凡事没有看上去那么简单、尽量降低风险 => 先做好设计，再去实现\n\n# 普通程序员如何改善团队工作方式\n* 放手去做\n* 借助病毒营销的方式\n* 营造以防优秀的小天地\n* 干掉害群之马\n* 屏蔽干扰 \n* 提升自身对团队的价值\n\n\n# 企业发展战略\n关于创业的想法,需要时请再重点阅读，分清楚两种不同的公司发展道路。\n>亚马逊和本杰瑞\n先有鸡还是先有蛋\n让我回到过去\n膨件和二八法则\n开源软件经济学\n\n","source":"_posts/joel-on-software-1-2.md","raw":"---\ntitle: 软件随想录-卷1（下） \ndate: 2016-06-21 11:18:18\ntags: 随笔\n---\njoel Spolsky著  杨帆译   \n[Gitbook 电子书](https://wizardforcel.gitbooks.io/joel-on-software/content/index.html  \"软件随想录\")\n\n# 开发人员的管理 \n* 面试游击指南\n面试官有时候会故意刁难你，看你能否坚持自己的正确的观点。好的面试者会坚持自己的观点，然后试图说服面试官。\n* 重金激励害多利少\n原因是绩效考核的不正确，负面评价对士气伤害很大，而正面评价对士气的激励也不如想象的那么好(会让他们觉得自己是为了拿到好成绩才好好工作的，就像巴普洛夫的狗)。\n大多数人都认为自己把事情做得很好(即使不是)。评价机制的不合理：比如某人是团队的粘合剂，总是能够在士气低落的时候激励大家，某人总喜欢研究新技术，别人有问题总要靠他解决，但是他写的代码量很少，这两种人可能得到的评价很低，但是不可否认他们的作用很大。\n绩效考评会使团队产生间隙。\n\n<!-- more -->\n\n# 冰山理论 \n程序员和非技术人员的思考语言不一样。客户不知道他们要什么，别再期望客户知道他们要什么。原型图上漂亮的接口只占10%的工作，而真正的90%的程序设计都是看不到的。\n>推论1：把使用接口的画面展示给非程序人员看时，如果这个接口很不好，对方会以为你整个程序也是很不好的。\n推论2：相反，如果这个接口很漂亮，对方会以为这个程序几乎已经完工。\n推论3：展示时唯一重要的就是外观。一定要让它美得冒泡\n推论4：**当公司规定不会编程的管理人员或客户要对项目\"签字验收\"的时候,提供多个版本的图形设计方案，供他们选择。微调页面的排版方式、视觉样式、字体等，把产品标识\n改大或改小一点，给他们一些决定无关痛痒事情的权力,让他们觉得自己的意见举足轻重。**\n推论5：做产品演示的时候，唯一起作用的就是产品截图，一定要让截图100%完美。\n\n如何绕过冰山：任何在幻灯片展示的产品都只是*一堆像素*而已。如果可以的话，在demo的用户界面中，把那些为完成的部分画成未完成的燕子。比如在工具栏图标\n对应的功能完成之前，把它画成潦草的手绘风格。在构建网络服务的时候，可以先不把为完成的功能展示在网站首页。这样随着开发的进展，人们就能看到首页的\n功能按键从3项逐渐增加到20项。**更重要的是**，要掌控人们对于开发进度的预期。用Excel文件的形式提供一份详细的开发进度规划表。\n\n# 漏洞抽象定律\n>可靠的tcp建立在不可靠的ip协议之上。   \n所有重大的抽象机制在某种程序上都是有漏洞的，所以我们有一天遇到了这些漏洞，不得不去学习它的底层实现。 \n比如二维数组的访问(内存分页)，sql语句查询的快慢(逻辑上等义的sql语句查询起来速度完全不一样)，c++的字符串处理。\n\n# 凡事没有看上去那么简单\n凡事没有看上去那么简单、尽量降低风险 => 先做好设计，再去实现\n\n# 普通程序员如何改善团队工作方式\n* 放手去做\n* 借助病毒营销的方式\n* 营造以防优秀的小天地\n* 干掉害群之马\n* 屏蔽干扰 \n* 提升自身对团队的价值\n\n\n# 企业发展战略\n关于创业的想法,需要时请再重点阅读，分清楚两种不同的公司发展道路。\n>亚马逊和本杰瑞\n先有鸡还是先有蛋\n让我回到过去\n膨件和二八法则\n开源软件经济学\n\n","slug":"joel-on-software-1-2","published":1,"updated":"2018-04-27T04:11:03.352Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjhajczoq000je4rw5qye8mqf","content":"<p>joel Spolsky著  杨帆译<br><a href=\"https://wizardforcel.gitbooks.io/joel-on-software/content/index.html\" title=\"软件随想录\" target=\"_blank\" rel=\"external\">Gitbook 电子书</a></p>\n<h1 id=\"开发人员的管理\"><a href=\"#开发人员的管理\" class=\"headerlink\" title=\"开发人员的管理\"></a>开发人员的管理</h1><ul>\n<li>面试游击指南<br>面试官有时候会故意刁难你，看你能否坚持自己的正确的观点。好的面试者会坚持自己的观点，然后试图说服面试官。</li>\n<li>重金激励害多利少<br>原因是绩效考核的不正确，负面评价对士气伤害很大，而正面评价对士气的激励也不如想象的那么好(会让他们觉得自己是为了拿到好成绩才好好工作的，就像巴普洛夫的狗)。<br>大多数人都认为自己把事情做得很好(即使不是)。评价机制的不合理：比如某人是团队的粘合剂，总是能够在士气低落的时候激励大家，某人总喜欢研究新技术，别人有问题总要靠他解决，但是他写的代码量很少，这两种人可能得到的评价很低，但是不可否认他们的作用很大。<br>绩效考评会使团队产生间隙。</li>\n</ul>\n<a id=\"more\"></a>\n<h1 id=\"冰山理论\"><a href=\"#冰山理论\" class=\"headerlink\" title=\"冰山理论\"></a>冰山理论</h1><p>程序员和非技术人员的思考语言不一样。客户不知道他们要什么，别再期望客户知道他们要什么。原型图上漂亮的接口只占10%的工作，而真正的90%的程序设计都是看不到的。</p>\n<blockquote>\n<p>推论1：把使用接口的画面展示给非程序人员看时，如果这个接口很不好，对方会以为你整个程序也是很不好的。<br>推论2：相反，如果这个接口很漂亮，对方会以为这个程序几乎已经完工。<br>推论3：展示时唯一重要的就是外观。一定要让它美得冒泡<br>推论4：<strong>当公司规定不会编程的管理人员或客户要对项目”签字验收”的时候,提供多个版本的图形设计方案，供他们选择。微调页面的排版方式、视觉样式、字体等，把产品标识<br>改大或改小一点，给他们一些决定无关痛痒事情的权力,让他们觉得自己的意见举足轻重。</strong><br>推论5：做产品演示的时候，唯一起作用的就是产品截图，一定要让截图100%完美。</p>\n</blockquote>\n<p>如何绕过冰山：任何在幻灯片展示的产品都只是<em>一堆像素</em>而已。如果可以的话，在demo的用户界面中，把那些为完成的部分画成未完成的燕子。比如在工具栏图标<br>对应的功能完成之前，把它画成潦草的手绘风格。在构建网络服务的时候，可以先不把为完成的功能展示在网站首页。这样随着开发的进展，人们就能看到首页的<br>功能按键从3项逐渐增加到20项。<strong>更重要的是</strong>，要掌控人们对于开发进度的预期。用Excel文件的形式提供一份详细的开发进度规划表。</p>\n<h1 id=\"漏洞抽象定律\"><a href=\"#漏洞抽象定律\" class=\"headerlink\" title=\"漏洞抽象定律\"></a>漏洞抽象定律</h1><blockquote>\n<p>可靠的tcp建立在不可靠的ip协议之上。<br>所有重大的抽象机制在某种程序上都是有漏洞的，所以我们有一天遇到了这些漏洞，不得不去学习它的底层实现。<br>比如二维数组的访问(内存分页)，sql语句查询的快慢(逻辑上等义的sql语句查询起来速度完全不一样)，c++的字符串处理。</p>\n</blockquote>\n<h1 id=\"凡事没有看上去那么简单\"><a href=\"#凡事没有看上去那么简单\" class=\"headerlink\" title=\"凡事没有看上去那么简单\"></a>凡事没有看上去那么简单</h1><p>凡事没有看上去那么简单、尽量降低风险 =&gt; 先做好设计，再去实现</p>\n<h1 id=\"普通程序员如何改善团队工作方式\"><a href=\"#普通程序员如何改善团队工作方式\" class=\"headerlink\" title=\"普通程序员如何改善团队工作方式\"></a>普通程序员如何改善团队工作方式</h1><ul>\n<li>放手去做</li>\n<li>借助病毒营销的方式</li>\n<li>营造以防优秀的小天地</li>\n<li>干掉害群之马</li>\n<li>屏蔽干扰 </li>\n<li>提升自身对团队的价值</li>\n</ul>\n<h1 id=\"企业发展战略\"><a href=\"#企业发展战略\" class=\"headerlink\" title=\"企业发展战略\"></a>企业发展战略</h1><p>关于创业的想法,需要时请再重点阅读，分清楚两种不同的公司发展道路。</p>\n<blockquote>\n<p>亚马逊和本杰瑞<br>先有鸡还是先有蛋<br>让我回到过去<br>膨件和二八法则<br>开源软件经济学</p>\n</blockquote>\n","site":{"data":{}},"excerpt":"<p>joel Spolsky著  杨帆译<br><a href=\"https://wizardforcel.gitbooks.io/joel-on-software/content/index.html\" title=\"软件随想录\" target=\"_blank\" rel=\"external\">Gitbook 电子书</a></p>\n<h1 id=\"开发人员的管理\"><a href=\"#开发人员的管理\" class=\"headerlink\" title=\"开发人员的管理\"></a>开发人员的管理</h1><ul>\n<li>面试游击指南<br>面试官有时候会故意刁难你，看你能否坚持自己的正确的观点。好的面试者会坚持自己的观点，然后试图说服面试官。</li>\n<li>重金激励害多利少<br>原因是绩效考核的不正确，负面评价对士气伤害很大，而正面评价对士气的激励也不如想象的那么好(会让他们觉得自己是为了拿到好成绩才好好工作的，就像巴普洛夫的狗)。<br>大多数人都认为自己把事情做得很好(即使不是)。评价机制的不合理：比如某人是团队的粘合剂，总是能够在士气低落的时候激励大家，某人总喜欢研究新技术，别人有问题总要靠他解决，但是他写的代码量很少，这两种人可能得到的评价很低，但是不可否认他们的作用很大。<br>绩效考评会使团队产生间隙。</li>\n</ul>","more":"<h1 id=\"冰山理论\"><a href=\"#冰山理论\" class=\"headerlink\" title=\"冰山理论\"></a>冰山理论</h1><p>程序员和非技术人员的思考语言不一样。客户不知道他们要什么，别再期望客户知道他们要什么。原型图上漂亮的接口只占10%的工作，而真正的90%的程序设计都是看不到的。</p>\n<blockquote>\n<p>推论1：把使用接口的画面展示给非程序人员看时，如果这个接口很不好，对方会以为你整个程序也是很不好的。<br>推论2：相反，如果这个接口很漂亮，对方会以为这个程序几乎已经完工。<br>推论3：展示时唯一重要的就是外观。一定要让它美得冒泡<br>推论4：<strong>当公司规定不会编程的管理人员或客户要对项目”签字验收”的时候,提供多个版本的图形设计方案，供他们选择。微调页面的排版方式、视觉样式、字体等，把产品标识<br>改大或改小一点，给他们一些决定无关痛痒事情的权力,让他们觉得自己的意见举足轻重。</strong><br>推论5：做产品演示的时候，唯一起作用的就是产品截图，一定要让截图100%完美。</p>\n</blockquote>\n<p>如何绕过冰山：任何在幻灯片展示的产品都只是<em>一堆像素</em>而已。如果可以的话，在demo的用户界面中，把那些为完成的部分画成未完成的燕子。比如在工具栏图标<br>对应的功能完成之前，把它画成潦草的手绘风格。在构建网络服务的时候，可以先不把为完成的功能展示在网站首页。这样随着开发的进展，人们就能看到首页的<br>功能按键从3项逐渐增加到20项。<strong>更重要的是</strong>，要掌控人们对于开发进度的预期。用Excel文件的形式提供一份详细的开发进度规划表。</p>\n<h1 id=\"漏洞抽象定律\"><a href=\"#漏洞抽象定律\" class=\"headerlink\" title=\"漏洞抽象定律\"></a>漏洞抽象定律</h1><blockquote>\n<p>可靠的tcp建立在不可靠的ip协议之上。<br>所有重大的抽象机制在某种程序上都是有漏洞的，所以我们有一天遇到了这些漏洞，不得不去学习它的底层实现。<br>比如二维数组的访问(内存分页)，sql语句查询的快慢(逻辑上等义的sql语句查询起来速度完全不一样)，c++的字符串处理。</p>\n</blockquote>\n<h1 id=\"凡事没有看上去那么简单\"><a href=\"#凡事没有看上去那么简单\" class=\"headerlink\" title=\"凡事没有看上去那么简单\"></a>凡事没有看上去那么简单</h1><p>凡事没有看上去那么简单、尽量降低风险 =&gt; 先做好设计，再去实现</p>\n<h1 id=\"普通程序员如何改善团队工作方式\"><a href=\"#普通程序员如何改善团队工作方式\" class=\"headerlink\" title=\"普通程序员如何改善团队工作方式\"></a>普通程序员如何改善团队工作方式</h1><ul>\n<li>放手去做</li>\n<li>借助病毒营销的方式</li>\n<li>营造以防优秀的小天地</li>\n<li>干掉害群之马</li>\n<li>屏蔽干扰 </li>\n<li>提升自身对团队的价值</li>\n</ul>\n<h1 id=\"企业发展战略\"><a href=\"#企业发展战略\" class=\"headerlink\" title=\"企业发展战略\"></a>企业发展战略</h1><p>关于创业的想法,需要时请再重点阅读，分清楚两种不同的公司发展道路。</p>\n<blockquote>\n<p>亚马逊和本杰瑞<br>先有鸡还是先有蛋<br>让我回到过去<br>膨件和二八法则<br>开源软件经济学</p>\n</blockquote>"},{"title":"LAMP站点搭建过程分析","date":"2017-03-20T11:34:19.000Z","_content":"LAMP 即 LINUX + APACHE + MYSQL +PHP 搭建网站的四大组件。下面详细介绍其普通的安装方法和配置。\n\n# LINUX \n咳，操作系统安装，可以使用VirtualBox创建虚拟机即可。这个就不细说了。推荐使用Centos6+\n\n# Apache httpd  \n1. 安装\n`yum install httpd`\n2. 配置\n查找出Httpd相关配置文件 `rpm -ql httpd|grep '/etc'`\n根据上述命令，查询主要的配置文件有/etc/httpd/conf/httpd.conf。 logs是日志链接目录。modules是支持的模块库目录。run是进程pid文件。\n3. 可执行文件\n>/usr/sbin/apachectl  #这个就是 Apache 的主要执行档，这个执行档其实是 shell script 而已， 他可以主动的侦测系统上面的一些设定值，好让你启动 Apache 时更简单！\n/usr/sbin/httpd     #这个才是主要的 Apache 二进制执行文件啦！\n/usr/bin/htpasswd   #(Apache 密码保护) 在某些网页当你想要登入时你需要输入账号与密码对吧！那 Apache 本身就提供一个最基本的密码保护方式， 该密码的产生就是透过这个指令来达成的！相关的设定方式我们会在 WWW 进阶设定当中说明的。\n4. 网页数据\n/var/www 目录下，html是放置网页内容的目录，error是错误信息的内容，icons是自带的一些图标目录，cgi-bin是默认网页可执行程序放置的地方。\n\n<!-- more -->\n\n# MYSQL   \n1. 安装 \n`yum install mysql mysql-server`\n2. 配置 \n查找配置文件与Apache Httpd 类似的，这里就不赘述了。主要配置文件是/etc/my.cnf 通过查看该文件，可看到日志的信息等。\n`cat /etc/my.cnf `\n>[mysqld]\ndatadir=/var/lib/mysql\nsocket=/var/lib/mysql/mysql.sock\nuser=mysql\n#Disabling symbolic-links is recommended to prevent assorted security risks\nsymbolic-links=0\n[mysqld_safe]\nlog-error=/var/log/mysqld.log\npid-file=/var/run/mysqld/mysqld.pid\n\n# PHP \n1. 安装 \n`yum install php php-mysql`\n2. 配置\n/etc/php.ini\n>/etc/php.ini #就是 PHP 的主要配置文件，包括你的 PHP 能不能允许使用者上传档案？能不能允许某些低安全性的标志等等， 都在这个配置文件当中设定的啦！\n/usr/lib64/httpd/modules/libphp5.so # PHP 这个软件提供给 Apache 使用的模块！这也是我们能否Apache 网页上面设计 PHP 程序语言的最重要的咚咚！ 务必要存在才行！\n/etc/php.d/mysql.ini, /usr/lib64/php/modules/mysql.so #你的 PHP 是否可以支持 MySQL 接口呢？就看这两个东西啦！这两个咚咚是由php-mysql 软件提供的呢！\n/usr/bin/phpize, /usr/include/php/ #如果你未来想要安装类似 PHP 加速器以让浏览速度加快的话，那么这个档案与目录就得要存在， 否则加速器软件可无法编译成功喔！这两个数据也是php-devel 软件所提供的啦！\n\n>[root@jinqiu mysql]# rpm -ql php\n/etc/httpd/conf.d/php.conf\n/usr/lib64/httpd/modules/libphp5.so\n/var/lib/php/session\n/var/www/icons/php.gif  \n\n/etc/httpd/conf.d/php.conf 是与apache相关联的配置文件，不需要手动写入/etc/httpd/conf/httpd.conf文件中，httpd服务会自动到/etc/httpd/conf.d 目录下读取该文件。\n\n以上就是基本的LAMP的搭建过程。具体的httpd.conf要再去查阅相关文档。建议安装http-manual软件，提供操作手册,可查询。\n同时为支持其他脚本语言，也可安装mrtg画图软件，mod-perl ,mod-python,mod-ssl。\n\n# 初试网站搭建\n当上述四个步骤都操作完成之后，接下来就是验证功能咯。各位请看\n* 静态网页   \n\n在/var/www/html 目录下新建php文件，演示如下。并通过浏览器查询界面。\n>[root@jinqiu html]# pwd\n/var/www/html\n[root@jinqiu html]# ls\nhello  index.html  phpinfo.php\n[root@jinqiu html]# cat phpinfo.php \n<?php phpinfo(); ?>\n\n`<?php ... ?> `是嵌入在 HTML 档案内的 PHP 程序语法，在这两个标签内的就是 PHP 的程序代码。那么 phpinfo(); 就是 PHP 程序提供的一个函式库，这个函式库可 以显示出你 WWW 服务器内的相关服务信息， 包括主要的 Apache 信息与 PHP 信息等等  \n* 简单动态网页\n修改配置执行perl 脚本。\n>vi /etc/httpd/conf/httpd.conf\nAddHandler cgi-script .cgi .pl\n<Directory \"/var/www/html/cgi\">\n        Options +ExecCGI\n        AllowOverride None\n        Order allow,deny\n        Allow from all\n</Directory>\n\n\n在/var/www/html/cgi 目录下新建perl文件\n>[root@www ~]# mkdir /var/www/html/cgi  \n[root@www ~]# vim /var/www/html/cgi/helloworld.pl\n#!/usr/bin/perl\nprint \"Content-type: text/html\\r\\n\\r\\n\";\nprint \"Hello, World.\";\n[root@www ~]# chmod a+x /var/www/html/cgi/helloworld.pl  #赋权 非常重要\n\n\n\n# phpBB3 搭建论坛  \n首先到官网上下载zip安装包，和汉化包。然后将安装包解压到/var/www/html apache网站目录下，重命名解压后的文件夹名称phpBB3。然后通过浏览器url: http://localhost:80/phpBB3/install/index.php 安装界面进行安装。 \n在新建论坛之后，要记得对论坛赋权限。然后还有把网站下的install文件删除或者重命名。否则看不见论坛，处于关闭状态。\n\n# Wordpress 搭建个人博客\n首先到官网上下载zip安装包。与phpBB3一样，将其解压后的文件夹复制到/var/www/html/目录下，重命名为wordpress。然后到通过浏览器访问http://localhost:80/wordpress/ 访问，即可自动跳转至配置界面，进行数据库的配置，最后会在/wordpress目录下生成wp_config.php配置文件，然后进入安装界面。配置用户名密码即可完成安装。\nps:在配置过程中，如果出现没有权限生成wp_config.php。手动复制文件内容自己创建也可。我在安装过程中出现问题，就是配置完文件后，并且生成wp_config.php后，一直又重复回到配置界面。仿佛就像他没有检测到生成的wp_config.php文件一样，及时我重启httpd服务也是如此。后来我把/wordpress 目录下的文件全部删除，重新配置一把就可以了。\n","source":"_posts/lamp-build.md","raw":"---\ntitle: LAMP站点搭建过程分析\ndate: 2017-03-20 19:34:19\ntags: LAMP\ncategory: 技术 \n---\nLAMP 即 LINUX + APACHE + MYSQL +PHP 搭建网站的四大组件。下面详细介绍其普通的安装方法和配置。\n\n# LINUX \n咳，操作系统安装，可以使用VirtualBox创建虚拟机即可。这个就不细说了。推荐使用Centos6+\n\n# Apache httpd  \n1. 安装\n`yum install httpd`\n2. 配置\n查找出Httpd相关配置文件 `rpm -ql httpd|grep '/etc'`\n根据上述命令，查询主要的配置文件有/etc/httpd/conf/httpd.conf。 logs是日志链接目录。modules是支持的模块库目录。run是进程pid文件。\n3. 可执行文件\n>/usr/sbin/apachectl  #这个就是 Apache 的主要执行档，这个执行档其实是 shell script 而已， 他可以主动的侦测系统上面的一些设定值，好让你启动 Apache 时更简单！\n/usr/sbin/httpd     #这个才是主要的 Apache 二进制执行文件啦！\n/usr/bin/htpasswd   #(Apache 密码保护) 在某些网页当你想要登入时你需要输入账号与密码对吧！那 Apache 本身就提供一个最基本的密码保护方式， 该密码的产生就是透过这个指令来达成的！相关的设定方式我们会在 WWW 进阶设定当中说明的。\n4. 网页数据\n/var/www 目录下，html是放置网页内容的目录，error是错误信息的内容，icons是自带的一些图标目录，cgi-bin是默认网页可执行程序放置的地方。\n\n<!-- more -->\n\n# MYSQL   \n1. 安装 \n`yum install mysql mysql-server`\n2. 配置 \n查找配置文件与Apache Httpd 类似的，这里就不赘述了。主要配置文件是/etc/my.cnf 通过查看该文件，可看到日志的信息等。\n`cat /etc/my.cnf `\n>[mysqld]\ndatadir=/var/lib/mysql\nsocket=/var/lib/mysql/mysql.sock\nuser=mysql\n#Disabling symbolic-links is recommended to prevent assorted security risks\nsymbolic-links=0\n[mysqld_safe]\nlog-error=/var/log/mysqld.log\npid-file=/var/run/mysqld/mysqld.pid\n\n# PHP \n1. 安装 \n`yum install php php-mysql`\n2. 配置\n/etc/php.ini\n>/etc/php.ini #就是 PHP 的主要配置文件，包括你的 PHP 能不能允许使用者上传档案？能不能允许某些低安全性的标志等等， 都在这个配置文件当中设定的啦！\n/usr/lib64/httpd/modules/libphp5.so # PHP 这个软件提供给 Apache 使用的模块！这也是我们能否Apache 网页上面设计 PHP 程序语言的最重要的咚咚！ 务必要存在才行！\n/etc/php.d/mysql.ini, /usr/lib64/php/modules/mysql.so #你的 PHP 是否可以支持 MySQL 接口呢？就看这两个东西啦！这两个咚咚是由php-mysql 软件提供的呢！\n/usr/bin/phpize, /usr/include/php/ #如果你未来想要安装类似 PHP 加速器以让浏览速度加快的话，那么这个档案与目录就得要存在， 否则加速器软件可无法编译成功喔！这两个数据也是php-devel 软件所提供的啦！\n\n>[root@jinqiu mysql]# rpm -ql php\n/etc/httpd/conf.d/php.conf\n/usr/lib64/httpd/modules/libphp5.so\n/var/lib/php/session\n/var/www/icons/php.gif  \n\n/etc/httpd/conf.d/php.conf 是与apache相关联的配置文件，不需要手动写入/etc/httpd/conf/httpd.conf文件中，httpd服务会自动到/etc/httpd/conf.d 目录下读取该文件。\n\n以上就是基本的LAMP的搭建过程。具体的httpd.conf要再去查阅相关文档。建议安装http-manual软件，提供操作手册,可查询。\n同时为支持其他脚本语言，也可安装mrtg画图软件，mod-perl ,mod-python,mod-ssl。\n\n# 初试网站搭建\n当上述四个步骤都操作完成之后，接下来就是验证功能咯。各位请看\n* 静态网页   \n\n在/var/www/html 目录下新建php文件，演示如下。并通过浏览器查询界面。\n>[root@jinqiu html]# pwd\n/var/www/html\n[root@jinqiu html]# ls\nhello  index.html  phpinfo.php\n[root@jinqiu html]# cat phpinfo.php \n<?php phpinfo(); ?>\n\n`<?php ... ?> `是嵌入在 HTML 档案内的 PHP 程序语法，在这两个标签内的就是 PHP 的程序代码。那么 phpinfo(); 就是 PHP 程序提供的一个函式库，这个函式库可 以显示出你 WWW 服务器内的相关服务信息， 包括主要的 Apache 信息与 PHP 信息等等  \n* 简单动态网页\n修改配置执行perl 脚本。\n>vi /etc/httpd/conf/httpd.conf\nAddHandler cgi-script .cgi .pl\n<Directory \"/var/www/html/cgi\">\n        Options +ExecCGI\n        AllowOverride None\n        Order allow,deny\n        Allow from all\n</Directory>\n\n\n在/var/www/html/cgi 目录下新建perl文件\n>[root@www ~]# mkdir /var/www/html/cgi  \n[root@www ~]# vim /var/www/html/cgi/helloworld.pl\n#!/usr/bin/perl\nprint \"Content-type: text/html\\r\\n\\r\\n\";\nprint \"Hello, World.\";\n[root@www ~]# chmod a+x /var/www/html/cgi/helloworld.pl  #赋权 非常重要\n\n\n\n# phpBB3 搭建论坛  \n首先到官网上下载zip安装包，和汉化包。然后将安装包解压到/var/www/html apache网站目录下，重命名解压后的文件夹名称phpBB3。然后通过浏览器url: http://localhost:80/phpBB3/install/index.php 安装界面进行安装。 \n在新建论坛之后，要记得对论坛赋权限。然后还有把网站下的install文件删除或者重命名。否则看不见论坛，处于关闭状态。\n\n# Wordpress 搭建个人博客\n首先到官网上下载zip安装包。与phpBB3一样，将其解压后的文件夹复制到/var/www/html/目录下，重命名为wordpress。然后到通过浏览器访问http://localhost:80/wordpress/ 访问，即可自动跳转至配置界面，进行数据库的配置，最后会在/wordpress目录下生成wp_config.php配置文件，然后进入安装界面。配置用户名密码即可完成安装。\nps:在配置过程中，如果出现没有权限生成wp_config.php。手动复制文件内容自己创建也可。我在安装过程中出现问题，就是配置完文件后，并且生成wp_config.php后，一直又重复回到配置界面。仿佛就像他没有检测到生成的wp_config.php文件一样，及时我重启httpd服务也是如此。后来我把/wordpress 目录下的文件全部删除，重新配置一把就可以了。\n","slug":"lamp-build","published":1,"updated":"2018-04-27T04:11:03.356Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjhajczos000me4rwfi9ljy1r","content":"<p>LAMP 即 LINUX + APACHE + MYSQL +PHP 搭建网站的四大组件。下面详细介绍其普通的安装方法和配置。</p>\n<h1 id=\"LINUX\"><a href=\"#LINUX\" class=\"headerlink\" title=\"LINUX\"></a>LINUX</h1><p>咳，操作系统安装，可以使用VirtualBox创建虚拟机即可。这个就不细说了。推荐使用Centos6+</p>\n<h1 id=\"Apache-httpd\"><a href=\"#Apache-httpd\" class=\"headerlink\" title=\"Apache httpd\"></a>Apache httpd</h1><ol>\n<li>安装<br><code>yum install httpd</code></li>\n<li>配置<br>查找出Httpd相关配置文件 <code>rpm -ql httpd|grep &#39;/etc&#39;</code><br>根据上述命令，查询主要的配置文件有/etc/httpd/conf/httpd.conf。 logs是日志链接目录。modules是支持的模块库目录。run是进程pid文件。</li>\n<li>可执行文件<blockquote>\n<p>/usr/sbin/apachectl  #这个就是 Apache 的主要执行档，这个执行档其实是 shell script 而已， 他可以主动的侦测系统上面的一些设定值，好让你启动 Apache 时更简单！<br>/usr/sbin/httpd     #这个才是主要的 Apache 二进制执行文件啦！<br>/usr/bin/htpasswd   #(Apache 密码保护) 在某些网页当你想要登入时你需要输入账号与密码对吧！那 Apache 本身就提供一个最基本的密码保护方式， 该密码的产生就是透过这个指令来达成的！相关的设定方式我们会在 WWW 进阶设定当中说明的。</p>\n</blockquote>\n</li>\n<li>网页数据<br>/var/www 目录下，html是放置网页内容的目录，error是错误信息的内容，icons是自带的一些图标目录，cgi-bin是默认网页可执行程序放置的地方。</li>\n</ol>\n<a id=\"more\"></a>\n<h1 id=\"MYSQL\"><a href=\"#MYSQL\" class=\"headerlink\" title=\"MYSQL\"></a>MYSQL</h1><ol>\n<li>安装<br><code>yum install mysql mysql-server</code></li>\n<li>配置<br>查找配置文件与Apache Httpd 类似的，这里就不赘述了。主要配置文件是/etc/my.cnf 通过查看该文件，可看到日志的信息等。<br><code>cat /etc/my.cnf</code><blockquote>\n<p>[mysqld]<br>datadir=/var/lib/mysql<br>socket=/var/lib/mysql/mysql.sock<br>user=mysql<br>#Disabling symbolic-links is recommended to prevent assorted security risks<br>symbolic-links=0<br>[mysqld_safe]<br>log-error=/var/log/mysqld.log<br>pid-file=/var/run/mysqld/mysqld.pid</p>\n</blockquote>\n</li>\n</ol>\n<h1 id=\"PHP\"><a href=\"#PHP\" class=\"headerlink\" title=\"PHP\"></a>PHP</h1><ol>\n<li>安装<br><code>yum install php php-mysql</code></li>\n<li>配置<br>/etc/php.ini<blockquote>\n<p>/etc/php.ini #就是 PHP 的主要配置文件，包括你的 PHP 能不能允许使用者上传档案？能不能允许某些低安全性的标志等等， 都在这个配置文件当中设定的啦！<br>/usr/lib64/httpd/modules/libphp5.so # PHP 这个软件提供给 Apache 使用的模块！这也是我们能否Apache 网页上面设计 PHP 程序语言的最重要的咚咚！ 务必要存在才行！<br>/etc/php.d/mysql.ini, /usr/lib64/php/modules/mysql.so #你的 PHP 是否可以支持 MySQL 接口呢？就看这两个东西啦！这两个咚咚是由php-mysql 软件提供的呢！<br>/usr/bin/phpize, /usr/include/php/ #如果你未来想要安装类似 PHP 加速器以让浏览速度加快的话，那么这个档案与目录就得要存在， 否则加速器软件可无法编译成功喔！这两个数据也是php-devel 软件所提供的啦！</p>\n</blockquote>\n</li>\n</ol>\n<blockquote>\n<p>[root@jinqiu mysql]# rpm -ql php<br>/etc/httpd/conf.d/php.conf<br>/usr/lib64/httpd/modules/libphp5.so<br>/var/lib/php/session<br>/var/www/icons/php.gif  </p>\n</blockquote>\n<p>/etc/httpd/conf.d/php.conf 是与apache相关联的配置文件，不需要手动写入/etc/httpd/conf/httpd.conf文件中，httpd服务会自动到/etc/httpd/conf.d 目录下读取该文件。</p>\n<p>以上就是基本的LAMP的搭建过程。具体的httpd.conf要再去查阅相关文档。建议安装http-manual软件，提供操作手册,可查询。<br>同时为支持其他脚本语言，也可安装mrtg画图软件，mod-perl ,mod-python,mod-ssl。</p>\n<h1 id=\"初试网站搭建\"><a href=\"#初试网站搭建\" class=\"headerlink\" title=\"初试网站搭建\"></a>初试网站搭建</h1><p>当上述四个步骤都操作完成之后，接下来就是验证功能咯。各位请看</p>\n<ul>\n<li>静态网页   </li>\n</ul>\n<p>在/var/www/html 目录下新建php文件，演示如下。并通过浏览器查询界面。</p>\n<blockquote>\n<p>[root@jinqiu html]# pwd<br>/var/www/html<br>[root@jinqiu html]# ls<br>hello  index.html  phpinfo.php<br>[root@jinqiu html]# cat phpinfo.php<br>&lt;?php phpinfo(); ?&gt;</p>\n</blockquote>\n<p><code>&lt;?php ... ?&gt;</code>是嵌入在 HTML 档案内的 PHP 程序语法，在这两个标签内的就是 PHP 的程序代码。那么 phpinfo(); 就是 PHP 程序提供的一个函式库，这个函式库可 以显示出你 WWW 服务器内的相关服务信息， 包括主要的 Apache 信息与 PHP 信息等等  </p>\n<ul>\n<li>简单动态网页<br>修改配置执行perl 脚本。<blockquote>\n<p>vi /etc/httpd/conf/httpd.conf<br>AddHandler cgi-script .cgi .pl</p>\n<directory \"=\"\" var=\"\" www=\"\" html=\"\" cgi\"=\"\"><br>      Options +ExecCGI<br>      AllowOverride None<br>      Order allow,deny<br>      Allow from all<br></directory>\n\n\n</blockquote>\n</li>\n</ul>\n<p>在/var/www/html/cgi 目录下新建perl文件</p>\n<blockquote>\n<p>[root@www ~]# mkdir /var/www/html/cgi<br>[root@www ~]# vim /var/www/html/cgi/helloworld.pl</p>\n<p>#!/usr/bin/perl<br>print “Content-type: text/html\\r\\n\\r\\n”;<br>print “Hello, World.”;<br>[root@www ~]# chmod a+x /var/www/html/cgi/helloworld.pl  #赋权 非常重要</p>\n</blockquote>\n<h1 id=\"phpBB3-搭建论坛\"><a href=\"#phpBB3-搭建论坛\" class=\"headerlink\" title=\"phpBB3 搭建论坛\"></a>phpBB3 搭建论坛</h1><p>首先到官网上下载zip安装包，和汉化包。然后将安装包解压到/var/www/html apache网站目录下，重命名解压后的文件夹名称phpBB3。然后通过浏览器url: <a href=\"http://localhost:80/phpBB3/install/index.php\" target=\"_blank\" rel=\"external\">http://localhost:80/phpBB3/install/index.php</a> 安装界面进行安装。<br>在新建论坛之后，要记得对论坛赋权限。然后还有把网站下的install文件删除或者重命名。否则看不见论坛，处于关闭状态。</p>\n<h1 id=\"Wordpress-搭建个人博客\"><a href=\"#Wordpress-搭建个人博客\" class=\"headerlink\" title=\"Wordpress 搭建个人博客\"></a>Wordpress 搭建个人博客</h1><p>首先到官网上下载zip安装包。与phpBB3一样，将其解压后的文件夹复制到/var/www/html/目录下，重命名为wordpress。然后到通过浏览器访问<a href=\"http://localhost:80/wordpress/\" target=\"_blank\" rel=\"external\">http://localhost:80/wordpress/</a> 访问，即可自动跳转至配置界面，进行数据库的配置，最后会在/wordpress目录下生成wp_config.php配置文件，然后进入安装界面。配置用户名密码即可完成安装。<br>ps:在配置过程中，如果出现没有权限生成wp_config.php。手动复制文件内容自己创建也可。我在安装过程中出现问题，就是配置完文件后，并且生成wp_config.php后，一直又重复回到配置界面。仿佛就像他没有检测到生成的wp_config.php文件一样，及时我重启httpd服务也是如此。后来我把/wordpress 目录下的文件全部删除，重新配置一把就可以了。</p>\n","site":{"data":{}},"excerpt":"<p>LAMP 即 LINUX + APACHE + MYSQL +PHP 搭建网站的四大组件。下面详细介绍其普通的安装方法和配置。</p>\n<h1 id=\"LINUX\"><a href=\"#LINUX\" class=\"headerlink\" title=\"LINUX\"></a>LINUX</h1><p>咳，操作系统安装，可以使用VirtualBox创建虚拟机即可。这个就不细说了。推荐使用Centos6+</p>\n<h1 id=\"Apache-httpd\"><a href=\"#Apache-httpd\" class=\"headerlink\" title=\"Apache httpd\"></a>Apache httpd</h1><ol>\n<li>安装<br><code>yum install httpd</code></li>\n<li>配置<br>查找出Httpd相关配置文件 <code>rpm -ql httpd|grep &#39;/etc&#39;</code><br>根据上述命令，查询主要的配置文件有/etc/httpd/conf/httpd.conf。 logs是日志链接目录。modules是支持的模块库目录。run是进程pid文件。</li>\n<li>可执行文件<blockquote>\n<p>/usr/sbin/apachectl  #这个就是 Apache 的主要执行档，这个执行档其实是 shell script 而已， 他可以主动的侦测系统上面的一些设定值，好让你启动 Apache 时更简单！<br>/usr/sbin/httpd     #这个才是主要的 Apache 二进制执行文件啦！<br>/usr/bin/htpasswd   #(Apache 密码保护) 在某些网页当你想要登入时你需要输入账号与密码对吧！那 Apache 本身就提供一个最基本的密码保护方式， 该密码的产生就是透过这个指令来达成的！相关的设定方式我们会在 WWW 进阶设定当中说明的。</p>\n</blockquote>\n</li>\n<li>网页数据<br>/var/www 目录下，html是放置网页内容的目录，error是错误信息的内容，icons是自带的一些图标目录，cgi-bin是默认网页可执行程序放置的地方。</li>\n</ol>","more":"<h1 id=\"MYSQL\"><a href=\"#MYSQL\" class=\"headerlink\" title=\"MYSQL\"></a>MYSQL</h1><ol>\n<li>安装<br><code>yum install mysql mysql-server</code></li>\n<li>配置<br>查找配置文件与Apache Httpd 类似的，这里就不赘述了。主要配置文件是/etc/my.cnf 通过查看该文件，可看到日志的信息等。<br><code>cat /etc/my.cnf</code><blockquote>\n<p>[mysqld]<br>datadir=/var/lib/mysql<br>socket=/var/lib/mysql/mysql.sock<br>user=mysql<br>#Disabling symbolic-links is recommended to prevent assorted security risks<br>symbolic-links=0<br>[mysqld_safe]<br>log-error=/var/log/mysqld.log<br>pid-file=/var/run/mysqld/mysqld.pid</p>\n</blockquote>\n</li>\n</ol>\n<h1 id=\"PHP\"><a href=\"#PHP\" class=\"headerlink\" title=\"PHP\"></a>PHP</h1><ol>\n<li>安装<br><code>yum install php php-mysql</code></li>\n<li>配置<br>/etc/php.ini<blockquote>\n<p>/etc/php.ini #就是 PHP 的主要配置文件，包括你的 PHP 能不能允许使用者上传档案？能不能允许某些低安全性的标志等等， 都在这个配置文件当中设定的啦！<br>/usr/lib64/httpd/modules/libphp5.so # PHP 这个软件提供给 Apache 使用的模块！这也是我们能否Apache 网页上面设计 PHP 程序语言的最重要的咚咚！ 务必要存在才行！<br>/etc/php.d/mysql.ini, /usr/lib64/php/modules/mysql.so #你的 PHP 是否可以支持 MySQL 接口呢？就看这两个东西啦！这两个咚咚是由php-mysql 软件提供的呢！<br>/usr/bin/phpize, /usr/include/php/ #如果你未来想要安装类似 PHP 加速器以让浏览速度加快的话，那么这个档案与目录就得要存在， 否则加速器软件可无法编译成功喔！这两个数据也是php-devel 软件所提供的啦！</p>\n</blockquote>\n</li>\n</ol>\n<blockquote>\n<p>[root@jinqiu mysql]# rpm -ql php<br>/etc/httpd/conf.d/php.conf<br>/usr/lib64/httpd/modules/libphp5.so<br>/var/lib/php/session<br>/var/www/icons/php.gif  </p>\n</blockquote>\n<p>/etc/httpd/conf.d/php.conf 是与apache相关联的配置文件，不需要手动写入/etc/httpd/conf/httpd.conf文件中，httpd服务会自动到/etc/httpd/conf.d 目录下读取该文件。</p>\n<p>以上就是基本的LAMP的搭建过程。具体的httpd.conf要再去查阅相关文档。建议安装http-manual软件，提供操作手册,可查询。<br>同时为支持其他脚本语言，也可安装mrtg画图软件，mod-perl ,mod-python,mod-ssl。</p>\n<h1 id=\"初试网站搭建\"><a href=\"#初试网站搭建\" class=\"headerlink\" title=\"初试网站搭建\"></a>初试网站搭建</h1><p>当上述四个步骤都操作完成之后，接下来就是验证功能咯。各位请看</p>\n<ul>\n<li>静态网页   </li>\n</ul>\n<p>在/var/www/html 目录下新建php文件，演示如下。并通过浏览器查询界面。</p>\n<blockquote>\n<p>[root@jinqiu html]# pwd<br>/var/www/html<br>[root@jinqiu html]# ls<br>hello  index.html  phpinfo.php<br>[root@jinqiu html]# cat phpinfo.php<br>&lt;?php phpinfo(); ?&gt;</p>\n</blockquote>\n<p><code>&lt;?php ... ?&gt;</code>是嵌入在 HTML 档案内的 PHP 程序语法，在这两个标签内的就是 PHP 的程序代码。那么 phpinfo(); 就是 PHP 程序提供的一个函式库，这个函式库可 以显示出你 WWW 服务器内的相关服务信息， 包括主要的 Apache 信息与 PHP 信息等等  </p>\n<ul>\n<li>简单动态网页<br>修改配置执行perl 脚本。<blockquote>\n<p>vi /etc/httpd/conf/httpd.conf<br>AddHandler cgi-script .cgi .pl</p>\n<directory \"=\"\" var=\"\" www=\"\" html=\"\" cgi\"=\"\"><br>      Options +ExecCGI<br>      AllowOverride None<br>      Order allow,deny<br>      Allow from all<br></directory>\n\n\n</blockquote>\n</li>\n</ul>\n<p>在/var/www/html/cgi 目录下新建perl文件</p>\n<blockquote>\n<p>[root@www ~]# mkdir /var/www/html/cgi<br>[root@www ~]# vim /var/www/html/cgi/helloworld.pl</p>\n<p>#!/usr/bin/perl<br>print “Content-type: text/html\\r\\n\\r\\n”;<br>print “Hello, World.”;<br>[root@www ~]# chmod a+x /var/www/html/cgi/helloworld.pl  #赋权 非常重要</p>\n</blockquote>\n<h1 id=\"phpBB3-搭建论坛\"><a href=\"#phpBB3-搭建论坛\" class=\"headerlink\" title=\"phpBB3 搭建论坛\"></a>phpBB3 搭建论坛</h1><p>首先到官网上下载zip安装包，和汉化包。然后将安装包解压到/var/www/html apache网站目录下，重命名解压后的文件夹名称phpBB3。然后通过浏览器url: <a href=\"http://localhost:80/phpBB3/install/index.php\" target=\"_blank\" rel=\"external\">http://localhost:80/phpBB3/install/index.php</a> 安装界面进行安装。<br>在新建论坛之后，要记得对论坛赋权限。然后还有把网站下的install文件删除或者重命名。否则看不见论坛，处于关闭状态。</p>\n<h1 id=\"Wordpress-搭建个人博客\"><a href=\"#Wordpress-搭建个人博客\" class=\"headerlink\" title=\"Wordpress 搭建个人博客\"></a>Wordpress 搭建个人博客</h1><p>首先到官网上下载zip安装包。与phpBB3一样，将其解压后的文件夹复制到/var/www/html/目录下，重命名为wordpress。然后到通过浏览器访问<a href=\"http://localhost:80/wordpress/\" target=\"_blank\" rel=\"external\">http://localhost:80/wordpress/</a> 访问，即可自动跳转至配置界面，进行数据库的配置，最后会在/wordpress目录下生成wp_config.php配置文件，然后进入安装界面。配置用户名密码即可完成安装。<br>ps:在配置过程中，如果出现没有权限生成wp_config.php。手动复制文件内容自己创建也可。我在安装过程中出现问题，就是配置完文件后，并且生成wp_config.php后，一直又重复回到配置界面。仿佛就像他没有检测到生成的wp_config.php文件一样，及时我重启httpd服务也是如此。后来我把/wordpress 目录下的文件全部删除，重新配置一把就可以了。</p>"},{"title":"软件随想录-卷2","date":"2017-02-20T09:36:29.000Z","_content":"\njoel Spolsky著 阮一峰译  \n[Gitbook 电子书](https://wizardforcel.gitbooks.io/joel-on-software/content/index.html  \"软件随想录\")\n\n# 寻找优秀程序员的三种办法\n>社招(简历)不靠谱，员工推荐不太靠谱\n走出去，参加开发者大会\n找实习生。\n创造自己的知名社区网站。\n\n# 写给程序员的建议\n任何时刻不要想着推倒重来，重构而不是重新做一遍\n不添加任何新功能\n无论任何时刻向源码库添加代码，都要保证程序能够正常运行\n所要做的只是一些合乎逻辑的变换，几乎都是机械性的，而且能够立刻确定不会改变代码行为\n并不是每个人都适合当程序员\n\n<!-- more -->\n\n# 为什么做一个内部程序员就槽糕透了呢\n1. 你永远无法用正确的事情做事情，总是被迫用最保险的方法做事。\n2. 一旦你的程序可以用了，你就不得不停止项目，转而开发其他项目。\n3. 如果你在专业的软件公司中编程，你的工作与公司的主营业务是直接相关，是能够为公司直接带来收入的，这至少意味着一件事情，就是管理层会想到你。\n\n# 程序员管理的三种方法\n| 方法   | 说明 |  缺点   |\n| :-------: | :----: | :---: |\n| 军事化管理法 | 军队可以让士兵绝对服从命令 |  1.会使团队人员不爽 2.不可能管理到所有细节 3.对于技术问题，程序员才是做决策的最佳人选，因为他们知道更多的技术细节。    |\n| 经济利益驱动法 | 假设每个人的行为动机都是钱| 1.会降低团队人员的内部驱动力，当你停止付钱或者他们不再需要钱，他们的驱动力就会消失。2.员工会追求局部利益最大化，最终为了自己的利益可能做出对公司不好的行为。3.会鼓励员工与制度博弈，寻找制度的缺失之处来为自己谋利。经济利益驱动法更像是管理的退位，表面上简化了管理，但实际上却是管理的缺失。   |\n| 认同法     | 方法：1.培养认同感。设定积极的目标和价值观。2.培养凝聚力和归属感。团队的人要一起吃饭，多组织集体活动。3.信息共享。和员工要做到信息共享，以便员工更好的工作。  |  实施起来不容易。|\n\n推荐做法：***认同法***\n\n# 设计的作用\n> 字体平滑、反锯齿和次像素渲染\n苹果公司的设计有利于频幕界面和印刷一致\n微软公司的设计有利于屏幕阅读\n苹果的字体放到微软的系统上面会有一点模糊，比如在mac上面装windows系统，字体会变模糊。 \n寸土必争\n随便找一个东西，如果你找不到它的缺点，那就说明你的转型还没有成功。\n大构想的陷阱\n阅读的时候，眼镜只会盯住一个点，而其它地方像素很低，但是由于眼镜的注意力快速移动，让你产生了你已经看到了所有细节的错觉。软件设计中，你以为你可以做出来，看上去大体流程很清楚，但是一旦你考虑细节的时候你才会知道，你少考虑了很多东西。\n\n# 别让用户做太多选择   \n微软的关机选项有7种，再加上电源键(点击和长按)或者一些电脑上有快捷键关机或者重启，合下屏幕也可以睡眠或者关机。这样给用户的选择就有10种左右，而过多的选择会让用户产生选择困难而失去幸福感。\n如何改进？\n>Switch User之前肯定要Lock，这两个按钮可以合并。\nLog Off就是为了退出当前运行的所有程序，可以和ShutDown按钮合并。\nRestart和shutdown是一样的，只需要按两次shutdown按钮即可(假设关机速度很快，而且不是远程操控(一般远程操控是命令行操作))\nSleep和Hibernate说实话我不太懂他们的区别。\n现在我们只剩下 switch user/lock     log off/shutdown     hibernate/sleep三个按钮。\n考虑switch user/lock     hibernate/sleep这两个按钮的合并，当我按下这一个复合按钮的时候，会弹出一个切换用户的对话框，然后30秒内没有切换用户会进入休眠状态，而这个过程中计算机是一直锁定的。\n现在计算机只剩两个按钮了，就是这两种状态：\n1、我要离开电脑一会儿\n2、我要离开电脑很久，需要拔下电源   \n现在考虑能不能合并这两个按钮，现在的电源管理系统已经能够做到你在睡眠状态下不关闭电脑也只耗很低的电源，即使你拔掉电源也能够保存你的数据，不会损失你的任何数据，所以这两个按钮完全可以合并。\n\n# 管理大型项目\n火星人的耳机：这一部分主要讲述了为什么现在的web标准如此复杂。\n>* 一对多模式：假设你在火星上发现火星人还在使用录音机，你发现了商机，你决定卖mp3给他们，为此你要制造一种耳机，于是你写了一份规格书，让其他厂商制造耳机，但是你的规格书里误写了电压的参数，结果厂商做出来的耳机总是爆炸，然后经过调整声音的赫兹终于能正常使用了，就这样你让火星人大量买了你的耳机，然后你想让你的耳机拥有打电话的功能，就是加一个麦克风，然后你重新设计了一个适配器，这个适配器考虑了以后的升级问题，但是推出以后火星人根本没人买，他们只在乎家里收藏了一衣柜的耳机，根本不在乎什么麦克风功能。然后你只有在耳机的接口处扩展功能，把耳机的金属轴接口分成三份，其中一份可以用来做麦克风功能，你又设计了一份协议，在接口处发射一个信号，当mp3找到这个信号的时候，就开启麦克风模式，否则就开启向后兼容的耳机模式，现在，整个市场就变成了一对多模式，同一个耳机有多个版本了。  \n\n\n>* 多对多模式：由于你不停的给mp3增加功能，每个生产耳机的厂商都要把耳机在每一个mp3型号测试一遍，由于规格书的繁琐难读，有些产品出现了不兼容的现象，然后为了降低成本，厂商不得不只让耳机适配最流行的mp3播放器，这样，当耳机插入其他mp3的时候，会播放不出来或者更严重的会爆炸。原因是规格书有一个功能写得不清楚，比如，如果下雨，电压会升高，如果不下雨，电压不变。但是关于下雪，规格书没有讲，有的地方把下雪当成了下雨的一种，原因是都要用到雨刮器。有的地方从不下雪，所以就不是下雨。\n\n\n>* 然后有一个无聊的家伙，发表了一篇文章，利用漏洞可以解决兼容问题，比如用程序把下雪也误判成下雨，于是市场上就出现了多个版本的耳机和mp3，这就是多对多。\n\n微软的ie8就是类似上面的流程\n* 为什么微软office的文件格式如此复杂\n* 要挣钱就别嫌脏\n\n# 编程建议\n* 询证式日程规划\n完后速率 = 估计用时/实际用时\n蒙特卡洛方法估计值：\n![about time](/img/about_time.jpg )\n* 让错误的代码显而易见\n网站中，用户输入的字符串必须经过编码后才能使用，否则有可能会受到攻击。\n怎样才能尽量避免这种出错方式呢？\n>第一种办法：当用户的数据一传进来，就对数据进行编码。但是当需要直接存到数据库的时候实际上是要存储原来的字符的，所以不行。\n第二种办法：制定一种规范，只有输出字符串的时候才进行编码，但是有个问题，有些要输出的字符串是不可以进行编码的，比如包含html语句的(换行等)。\n第三种方法：设置中间变量，所有用户输入的字符串都必须赋给以us开头的变量，所有已知安全的字符串和包含html的字符换，都赋给s开头的变量。\n![judge code](/img/judge_code.jpg)\n\n* 把相同的代码放在一起\n把相同的代码尽量放到一起，将有助于帮你发现你程序的错误。比如：`i = j*5` c语言可以一眼看出这句代码的意思，但是c++中必须要考虑到i和j的数据类型，以防进行了运算符重载，还要考虑继承和多态的问题。所以在c++中这样抽象不是好的行为\n\n# 开办软件公司\n软件个体户\n乔尔创建Fog Creek的真正目的：那就是创造一家我们愿意为之工作的软件公司。\n优秀的程序员和平庸的程序员差距非常大。\n\n# 经营软件公司\n* 仿生学办公室\n* 他山之石，不可攻玉\n* 简化性\n* 揉一揉，搓一搓\n* 组织beta测试的十二个最高秘诀\n* 建立优质客户服务的7个步骤\n>1.每件事都有两种做法：a.表面的、快速的解决方法;b.思考、防止类似的问题再次发生\n2.建议吹掉灰尘：客户忘记插好接口，可以以其他委婉的方式提醒客户\n3.让客户迷上你\n4.承受责备\n5.学会说软话\n6.学会做木偶\n7.贪婪让你一无所获\n\n# 发布软件\n* 挑选发布日期\n>1.确定发布日期，这个日期可以根据可观情况也可根据主管愿望进行选择\n2.列出软件要实现的功能，然后按照优先级排序\n3.每当你落后于预定进度时，就把排在最后的功能砍掉\n\n* 如何为软件定价：招无定式\n\n# 修订软件\n* 五个为什么\n* 确定优先顺序\n\n\n","source":"_posts/more-joel-on-software.md","raw":"---\ntitle: 软件随想录-卷2\ndate: 2017-02-20 17:36:29\ntags: 随笔\n---\n\njoel Spolsky著 阮一峰译  \n[Gitbook 电子书](https://wizardforcel.gitbooks.io/joel-on-software/content/index.html  \"软件随想录\")\n\n# 寻找优秀程序员的三种办法\n>社招(简历)不靠谱，员工推荐不太靠谱\n走出去，参加开发者大会\n找实习生。\n创造自己的知名社区网站。\n\n# 写给程序员的建议\n任何时刻不要想着推倒重来，重构而不是重新做一遍\n不添加任何新功能\n无论任何时刻向源码库添加代码，都要保证程序能够正常运行\n所要做的只是一些合乎逻辑的变换，几乎都是机械性的，而且能够立刻确定不会改变代码行为\n并不是每个人都适合当程序员\n\n<!-- more -->\n\n# 为什么做一个内部程序员就槽糕透了呢\n1. 你永远无法用正确的事情做事情，总是被迫用最保险的方法做事。\n2. 一旦你的程序可以用了，你就不得不停止项目，转而开发其他项目。\n3. 如果你在专业的软件公司中编程，你的工作与公司的主营业务是直接相关，是能够为公司直接带来收入的，这至少意味着一件事情，就是管理层会想到你。\n\n# 程序员管理的三种方法\n| 方法   | 说明 |  缺点   |\n| :-------: | :----: | :---: |\n| 军事化管理法 | 军队可以让士兵绝对服从命令 |  1.会使团队人员不爽 2.不可能管理到所有细节 3.对于技术问题，程序员才是做决策的最佳人选，因为他们知道更多的技术细节。    |\n| 经济利益驱动法 | 假设每个人的行为动机都是钱| 1.会降低团队人员的内部驱动力，当你停止付钱或者他们不再需要钱，他们的驱动力就会消失。2.员工会追求局部利益最大化，最终为了自己的利益可能做出对公司不好的行为。3.会鼓励员工与制度博弈，寻找制度的缺失之处来为自己谋利。经济利益驱动法更像是管理的退位，表面上简化了管理，但实际上却是管理的缺失。   |\n| 认同法     | 方法：1.培养认同感。设定积极的目标和价值观。2.培养凝聚力和归属感。团队的人要一起吃饭，多组织集体活动。3.信息共享。和员工要做到信息共享，以便员工更好的工作。  |  实施起来不容易。|\n\n推荐做法：***认同法***\n\n# 设计的作用\n> 字体平滑、反锯齿和次像素渲染\n苹果公司的设计有利于频幕界面和印刷一致\n微软公司的设计有利于屏幕阅读\n苹果的字体放到微软的系统上面会有一点模糊，比如在mac上面装windows系统，字体会变模糊。 \n寸土必争\n随便找一个东西，如果你找不到它的缺点，那就说明你的转型还没有成功。\n大构想的陷阱\n阅读的时候，眼镜只会盯住一个点，而其它地方像素很低，但是由于眼镜的注意力快速移动，让你产生了你已经看到了所有细节的错觉。软件设计中，你以为你可以做出来，看上去大体流程很清楚，但是一旦你考虑细节的时候你才会知道，你少考虑了很多东西。\n\n# 别让用户做太多选择   \n微软的关机选项有7种，再加上电源键(点击和长按)或者一些电脑上有快捷键关机或者重启，合下屏幕也可以睡眠或者关机。这样给用户的选择就有10种左右，而过多的选择会让用户产生选择困难而失去幸福感。\n如何改进？\n>Switch User之前肯定要Lock，这两个按钮可以合并。\nLog Off就是为了退出当前运行的所有程序，可以和ShutDown按钮合并。\nRestart和shutdown是一样的，只需要按两次shutdown按钮即可(假设关机速度很快，而且不是远程操控(一般远程操控是命令行操作))\nSleep和Hibernate说实话我不太懂他们的区别。\n现在我们只剩下 switch user/lock     log off/shutdown     hibernate/sleep三个按钮。\n考虑switch user/lock     hibernate/sleep这两个按钮的合并，当我按下这一个复合按钮的时候，会弹出一个切换用户的对话框，然后30秒内没有切换用户会进入休眠状态，而这个过程中计算机是一直锁定的。\n现在计算机只剩两个按钮了，就是这两种状态：\n1、我要离开电脑一会儿\n2、我要离开电脑很久，需要拔下电源   \n现在考虑能不能合并这两个按钮，现在的电源管理系统已经能够做到你在睡眠状态下不关闭电脑也只耗很低的电源，即使你拔掉电源也能够保存你的数据，不会损失你的任何数据，所以这两个按钮完全可以合并。\n\n# 管理大型项目\n火星人的耳机：这一部分主要讲述了为什么现在的web标准如此复杂。\n>* 一对多模式：假设你在火星上发现火星人还在使用录音机，你发现了商机，你决定卖mp3给他们，为此你要制造一种耳机，于是你写了一份规格书，让其他厂商制造耳机，但是你的规格书里误写了电压的参数，结果厂商做出来的耳机总是爆炸，然后经过调整声音的赫兹终于能正常使用了，就这样你让火星人大量买了你的耳机，然后你想让你的耳机拥有打电话的功能，就是加一个麦克风，然后你重新设计了一个适配器，这个适配器考虑了以后的升级问题，但是推出以后火星人根本没人买，他们只在乎家里收藏了一衣柜的耳机，根本不在乎什么麦克风功能。然后你只有在耳机的接口处扩展功能，把耳机的金属轴接口分成三份，其中一份可以用来做麦克风功能，你又设计了一份协议，在接口处发射一个信号，当mp3找到这个信号的时候，就开启麦克风模式，否则就开启向后兼容的耳机模式，现在，整个市场就变成了一对多模式，同一个耳机有多个版本了。  \n\n\n>* 多对多模式：由于你不停的给mp3增加功能，每个生产耳机的厂商都要把耳机在每一个mp3型号测试一遍，由于规格书的繁琐难读，有些产品出现了不兼容的现象，然后为了降低成本，厂商不得不只让耳机适配最流行的mp3播放器，这样，当耳机插入其他mp3的时候，会播放不出来或者更严重的会爆炸。原因是规格书有一个功能写得不清楚，比如，如果下雨，电压会升高，如果不下雨，电压不变。但是关于下雪，规格书没有讲，有的地方把下雪当成了下雨的一种，原因是都要用到雨刮器。有的地方从不下雪，所以就不是下雨。\n\n\n>* 然后有一个无聊的家伙，发表了一篇文章，利用漏洞可以解决兼容问题，比如用程序把下雪也误判成下雨，于是市场上就出现了多个版本的耳机和mp3，这就是多对多。\n\n微软的ie8就是类似上面的流程\n* 为什么微软office的文件格式如此复杂\n* 要挣钱就别嫌脏\n\n# 编程建议\n* 询证式日程规划\n完后速率 = 估计用时/实际用时\n蒙特卡洛方法估计值：\n![about time](/img/about_time.jpg )\n* 让错误的代码显而易见\n网站中，用户输入的字符串必须经过编码后才能使用，否则有可能会受到攻击。\n怎样才能尽量避免这种出错方式呢？\n>第一种办法：当用户的数据一传进来，就对数据进行编码。但是当需要直接存到数据库的时候实际上是要存储原来的字符的，所以不行。\n第二种办法：制定一种规范，只有输出字符串的时候才进行编码，但是有个问题，有些要输出的字符串是不可以进行编码的，比如包含html语句的(换行等)。\n第三种方法：设置中间变量，所有用户输入的字符串都必须赋给以us开头的变量，所有已知安全的字符串和包含html的字符换，都赋给s开头的变量。\n![judge code](/img/judge_code.jpg)\n\n* 把相同的代码放在一起\n把相同的代码尽量放到一起，将有助于帮你发现你程序的错误。比如：`i = j*5` c语言可以一眼看出这句代码的意思，但是c++中必须要考虑到i和j的数据类型，以防进行了运算符重载，还要考虑继承和多态的问题。所以在c++中这样抽象不是好的行为\n\n# 开办软件公司\n软件个体户\n乔尔创建Fog Creek的真正目的：那就是创造一家我们愿意为之工作的软件公司。\n优秀的程序员和平庸的程序员差距非常大。\n\n# 经营软件公司\n* 仿生学办公室\n* 他山之石，不可攻玉\n* 简化性\n* 揉一揉，搓一搓\n* 组织beta测试的十二个最高秘诀\n* 建立优质客户服务的7个步骤\n>1.每件事都有两种做法：a.表面的、快速的解决方法;b.思考、防止类似的问题再次发生\n2.建议吹掉灰尘：客户忘记插好接口，可以以其他委婉的方式提醒客户\n3.让客户迷上你\n4.承受责备\n5.学会说软话\n6.学会做木偶\n7.贪婪让你一无所获\n\n# 发布软件\n* 挑选发布日期\n>1.确定发布日期，这个日期可以根据可观情况也可根据主管愿望进行选择\n2.列出软件要实现的功能，然后按照优先级排序\n3.每当你落后于预定进度时，就把排在最后的功能砍掉\n\n* 如何为软件定价：招无定式\n\n# 修订软件\n* 五个为什么\n* 确定优先顺序\n\n\n","slug":"more-joel-on-software","published":1,"updated":"2018-04-27T04:11:03.361Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjhajczov000qe4rws0y5ozjl","content":"<p>joel Spolsky著 阮一峰译<br><a href=\"https://wizardforcel.gitbooks.io/joel-on-software/content/index.html\" title=\"软件随想录\" target=\"_blank\" rel=\"external\">Gitbook 电子书</a></p>\n<h1 id=\"寻找优秀程序员的三种办法\"><a href=\"#寻找优秀程序员的三种办法\" class=\"headerlink\" title=\"寻找优秀程序员的三种办法\"></a>寻找优秀程序员的三种办法</h1><blockquote>\n<p>社招(简历)不靠谱，员工推荐不太靠谱<br>走出去，参加开发者大会<br>找实习生。<br>创造自己的知名社区网站。</p>\n</blockquote>\n<h1 id=\"写给程序员的建议\"><a href=\"#写给程序员的建议\" class=\"headerlink\" title=\"写给程序员的建议\"></a>写给程序员的建议</h1><p>任何时刻不要想着推倒重来，重构而不是重新做一遍<br>不添加任何新功能<br>无论任何时刻向源码库添加代码，都要保证程序能够正常运行<br>所要做的只是一些合乎逻辑的变换，几乎都是机械性的，而且能够立刻确定不会改变代码行为<br>并不是每个人都适合当程序员</p>\n<a id=\"more\"></a>\n<h1 id=\"为什么做一个内部程序员就槽糕透了呢\"><a href=\"#为什么做一个内部程序员就槽糕透了呢\" class=\"headerlink\" title=\"为什么做一个内部程序员就槽糕透了呢\"></a>为什么做一个内部程序员就槽糕透了呢</h1><ol>\n<li>你永远无法用正确的事情做事情，总是被迫用最保险的方法做事。</li>\n<li>一旦你的程序可以用了，你就不得不停止项目，转而开发其他项目。</li>\n<li>如果你在专业的软件公司中编程，你的工作与公司的主营业务是直接相关，是能够为公司直接带来收入的，这至少意味着一件事情，就是管理层会想到你。</li>\n</ol>\n<h1 id=\"程序员管理的三种方法\"><a href=\"#程序员管理的三种方法\" class=\"headerlink\" title=\"程序员管理的三种方法\"></a>程序员管理的三种方法</h1><table>\n<thead>\n<tr>\n<th style=\"text-align:center\">方法</th>\n<th style=\"text-align:center\">说明</th>\n<th style=\"text-align:center\">缺点</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">军事化管理法</td>\n<td style=\"text-align:center\">军队可以让士兵绝对服从命令</td>\n<td style=\"text-align:center\">1.会使团队人员不爽 2.不可能管理到所有细节 3.对于技术问题，程序员才是做决策的最佳人选，因为他们知道更多的技术细节。</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">经济利益驱动法</td>\n<td style=\"text-align:center\">假设每个人的行为动机都是钱</td>\n<td style=\"text-align:center\">1.会降低团队人员的内部驱动力，当你停止付钱或者他们不再需要钱，他们的驱动力就会消失。2.员工会追求局部利益最大化，最终为了自己的利益可能做出对公司不好的行为。3.会鼓励员工与制度博弈，寻找制度的缺失之处来为自己谋利。经济利益驱动法更像是管理的退位，表面上简化了管理，但实际上却是管理的缺失。</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">认同法</td>\n<td style=\"text-align:center\">方法：1.培养认同感。设定积极的目标和价值观。2.培养凝聚力和归属感。团队的人要一起吃饭，多组织集体活动。3.信息共享。和员工要做到信息共享，以便员工更好的工作。</td>\n<td style=\"text-align:center\">实施起来不容易。</td>\n</tr>\n</tbody>\n</table>\n<p>推荐做法：<strong><em>认同法</em></strong></p>\n<h1 id=\"设计的作用\"><a href=\"#设计的作用\" class=\"headerlink\" title=\"设计的作用\"></a>设计的作用</h1><blockquote>\n<p>字体平滑、反锯齿和次像素渲染<br>苹果公司的设计有利于频幕界面和印刷一致<br>微软公司的设计有利于屏幕阅读<br>苹果的字体放到微软的系统上面会有一点模糊，比如在mac上面装windows系统，字体会变模糊。<br>寸土必争<br>随便找一个东西，如果你找不到它的缺点，那就说明你的转型还没有成功。<br>大构想的陷阱<br>阅读的时候，眼镜只会盯住一个点，而其它地方像素很低，但是由于眼镜的注意力快速移动，让你产生了你已经看到了所有细节的错觉。软件设计中，你以为你可以做出来，看上去大体流程很清楚，但是一旦你考虑细节的时候你才会知道，你少考虑了很多东西。</p>\n</blockquote>\n<h1 id=\"别让用户做太多选择\"><a href=\"#别让用户做太多选择\" class=\"headerlink\" title=\"别让用户做太多选择\"></a>别让用户做太多选择</h1><p>微软的关机选项有7种，再加上电源键(点击和长按)或者一些电脑上有快捷键关机或者重启，合下屏幕也可以睡眠或者关机。这样给用户的选择就有10种左右，而过多的选择会让用户产生选择困难而失去幸福感。<br>如何改进？</p>\n<blockquote>\n<p>Switch User之前肯定要Lock，这两个按钮可以合并。<br>Log Off就是为了退出当前运行的所有程序，可以和ShutDown按钮合并。<br>Restart和shutdown是一样的，只需要按两次shutdown按钮即可(假设关机速度很快，而且不是远程操控(一般远程操控是命令行操作))<br>Sleep和Hibernate说实话我不太懂他们的区别。<br>现在我们只剩下 switch user/lock     log off/shutdown     hibernate/sleep三个按钮。<br>考虑switch user/lock     hibernate/sleep这两个按钮的合并，当我按下这一个复合按钮的时候，会弹出一个切换用户的对话框，然后30秒内没有切换用户会进入休眠状态，而这个过程中计算机是一直锁定的。<br>现在计算机只剩两个按钮了，就是这两种状态：<br>1、我要离开电脑一会儿<br>2、我要离开电脑很久，需要拔下电源<br>现在考虑能不能合并这两个按钮，现在的电源管理系统已经能够做到你在睡眠状态下不关闭电脑也只耗很低的电源，即使你拔掉电源也能够保存你的数据，不会损失你的任何数据，所以这两个按钮完全可以合并。</p>\n</blockquote>\n<h1 id=\"管理大型项目\"><a href=\"#管理大型项目\" class=\"headerlink\" title=\"管理大型项目\"></a>管理大型项目</h1><p>火星人的耳机：这一部分主要讲述了为什么现在的web标准如此复杂。</p>\n<blockquote>\n<ul>\n<li>一对多模式：假设你在火星上发现火星人还在使用录音机，你发现了商机，你决定卖mp3给他们，为此你要制造一种耳机，于是你写了一份规格书，让其他厂商制造耳机，但是你的规格书里误写了电压的参数，结果厂商做出来的耳机总是爆炸，然后经过调整声音的赫兹终于能正常使用了，就这样你让火星人大量买了你的耳机，然后你想让你的耳机拥有打电话的功能，就是加一个麦克风，然后你重新设计了一个适配器，这个适配器考虑了以后的升级问题，但是推出以后火星人根本没人买，他们只在乎家里收藏了一衣柜的耳机，根本不在乎什么麦克风功能。然后你只有在耳机的接口处扩展功能，把耳机的金属轴接口分成三份，其中一份可以用来做麦克风功能，你又设计了一份协议，在接口处发射一个信号，当mp3找到这个信号的时候，就开启麦克风模式，否则就开启向后兼容的耳机模式，现在，整个市场就变成了一对多模式，同一个耳机有多个版本了。  </li>\n</ul>\n<ul>\n<li>多对多模式：由于你不停的给mp3增加功能，每个生产耳机的厂商都要把耳机在每一个mp3型号测试一遍，由于规格书的繁琐难读，有些产品出现了不兼容的现象，然后为了降低成本，厂商不得不只让耳机适配最流行的mp3播放器，这样，当耳机插入其他mp3的时候，会播放不出来或者更严重的会爆炸。原因是规格书有一个功能写得不清楚，比如，如果下雨，电压会升高，如果不下雨，电压不变。但是关于下雪，规格书没有讲，有的地方把下雪当成了下雨的一种，原因是都要用到雨刮器。有的地方从不下雪，所以就不是下雨。</li>\n</ul>\n<ul>\n<li>然后有一个无聊的家伙，发表了一篇文章，利用漏洞可以解决兼容问题，比如用程序把下雪也误判成下雨，于是市场上就出现了多个版本的耳机和mp3，这就是多对多。</li>\n</ul>\n</blockquote>\n<p>微软的ie8就是类似上面的流程</p>\n<ul>\n<li>为什么微软office的文件格式如此复杂</li>\n<li>要挣钱就别嫌脏</li>\n</ul>\n<h1 id=\"编程建议\"><a href=\"#编程建议\" class=\"headerlink\" title=\"编程建议\"></a>编程建议</h1><ul>\n<li>询证式日程规划<br>完后速率 = 估计用时/实际用时<br>蒙特卡洛方法估计值：<br><img src=\"/img/about_time.jpg\" alt=\"about time\"></li>\n<li><p>让错误的代码显而易见<br>网站中，用户输入的字符串必须经过编码后才能使用，否则有可能会受到攻击。<br>怎样才能尽量避免这种出错方式呢？</p>\n<blockquote>\n<p>第一种办法：当用户的数据一传进来，就对数据进行编码。但是当需要直接存到数据库的时候实际上是要存储原来的字符的，所以不行。<br>第二种办法：制定一种规范，只有输出字符串的时候才进行编码，但是有个问题，有些要输出的字符串是不可以进行编码的，比如包含html语句的(换行等)。<br>第三种方法：设置中间变量，所有用户输入的字符串都必须赋给以us开头的变量，所有已知安全的字符串和包含html的字符换，都赋给s开头的变量。<br><img src=\"/img/judge_code.jpg\" alt=\"judge code\"></p>\n</blockquote>\n</li>\n<li><p>把相同的代码放在一起<br>把相同的代码尽量放到一起，将有助于帮你发现你程序的错误。比如：<code>i = j*5</code> c语言可以一眼看出这句代码的意思，但是c++中必须要考虑到i和j的数据类型，以防进行了运算符重载，还要考虑继承和多态的问题。所以在c++中这样抽象不是好的行为</p>\n</li>\n</ul>\n<h1 id=\"开办软件公司\"><a href=\"#开办软件公司\" class=\"headerlink\" title=\"开办软件公司\"></a>开办软件公司</h1><p>软件个体户<br>乔尔创建Fog Creek的真正目的：那就是创造一家我们愿意为之工作的软件公司。<br>优秀的程序员和平庸的程序员差距非常大。</p>\n<h1 id=\"经营软件公司\"><a href=\"#经营软件公司\" class=\"headerlink\" title=\"经营软件公司\"></a>经营软件公司</h1><ul>\n<li>仿生学办公室</li>\n<li>他山之石，不可攻玉</li>\n<li>简化性</li>\n<li>揉一揉，搓一搓</li>\n<li>组织beta测试的十二个最高秘诀</li>\n<li>建立优质客户服务的7个步骤<blockquote>\n<p>1.每件事都有两种做法：a.表面的、快速的解决方法;b.思考、防止类似的问题再次发生<br>2.建议吹掉灰尘：客户忘记插好接口，可以以其他委婉的方式提醒客户<br>3.让客户迷上你<br>4.承受责备<br>5.学会说软话<br>6.学会做木偶<br>7.贪婪让你一无所获</p>\n</blockquote>\n</li>\n</ul>\n<h1 id=\"发布软件\"><a href=\"#发布软件\" class=\"headerlink\" title=\"发布软件\"></a>发布软件</h1><ul>\n<li><p>挑选发布日期</p>\n<blockquote>\n<p>1.确定发布日期，这个日期可以根据可观情况也可根据主管愿望进行选择<br>2.列出软件要实现的功能，然后按照优先级排序<br>3.每当你落后于预定进度时，就把排在最后的功能砍掉</p>\n</blockquote>\n</li>\n<li><p>如何为软件定价：招无定式</p>\n</li>\n</ul>\n<h1 id=\"修订软件\"><a href=\"#修订软件\" class=\"headerlink\" title=\"修订软件\"></a>修订软件</h1><ul>\n<li>五个为什么</li>\n<li>确定优先顺序</li>\n</ul>\n","site":{"data":{}},"excerpt":"<p>joel Spolsky著 阮一峰译<br><a href=\"https://wizardforcel.gitbooks.io/joel-on-software/content/index.html\" title=\"软件随想录\" target=\"_blank\" rel=\"external\">Gitbook 电子书</a></p>\n<h1 id=\"寻找优秀程序员的三种办法\"><a href=\"#寻找优秀程序员的三种办法\" class=\"headerlink\" title=\"寻找优秀程序员的三种办法\"></a>寻找优秀程序员的三种办法</h1><blockquote>\n<p>社招(简历)不靠谱，员工推荐不太靠谱<br>走出去，参加开发者大会<br>找实习生。<br>创造自己的知名社区网站。</p>\n</blockquote>\n<h1 id=\"写给程序员的建议\"><a href=\"#写给程序员的建议\" class=\"headerlink\" title=\"写给程序员的建议\"></a>写给程序员的建议</h1><p>任何时刻不要想着推倒重来，重构而不是重新做一遍<br>不添加任何新功能<br>无论任何时刻向源码库添加代码，都要保证程序能够正常运行<br>所要做的只是一些合乎逻辑的变换，几乎都是机械性的，而且能够立刻确定不会改变代码行为<br>并不是每个人都适合当程序员</p>","more":"<h1 id=\"为什么做一个内部程序员就槽糕透了呢\"><a href=\"#为什么做一个内部程序员就槽糕透了呢\" class=\"headerlink\" title=\"为什么做一个内部程序员就槽糕透了呢\"></a>为什么做一个内部程序员就槽糕透了呢</h1><ol>\n<li>你永远无法用正确的事情做事情，总是被迫用最保险的方法做事。</li>\n<li>一旦你的程序可以用了，你就不得不停止项目，转而开发其他项目。</li>\n<li>如果你在专业的软件公司中编程，你的工作与公司的主营业务是直接相关，是能够为公司直接带来收入的，这至少意味着一件事情，就是管理层会想到你。</li>\n</ol>\n<h1 id=\"程序员管理的三种方法\"><a href=\"#程序员管理的三种方法\" class=\"headerlink\" title=\"程序员管理的三种方法\"></a>程序员管理的三种方法</h1><table>\n<thead>\n<tr>\n<th style=\"text-align:center\">方法</th>\n<th style=\"text-align:center\">说明</th>\n<th style=\"text-align:center\">缺点</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">军事化管理法</td>\n<td style=\"text-align:center\">军队可以让士兵绝对服从命令</td>\n<td style=\"text-align:center\">1.会使团队人员不爽 2.不可能管理到所有细节 3.对于技术问题，程序员才是做决策的最佳人选，因为他们知道更多的技术细节。</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">经济利益驱动法</td>\n<td style=\"text-align:center\">假设每个人的行为动机都是钱</td>\n<td style=\"text-align:center\">1.会降低团队人员的内部驱动力，当你停止付钱或者他们不再需要钱，他们的驱动力就会消失。2.员工会追求局部利益最大化，最终为了自己的利益可能做出对公司不好的行为。3.会鼓励员工与制度博弈，寻找制度的缺失之处来为自己谋利。经济利益驱动法更像是管理的退位，表面上简化了管理，但实际上却是管理的缺失。</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">认同法</td>\n<td style=\"text-align:center\">方法：1.培养认同感。设定积极的目标和价值观。2.培养凝聚力和归属感。团队的人要一起吃饭，多组织集体活动。3.信息共享。和员工要做到信息共享，以便员工更好的工作。</td>\n<td style=\"text-align:center\">实施起来不容易。</td>\n</tr>\n</tbody>\n</table>\n<p>推荐做法：<strong><em>认同法</em></strong></p>\n<h1 id=\"设计的作用\"><a href=\"#设计的作用\" class=\"headerlink\" title=\"设计的作用\"></a>设计的作用</h1><blockquote>\n<p>字体平滑、反锯齿和次像素渲染<br>苹果公司的设计有利于频幕界面和印刷一致<br>微软公司的设计有利于屏幕阅读<br>苹果的字体放到微软的系统上面会有一点模糊，比如在mac上面装windows系统，字体会变模糊。<br>寸土必争<br>随便找一个东西，如果你找不到它的缺点，那就说明你的转型还没有成功。<br>大构想的陷阱<br>阅读的时候，眼镜只会盯住一个点，而其它地方像素很低，但是由于眼镜的注意力快速移动，让你产生了你已经看到了所有细节的错觉。软件设计中，你以为你可以做出来，看上去大体流程很清楚，但是一旦你考虑细节的时候你才会知道，你少考虑了很多东西。</p>\n</blockquote>\n<h1 id=\"别让用户做太多选择\"><a href=\"#别让用户做太多选择\" class=\"headerlink\" title=\"别让用户做太多选择\"></a>别让用户做太多选择</h1><p>微软的关机选项有7种，再加上电源键(点击和长按)或者一些电脑上有快捷键关机或者重启，合下屏幕也可以睡眠或者关机。这样给用户的选择就有10种左右，而过多的选择会让用户产生选择困难而失去幸福感。<br>如何改进？</p>\n<blockquote>\n<p>Switch User之前肯定要Lock，这两个按钮可以合并。<br>Log Off就是为了退出当前运行的所有程序，可以和ShutDown按钮合并。<br>Restart和shutdown是一样的，只需要按两次shutdown按钮即可(假设关机速度很快，而且不是远程操控(一般远程操控是命令行操作))<br>Sleep和Hibernate说实话我不太懂他们的区别。<br>现在我们只剩下 switch user/lock     log off/shutdown     hibernate/sleep三个按钮。<br>考虑switch user/lock     hibernate/sleep这两个按钮的合并，当我按下这一个复合按钮的时候，会弹出一个切换用户的对话框，然后30秒内没有切换用户会进入休眠状态，而这个过程中计算机是一直锁定的。<br>现在计算机只剩两个按钮了，就是这两种状态：<br>1、我要离开电脑一会儿<br>2、我要离开电脑很久，需要拔下电源<br>现在考虑能不能合并这两个按钮，现在的电源管理系统已经能够做到你在睡眠状态下不关闭电脑也只耗很低的电源，即使你拔掉电源也能够保存你的数据，不会损失你的任何数据，所以这两个按钮完全可以合并。</p>\n</blockquote>\n<h1 id=\"管理大型项目\"><a href=\"#管理大型项目\" class=\"headerlink\" title=\"管理大型项目\"></a>管理大型项目</h1><p>火星人的耳机：这一部分主要讲述了为什么现在的web标准如此复杂。</p>\n<blockquote>\n<ul>\n<li>一对多模式：假设你在火星上发现火星人还在使用录音机，你发现了商机，你决定卖mp3给他们，为此你要制造一种耳机，于是你写了一份规格书，让其他厂商制造耳机，但是你的规格书里误写了电压的参数，结果厂商做出来的耳机总是爆炸，然后经过调整声音的赫兹终于能正常使用了，就这样你让火星人大量买了你的耳机，然后你想让你的耳机拥有打电话的功能，就是加一个麦克风，然后你重新设计了一个适配器，这个适配器考虑了以后的升级问题，但是推出以后火星人根本没人买，他们只在乎家里收藏了一衣柜的耳机，根本不在乎什么麦克风功能。然后你只有在耳机的接口处扩展功能，把耳机的金属轴接口分成三份，其中一份可以用来做麦克风功能，你又设计了一份协议，在接口处发射一个信号，当mp3找到这个信号的时候，就开启麦克风模式，否则就开启向后兼容的耳机模式，现在，整个市场就变成了一对多模式，同一个耳机有多个版本了。  </li>\n</ul>\n<ul>\n<li>多对多模式：由于你不停的给mp3增加功能，每个生产耳机的厂商都要把耳机在每一个mp3型号测试一遍，由于规格书的繁琐难读，有些产品出现了不兼容的现象，然后为了降低成本，厂商不得不只让耳机适配最流行的mp3播放器，这样，当耳机插入其他mp3的时候，会播放不出来或者更严重的会爆炸。原因是规格书有一个功能写得不清楚，比如，如果下雨，电压会升高，如果不下雨，电压不变。但是关于下雪，规格书没有讲，有的地方把下雪当成了下雨的一种，原因是都要用到雨刮器。有的地方从不下雪，所以就不是下雨。</li>\n</ul>\n<ul>\n<li>然后有一个无聊的家伙，发表了一篇文章，利用漏洞可以解决兼容问题，比如用程序把下雪也误判成下雨，于是市场上就出现了多个版本的耳机和mp3，这就是多对多。</li>\n</ul>\n</blockquote>\n<p>微软的ie8就是类似上面的流程</p>\n<ul>\n<li>为什么微软office的文件格式如此复杂</li>\n<li>要挣钱就别嫌脏</li>\n</ul>\n<h1 id=\"编程建议\"><a href=\"#编程建议\" class=\"headerlink\" title=\"编程建议\"></a>编程建议</h1><ul>\n<li>询证式日程规划<br>完后速率 = 估计用时/实际用时<br>蒙特卡洛方法估计值：<br><img src=\"/img/about_time.jpg\" alt=\"about time\"></li>\n<li><p>让错误的代码显而易见<br>网站中，用户输入的字符串必须经过编码后才能使用，否则有可能会受到攻击。<br>怎样才能尽量避免这种出错方式呢？</p>\n<blockquote>\n<p>第一种办法：当用户的数据一传进来，就对数据进行编码。但是当需要直接存到数据库的时候实际上是要存储原来的字符的，所以不行。<br>第二种办法：制定一种规范，只有输出字符串的时候才进行编码，但是有个问题，有些要输出的字符串是不可以进行编码的，比如包含html语句的(换行等)。<br>第三种方法：设置中间变量，所有用户输入的字符串都必须赋给以us开头的变量，所有已知安全的字符串和包含html的字符换，都赋给s开头的变量。<br><img src=\"/img/judge_code.jpg\" alt=\"judge code\"></p>\n</blockquote>\n</li>\n<li><p>把相同的代码放在一起<br>把相同的代码尽量放到一起，将有助于帮你发现你程序的错误。比如：<code>i = j*5</code> c语言可以一眼看出这句代码的意思，但是c++中必须要考虑到i和j的数据类型，以防进行了运算符重载，还要考虑继承和多态的问题。所以在c++中这样抽象不是好的行为</p>\n</li>\n</ul>\n<h1 id=\"开办软件公司\"><a href=\"#开办软件公司\" class=\"headerlink\" title=\"开办软件公司\"></a>开办软件公司</h1><p>软件个体户<br>乔尔创建Fog Creek的真正目的：那就是创造一家我们愿意为之工作的软件公司。<br>优秀的程序员和平庸的程序员差距非常大。</p>\n<h1 id=\"经营软件公司\"><a href=\"#经营软件公司\" class=\"headerlink\" title=\"经营软件公司\"></a>经营软件公司</h1><ul>\n<li>仿生学办公室</li>\n<li>他山之石，不可攻玉</li>\n<li>简化性</li>\n<li>揉一揉，搓一搓</li>\n<li>组织beta测试的十二个最高秘诀</li>\n<li>建立优质客户服务的7个步骤<blockquote>\n<p>1.每件事都有两种做法：a.表面的、快速的解决方法;b.思考、防止类似的问题再次发生<br>2.建议吹掉灰尘：客户忘记插好接口，可以以其他委婉的方式提醒客户<br>3.让客户迷上你<br>4.承受责备<br>5.学会说软话<br>6.学会做木偶<br>7.贪婪让你一无所获</p>\n</blockquote>\n</li>\n</ul>\n<h1 id=\"发布软件\"><a href=\"#发布软件\" class=\"headerlink\" title=\"发布软件\"></a>发布软件</h1><ul>\n<li><p>挑选发布日期</p>\n<blockquote>\n<p>1.确定发布日期，这个日期可以根据可观情况也可根据主管愿望进行选择<br>2.列出软件要实现的功能，然后按照优先级排序<br>3.每当你落后于预定进度时，就把排在最后的功能砍掉</p>\n</blockquote>\n</li>\n<li><p>如何为软件定价：招无定式</p>\n</li>\n</ul>\n<h1 id=\"修订软件\"><a href=\"#修订软件\" class=\"headerlink\" title=\"修订软件\"></a>修订软件</h1><ul>\n<li>五个为什么</li>\n<li>确定优先顺序</li>\n</ul>"},{"title":"Open-falcon 监控体系介绍","date":"2018-04-27T04:51:34.000Z","_content":"\n## 1.Openfalcon安装部署\n\n### 简介\n\n[官方文档 v2.0](https://book.open-falcon.org/zh_0_2/intro/)\n\n- 强大灵活的数据采集：自动发现，支持falcon-agent、snmp、支持用户主动push、用户自定义插件支持、opentsdb data model like（timestamp、endpoint、metric、key-value tags）\n- 水平扩展能力：支持每个周期上亿次的数据采集、告警判定、历史数据存储和查询\n- 高效率的告警策略管理：高效的portal、支持策略模板、模板继承和覆盖、多种告警方式、支持callback调用\n- 人性化的告警设置：最大告警次数、告警级别、告警恢复通知、告警暂停、不同时段不同阈值、支持维护周期\n- 高效率的graph组件：单机支撑200万metric的上报、归档、存储（周期为1分钟）\n- 高效的历史数据query组件：采用rrdtool的数据归档策略，秒级返回上百个metric一年的历史数据\n- dashboard：多维度的数据展示，用户自定义Screen\n- 高可用：整个系统无核心单点，易运维，易部署，可水平扩展\n- 开发语言： 整个系统的后端，全部golang编写，portal和dashboard使用python编写。\n\n### 框架\n\n![func_tro](/img/openfalcon-func.png)\n\n\n\n<!-- more -->\n\n### 后端安装(docker)\n\n/opt/openfalcon/falcon-plus\n\n[官方教程](https://github.com/open-falcon/falcon-plus/blob/master/docker/README.md)\n\n```shell\n## Running falcon-plus container\n\n    docker pull openfalcon/falcon-plus:0.2.0\n    docker run -itd -p 8081:8081 openfalcon/falcon-plus:0.2.0 bash /run.sh hbs\n\n## Running falcon-plus container with docker-compose\n\n    docker-compose -f init.yml up -d falcon-plus\n\n## Running mysql and redis container\n\n    docker-compose -f init.yml up -d mysql redis\n\n## Stop and Remove containers\n\n    docker-compose -f init.yml rm -f\n```\n\ninit.yml\n\n```yaml\nversion: '2'\nservices:\n  mysql:\n    environment:\n    - MYSQL_ROOT_PASSWORD=password\n    extends:\n      file: common.yml\n      service: template\n    hostname: docker-mysql\n    image: mysql:5.7\n    labels:\n      owl: mysql\n    ports:\n    - 3306:3306\n    restart: always\n    volumes:\n    - ../scripts/mysql/db_schema:/docker-entrypoint-initdb.d\n    - ./mysql.cnf:/etc/mysql/conf.d/mysql.cnf:ro\n  redis:\n    command: redis-server /redis.conf\n    extends:\n      file: common.yml\n      service: template-backend\n    hostname: docker-redis\n    image: redis:3.0\n    labels:\n      owl: redis\n    ports:\n    - 6379:6379\n    restart: always\n    volumes:\n    - ./redis.conf:/redis.conf\n  falcon-plus:\n    command: bash /run.sh hbs\n    image: openfalcon/falcon-plus:0.2.0\n    ports:\n    - 8081:8081\n    restart: always\n```\n\n**注意**  需要打包mysql初始化脚本到这个路径下```../scripts/mysql/db_schema``` ，具体文件[在这里](https://github.com/open-falcon/falcon-plus/tree/master/scripts/mysql/db_schema)\n\nmysql登陆\n\n```\n#默认mysql密码  password\ndocker exec -it <容器id> mysql -p \n```\n\nweb 登陆 http://192.168.14.165:8081   新建用户\n\n![web](/img/openfalcon-web.png)\n\n### 前端安装(docker)\n\n/opt/openfalcon/dashboard\n\n[官方教程](https://github.com/open-falcon/dashboard/blob/master/README.md)\n\n```shell\n# make the image，run commands under dir of dashboard:\ndocker build -t falcon-dashboard:v1.0 .\n\n# start the container\ndocker run -itd --name aaa --net host \\\n\t-e API_ADDR=http://127.0.0.1:8080/api/v1 \\\n\t-e PORTAL_DB_HOST=127.0.0.1 \\\n\t-e PORTAL_DB_PORT=3306 \\\n\t-e PORTAL_DB_USER=root \\\n\t-e PORTAL_DB_PASS=123456 \\\n\t-e PORTAL_DB_NAME=falcon_portal \\\n\t-e ALARM_DB_PASS=123456 \\\n\t-e ALARM_DB_HOST=127.0.0.1 \\\n\t-e ALARM_DB_PORT=3306 \\\n\t-e ALARM_DB_USER=root \\\n\t-e ALARM_DB_PASS=123456 \\\n\t-e ALARM_DB_NAME=alarms \\\n\tfalcon-dashboard:v1.0\n```\n\nDockerfile\n\n```\nFROM centos:7.3.1611\n\nRUN yum clean all && yum install -y epel-release && yum -y update && \\\nyum install -y git python-virtualenv python-devel openldap-devel mysql-devel && \\\nyum groupinstall -y \"Development tools\"\n\nRUN export HOME=/home/work/ && mkdir -p $HOME/open-falcon/dashboard && cd $HOME/open-falcon/dashboard\nWORKDIR /home/work/open-falcon/dashboard\nADD ./ ./\nRUN virtualenv ./env && ./env/bin/pip install -r pip_requirements.txt -i http://pypi.douban.com/simple\n\nADD ./entrypoint.sh /entrypoint.sh\nRUN chmod +x /entrypoint.sh\n\nENTRYPOINT [\"/entrypoint.sh\"]\n```\n\n\n\n\n\n## 2.Openfalcon使用\n\n### 采集模式\n\n[入门手册](https://book.open-falcon.org/zh_0_2/usage/getting-started.html)\n\nfalcon-agent自动开始采集各项指标，主动上报，不需要用户在server做任何配置（这和zabbix有很大的不同），这样做的好处，就是用户维护方便，覆盖率高。当然这样做也会server端造成较大的压力，不过open-falcon的服务端组件单机性能足够高，同时都可以水平扩展，所以自动多采集足够多的数据，反而是一件好事情，对于SRE和DEV来讲，事后追查问题，不再是难题。\n\n另外，falcon-agent提供了一个proxy-gateway，用户可以方便的通过http接口，push数据到本机的gateway，gateway会帮忙高效率的转发到server端。\n\n\n\n**机器负载信息**\n\n这部分比较通用，我们提供了一个agent部署在所有机器上去采集。不像zabbix，要采集什么数据需要在服务端配置，falcon无需配置，只要agent部署到机器上，配置好heartbeat和Transfer地址，就自动开始采集了，省去了用户配置的麻烦。目前agent只支持64位Linux，Mac、Windows均不支持。\n\n**硬件信息**\n\n硬件信息的采集脚本由系统组同学提供，作为plugin依托于agent运行，plugin机制介绍请看[这里](http://book.open-falcon.org/zh_0_2/philosophy/plugin.html)。\n\n**服务监控数据**\n\n服务的监控指标采集脚本，通常都是跟着服务的code走的，服务上线或者扩容，这个脚本也跟着上线或者扩容，服务下线，这个采集脚本也要相应下线。公司里Java的项目有不少，研发那边就提供了一个通用jar包，只要引入这个jar包，就可以自动采集接口的调用次数、延迟时间等数据。然后将采集到的数据push给监控，一分钟push一次。目前falcon的agent提供了一个简单的http接口，这个jar包采集到数据之后是post给本机agent。向agent推送数据的一个简单例子，如下：\n\n```\ncurl -X POST -d '[{\"metric\": \"qps\", \"endpoint\": \"open-falcon-graph01.bj\", \"timestamp\": 1431347802, \"step\": 60,\"value\": 9,\"counterType\": \"GAUGE\",\"tags\": \"project=falcon,module=graph\"}]' http://127.0.0.1:1988/v1/push\n\n```\n\n**各种开源软件的监控指标**\n\n这都是大用户，比如DBA自己写一些采集脚本，连到各个MySQL实例上去采集数据，完事直接调用server端的jsonrpc汇报数据，一分钟一次，每次甚至push几十万条数据，比较好的发送方式是500条数据做一个batch，别几十万数据一次性发送。\n\n\n\n由上可知，falcon-agent本身仅支持主机类的指标采集，对于其他业务系统、开源软件，需独立的使用采集脚本，将数据按照规定格式，发送到falcon-agent 所在的监听端口。最终再由falcon-agent 转发到服务端。\n\n","source":"_posts/open-falcon.md","raw":"---\ntitle: Open-falcon 监控体系介绍\ndate: 2018-04-27 12:51:34\ntags: \n - Open-falcon\n - Go\ncategories: 技术\n---\n\n## 1.Openfalcon安装部署\n\n### 简介\n\n[官方文档 v2.0](https://book.open-falcon.org/zh_0_2/intro/)\n\n- 强大灵活的数据采集：自动发现，支持falcon-agent、snmp、支持用户主动push、用户自定义插件支持、opentsdb data model like（timestamp、endpoint、metric、key-value tags）\n- 水平扩展能力：支持每个周期上亿次的数据采集、告警判定、历史数据存储和查询\n- 高效率的告警策略管理：高效的portal、支持策略模板、模板继承和覆盖、多种告警方式、支持callback调用\n- 人性化的告警设置：最大告警次数、告警级别、告警恢复通知、告警暂停、不同时段不同阈值、支持维护周期\n- 高效率的graph组件：单机支撑200万metric的上报、归档、存储（周期为1分钟）\n- 高效的历史数据query组件：采用rrdtool的数据归档策略，秒级返回上百个metric一年的历史数据\n- dashboard：多维度的数据展示，用户自定义Screen\n- 高可用：整个系统无核心单点，易运维，易部署，可水平扩展\n- 开发语言： 整个系统的后端，全部golang编写，portal和dashboard使用python编写。\n\n### 框架\n\n![func_tro](/img/openfalcon-func.png)\n\n\n\n<!-- more -->\n\n### 后端安装(docker)\n\n/opt/openfalcon/falcon-plus\n\n[官方教程](https://github.com/open-falcon/falcon-plus/blob/master/docker/README.md)\n\n```shell\n## Running falcon-plus container\n\n    docker pull openfalcon/falcon-plus:0.2.0\n    docker run -itd -p 8081:8081 openfalcon/falcon-plus:0.2.0 bash /run.sh hbs\n\n## Running falcon-plus container with docker-compose\n\n    docker-compose -f init.yml up -d falcon-plus\n\n## Running mysql and redis container\n\n    docker-compose -f init.yml up -d mysql redis\n\n## Stop and Remove containers\n\n    docker-compose -f init.yml rm -f\n```\n\ninit.yml\n\n```yaml\nversion: '2'\nservices:\n  mysql:\n    environment:\n    - MYSQL_ROOT_PASSWORD=password\n    extends:\n      file: common.yml\n      service: template\n    hostname: docker-mysql\n    image: mysql:5.7\n    labels:\n      owl: mysql\n    ports:\n    - 3306:3306\n    restart: always\n    volumes:\n    - ../scripts/mysql/db_schema:/docker-entrypoint-initdb.d\n    - ./mysql.cnf:/etc/mysql/conf.d/mysql.cnf:ro\n  redis:\n    command: redis-server /redis.conf\n    extends:\n      file: common.yml\n      service: template-backend\n    hostname: docker-redis\n    image: redis:3.0\n    labels:\n      owl: redis\n    ports:\n    - 6379:6379\n    restart: always\n    volumes:\n    - ./redis.conf:/redis.conf\n  falcon-plus:\n    command: bash /run.sh hbs\n    image: openfalcon/falcon-plus:0.2.0\n    ports:\n    - 8081:8081\n    restart: always\n```\n\n**注意**  需要打包mysql初始化脚本到这个路径下```../scripts/mysql/db_schema``` ，具体文件[在这里](https://github.com/open-falcon/falcon-plus/tree/master/scripts/mysql/db_schema)\n\nmysql登陆\n\n```\n#默认mysql密码  password\ndocker exec -it <容器id> mysql -p \n```\n\nweb 登陆 http://192.168.14.165:8081   新建用户\n\n![web](/img/openfalcon-web.png)\n\n### 前端安装(docker)\n\n/opt/openfalcon/dashboard\n\n[官方教程](https://github.com/open-falcon/dashboard/blob/master/README.md)\n\n```shell\n# make the image，run commands under dir of dashboard:\ndocker build -t falcon-dashboard:v1.0 .\n\n# start the container\ndocker run -itd --name aaa --net host \\\n\t-e API_ADDR=http://127.0.0.1:8080/api/v1 \\\n\t-e PORTAL_DB_HOST=127.0.0.1 \\\n\t-e PORTAL_DB_PORT=3306 \\\n\t-e PORTAL_DB_USER=root \\\n\t-e PORTAL_DB_PASS=123456 \\\n\t-e PORTAL_DB_NAME=falcon_portal \\\n\t-e ALARM_DB_PASS=123456 \\\n\t-e ALARM_DB_HOST=127.0.0.1 \\\n\t-e ALARM_DB_PORT=3306 \\\n\t-e ALARM_DB_USER=root \\\n\t-e ALARM_DB_PASS=123456 \\\n\t-e ALARM_DB_NAME=alarms \\\n\tfalcon-dashboard:v1.0\n```\n\nDockerfile\n\n```\nFROM centos:7.3.1611\n\nRUN yum clean all && yum install -y epel-release && yum -y update && \\\nyum install -y git python-virtualenv python-devel openldap-devel mysql-devel && \\\nyum groupinstall -y \"Development tools\"\n\nRUN export HOME=/home/work/ && mkdir -p $HOME/open-falcon/dashboard && cd $HOME/open-falcon/dashboard\nWORKDIR /home/work/open-falcon/dashboard\nADD ./ ./\nRUN virtualenv ./env && ./env/bin/pip install -r pip_requirements.txt -i http://pypi.douban.com/simple\n\nADD ./entrypoint.sh /entrypoint.sh\nRUN chmod +x /entrypoint.sh\n\nENTRYPOINT [\"/entrypoint.sh\"]\n```\n\n\n\n\n\n## 2.Openfalcon使用\n\n### 采集模式\n\n[入门手册](https://book.open-falcon.org/zh_0_2/usage/getting-started.html)\n\nfalcon-agent自动开始采集各项指标，主动上报，不需要用户在server做任何配置（这和zabbix有很大的不同），这样做的好处，就是用户维护方便，覆盖率高。当然这样做也会server端造成较大的压力，不过open-falcon的服务端组件单机性能足够高，同时都可以水平扩展，所以自动多采集足够多的数据，反而是一件好事情，对于SRE和DEV来讲，事后追查问题，不再是难题。\n\n另外，falcon-agent提供了一个proxy-gateway，用户可以方便的通过http接口，push数据到本机的gateway，gateway会帮忙高效率的转发到server端。\n\n\n\n**机器负载信息**\n\n这部分比较通用，我们提供了一个agent部署在所有机器上去采集。不像zabbix，要采集什么数据需要在服务端配置，falcon无需配置，只要agent部署到机器上，配置好heartbeat和Transfer地址，就自动开始采集了，省去了用户配置的麻烦。目前agent只支持64位Linux，Mac、Windows均不支持。\n\n**硬件信息**\n\n硬件信息的采集脚本由系统组同学提供，作为plugin依托于agent运行，plugin机制介绍请看[这里](http://book.open-falcon.org/zh_0_2/philosophy/plugin.html)。\n\n**服务监控数据**\n\n服务的监控指标采集脚本，通常都是跟着服务的code走的，服务上线或者扩容，这个脚本也跟着上线或者扩容，服务下线，这个采集脚本也要相应下线。公司里Java的项目有不少，研发那边就提供了一个通用jar包，只要引入这个jar包，就可以自动采集接口的调用次数、延迟时间等数据。然后将采集到的数据push给监控，一分钟push一次。目前falcon的agent提供了一个简单的http接口，这个jar包采集到数据之后是post给本机agent。向agent推送数据的一个简单例子，如下：\n\n```\ncurl -X POST -d '[{\"metric\": \"qps\", \"endpoint\": \"open-falcon-graph01.bj\", \"timestamp\": 1431347802, \"step\": 60,\"value\": 9,\"counterType\": \"GAUGE\",\"tags\": \"project=falcon,module=graph\"}]' http://127.0.0.1:1988/v1/push\n\n```\n\n**各种开源软件的监控指标**\n\n这都是大用户，比如DBA自己写一些采集脚本，连到各个MySQL实例上去采集数据，完事直接调用server端的jsonrpc汇报数据，一分钟一次，每次甚至push几十万条数据，比较好的发送方式是500条数据做一个batch，别几十万数据一次性发送。\n\n\n\n由上可知，falcon-agent本身仅支持主机类的指标采集，对于其他业务系统、开源软件，需独立的使用采集脚本，将数据按照规定格式，发送到falcon-agent 所在的监听端口。最终再由falcon-agent 转发到服务端。\n\n","slug":"open-falcon","published":1,"updated":"2018-10-20T08:15:43.513Z","_id":"cjhajczoy000se4rwaasmefg4","comments":1,"layout":"post","photos":[],"link":"","content":"<h2 id=\"1-Openfalcon安装部署\"><a href=\"#1-Openfalcon安装部署\" class=\"headerlink\" title=\"1.Openfalcon安装部署\"></a>1.Openfalcon安装部署</h2><h3 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h3><p><a href=\"https://book.open-falcon.org/zh_0_2/intro/\" target=\"_blank\" rel=\"noopener\">官方文档 v2.0</a></p>\n<ul>\n<li>强大灵活的数据采集：自动发现，支持falcon-agent、snmp、支持用户主动push、用户自定义插件支持、opentsdb data model like（timestamp、endpoint、metric、key-value tags）</li>\n<li>水平扩展能力：支持每个周期上亿次的数据采集、告警判定、历史数据存储和查询</li>\n<li>高效率的告警策略管理：高效的portal、支持策略模板、模板继承和覆盖、多种告警方式、支持callback调用</li>\n<li>人性化的告警设置：最大告警次数、告警级别、告警恢复通知、告警暂停、不同时段不同阈值、支持维护周期</li>\n<li>高效率的graph组件：单机支撑200万metric的上报、归档、存储（周期为1分钟）</li>\n<li>高效的历史数据query组件：采用rrdtool的数据归档策略，秒级返回上百个metric一年的历史数据</li>\n<li>dashboard：多维度的数据展示，用户自定义Screen</li>\n<li>高可用：整个系统无核心单点，易运维，易部署，可水平扩展</li>\n<li>开发语言： 整个系统的后端，全部golang编写，portal和dashboard使用python编写。</li>\n</ul>\n<h3 id=\"框架\"><a href=\"#框架\" class=\"headerlink\" title=\"框架\"></a>框架</h3><p><img src=\"/img/openfalcon-func.png\" alt=\"func_tro\"></p>\n<a id=\"more\"></a>\n<h3 id=\"后端安装-docker\"><a href=\"#后端安装-docker\" class=\"headerlink\" title=\"后端安装(docker)\"></a>后端安装(docker)</h3><p>/opt/openfalcon/falcon-plus</p>\n<p><a href=\"https://github.com/open-falcon/falcon-plus/blob/master/docker/README.md\" target=\"_blank\" rel=\"noopener\">官方教程</a></p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#</span># Running falcon-plus container</span><br><span class=\"line\"></span><br><span class=\"line\">    docker pull openfalcon/falcon-plus:0.2.0</span><br><span class=\"line\">    docker run -itd -p 8081:8081 openfalcon/falcon-plus:0.2.0 bash /run.sh hbs</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">#</span># Running falcon-plus container with docker-compose</span><br><span class=\"line\"></span><br><span class=\"line\">    docker-compose -f init.yml up -d falcon-plus</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">#</span># Running mysql and redis container</span><br><span class=\"line\"></span><br><span class=\"line\">    docker-compose -f init.yml up -d mysql redis</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">#</span># Stop and Remove containers</span><br><span class=\"line\"></span><br><span class=\"line\">    docker-compose -f init.yml rm -f</span><br></pre></td></tr></table></figure>\n<p>init.yml</p>\n<figure class=\"highlight yaml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">version:</span> <span class=\"string\">'2'</span></span><br><span class=\"line\"><span class=\"attr\">services:</span></span><br><span class=\"line\"><span class=\"attr\">  mysql:</span></span><br><span class=\"line\"><span class=\"attr\">    environment:</span></span><br><span class=\"line\"><span class=\"bullet\">    -</span> <span class=\"string\">MYSQL_ROOT_PASSWORD=password</span></span><br><span class=\"line\"><span class=\"attr\">    extends:</span></span><br><span class=\"line\"><span class=\"attr\">      file:</span> <span class=\"string\">common.yml</span></span><br><span class=\"line\"><span class=\"attr\">      service:</span> <span class=\"string\">template</span></span><br><span class=\"line\"><span class=\"attr\">    hostname:</span> <span class=\"string\">docker-mysql</span></span><br><span class=\"line\"><span class=\"attr\">    image:</span> <span class=\"attr\">mysql:5.7</span></span><br><span class=\"line\"><span class=\"attr\">    labels:</span></span><br><span class=\"line\"><span class=\"attr\">      owl:</span> <span class=\"string\">mysql</span></span><br><span class=\"line\"><span class=\"attr\">    ports:</span></span><br><span class=\"line\"><span class=\"bullet\">    -</span> <span class=\"number\">3306</span><span class=\"string\">:3306</span></span><br><span class=\"line\"><span class=\"attr\">    restart:</span> <span class=\"string\">always</span></span><br><span class=\"line\"><span class=\"attr\">    volumes:</span></span><br><span class=\"line\"><span class=\"bullet\">    -</span> <span class=\"string\">../scripts/mysql/db_schema:/docker-entrypoint-initdb.d</span></span><br><span class=\"line\"><span class=\"bullet\">    -</span> <span class=\"string\">./mysql.cnf:/etc/mysql/conf.d/mysql.cnf:ro</span></span><br><span class=\"line\"><span class=\"attr\">  redis:</span></span><br><span class=\"line\"><span class=\"attr\">    command:</span> <span class=\"string\">redis-server</span> <span class=\"string\">/redis.conf</span></span><br><span class=\"line\"><span class=\"attr\">    extends:</span></span><br><span class=\"line\"><span class=\"attr\">      file:</span> <span class=\"string\">common.yml</span></span><br><span class=\"line\"><span class=\"attr\">      service:</span> <span class=\"string\">template-backend</span></span><br><span class=\"line\"><span class=\"attr\">    hostname:</span> <span class=\"string\">docker-redis</span></span><br><span class=\"line\"><span class=\"attr\">    image:</span> <span class=\"attr\">redis:3.0</span></span><br><span class=\"line\"><span class=\"attr\">    labels:</span></span><br><span class=\"line\"><span class=\"attr\">      owl:</span> <span class=\"string\">redis</span></span><br><span class=\"line\"><span class=\"attr\">    ports:</span></span><br><span class=\"line\"><span class=\"bullet\">    -</span> <span class=\"number\">6379</span><span class=\"string\">:6379</span></span><br><span class=\"line\"><span class=\"attr\">    restart:</span> <span class=\"string\">always</span></span><br><span class=\"line\"><span class=\"attr\">    volumes:</span></span><br><span class=\"line\"><span class=\"bullet\">    -</span> <span class=\"string\">./redis.conf:/redis.conf</span></span><br><span class=\"line\"><span class=\"attr\">  falcon-plus:</span></span><br><span class=\"line\"><span class=\"attr\">    command:</span> <span class=\"string\">bash</span> <span class=\"string\">/run.sh</span> <span class=\"string\">hbs</span></span><br><span class=\"line\"><span class=\"attr\">    image:</span> <span class=\"string\">openfalcon/falcon-plus:0.2.0</span></span><br><span class=\"line\"><span class=\"attr\">    ports:</span></span><br><span class=\"line\"><span class=\"bullet\">    -</span> <span class=\"number\">8081</span><span class=\"string\">:8081</span></span><br><span class=\"line\"><span class=\"attr\">    restart:</span> <span class=\"string\">always</span></span><br></pre></td></tr></table></figure>\n<p><strong>注意</strong>  需要打包mysql初始化脚本到这个路径下<figure class=\"highlight plain\"><figcaption><span>，具体文件[在这里](https://github.com/open-falcon/falcon-plus/tree/master/scripts/mysql/db_schema)</span></figcaption><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">mysql登陆</span><br></pre></td></tr></table></figure></p>\n<p>#默认mysql密码  password<br>docker exec -it &lt;容器id&gt; mysql -p<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">web 登陆 http://192.168.14.165:8081   新建用户</span><br><span class=\"line\"></span><br><span class=\"line\">![web](/img/openfalcon-web.png)</span><br><span class=\"line\"></span><br><span class=\"line\">### 前端安装(docker)</span><br><span class=\"line\"></span><br><span class=\"line\">/opt/openfalcon/dashboard</span><br><span class=\"line\"></span><br><span class=\"line\">[官方教程](https://github.com/open-falcon/dashboard/blob/master/README.md)</span><br><span class=\"line\"></span><br><span class=\"line\">```shell</span><br><span class=\"line\"># make the image，run commands under dir of dashboard:</span><br><span class=\"line\">docker build -t falcon-dashboard:v1.0 .</span><br><span class=\"line\"></span><br><span class=\"line\"># start the container</span><br><span class=\"line\">docker run -itd --name aaa --net host \\</span><br><span class=\"line\">\t-e API_ADDR=http://127.0.0.1:8080/api/v1 \\</span><br><span class=\"line\">\t-e PORTAL_DB_HOST=127.0.0.1 \\</span><br><span class=\"line\">\t-e PORTAL_DB_PORT=3306 \\</span><br><span class=\"line\">\t-e PORTAL_DB_USER=root \\</span><br><span class=\"line\">\t-e PORTAL_DB_PASS=123456 \\</span><br><span class=\"line\">\t-e PORTAL_DB_NAME=falcon_portal \\</span><br><span class=\"line\">\t-e ALARM_DB_PASS=123456 \\</span><br><span class=\"line\">\t-e ALARM_DB_HOST=127.0.0.1 \\</span><br><span class=\"line\">\t-e ALARM_DB_PORT=3306 \\</span><br><span class=\"line\">\t-e ALARM_DB_USER=root \\</span><br><span class=\"line\">\t-e ALARM_DB_PASS=123456 \\</span><br><span class=\"line\">\t-e ALARM_DB_NAME=alarms \\</span><br><span class=\"line\">\tfalcon-dashboard:v1.0</span><br></pre></td></tr></table></figure></p>\n<p>Dockerfile</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">FROM centos:7.3.1611</span><br><span class=\"line\"></span><br><span class=\"line\">RUN yum clean all &amp;&amp; yum install -y epel-release &amp;&amp; yum -y update &amp;&amp; \\</span><br><span class=\"line\">yum install -y git python-virtualenv python-devel openldap-devel mysql-devel &amp;&amp; \\</span><br><span class=\"line\">yum groupinstall -y &quot;Development tools&quot;</span><br><span class=\"line\"></span><br><span class=\"line\">RUN export HOME=/home/work/ &amp;&amp; mkdir -p $HOME/open-falcon/dashboard &amp;&amp; cd $HOME/open-falcon/dashboard</span><br><span class=\"line\">WORKDIR /home/work/open-falcon/dashboard</span><br><span class=\"line\">ADD ./ ./</span><br><span class=\"line\">RUN virtualenv ./env &amp;&amp; ./env/bin/pip install -r pip_requirements.txt -i http://pypi.douban.com/simple</span><br><span class=\"line\"></span><br><span class=\"line\">ADD ./entrypoint.sh /entrypoint.sh</span><br><span class=\"line\">RUN chmod +x /entrypoint.sh</span><br><span class=\"line\"></span><br><span class=\"line\">ENTRYPOINT [&quot;/entrypoint.sh&quot;]</span><br></pre></td></tr></table></figure>\n<h2 id=\"2-Openfalcon使用\"><a href=\"#2-Openfalcon使用\" class=\"headerlink\" title=\"2.Openfalcon使用\"></a>2.Openfalcon使用</h2><h3 id=\"采集模式\"><a href=\"#采集模式\" class=\"headerlink\" title=\"采集模式\"></a>采集模式</h3><p><a href=\"https://book.open-falcon.org/zh_0_2/usage/getting-started.html\" target=\"_blank\" rel=\"noopener\">入门手册</a></p>\n<p>falcon-agent自动开始采集各项指标，主动上报，不需要用户在server做任何配置（这和zabbix有很大的不同），这样做的好处，就是用户维护方便，覆盖率高。当然这样做也会server端造成较大的压力，不过open-falcon的服务端组件单机性能足够高，同时都可以水平扩展，所以自动多采集足够多的数据，反而是一件好事情，对于SRE和DEV来讲，事后追查问题，不再是难题。</p>\n<p>另外，falcon-agent提供了一个proxy-gateway，用户可以方便的通过http接口，push数据到本机的gateway，gateway会帮忙高效率的转发到server端。</p>\n<p><strong>机器负载信息</strong></p>\n<p>这部分比较通用，我们提供了一个agent部署在所有机器上去采集。不像zabbix，要采集什么数据需要在服务端配置，falcon无需配置，只要agent部署到机器上，配置好heartbeat和Transfer地址，就自动开始采集了，省去了用户配置的麻烦。目前agent只支持64位Linux，Mac、Windows均不支持。</p>\n<p><strong>硬件信息</strong></p>\n<p>硬件信息的采集脚本由系统组同学提供，作为plugin依托于agent运行，plugin机制介绍请看<a href=\"http://book.open-falcon.org/zh_0_2/philosophy/plugin.html\" target=\"_blank\" rel=\"noopener\">这里</a>。</p>\n<p><strong>服务监控数据</strong></p>\n<p>服务的监控指标采集脚本，通常都是跟着服务的code走的，服务上线或者扩容，这个脚本也跟着上线或者扩容，服务下线，这个采集脚本也要相应下线。公司里Java的项目有不少，研发那边就提供了一个通用jar包，只要引入这个jar包，就可以自动采集接口的调用次数、延迟时间等数据。然后将采集到的数据push给监控，一分钟push一次。目前falcon的agent提供了一个简单的http接口，这个jar包采集到数据之后是post给本机agent。向agent推送数据的一个简单例子，如下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">curl -X POST -d &apos;[&#123;&quot;metric&quot;: &quot;qps&quot;, &quot;endpoint&quot;: &quot;open-falcon-graph01.bj&quot;, &quot;timestamp&quot;: 1431347802, &quot;step&quot;: 60,&quot;value&quot;: 9,&quot;counterType&quot;: &quot;GAUGE&quot;,&quot;tags&quot;: &quot;project=falcon,module=graph&quot;&#125;]&apos; http://127.0.0.1:1988/v1/push</span><br></pre></td></tr></table></figure>\n<p><strong>各种开源软件的监控指标</strong></p>\n<p>这都是大用户，比如DBA自己写一些采集脚本，连到各个MySQL实例上去采集数据，完事直接调用server端的jsonrpc汇报数据，一分钟一次，每次甚至push几十万条数据，比较好的发送方式是500条数据做一个batch，别几十万数据一次性发送。</p>\n<p>由上可知，falcon-agent本身仅支持主机类的指标采集，对于其他业务系统、开源软件，需独立的使用采集脚本，将数据按照规定格式，发送到falcon-agent 所在的监听端口。最终再由falcon-agent 转发到服务端。</p>\n","site":{"data":{}},"excerpt":"<h2 id=\"1-Openfalcon安装部署\"><a href=\"#1-Openfalcon安装部署\" class=\"headerlink\" title=\"1.Openfalcon安装部署\"></a>1.Openfalcon安装部署</h2><h3 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h3><p><a href=\"https://book.open-falcon.org/zh_0_2/intro/\" target=\"_blank\" rel=\"noopener\">官方文档 v2.0</a></p>\n<ul>\n<li>强大灵活的数据采集：自动发现，支持falcon-agent、snmp、支持用户主动push、用户自定义插件支持、opentsdb data model like（timestamp、endpoint、metric、key-value tags）</li>\n<li>水平扩展能力：支持每个周期上亿次的数据采集、告警判定、历史数据存储和查询</li>\n<li>高效率的告警策略管理：高效的portal、支持策略模板、模板继承和覆盖、多种告警方式、支持callback调用</li>\n<li>人性化的告警设置：最大告警次数、告警级别、告警恢复通知、告警暂停、不同时段不同阈值、支持维护周期</li>\n<li>高效率的graph组件：单机支撑200万metric的上报、归档、存储（周期为1分钟）</li>\n<li>高效的历史数据query组件：采用rrdtool的数据归档策略，秒级返回上百个metric一年的历史数据</li>\n<li>dashboard：多维度的数据展示，用户自定义Screen</li>\n<li>高可用：整个系统无核心单点，易运维，易部署，可水平扩展</li>\n<li>开发语言： 整个系统的后端，全部golang编写，portal和dashboard使用python编写。</li>\n</ul>\n<h3 id=\"框架\"><a href=\"#框架\" class=\"headerlink\" title=\"框架\"></a>框架</h3><p><img src=\"/img/openfalcon-func.png\" alt=\"func_tro\"></p>","more":"<h3 id=\"后端安装-docker\"><a href=\"#后端安装-docker\" class=\"headerlink\" title=\"后端安装(docker)\"></a>后端安装(docker)</h3><p>/opt/openfalcon/falcon-plus</p>\n<p><a href=\"https://github.com/open-falcon/falcon-plus/blob/master/docker/README.md\" target=\"_blank\" rel=\"noopener\">官方教程</a></p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#</span># Running falcon-plus container</span><br><span class=\"line\"></span><br><span class=\"line\">    docker pull openfalcon/falcon-plus:0.2.0</span><br><span class=\"line\">    docker run -itd -p 8081:8081 openfalcon/falcon-plus:0.2.0 bash /run.sh hbs</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">#</span># Running falcon-plus container with docker-compose</span><br><span class=\"line\"></span><br><span class=\"line\">    docker-compose -f init.yml up -d falcon-plus</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">#</span># Running mysql and redis container</span><br><span class=\"line\"></span><br><span class=\"line\">    docker-compose -f init.yml up -d mysql redis</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">#</span># Stop and Remove containers</span><br><span class=\"line\"></span><br><span class=\"line\">    docker-compose -f init.yml rm -f</span><br></pre></td></tr></table></figure>\n<p>init.yml</p>\n<figure class=\"highlight yaml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">version:</span> <span class=\"string\">'2'</span></span><br><span class=\"line\"><span class=\"attr\">services:</span></span><br><span class=\"line\"><span class=\"attr\">  mysql:</span></span><br><span class=\"line\"><span class=\"attr\">    environment:</span></span><br><span class=\"line\"><span class=\"bullet\">    -</span> <span class=\"string\">MYSQL_ROOT_PASSWORD=password</span></span><br><span class=\"line\"><span class=\"attr\">    extends:</span></span><br><span class=\"line\"><span class=\"attr\">      file:</span> <span class=\"string\">common.yml</span></span><br><span class=\"line\"><span class=\"attr\">      service:</span> <span class=\"string\">template</span></span><br><span class=\"line\"><span class=\"attr\">    hostname:</span> <span class=\"string\">docker-mysql</span></span><br><span class=\"line\"><span class=\"attr\">    image:</span> <span class=\"attr\">mysql:5.7</span></span><br><span class=\"line\"><span class=\"attr\">    labels:</span></span><br><span class=\"line\"><span class=\"attr\">      owl:</span> <span class=\"string\">mysql</span></span><br><span class=\"line\"><span class=\"attr\">    ports:</span></span><br><span class=\"line\"><span class=\"bullet\">    -</span> <span class=\"number\">3306</span><span class=\"string\">:3306</span></span><br><span class=\"line\"><span class=\"attr\">    restart:</span> <span class=\"string\">always</span></span><br><span class=\"line\"><span class=\"attr\">    volumes:</span></span><br><span class=\"line\"><span class=\"bullet\">    -</span> <span class=\"string\">../scripts/mysql/db_schema:/docker-entrypoint-initdb.d</span></span><br><span class=\"line\"><span class=\"bullet\">    -</span> <span class=\"string\">./mysql.cnf:/etc/mysql/conf.d/mysql.cnf:ro</span></span><br><span class=\"line\"><span class=\"attr\">  redis:</span></span><br><span class=\"line\"><span class=\"attr\">    command:</span> <span class=\"string\">redis-server</span> <span class=\"string\">/redis.conf</span></span><br><span class=\"line\"><span class=\"attr\">    extends:</span></span><br><span class=\"line\"><span class=\"attr\">      file:</span> <span class=\"string\">common.yml</span></span><br><span class=\"line\"><span class=\"attr\">      service:</span> <span class=\"string\">template-backend</span></span><br><span class=\"line\"><span class=\"attr\">    hostname:</span> <span class=\"string\">docker-redis</span></span><br><span class=\"line\"><span class=\"attr\">    image:</span> <span class=\"attr\">redis:3.0</span></span><br><span class=\"line\"><span class=\"attr\">    labels:</span></span><br><span class=\"line\"><span class=\"attr\">      owl:</span> <span class=\"string\">redis</span></span><br><span class=\"line\"><span class=\"attr\">    ports:</span></span><br><span class=\"line\"><span class=\"bullet\">    -</span> <span class=\"number\">6379</span><span class=\"string\">:6379</span></span><br><span class=\"line\"><span class=\"attr\">    restart:</span> <span class=\"string\">always</span></span><br><span class=\"line\"><span class=\"attr\">    volumes:</span></span><br><span class=\"line\"><span class=\"bullet\">    -</span> <span class=\"string\">./redis.conf:/redis.conf</span></span><br><span class=\"line\"><span class=\"attr\">  falcon-plus:</span></span><br><span class=\"line\"><span class=\"attr\">    command:</span> <span class=\"string\">bash</span> <span class=\"string\">/run.sh</span> <span class=\"string\">hbs</span></span><br><span class=\"line\"><span class=\"attr\">    image:</span> <span class=\"string\">openfalcon/falcon-plus:0.2.0</span></span><br><span class=\"line\"><span class=\"attr\">    ports:</span></span><br><span class=\"line\"><span class=\"bullet\">    -</span> <span class=\"number\">8081</span><span class=\"string\">:8081</span></span><br><span class=\"line\"><span class=\"attr\">    restart:</span> <span class=\"string\">always</span></span><br></pre></td></tr></table></figure>\n<p><strong>注意</strong>  需要打包mysql初始化脚本到这个路径下<figure class=\"highlight plain\"><figcaption><span>，具体文件[在这里](https://github.com/open-falcon/falcon-plus/tree/master/scripts/mysql/db_schema)</span></figcaption><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">mysql登陆</span><br></pre></td></tr></table></figure></p>\n<p>#默认mysql密码  password<br>docker exec -it &lt;容器id&gt; mysql -p<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">web 登陆 http://192.168.14.165:8081   新建用户</span><br><span class=\"line\"></span><br><span class=\"line\">![web](/img/openfalcon-web.png)</span><br><span class=\"line\"></span><br><span class=\"line\">### 前端安装(docker)</span><br><span class=\"line\"></span><br><span class=\"line\">/opt/openfalcon/dashboard</span><br><span class=\"line\"></span><br><span class=\"line\">[官方教程](https://github.com/open-falcon/dashboard/blob/master/README.md)</span><br><span class=\"line\"></span><br><span class=\"line\">```shell</span><br><span class=\"line\"># make the image，run commands under dir of dashboard:</span><br><span class=\"line\">docker build -t falcon-dashboard:v1.0 .</span><br><span class=\"line\"></span><br><span class=\"line\"># start the container</span><br><span class=\"line\">docker run -itd --name aaa --net host \\</span><br><span class=\"line\">\t-e API_ADDR=http://127.0.0.1:8080/api/v1 \\</span><br><span class=\"line\">\t-e PORTAL_DB_HOST=127.0.0.1 \\</span><br><span class=\"line\">\t-e PORTAL_DB_PORT=3306 \\</span><br><span class=\"line\">\t-e PORTAL_DB_USER=root \\</span><br><span class=\"line\">\t-e PORTAL_DB_PASS=123456 \\</span><br><span class=\"line\">\t-e PORTAL_DB_NAME=falcon_portal \\</span><br><span class=\"line\">\t-e ALARM_DB_PASS=123456 \\</span><br><span class=\"line\">\t-e ALARM_DB_HOST=127.0.0.1 \\</span><br><span class=\"line\">\t-e ALARM_DB_PORT=3306 \\</span><br><span class=\"line\">\t-e ALARM_DB_USER=root \\</span><br><span class=\"line\">\t-e ALARM_DB_PASS=123456 \\</span><br><span class=\"line\">\t-e ALARM_DB_NAME=alarms \\</span><br><span class=\"line\">\tfalcon-dashboard:v1.0</span><br></pre></td></tr></table></figure></p>\n<p>Dockerfile</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">FROM centos:7.3.1611</span><br><span class=\"line\"></span><br><span class=\"line\">RUN yum clean all &amp;&amp; yum install -y epel-release &amp;&amp; yum -y update &amp;&amp; \\</span><br><span class=\"line\">yum install -y git python-virtualenv python-devel openldap-devel mysql-devel &amp;&amp; \\</span><br><span class=\"line\">yum groupinstall -y &quot;Development tools&quot;</span><br><span class=\"line\"></span><br><span class=\"line\">RUN export HOME=/home/work/ &amp;&amp; mkdir -p $HOME/open-falcon/dashboard &amp;&amp; cd $HOME/open-falcon/dashboard</span><br><span class=\"line\">WORKDIR /home/work/open-falcon/dashboard</span><br><span class=\"line\">ADD ./ ./</span><br><span class=\"line\">RUN virtualenv ./env &amp;&amp; ./env/bin/pip install -r pip_requirements.txt -i http://pypi.douban.com/simple</span><br><span class=\"line\"></span><br><span class=\"line\">ADD ./entrypoint.sh /entrypoint.sh</span><br><span class=\"line\">RUN chmod +x /entrypoint.sh</span><br><span class=\"line\"></span><br><span class=\"line\">ENTRYPOINT [&quot;/entrypoint.sh&quot;]</span><br></pre></td></tr></table></figure>\n<h2 id=\"2-Openfalcon使用\"><a href=\"#2-Openfalcon使用\" class=\"headerlink\" title=\"2.Openfalcon使用\"></a>2.Openfalcon使用</h2><h3 id=\"采集模式\"><a href=\"#采集模式\" class=\"headerlink\" title=\"采集模式\"></a>采集模式</h3><p><a href=\"https://book.open-falcon.org/zh_0_2/usage/getting-started.html\" target=\"_blank\" rel=\"noopener\">入门手册</a></p>\n<p>falcon-agent自动开始采集各项指标，主动上报，不需要用户在server做任何配置（这和zabbix有很大的不同），这样做的好处，就是用户维护方便，覆盖率高。当然这样做也会server端造成较大的压力，不过open-falcon的服务端组件单机性能足够高，同时都可以水平扩展，所以自动多采集足够多的数据，反而是一件好事情，对于SRE和DEV来讲，事后追查问题，不再是难题。</p>\n<p>另外，falcon-agent提供了一个proxy-gateway，用户可以方便的通过http接口，push数据到本机的gateway，gateway会帮忙高效率的转发到server端。</p>\n<p><strong>机器负载信息</strong></p>\n<p>这部分比较通用，我们提供了一个agent部署在所有机器上去采集。不像zabbix，要采集什么数据需要在服务端配置，falcon无需配置，只要agent部署到机器上，配置好heartbeat和Transfer地址，就自动开始采集了，省去了用户配置的麻烦。目前agent只支持64位Linux，Mac、Windows均不支持。</p>\n<p><strong>硬件信息</strong></p>\n<p>硬件信息的采集脚本由系统组同学提供，作为plugin依托于agent运行，plugin机制介绍请看<a href=\"http://book.open-falcon.org/zh_0_2/philosophy/plugin.html\" target=\"_blank\" rel=\"noopener\">这里</a>。</p>\n<p><strong>服务监控数据</strong></p>\n<p>服务的监控指标采集脚本，通常都是跟着服务的code走的，服务上线或者扩容，这个脚本也跟着上线或者扩容，服务下线，这个采集脚本也要相应下线。公司里Java的项目有不少，研发那边就提供了一个通用jar包，只要引入这个jar包，就可以自动采集接口的调用次数、延迟时间等数据。然后将采集到的数据push给监控，一分钟push一次。目前falcon的agent提供了一个简单的http接口，这个jar包采集到数据之后是post给本机agent。向agent推送数据的一个简单例子，如下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">curl -X POST -d &apos;[&#123;&quot;metric&quot;: &quot;qps&quot;, &quot;endpoint&quot;: &quot;open-falcon-graph01.bj&quot;, &quot;timestamp&quot;: 1431347802, &quot;step&quot;: 60,&quot;value&quot;: 9,&quot;counterType&quot;: &quot;GAUGE&quot;,&quot;tags&quot;: &quot;project=falcon,module=graph&quot;&#125;]&apos; http://127.0.0.1:1988/v1/push</span><br></pre></td></tr></table></figure>\n<p><strong>各种开源软件的监控指标</strong></p>\n<p>这都是大用户，比如DBA自己写一些采集脚本，连到各个MySQL实例上去采集数据，完事直接调用server端的jsonrpc汇报数据，一分钟一次，每次甚至push几十万条数据，比较好的发送方式是500条数据做一个batch，别几十万数据一次性发送。</p>\n<p>由上可知，falcon-agent本身仅支持主机类的指标采集，对于其他业务系统、开源软件，需独立的使用采集脚本，将数据按照规定格式，发送到falcon-agent 所在的监听端口。最终再由falcon-agent 转发到服务端。</p>"},{"title":"凤凰项目-一个IT运维的传奇故事","date":"2018-05-17T12:51:43.000Z","_content":"\n本书讲述了一位IT经理临危受命，在未来董事的帮助和自己“三步工作法”理念的支撑下，最终挽救了一家具有悠久历史的汽车配件制造商的故事。小说揭示了管理现代IT组织与管理传统工厂的共通之处，让读者不仅能对如何管理IT组织心领神会，更重要的是将以完全不同于以往的视角来看待自己的工作环境。\n\n只有掌握了战术，才能实现战略目标。\n半成品是隐形杀手。\n约束理论：在瓶颈之外的任何地方作出的改进都是假象。在瓶颈之后作出的任何改进都是徒劳的，因为只能干等着瓶颈把工作传送过来。而在瓶颈之前作出的任何改进则只会导致瓶颈处堆积更多的库存。\n三步工作法：\n\n- 第一工作法帮助我们理解在工作从开发部移向运维部时该如何建立快速工作流，因为那就是业务部门与客户之间的衔接。\n- 第二工作法告诉我们如何缩短以及放大反馈环路，从而在源头上解决质量问题，避免返工。\n- 第三工作法告诉我们如何建立一种文化，既能鼓励探索、从失败中吸取教训，又能理解反复的实践是精通工作的先觉条件。","source":"_posts/phoenix.md","raw":"---\ntitle: 凤凰项目-一个IT运维的传奇故事\ndate: 2018-05-17 20:51:43\ntags: 随笔\n---\n\n本书讲述了一位IT经理临危受命，在未来董事的帮助和自己“三步工作法”理念的支撑下，最终挽救了一家具有悠久历史的汽车配件制造商的故事。小说揭示了管理现代IT组织与管理传统工厂的共通之处，让读者不仅能对如何管理IT组织心领神会，更重要的是将以完全不同于以往的视角来看待自己的工作环境。\n\n只有掌握了战术，才能实现战略目标。\n半成品是隐形杀手。\n约束理论：在瓶颈之外的任何地方作出的改进都是假象。在瓶颈之后作出的任何改进都是徒劳的，因为只能干等着瓶颈把工作传送过来。而在瓶颈之前作出的任何改进则只会导致瓶颈处堆积更多的库存。\n三步工作法：\n\n- 第一工作法帮助我们理解在工作从开发部移向运维部时该如何建立快速工作流，因为那就是业务部门与客户之间的衔接。\n- 第二工作法告诉我们如何缩短以及放大反馈环路，从而在源头上解决质量问题，避免返工。\n- 第三工作法告诉我们如何建立一种文化，既能鼓励探索、从失败中吸取教训，又能理解反复的实践是精通工作的先觉条件。","slug":"phoenix","published":1,"updated":"2018-10-20T08:15:43.513Z","_id":"cjhajczp0000we4rwkfbth5ah","comments":1,"layout":"post","photos":[],"link":"","content":"<p>本书讲述了一位IT经理临危受命，在未来董事的帮助和自己“三步工作法”理念的支撑下，最终挽救了一家具有悠久历史的汽车配件制造商的故事。小说揭示了管理现代IT组织与管理传统工厂的共通之处，让读者不仅能对如何管理IT组织心领神会，更重要的是将以完全不同于以往的视角来看待自己的工作环境。</p>\n<p>只有掌握了战术，才能实现战略目标。<br>半成品是隐形杀手。<br>约束理论：在瓶颈之外的任何地方作出的改进都是假象。在瓶颈之后作出的任何改进都是徒劳的，因为只能干等着瓶颈把工作传送过来。而在瓶颈之前作出的任何改进则只会导致瓶颈处堆积更多的库存。<br>三步工作法：</p>\n<ul>\n<li>第一工作法帮助我们理解在工作从开发部移向运维部时该如何建立快速工作流，因为那就是业务部门与客户之间的衔接。</li>\n<li>第二工作法告诉我们如何缩短以及放大反馈环路，从而在源头上解决质量问题，避免返工。</li>\n<li>第三工作法告诉我们如何建立一种文化，既能鼓励探索、从失败中吸取教训，又能理解反复的实践是精通工作的先觉条件。</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<p>本书讲述了一位IT经理临危受命，在未来董事的帮助和自己“三步工作法”理念的支撑下，最终挽救了一家具有悠久历史的汽车配件制造商的故事。小说揭示了管理现代IT组织与管理传统工厂的共通之处，让读者不仅能对如何管理IT组织心领神会，更重要的是将以完全不同于以往的视角来看待自己的工作环境。</p>\n<p>只有掌握了战术，才能实现战略目标。<br>半成品是隐形杀手。<br>约束理论：在瓶颈之外的任何地方作出的改进都是假象。在瓶颈之后作出的任何改进都是徒劳的，因为只能干等着瓶颈把工作传送过来。而在瓶颈之前作出的任何改进则只会导致瓶颈处堆积更多的库存。<br>三步工作法：</p>\n<ul>\n<li>第一工作法帮助我们理解在工作从开发部移向运维部时该如何建立快速工作流，因为那就是业务部门与客户之间的衔接。</li>\n<li>第二工作法告诉我们如何缩短以及放大反馈环路，从而在源头上解决质量问题，避免返工。</li>\n<li>第三工作法告诉我们如何建立一种文化，既能鼓励探索、从失败中吸取教训，又能理解反复的实践是精通工作的先觉条件。</li>\n</ul>\n"},{"title":"程序员的思维训练","date":"2015-04-20T13:05:49.000Z","_content":"这是一本关于训练程序员思维的书，当然也不仅仅局限于此。各行各业的本质很多都是相同。在本书中，作者提出了程序员的两大重要能力：沟通能力以及学习和思考能力。同时作者也提出了情境的重要性，因为事物都之间都是互相联系的，关注情境才能把握大局。\n![programmer thought](/img/programmer.png)\n\n<!-- more -->\n\n# 介绍从新手到专家的历程\n    德雷福斯模型:\n    新手：严重依赖与规则，只能按照规则行事，不能独立的思考以解决问题。\n    高级新手：没有全局思维，只关注自己的工作部分。\n    胜任者：可以独立解决问题，也开始思考如何解决新的问题，过于依赖以前的经验，缺乏反思和自我纠正。\n    精通：能够自我纠正，反思，并且改进，以期做得更好。\n    专家：总是不断的寻找更好的方法和方式去做事，有丰富的经验，可以在恰当的情境中选取和应用这些经验。专家根据直觉工作。专家知道哪些是无关紧要的细节，哪些是非常重要的细节。专家非常擅长做有针对性的特征匹配。举例说明，一个众所周知的极限编程方法经验之谈是“测试一切可能出错的东西”。对于新手来说，这只是一个指令清单。测试什么？是所有的setter和getter方法，还是只是打印语句?他们最终会测试所有无关的东西。但是处于精通水平的人员知道，什么地方有可能出错，或者更确切的说，什么地方非常有可能出错。他们具有经验和判断力，能够理解这句格言在情境中意味着什么。事实证明，理解情境是成为专家的关键。\n    新手到专家的转变，最重要的三个变化是：从依赖规则到依赖直觉；观念的转变，问题已不再是一个相关度等同的所有单元的集合体，而是一个完整和独特的整体，其中只有某些单元是相关的；最后，从问题的旁观者转变为问题设计的系统本身的一部分。\n    诀窍：专家依赖直觉。知道你不知道什么。通过观察和模仿来学习。学习如何学习的技能。\n# 认识你的大脑\n    大脑的结构：单总线，双CPU。左右脑的区别，一个处理逻辑的线性的L型，一个则是感性的非线性的R型。这两种模式都需要：R型对直觉、问题解决和创造性非常重要；L型让你细致工作并实现目标。R型侧重全局，L型则关注局部。R型是异步处理任务的，他实时的记录存储着我们所感知的一切，但是并没有建立索引，换言之，我们见过的一切实物，可能都已经深深的印在脑海中，只是大脑认为这些不重要，因而没有被立即索引，就被抛在脑后了，当哪天我们在努力思考解决一个问题的时候，R型就会在脑海里异步搜索，并在某个时刻，以灵光一闪的惊喜给你答案。因此，我们需要7*24记录想法。\n    诀窍：综合学习和分析学习并重。\n\n# 利用右脑\n    既然右脑（R型）如此重要，那么我们如何来训练，以期充分地发掘它的潜力呢？R型开路，L型紧跟。R型只能邀请，不能强制命令。\n    举例：乐高积木的角色扮演。攀岩的例子，教练先让我们体验了一把真正的攀岩，然后再来解释其中的技巧和关键。这实际上是建立了从R型到L型的转换\n    诀窍：增加感官体验以促进大脑的使用。R型开路，L型紧跟。使用隐喻作为R型和L型的融合之所。晨写日志。离开键盘去解决难题。改变解决问题的角度。培养快速的洞察能力，寻找不相关事物之间的关系或者类比。\n\n\n\n# 调试大脑\n    如何调试大脑：认知偏见，思维如何被误导；时代影响，同代人如何影响你；个性倾向，个性如何影响思维；硬件故障，大脑较老区域如何压制脚聪明的区域。\n    诀窍：像高级动物一样行动，请做深呼吸，而不要张口嘶鸣。相信直觉，但是要验证。\n# 积极学习\n    大脑不是一个用于填充的容器，而是一束需要点燃的火焰。\n    瞄准SMART目标。SMART代表具体的、可度量的、可实现的、相关的和时间可控的（Specific,Measurable,Achieveable,Relevant and Time-boxed）..使用你的原生学习模式：视觉型，听觉型和动觉型。使用增强的学习方法，既然已经建立了主动学习的良好框架，我们需要看看学习本身。以下是好方法：\n\t主动阅读和总结书面材料的更好方式：使用SQ3R法主动阅读，调查（Survey）：扫描目录和每章总结，得出总体看法；问题（Question）：记录所有的问题；阅读（Read）:阅读全部内容；复述（Recite）：总结，做笔记，用自己的话来描述；回顾（Review）：重读，拓展笔记，与同时讨论。具体例子：假设我正在阅读一本有关于新编程语言D,Erlang或者Ruby的书。我翻阅目录，看看书的主要内容。噢，一些语法的介绍，几个简单项目，还有我目前不感兴趣的高级特性。嗯，它是单继承、多继承还是混合的？我想知道迭代器在该语言中是怎么用的？如何创建和管理包或者模块？运行时性能如何？接下来看书-如果可能的话就多看点，如果时间紧的话就少看点。接下来是复述，即改写。整个事件流听起来很熟悉，是吗？我想它清晰的反映了R型到L型的转换，就像攀岩体验一样，首先是一种全盘、浅显但是广泛的调查，然后转换到传统的L型活动，扩大到多重感官的参与（讨论，笔记，图片，隐喻等。）\n\t使用思维导图探索和发现模式和关系。 手写思维导图，更加的形象，易于被大脑（R型）接受。\n\t以教代学。和橡皮鸭聊天。学习某项实物的最简单和有效的方法就是尝试教别人。\n    诀窍：写文档的过程比文档本身更重要。观察，实践，教学。\n\n# 积累经验\n    积累经验是学习和成长的关键--我们通过实践的方法学习，效果最好。运用内在诀窍的关键要素：不要把精力放在纠正一个一个的细节上，只需要具有意识。举例：在采取纠正行动之前，先知道“这是什么”对于调试非常重要。太多的程序员往往没有完全明白正真的错误是什么就着急修正它。匆忙的做出判断或者过早的就进行修补。你需要首先完全明白系统的原理，然后在判断哪部分错了，最后提供解决方案。现在闭上眼睛，想象一下错误代码的位置。把它看作是地震的震中，你可能感觉到地面到处在抖动，但是震中最为明显。出错的代码应该是什么样的？周围的代码呢？\n    诀窍：为了更好的学习，请更好的玩。从相似点中学习，从差异中忘却。利用大脑模拟成功，像专家一样学习。\n# 控制注意力\n    通过以下三点更好的管理思维：增强注意力，管理你的知识，优化当前情境。\n    其实就是减少干扰，营造学习的良好环境。控制注意力，有助于我们更好的学习。\n    记住这三件事情：1.学会安抚喋喋不休的L型思维；2主动在前进中思考和增强思想，即使是不成熟的；3明确情境切换的昂贵代价，尽可能的避免。\n# 超越专家\n    保持好奇心，学习新知识，成为新的“新手”。认识你自己，认识当前时刻，认识你所处的情境。自由的代价是永远提高警惕。\n\n\n","source":"_posts/programmer-thought.md","raw":"---\ntitle: 程序员的思维训练\ndate: 2015-04-20 21:05:49\ntags: 随笔\n---\n这是一本关于训练程序员思维的书，当然也不仅仅局限于此。各行各业的本质很多都是相同。在本书中，作者提出了程序员的两大重要能力：沟通能力以及学习和思考能力。同时作者也提出了情境的重要性，因为事物都之间都是互相联系的，关注情境才能把握大局。\n![programmer thought](/img/programmer.png)\n\n<!-- more -->\n\n# 介绍从新手到专家的历程\n    德雷福斯模型:\n    新手：严重依赖与规则，只能按照规则行事，不能独立的思考以解决问题。\n    高级新手：没有全局思维，只关注自己的工作部分。\n    胜任者：可以独立解决问题，也开始思考如何解决新的问题，过于依赖以前的经验，缺乏反思和自我纠正。\n    精通：能够自我纠正，反思，并且改进，以期做得更好。\n    专家：总是不断的寻找更好的方法和方式去做事，有丰富的经验，可以在恰当的情境中选取和应用这些经验。专家根据直觉工作。专家知道哪些是无关紧要的细节，哪些是非常重要的细节。专家非常擅长做有针对性的特征匹配。举例说明，一个众所周知的极限编程方法经验之谈是“测试一切可能出错的东西”。对于新手来说，这只是一个指令清单。测试什么？是所有的setter和getter方法，还是只是打印语句?他们最终会测试所有无关的东西。但是处于精通水平的人员知道，什么地方有可能出错，或者更确切的说，什么地方非常有可能出错。他们具有经验和判断力，能够理解这句格言在情境中意味着什么。事实证明，理解情境是成为专家的关键。\n    新手到专家的转变，最重要的三个变化是：从依赖规则到依赖直觉；观念的转变，问题已不再是一个相关度等同的所有单元的集合体，而是一个完整和独特的整体，其中只有某些单元是相关的；最后，从问题的旁观者转变为问题设计的系统本身的一部分。\n    诀窍：专家依赖直觉。知道你不知道什么。通过观察和模仿来学习。学习如何学习的技能。\n# 认识你的大脑\n    大脑的结构：单总线，双CPU。左右脑的区别，一个处理逻辑的线性的L型，一个则是感性的非线性的R型。这两种模式都需要：R型对直觉、问题解决和创造性非常重要；L型让你细致工作并实现目标。R型侧重全局，L型则关注局部。R型是异步处理任务的，他实时的记录存储着我们所感知的一切，但是并没有建立索引，换言之，我们见过的一切实物，可能都已经深深的印在脑海中，只是大脑认为这些不重要，因而没有被立即索引，就被抛在脑后了，当哪天我们在努力思考解决一个问题的时候，R型就会在脑海里异步搜索，并在某个时刻，以灵光一闪的惊喜给你答案。因此，我们需要7*24记录想法。\n    诀窍：综合学习和分析学习并重。\n\n# 利用右脑\n    既然右脑（R型）如此重要，那么我们如何来训练，以期充分地发掘它的潜力呢？R型开路，L型紧跟。R型只能邀请，不能强制命令。\n    举例：乐高积木的角色扮演。攀岩的例子，教练先让我们体验了一把真正的攀岩，然后再来解释其中的技巧和关键。这实际上是建立了从R型到L型的转换\n    诀窍：增加感官体验以促进大脑的使用。R型开路，L型紧跟。使用隐喻作为R型和L型的融合之所。晨写日志。离开键盘去解决难题。改变解决问题的角度。培养快速的洞察能力，寻找不相关事物之间的关系或者类比。\n\n\n\n# 调试大脑\n    如何调试大脑：认知偏见，思维如何被误导；时代影响，同代人如何影响你；个性倾向，个性如何影响思维；硬件故障，大脑较老区域如何压制脚聪明的区域。\n    诀窍：像高级动物一样行动，请做深呼吸，而不要张口嘶鸣。相信直觉，但是要验证。\n# 积极学习\n    大脑不是一个用于填充的容器，而是一束需要点燃的火焰。\n    瞄准SMART目标。SMART代表具体的、可度量的、可实现的、相关的和时间可控的（Specific,Measurable,Achieveable,Relevant and Time-boxed）..使用你的原生学习模式：视觉型，听觉型和动觉型。使用增强的学习方法，既然已经建立了主动学习的良好框架，我们需要看看学习本身。以下是好方法：\n\t主动阅读和总结书面材料的更好方式：使用SQ3R法主动阅读，调查（Survey）：扫描目录和每章总结，得出总体看法；问题（Question）：记录所有的问题；阅读（Read）:阅读全部内容；复述（Recite）：总结，做笔记，用自己的话来描述；回顾（Review）：重读，拓展笔记，与同时讨论。具体例子：假设我正在阅读一本有关于新编程语言D,Erlang或者Ruby的书。我翻阅目录，看看书的主要内容。噢，一些语法的介绍，几个简单项目，还有我目前不感兴趣的高级特性。嗯，它是单继承、多继承还是混合的？我想知道迭代器在该语言中是怎么用的？如何创建和管理包或者模块？运行时性能如何？接下来看书-如果可能的话就多看点，如果时间紧的话就少看点。接下来是复述，即改写。整个事件流听起来很熟悉，是吗？我想它清晰的反映了R型到L型的转换，就像攀岩体验一样，首先是一种全盘、浅显但是广泛的调查，然后转换到传统的L型活动，扩大到多重感官的参与（讨论，笔记，图片，隐喻等。）\n\t使用思维导图探索和发现模式和关系。 手写思维导图，更加的形象，易于被大脑（R型）接受。\n\t以教代学。和橡皮鸭聊天。学习某项实物的最简单和有效的方法就是尝试教别人。\n    诀窍：写文档的过程比文档本身更重要。观察，实践，教学。\n\n# 积累经验\n    积累经验是学习和成长的关键--我们通过实践的方法学习，效果最好。运用内在诀窍的关键要素：不要把精力放在纠正一个一个的细节上，只需要具有意识。举例：在采取纠正行动之前，先知道“这是什么”对于调试非常重要。太多的程序员往往没有完全明白正真的错误是什么就着急修正它。匆忙的做出判断或者过早的就进行修补。你需要首先完全明白系统的原理，然后在判断哪部分错了，最后提供解决方案。现在闭上眼睛，想象一下错误代码的位置。把它看作是地震的震中，你可能感觉到地面到处在抖动，但是震中最为明显。出错的代码应该是什么样的？周围的代码呢？\n    诀窍：为了更好的学习，请更好的玩。从相似点中学习，从差异中忘却。利用大脑模拟成功，像专家一样学习。\n# 控制注意力\n    通过以下三点更好的管理思维：增强注意力，管理你的知识，优化当前情境。\n    其实就是减少干扰，营造学习的良好环境。控制注意力，有助于我们更好的学习。\n    记住这三件事情：1.学会安抚喋喋不休的L型思维；2主动在前进中思考和增强思想，即使是不成熟的；3明确情境切换的昂贵代价，尽可能的避免。\n# 超越专家\n    保持好奇心，学习新知识，成为新的“新手”。认识你自己，认识当前时刻，认识你所处的情境。自由的代价是永远提高警惕。\n\n\n","slug":"programmer-thought","published":1,"updated":"2018-04-27T04:11:03.364Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjhajczp2000ye4rw357hwar7","content":"<p>这是一本关于训练程序员思维的书，当然也不仅仅局限于此。各行各业的本质很多都是相同。在本书中，作者提出了程序员的两大重要能力：沟通能力以及学习和思考能力。同时作者也提出了情境的重要性，因为事物都之间都是互相联系的，关注情境才能把握大局。<br><img src=\"/img/programmer.png\" alt=\"programmer thought\"></p>\n<a id=\"more\"></a>\n<h1 id=\"介绍从新手到专家的历程\"><a href=\"#介绍从新手到专家的历程\" class=\"headerlink\" title=\"介绍从新手到专家的历程\"></a>介绍从新手到专家的历程</h1><pre><code>德雷福斯模型:\n新手：严重依赖与规则，只能按照规则行事，不能独立的思考以解决问题。\n高级新手：没有全局思维，只关注自己的工作部分。\n胜任者：可以独立解决问题，也开始思考如何解决新的问题，过于依赖以前的经验，缺乏反思和自我纠正。\n精通：能够自我纠正，反思，并且改进，以期做得更好。\n专家：总是不断的寻找更好的方法和方式去做事，有丰富的经验，可以在恰当的情境中选取和应用这些经验。专家根据直觉工作。专家知道哪些是无关紧要的细节，哪些是非常重要的细节。专家非常擅长做有针对性的特征匹配。举例说明，一个众所周知的极限编程方法经验之谈是“测试一切可能出错的东西”。对于新手来说，这只是一个指令清单。测试什么？是所有的setter和getter方法，还是只是打印语句?他们最终会测试所有无关的东西。但是处于精通水平的人员知道，什么地方有可能出错，或者更确切的说，什么地方非常有可能出错。他们具有经验和判断力，能够理解这句格言在情境中意味着什么。事实证明，理解情境是成为专家的关键。\n新手到专家的转变，最重要的三个变化是：从依赖规则到依赖直觉；观念的转变，问题已不再是一个相关度等同的所有单元的集合体，而是一个完整和独特的整体，其中只有某些单元是相关的；最后，从问题的旁观者转变为问题设计的系统本身的一部分。\n诀窍：专家依赖直觉。知道你不知道什么。通过观察和模仿来学习。学习如何学习的技能。\n</code></pre><h1 id=\"认识你的大脑\"><a href=\"#认识你的大脑\" class=\"headerlink\" title=\"认识你的大脑\"></a>认识你的大脑</h1><pre><code>大脑的结构：单总线，双CPU。左右脑的区别，一个处理逻辑的线性的L型，一个则是感性的非线性的R型。这两种模式都需要：R型对直觉、问题解决和创造性非常重要；L型让你细致工作并实现目标。R型侧重全局，L型则关注局部。R型是异步处理任务的，他实时的记录存储着我们所感知的一切，但是并没有建立索引，换言之，我们见过的一切实物，可能都已经深深的印在脑海中，只是大脑认为这些不重要，因而没有被立即索引，就被抛在脑后了，当哪天我们在努力思考解决一个问题的时候，R型就会在脑海里异步搜索，并在某个时刻，以灵光一闪的惊喜给你答案。因此，我们需要7*24记录想法。\n诀窍：综合学习和分析学习并重。\n</code></pre><h1 id=\"利用右脑\"><a href=\"#利用右脑\" class=\"headerlink\" title=\"利用右脑\"></a>利用右脑</h1><pre><code>既然右脑（R型）如此重要，那么我们如何来训练，以期充分地发掘它的潜力呢？R型开路，L型紧跟。R型只能邀请，不能强制命令。\n举例：乐高积木的角色扮演。攀岩的例子，教练先让我们体验了一把真正的攀岩，然后再来解释其中的技巧和关键。这实际上是建立了从R型到L型的转换\n诀窍：增加感官体验以促进大脑的使用。R型开路，L型紧跟。使用隐喻作为R型和L型的融合之所。晨写日志。离开键盘去解决难题。改变解决问题的角度。培养快速的洞察能力，寻找不相关事物之间的关系或者类比。\n</code></pre><h1 id=\"调试大脑\"><a href=\"#调试大脑\" class=\"headerlink\" title=\"调试大脑\"></a>调试大脑</h1><pre><code>如何调试大脑：认知偏见，思维如何被误导；时代影响，同代人如何影响你；个性倾向，个性如何影响思维；硬件故障，大脑较老区域如何压制脚聪明的区域。\n诀窍：像高级动物一样行动，请做深呼吸，而不要张口嘶鸣。相信直觉，但是要验证。\n</code></pre><h1 id=\"积极学习\"><a href=\"#积极学习\" class=\"headerlink\" title=\"积极学习\"></a>积极学习</h1><pre><code>大脑不是一个用于填充的容器，而是一束需要点燃的火焰。\n瞄准SMART目标。SMART代表具体的、可度量的、可实现的、相关的和时间可控的（Specific,Measurable,Achieveable,Relevant and Time-boxed）..使用你的原生学习模式：视觉型，听觉型和动觉型。使用增强的学习方法，既然已经建立了主动学习的良好框架，我们需要看看学习本身。以下是好方法：\n主动阅读和总结书面材料的更好方式：使用SQ3R法主动阅读，调查（Survey）：扫描目录和每章总结，得出总体看法；问题（Question）：记录所有的问题；阅读（Read）:阅读全部内容；复述（Recite）：总结，做笔记，用自己的话来描述；回顾（Review）：重读，拓展笔记，与同时讨论。具体例子：假设我正在阅读一本有关于新编程语言D,Erlang或者Ruby的书。我翻阅目录，看看书的主要内容。噢，一些语法的介绍，几个简单项目，还有我目前不感兴趣的高级特性。嗯，它是单继承、多继承还是混合的？我想知道迭代器在该语言中是怎么用的？如何创建和管理包或者模块？运行时性能如何？接下来看书-如果可能的话就多看点，如果时间紧的话就少看点。接下来是复述，即改写。整个事件流听起来很熟悉，是吗？我想它清晰的反映了R型到L型的转换，就像攀岩体验一样，首先是一种全盘、浅显但是广泛的调查，然后转换到传统的L型活动，扩大到多重感官的参与（讨论，笔记，图片，隐喻等。）\n使用思维导图探索和发现模式和关系。 手写思维导图，更加的形象，易于被大脑（R型）接受。\n以教代学。和橡皮鸭聊天。学习某项实物的最简单和有效的方法就是尝试教别人。\n诀窍：写文档的过程比文档本身更重要。观察，实践，教学。\n</code></pre><h1 id=\"积累经验\"><a href=\"#积累经验\" class=\"headerlink\" title=\"积累经验\"></a>积累经验</h1><pre><code>积累经验是学习和成长的关键--我们通过实践的方法学习，效果最好。运用内在诀窍的关键要素：不要把精力放在纠正一个一个的细节上，只需要具有意识。举例：在采取纠正行动之前，先知道“这是什么”对于调试非常重要。太多的程序员往往没有完全明白正真的错误是什么就着急修正它。匆忙的做出判断或者过早的就进行修补。你需要首先完全明白系统的原理，然后在判断哪部分错了，最后提供解决方案。现在闭上眼睛，想象一下错误代码的位置。把它看作是地震的震中，你可能感觉到地面到处在抖动，但是震中最为明显。出错的代码应该是什么样的？周围的代码呢？\n诀窍：为了更好的学习，请更好的玩。从相似点中学习，从差异中忘却。利用大脑模拟成功，像专家一样学习。\n</code></pre><h1 id=\"控制注意力\"><a href=\"#控制注意力\" class=\"headerlink\" title=\"控制注意力\"></a>控制注意力</h1><pre><code>通过以下三点更好的管理思维：增强注意力，管理你的知识，优化当前情境。\n其实就是减少干扰，营造学习的良好环境。控制注意力，有助于我们更好的学习。\n记住这三件事情：1.学会安抚喋喋不休的L型思维；2主动在前进中思考和增强思想，即使是不成熟的；3明确情境切换的昂贵代价，尽可能的避免。\n</code></pre><h1 id=\"超越专家\"><a href=\"#超越专家\" class=\"headerlink\" title=\"超越专家\"></a>超越专家</h1><pre><code>保持好奇心，学习新知识，成为新的“新手”。认识你自己，认识当前时刻，认识你所处的情境。自由的代价是永远提高警惕。\n</code></pre>","site":{"data":{}},"excerpt":"<p>这是一本关于训练程序员思维的书，当然也不仅仅局限于此。各行各业的本质很多都是相同。在本书中，作者提出了程序员的两大重要能力：沟通能力以及学习和思考能力。同时作者也提出了情境的重要性，因为事物都之间都是互相联系的，关注情境才能把握大局。<br><img src=\"/img/programmer.png\" alt=\"programmer thought\"></p>","more":"<h1 id=\"介绍从新手到专家的历程\"><a href=\"#介绍从新手到专家的历程\" class=\"headerlink\" title=\"介绍从新手到专家的历程\"></a>介绍从新手到专家的历程</h1><pre><code>德雷福斯模型:\n新手：严重依赖与规则，只能按照规则行事，不能独立的思考以解决问题。\n高级新手：没有全局思维，只关注自己的工作部分。\n胜任者：可以独立解决问题，也开始思考如何解决新的问题，过于依赖以前的经验，缺乏反思和自我纠正。\n精通：能够自我纠正，反思，并且改进，以期做得更好。\n专家：总是不断的寻找更好的方法和方式去做事，有丰富的经验，可以在恰当的情境中选取和应用这些经验。专家根据直觉工作。专家知道哪些是无关紧要的细节，哪些是非常重要的细节。专家非常擅长做有针对性的特征匹配。举例说明，一个众所周知的极限编程方法经验之谈是“测试一切可能出错的东西”。对于新手来说，这只是一个指令清单。测试什么？是所有的setter和getter方法，还是只是打印语句?他们最终会测试所有无关的东西。但是处于精通水平的人员知道，什么地方有可能出错，或者更确切的说，什么地方非常有可能出错。他们具有经验和判断力，能够理解这句格言在情境中意味着什么。事实证明，理解情境是成为专家的关键。\n新手到专家的转变，最重要的三个变化是：从依赖规则到依赖直觉；观念的转变，问题已不再是一个相关度等同的所有单元的集合体，而是一个完整和独特的整体，其中只有某些单元是相关的；最后，从问题的旁观者转变为问题设计的系统本身的一部分。\n诀窍：专家依赖直觉。知道你不知道什么。通过观察和模仿来学习。学习如何学习的技能。\n</code></pre><h1 id=\"认识你的大脑\"><a href=\"#认识你的大脑\" class=\"headerlink\" title=\"认识你的大脑\"></a>认识你的大脑</h1><pre><code>大脑的结构：单总线，双CPU。左右脑的区别，一个处理逻辑的线性的L型，一个则是感性的非线性的R型。这两种模式都需要：R型对直觉、问题解决和创造性非常重要；L型让你细致工作并实现目标。R型侧重全局，L型则关注局部。R型是异步处理任务的，他实时的记录存储着我们所感知的一切，但是并没有建立索引，换言之，我们见过的一切实物，可能都已经深深的印在脑海中，只是大脑认为这些不重要，因而没有被立即索引，就被抛在脑后了，当哪天我们在努力思考解决一个问题的时候，R型就会在脑海里异步搜索，并在某个时刻，以灵光一闪的惊喜给你答案。因此，我们需要7*24记录想法。\n诀窍：综合学习和分析学习并重。\n</code></pre><h1 id=\"利用右脑\"><a href=\"#利用右脑\" class=\"headerlink\" title=\"利用右脑\"></a>利用右脑</h1><pre><code>既然右脑（R型）如此重要，那么我们如何来训练，以期充分地发掘它的潜力呢？R型开路，L型紧跟。R型只能邀请，不能强制命令。\n举例：乐高积木的角色扮演。攀岩的例子，教练先让我们体验了一把真正的攀岩，然后再来解释其中的技巧和关键。这实际上是建立了从R型到L型的转换\n诀窍：增加感官体验以促进大脑的使用。R型开路，L型紧跟。使用隐喻作为R型和L型的融合之所。晨写日志。离开键盘去解决难题。改变解决问题的角度。培养快速的洞察能力，寻找不相关事物之间的关系或者类比。\n</code></pre><h1 id=\"调试大脑\"><a href=\"#调试大脑\" class=\"headerlink\" title=\"调试大脑\"></a>调试大脑</h1><pre><code>如何调试大脑：认知偏见，思维如何被误导；时代影响，同代人如何影响你；个性倾向，个性如何影响思维；硬件故障，大脑较老区域如何压制脚聪明的区域。\n诀窍：像高级动物一样行动，请做深呼吸，而不要张口嘶鸣。相信直觉，但是要验证。\n</code></pre><h1 id=\"积极学习\"><a href=\"#积极学习\" class=\"headerlink\" title=\"积极学习\"></a>积极学习</h1><pre><code>大脑不是一个用于填充的容器，而是一束需要点燃的火焰。\n瞄准SMART目标。SMART代表具体的、可度量的、可实现的、相关的和时间可控的（Specific,Measurable,Achieveable,Relevant and Time-boxed）..使用你的原生学习模式：视觉型，听觉型和动觉型。使用增强的学习方法，既然已经建立了主动学习的良好框架，我们需要看看学习本身。以下是好方法：\n主动阅读和总结书面材料的更好方式：使用SQ3R法主动阅读，调查（Survey）：扫描目录和每章总结，得出总体看法；问题（Question）：记录所有的问题；阅读（Read）:阅读全部内容；复述（Recite）：总结，做笔记，用自己的话来描述；回顾（Review）：重读，拓展笔记，与同时讨论。具体例子：假设我正在阅读一本有关于新编程语言D,Erlang或者Ruby的书。我翻阅目录，看看书的主要内容。噢，一些语法的介绍，几个简单项目，还有我目前不感兴趣的高级特性。嗯，它是单继承、多继承还是混合的？我想知道迭代器在该语言中是怎么用的？如何创建和管理包或者模块？运行时性能如何？接下来看书-如果可能的话就多看点，如果时间紧的话就少看点。接下来是复述，即改写。整个事件流听起来很熟悉，是吗？我想它清晰的反映了R型到L型的转换，就像攀岩体验一样，首先是一种全盘、浅显但是广泛的调查，然后转换到传统的L型活动，扩大到多重感官的参与（讨论，笔记，图片，隐喻等。）\n使用思维导图探索和发现模式和关系。 手写思维导图，更加的形象，易于被大脑（R型）接受。\n以教代学。和橡皮鸭聊天。学习某项实物的最简单和有效的方法就是尝试教别人。\n诀窍：写文档的过程比文档本身更重要。观察，实践，教学。\n</code></pre><h1 id=\"积累经验\"><a href=\"#积累经验\" class=\"headerlink\" title=\"积累经验\"></a>积累经验</h1><pre><code>积累经验是学习和成长的关键--我们通过实践的方法学习，效果最好。运用内在诀窍的关键要素：不要把精力放在纠正一个一个的细节上，只需要具有意识。举例：在采取纠正行动之前，先知道“这是什么”对于调试非常重要。太多的程序员往往没有完全明白正真的错误是什么就着急修正它。匆忙的做出判断或者过早的就进行修补。你需要首先完全明白系统的原理，然后在判断哪部分错了，最后提供解决方案。现在闭上眼睛，想象一下错误代码的位置。把它看作是地震的震中，你可能感觉到地面到处在抖动，但是震中最为明显。出错的代码应该是什么样的？周围的代码呢？\n诀窍：为了更好的学习，请更好的玩。从相似点中学习，从差异中忘却。利用大脑模拟成功，像专家一样学习。\n</code></pre><h1 id=\"控制注意力\"><a href=\"#控制注意力\" class=\"headerlink\" title=\"控制注意力\"></a>控制注意力</h1><pre><code>通过以下三点更好的管理思维：增强注意力，管理你的知识，优化当前情境。\n其实就是减少干扰，营造学习的良好环境。控制注意力，有助于我们更好的学习。\n记住这三件事情：1.学会安抚喋喋不休的L型思维；2主动在前进中思考和增强思想，即使是不成熟的；3明确情境切换的昂贵代价，尽可能的避免。\n</code></pre><h1 id=\"超越专家\"><a href=\"#超越专家\" class=\"headerlink\" title=\"超越专家\"></a>超越专家</h1><pre><code>保持好奇心，学习新知识，成为新的“新手”。认识你自己，认识当前时刻，认识你所处的情境。自由的代价是永远提高警惕。\n</code></pre>"},{"title":"Prometheus 监控体系","date":"2018-03-25T07:48:29.000Z","_content":"## 1 概述\n\n### 1.1 主要功能\n\n- 多维 [数据模型](https://prometheus.io/docs/concepts/data_model/)（时序由 metric 名字和 k/v 的 labels 构成）。\n- 灵活的查询语句（[PromQL](https://prometheus.io/docs/querying/basics/)）。\n- 无依赖存储，支持 local 和 remote 不同模型。\n- 采用 http 协议，使用 pull 模式，拉取数据，简单易懂。\n- 监控目标，可以采用服务发现或静态配置的方式。\n- 支持多种统计数据模型，图形化友好。\n\n### 1.2 核心组件\n\n- [Prometheus Server](https://github.com/prometheus/prometheus)， 主要用于抓取数据和存储时序数据，另外还提供查询和 Alert Rule 配置管理。\n- [client libraries](https://prometheus.io/docs/instrumenting/clientlibs/)，用于对接 Prometheus Server, 可以查询和上报数据。\n- [push gateway](https://github.com/prometheus/pushgateway) ，用于批量，短期的监控数据的汇总节点，主要用于业务数据汇报等。\n- 各种汇报数据的 [exporters](https://prometheus.io/docs/instrumenting/exporters/) ，例如汇报机器数据的 node_exporter,  汇报 MongoDB 信息的 [MongoDB exporter](https://github.com/dcu/mongodb_exporter) 等等。\n- 用于告警通知管理的 [alertmanager](https://github.com/prometheus/alertmanager) 。\n\n### 1.3 基础架构\n\n一图胜千言，先来张官方的架构图\n\n![img](/img/prometheus1.png)\n\n<!-- more -->\n\n从这个架构图，也可以看出 Prometheus 的主要模块包含， Server,  Exporters, Pushgateway, PromQL, Alertmanager, WebUI 等。\n\n它大致使用逻辑是这样：\n\n1. Prometheus server 定期从静态配置的 targets 或者服务发现的 targets 拉取数据。\n2. 当新拉取的数据大于配置内存缓存区的时候，Prometheus 会将数据持久化到磁盘（如果使用 remote storage 将持久化到云端）。\n3. Prometheus 可以配置 rules，然后定时查询数据，当条件触发的时候，会将 alert 推送到配置的 Alertmanager。\n4. Alertmanager 收到警告的时候，可以根据配置，聚合，去重，降噪，最后发送警告。\n5. 可以使用 API， Prometheus Console 或者 Grafana 查询和聚合数据。\n\n### 1.4 注意\n\n- Prometheus 的数据是基于时序的 float64 的值，如果你的数据值有更多类型，无法满足。\n- Prometheus 不适合做审计计费，因为它的数据是按一定时间采集的，关注的更多是系统的运行瞬时状态以及趋势，即使有少量数据没有采集也能容忍，但是审计计费需要记录每个请求，并且数据长期存储，这个和 Prometheus 无法满足，可能需要采用专门的审计系统。\n\n\n\n## 2 BO关注项\n\n### 2.1 数据收集方式\n\n使用 pull 模式，拉取数据。\n\n\n\n### 2.2 数据格式\n\nPrometheus 时序格式与 [OpenTSDB](http://opentsdb.net/) 相似：\n\n```\n<metric name>{<label name>=<label value>, ...}\n\n```\n\n其中包含时序名字以及时序的标签。\n\n\n\n#### 2.2.1 时序 4 种类型\n\nPrometheus 时序数据分为 [Counter](https://prometheus.io/docs/concepts/metric_types/#counter), [Gauge](https://prometheus.io/docs/concepts/metric_types/#gauge), [Histogram](https://prometheus.io/docs/concepts/metric_types/#histogram), [Summary](https://prometheus.io/docs/concepts/metric_types/#summary) 四种类型。\n\n##### 2.2.1.1 Counter\n\nCounter 表示收集的数据是按照某个趋势（增加／减少）一直变化的，我们往往用它记录服务请求总量，错误总数等。\n\n例如 Prometheus server 中 `http_requests_total`,  表示 Prometheus 处理的 http 请求总数，我们可以使用 `delta`, 很容易得到任意区间数据的增量，这个会在 PromQL 一节中细讲。\n\n```\n# HELP http_requests_total Total number of HTTP requests made.\n# TYPE http_requests_total counter\nhttp_requests_total{code=\"200\",handler=\"alerts\",method=\"get\"} 2\nhttp_requests_total{code=\"200\",handler=\"config\",method=\"get\"} 1\nhttp_requests_total{code=\"200\",handler=\"flags\",method=\"get\"} 2\nhttp_requests_total{code=\"200\",handler=\"graph\",method=\"get\"} 6\nhttp_requests_total{code=\"200\",handler=\"label_values\",method=\"get\"} 6\nhttp_requests_total{code=\"200\",handler=\"prometheus\",method=\"get\"} 24755\nhttp_requests_total{code=\"200\",handler=\"query\",method=\"get\"} 6\nhttp_requests_total{code=\"200\",handler=\"static\",method=\"get\"} 6\nhttp_requests_total{code=\"200\",handler=\"status\",method=\"get\"} 2\nhttp_requests_total{code=\"200\",handler=\"targets\",method=\"get\"} 4\nhttp_requests_total{code=\"304\",handler=\"static\",method=\"get\"} 4\n\n```\n\n##### 2.2.1.2 Gauge\n\nGauge 表示搜集的数据是一个瞬时的，与时间没有关系，可以任意变高变低，往往可以用来记录内存使用率、磁盘使用率等。\n\n例如 Prometheus server 中 `go_goroutines`,  表示 Prometheus 当前 goroutines 的数量。\n\n```\n# HELP go_goroutines Number of goroutines that currently exist.\n# TYPE go_goroutines gauge\ngo_goroutines 100\n\n```\n\n##### 2.2.1.3 Histogram\n\nHistogram 由 `<basename>_bucket{le=\"<upper inclusive bound>\"}`，`<basename>_bucket{le=\"+Inf\"}`, `<basename>_sum`，`<basename>_count` 组成，主要用于表示一段时间范围内对数据进行采样，（通常是请求持续时间或响应大小），并能够对其指定区间以及总数进行统计，通常我们用它计算分位数的直方图。\n\n例如 Prometheus server 中 `prometheus_local_storage_series_chunks_persisted`,  表示 Prometheus 中每个时序需要存储的 chunks 数量，我们可以用它计算待持久化的数据的分位数。\n\n```\n# HELP prometheus_tsdb_compaction_chunk_range Final time range of chunks on their first compaction\n# TYPE prometheus_tsdb_compaction_chunk_range histogram\nprometheus_tsdb_compaction_chunk_range_bucket{le=\"100\"} 0\nprometheus_tsdb_compaction_chunk_range_bucket{le=\"400\"} 0\nprometheus_tsdb_compaction_chunk_range_bucket{le=\"1600\"} 0\nprometheus_tsdb_compaction_chunk_range_bucket{le=\"6400\"} 0\nprometheus_tsdb_compaction_chunk_range_bucket{le=\"25600\"} 0\nprometheus_tsdb_compaction_chunk_range_bucket{le=\"102400\"} 0\nprometheus_tsdb_compaction_chunk_range_bucket{le=\"409600\"} 605\nprometheus_tsdb_compaction_chunk_range_bucket{le=\"1.6384e+06\"} 612\nprometheus_tsdb_compaction_chunk_range_bucket{le=\"6.5536e+06\"} 126358\nprometheus_tsdb_compaction_chunk_range_bucket{le=\"2.62144e+07\"} 126358\nprometheus_tsdb_compaction_chunk_range_bucket{le=\"+Inf\"} 126358\nprometheus_tsdb_compaction_chunk_range_sum 2.25313627417e+11\nprometheus_tsdb_compaction_chunk_range_count 126358\n\n```\n\n##### 2.2.1.4 Summary\n\nSummary 和 Histogram 类似，由 `<basename>{quantile=\"<φ>\"}`，`<basename>_sum`，`<basename>_count` 组成，主要用于表示一段时间内数据采样结果，（通常是请求持续时间或响应大小），它直接存储了 quantile 数据，而不是根据统计区间计算出来的。\n\n\n\n例如 Prometheus server 中 `prometheus_target_interval_length_seconds`。\n\n```\n# HELP prometheus_target_interval_length_seconds Actual intervals between scrapes.\n# TYPE prometheus_target_interval_length_seconds summary\nprometheus_target_interval_length_seconds{interval=\"15s\",quantile=\"0.01\"} 14.999987534\nprometheus_target_interval_length_seconds{interval=\"15s\",quantile=\"0.05\"} 14.999987534\nprometheus_target_interval_length_seconds{interval=\"15s\",quantile=\"0.5\"} 15.000020575\nprometheus_target_interval_length_seconds{interval=\"15s\",quantile=\"0.9\"} 15.000045415\nprometheus_target_interval_length_seconds{interval=\"15s\",quantile=\"0.99\"} 15.000050555\nprometheus_target_interval_length_seconds_sum{interval=\"15s\"} 371280.61110144516\nprometheus_target_interval_length_seconds_count{interval=\"15s\"} 24752\n\n```\n\n##### 2.2.1.5 Histogram vs Summary\n\n- 都包含 `<basename>_sum`，`<basename>_count`\n- Histogram 需要通过 `<basename>_bucket` 计算 quantile, 而 Summary 直接存储了 quantile 的值。\n\n\n\n### 2.2.3 数据存储方式\n\n​\t数据存在promethues自身的数据库，以数据文件的形式存储，有自身的查询方式：promql；详见https://songjiayang.gitbooks.io/prometheus/content/promql/summary.html\n\n### 2.2.4 数据输出方式\n\n​\tagent：被动拉取；\n\n​\tpromethues server：主动拉取客户端的数据。promethues将拉取到的数据存到data/目录。（除了 promethues 前台的PromQ查询页面，应该有某种工具可以直接在命令行查询promethues的历史数据（暂未找到）；多种导出工具，可以支持Prometheus存储数据转化为HAProxy、StatsD、Graphite等工具所需要的数据存储格式（工具未研究过）。）\n\n### 2.2.5 agent部署方式\n\n​\tpromethues未提供自动部署agent的功能。\n\n### 2.2.6 任务下发方式\n\n​\tagent每个周期固定采集设备的指定指标，若要自定义采集某些指标则需要修改agent源码。\n\n​\tpromethues server拉取数据的任务在prometheus.yml配置。\n\n## 3 promethues组件及部署\n\n### 3.1 promethues server\n\n#### 3.1.1 部署\n\n​\ttar包解压即可用\n\n#### 3.1.2 prometheus.yml配置举例\n\n```\nglobal:\n  scrape_interval:     15s # By default, scrape targets every 15 seconds.\n  evaluation_interval: 15s # By default, scrape targets every 15 seconds.\n\nrule_files:\n  - \"rules/node.rules\"\n\nscrape_configs:\n  - job_name: 'prometheus'\n    scrape_interval: 5s\n    static_configs:\n      - targets: ['localhost:9090']\n\n  - job_name: 'node'\n    scrape_interval: 8s\n    static_configs:\n      - targets: ['127.0.0.1:9100', '127.0.0.12:9100']\n\n  - job_name: 'mysqld'\n    static_configs:\n      - targets: ['127.0.0.1:9104']\n  - job_name: 'memcached'\n    static_configs:\n      - targets: ['127.0.0.1:9150']\n\n```\n\n#### 3.1.3 命令\n\n[chenrj@kfapp01 prometheus-2.0.0.linux-amd64]$ ./prometheus -h\nusage: prometheus [<flags>]\n\nThe Prometheus monitoring server\n\nFlags:\n  -h, --help                     Show context-sensitive help (also try --help-long and --help-man).\n#### 3.1.4 前台地址\n\n​\thttp://192.168.7.40:9090/graph\n\n​\t默认9090端口\n\n### 3.2 grafana\n\n​       [http://192.168.7.40:3000](http://192.168.7.40:3000/)\n\n​       端口默认3000，\n\n​       用户密码： admin/admin\n\n​\t./grafana-server\n\n### 3.3 主机节点\n\n​\thttp://10.140.20.142:9100/metrics\n\n### 3.4 redis节点\n\n​\thttp://10.140.20.143:9121/metrics \n\n### 3.5 elasticsearch节点\n\n​\thttp://10.140.20.146:9108/metrics\n\n## 4 数据查询\n\n### 4.1 http方式查询promethues数据\n\n​\thttps://prometheus.io/docs/prometheus/latest/querying/api/#querying-metadata\n\n#### 4.1.1 即时查询\n\n```\nGET /api/v1/query\n\n```\n\n**URL查询参数：**\n\n- `query=<string>`：普罗米修斯表达查询字符串。\n- `time=<rfc3339 | unix_timestamp>`：评估时间戳。可选的。\n- `timeout=<duration>`：评价超时。可选的。默认为，并通过价值上限`-query.timeout`标志。\n\n若省略时间time测试，则默认使用服务器时间\n\n\n\n**例：**查询2018-01-16T03:12:51.781这个时刻go_memstats_frees_total的值\n\n[logstash@CP-ITSM-OMC-ZSC05 supervisor]$ curl 'http://10.140.20.146:9090/api/v1/query?query=go_memstats_frees_total&time=2018-01-16T03:12:51.781Z'\n{\"status\":\"success\",\"data\":{\"resultType\":\"vector\",\"result\":[{\"metric\":{\"__name__\":\"go_memstats_frees_total\",\"instance\":\"10.140.20.142:9100\",\"job\":\"node\"},\"value\":[1516072371.781,\"5599415948\"]},{\"metric\":{\"__name__\":\"go_memstats_frees_total\",\"instance\":\"10.140.20.143:9100\",\"job\":\"node\"},\"value\":[1516072371.781,\"5152870637\"]},{\"metric\":{\"__name__\":\"go_memstats_frees_total\",\"instance\":\"10.140.20.143:9108\",\"job\":\"elasticsearch_exporter\"},\"value\":[1516072371.781,\"1385642849\"]},{\"metric\":{\"__name__\":\"go_memstats_frees_total\",\"instance\":\"10.140.20.143:9121\",\"job\":\"redis_exporter_143\"},\"value\":[1516072371.781,\"159639669\"]},{\"metric\":{\"__name__\":\"go_memstats_frees_total\",\"instance\":\"10.140.20.144:9100\",\"job\":\"node\"},\"value\":[1516072371.781,\"5167404030\"]},{\"metric\":{\"__name__\":\"go_memstats_frees_total\",\"instance\":\"10.140.20.144:9108\",\"job\":\"elasticsearch_exporter\"},\"value\":[1516072371.781,\"1383957758\"]},{\"metric\":{\"__name__\":\"go_memstats_frees_total\",\"instance\":\"10.140.20.144:9121\",\"job\":\"redis_exporter_144\"},\"value\":[1516072371.781,\"373190465\"]},{\"metric\":{\"__name__\":\"go_memstats_frees_total\",\"instance\":\"10.140.20.145:9100\",\"job\":\"node\"},\"value\":[1516072371.781,\"5124941908\"]},{\"metric\":{\"__name__\":\"go_memstats_frees_total\",\"instance\":\"10.140.20.145:9108\",\"job\":\"elasticsearch_exporter\"},\"value\":[1516072371.781,\"1370943258\"]},{\"metric\":{\"__name__\":\"go_memstats_frees_total\",\"instance\":\"10.140.20.146:9100\",\"job\":\"node\"},\"value\":[1516072371.781,\"4850755799\"]},{\"metric\":{\"__name__\":\"go_memstats_frees_total\",\"instance\":\"10.140.20.146:9108\",\"job\":\"elasticsearch_exporter\"},\"value\":[1516072371.781,\"1370683906\"]},{\"metric\":{\"__name__\":\"go_memstats_frees_total\",\"instance\":\"localhost:9090\",\"job\":\"prometheus\"},\"value\":[1516072371.781,\"2299674805\"]}]}}\n\n**注：\"value\":[时间戳,\"对应值\"]**\n\n\n\n#### 4.1.2 范围查询\n\n```\nGET /api/v1/query_range\n\n```\n\n**URL查询参数：**\n\n- `query=<string>`：普罗米修斯表达查询字符串。\n- `start=<rfc3339 | unix_timestamp>`：开始时间戳。\n- `end=<rfc3339 | unix_timestamp>`：结束时间戳。\n- `step=<duration>`：查询分辨率步的宽度。\n- `timeout=<duration>`：评价超时。可选的。默认为，并通过价值上限`-query.timeout`标志。\n\n\n\n例：时间在2018-01-01T20:10:30.781到2018-01-01T20:11:00.781范围内，间隔15秒，up的数据\n\n[logstash@CP-ITSM-OMC-ZSC05 supervisor]$ curl 'http://10.140.20.146:9090/api/v1/query_range?query=up&start=2018-01-01T20:10:30.781Z&end=2018-01-01T20:11:00.781Z&step=15s'\n{\"status\":\"success\",\"data\":{\"resultType\":\"matrix\",\"result\":[{\"metric\":{\"__name__\":\"up\",\"instance\":\"localhost:9090\",\"job\":\"prometheus\"},\"values\":[[1514837430.781,\"1\"],[1514837445.781,\"1\"],[1514837460.781,\"1\"]]},{\"metric\":{\"__name__\":\"up\",\"instance\":\"localhost:9100\",\"job\":\"node\"},\"values\":[[1514837430.781,\"0\"],[1514837445.781,\"0\"],[1514837460.781,\"0\"]]}]}}\n\n\n\n### 4.2 http查询方式作用未知系列？？\n\n#### Querying metadata\n\n##### Finding series by label matchers\n\nThe following endpoint returns the list of time series that match a certain label set.\n\n```\nGET /api/v1/series\n\n```\n\nURL query parameters:\n\n- `match[]=<series_selector>`: Repeated series selector argument that selects the series to return. At least one `match[]` argument must be provided.\n- `start=<rfc3339 | unix_timestamp>`: Start timestamp.\n- `end=<rfc3339 | unix_timestamp>`: End timestamp.\n\nThe `data` section of the query result consists of a list of objects that contain the label name/value pairs which identify each series.\n\nThe following example returns all series that match either of the selectors `up` or `process_start_time_seconds{job=\"prometheus\"}`:\n\n```\n$ curl -g 'http://localhost:9090/api/v1/series?match[]=up&match[]=process_start_time_seconds{job=\"prometheus\"}'\n{\n   \"status\" : \"success\",\n   \"data\" : [\n      {\n         \"__name__\" : \"up\",\n         \"job\" : \"prometheus\",\n         \"instance\" : \"localhost:9090\"\n      },\n      {\n         \"__name__\" : \"up\",\n         \"job\" : \"node\",\n         \"instance\" : \"localhost:9091\"\n      },\n      {\n         \"__name__\" : \"process_start_time_seconds\",\n         \"job\" : \"prometheus\",\n         \"instance\" : \"localhost:9090\"\n      }\n   ]\n}\n\n```\n\n##### Querying label values\n\nThe following endpoint returns a list of label values for a provided label name:\n\n```\nGET /api/v1/label/<label_name>/values\n\n```\n\nThe `data` section of the JSON response is a list of string label names.\n\nThis example queries for all label values for the `job` label:\n\n```\n$ curl http://localhost:9090/api/v1/label/job/values\n{\n   \"status\" : \"success\",\n   \"data\" : [\n      \"node\",\n      \"prometheus\"\n   ]\n}\n\n```\n\n#### Expression query result formats\n\nExpression queries may return the following response values in the `result` property of the `data` section. `<sample_value>` placeholders are numeric sample values. JSON does not support special float values such as `NaN`, `Inf`, and `-Inf`, so sample values are transferred as quoted JSON strings rather than raw numbers.\n\n##### Range vectors\n\nRange vectors are returned as result type `matrix`. The corresponding `result` property has the following format:\n\n```\n[\n  {\n    \"metric\": { \"<label_name>\": \"<label_value>\", ... },\n    \"values\": [ [ <unix_time>, \"<sample_value>\" ], ... ]\n  },\n  ...\n]\n\n```\n\n##### Instant vectors\n\nInstant vectors are returned as result type `vector`. The corresponding `result` property has the following format:\n\n```\n[\n  {\n    \"metric\": { \"<label_name>\": \"<label_value>\", ... },\n    \"value\": [ <unix_time>, \"<sample_value>\" ]\n  },\n  ...\n]\n\n```\n\n##### Scalars\n\nScalar results are returned as result type `scalar`. The corresponding `result` property has the following format:\n\n```\n[ <unix_time>, \"<scalar_value>\" ]\n\n```\n\n##### Strings\n\nString results are returned as result type `string`. The corresponding `result` property has the following format:\n\n```\n[ <unix_time>, \"<string_value>\" ]\n\n```\n\n#### Targets\n\n> This API is experimental as it is intended to be extended with targets dropped due to relabelling in the future.\n\nThe following endpoint returns an overview of the current state of the Prometheus target discovery:\n\n```\nGET /api/v1/targets\n\n```\n\nCurrently only the active targets are part of the response.\n\n```\n$ curl http://localhost:9090/api/v1/targets\n{\n  \"status\": \"success\",                                                                                                                                [3/11]\n  \"data\": {\n    \"activeTargets\": [\n      {\n        \"discoveredLabels\": {\n          \"__address__\": \"127.0.0.1:9090\",\n          \"__metrics_path__\": \"/metrics\",\n          \"__scheme__\": \"http\",\n          \"job\": \"prometheus\"\n        },\n        \"labels\": {\n          \"instance\": \"127.0.0.1:9090\",\n          \"job\": \"prometheus\"\n        },\n        \"scrapeUrl\": \"http://127.0.0.1:9090/metrics\",\n        \"lastError\": \"\",\n        \"lastScrape\": \"2017-01-17T15:07:44.723715405+01:00\",\n        \"health\": \"up\"\n      }\n    ]\n  }\n}\n\n```\n\n#### Alertmanagers\n\n> This API is experimental as it is intended to be extended with Alertmanagers dropped due to relabelling in the future.\n\nThe following endpoint returns an overview of the current state of the Prometheus alertmanager discovery:\n\n```\nGET /api/v1/alertmanagers\n\n```\n\nCurrently only the active Alertmanagers are part of the response.\n\n```\n$ curl http://localhost:9090/api/v1/alertmanagers\n{\n  \"status\": \"success\",\n  \"data\": {\n    \"activeAlertmanagers\": [\n      {\n        \"url\": \"http://127.0.0.1:9090/api/v1/alerts\"\n      }\n    ]\n  }\n}\n```\n\n\n\n\n\n查询指标标签：\tcurl -g 'http://192.168.7.40:9090/api/v1/series?match[]=up&match[]=process_start_time_seconds{job=\"prometheus\"}'\n\n\n\n查询标签 {\"status\":\"success\",\"data\":[]}[logstash@CP-ITSM-OMC-ZSC05 supervisor]$ curl http://10.140.20.146:9090/api/v1/label/job/values\n{\"status\":\"success\",\"data\":[\"elasticsearch_exporter\",\"node\",\"prometheus\",\"redis_exporter\",\"redis_exporter_143\",\"redis_exporter_144\"]}\n\n\n\n\n\n### 4.3 查询节点数据\n\n​\t查询节点exporter的所有数据：curl -s http://192.168.7.40:9100/metrics\n\n\n\n## 5 exporter格式\n\n基于协议缓冲区格式 和 文本格式\n\n客户端可以暴露promethues无法解析的其他格式\n\n\n\n### 5.1 基于协议缓冲区格式 和 文本格式 的区别\n\n|                                    | Protocol buffer format                   | Text format                              |\n| ---------------------------------- | ---------------------------------------- | ---------------------------------------- |\n| **Inception**                      | April 2014                               | April 2014                               |\n| **Supported in**                   | Prometheus version `>=0.4.0`             | Prometheus version `>=0.4.0`             |\n| **Transmission**                   | HTTP                                     | HTTP                                     |\n| **Encoding**                       | [32-bit varint-encoded record length-delimited](https://developers.google.com/protocol-buffers/docs/reference/java/com/google/protobuf/AbstractMessageLite#writeDelimitedTo(java.io.OutputStream)) Protocol Buffer messages of type [io.prometheus.client.MetricFamily](https://github.com/prometheus/client_model/blob/086fe7ca28bde6cec2acd5223423c1475a362858/metrics.proto#L76-%20%20L81) | UTF-8, `\\n` line endings                 |\n| **HTTP Content-Type**              | `application/vnd.google.protobuf; proto=io.prometheus.client.MetricFamily; encoding=delimited` | `text/plain; version=0.0.4` (A missing `version` value will lead to a fall-back to the most recent text format version.) |\n| **Optional HTTP Content-Encoding** | `gzip`                                   | `gzip`                                   |\n| **Advantages**                     | Cross-platformSizeEncoding and decoding costsStrict schemaSupports concatenation and theoretically streaming (only server-side behavior would need to change) | Human-readableEasy to assemble, especially for minimalistic cases (no nesting required)Readable line by line (with the exception of type hints and docstrings) |\n| **Limitations**                    | Not human-readable                       | VerboseTypes and docstrings not integral part of the syntax, meaning little-to-nonexistent metric contract validationParsing cost |\n| **Supported metric primitives**    | CounterGaugeHistogramSummaryUntyped      | CounterGaugeHistogramSummaryUntyped      |\n| **Compatibility**                  | Version `0.0.3` protocol buffers are also valid version `0.0.4` protocol buffers. | none                                     |\n\n\n\n### 5.2 基于协议缓冲区格式\n\n​\tReproducible sorting of the protocol buffer fields in repeated expositions is preferred but not required, i.e. do not sort if the computational cost is prohibitive.\n\n​\tEach `MetricFamily` within the same exposition must have a unique name. Each `Metric` within the same `MetricFamily` must have a unique set of `LabelPair` fields. Otherwise, the ingestion behavior is undefined.\n\n### 5.3 文本类型格式\n\n​\t＃打头的是注释行（除非＃之后的第一个标记是HELP或TYPE）。\n\n​\tHELP行：可能包含任何UTF-8字符序列（在指标名称之后），但反斜杠和换行字符必须分别转义为`\\\\`和`\\ n`。对于相同的指标名称，只能有一条HELP行，一个指标只能有一个HELP行。\n\nTYPE行：TPYE后的第一个参数是指标名，第二个参数是数据类型（可以是counter, gauge, histogram, summary,  untyped）。相同的指标名称，只能有一个TYPE行。如果指标名称没有TYPE行，则该类型设置为无类型。\n\n格式：\n\n```\nmetric_name [\n  \"{\" label_name \"=\" `\"` label_value `\"` { \",\" label_name \"=\" `\"` label_value `\"` } [ \",\" ] \"}\"\n] value [ timestamp ]\n```\n\nlabel_value可以是任何UTF-8格式的内容，但反斜杠、双引号、换行符 必须转义成\n\n```\n\\\\  \\\"  \\\\n\n```\n\nhistogram（直方图）  summary（汇总）类型的特别格式：\n\n1. 需要单独一行xxx_sum；\n2. 需要单独一行xxx_count；\n3. Each quantile of a summary named x is given as a separate sample line with the same name x and a label {quantile=\"y\"}；\n4. A histogram must have a bucket with {le=\"+Inf\"}. Its value must be identical to the value of x_count；\n5. histogram类型必须要有{le=\"+Inf\"}，并且值要和xxx_count一致；\n\n\n\n### 5.4 文本类型格式举例\n\n```\n# HELP http_requests_total The total number of HTTP requests.\n# TYPE http_requests_total counter\nhttp_requests_total{method=\"post\",code=\"200\"} 1027 1395066363000\nhttp_requests_total{method=\"post\",code=\"400\"}    3 1395066363000\n\n# Escaping in label values:\nmsdos_file_access_time_seconds{path=\"C:\\\\DIR\\\\FILE.TXT\",error=\"Cannot find file:\\n\\\"FILE.TXT\\\"\"} 1.458255915e9\n\n# Minimalistic line:\nmetric_without_timestamp_and_labels 12.47\n\n# A weird metric from before the epoch:\nsomething_weird{problem=\"division by zero\"} +Inf -3982045\n\n# A histogram, which has a pretty complex representation in the text format:\n# HELP http_request_duration_seconds A histogram of the request duration.\n# TYPE http_request_duration_seconds histogram\nhttp_request_duration_seconds_bucket{le=\"0.05\"} 24054\nhttp_request_duration_seconds_bucket{le=\"0.1\"} 33444\nhttp_request_duration_seconds_bucket{le=\"0.2\"} 100392\nhttp_request_duration_seconds_bucket{le=\"0.5\"} 129389\nhttp_request_duration_seconds_bucket{le=\"1\"} 133988\nhttp_request_duration_seconds_bucket{le=\"+Inf\"} 144320\nhttp_request_duration_seconds_sum 53423\nhttp_request_duration_seconds_count 144320\n\n# Finally a summary, which has a complex representation, too:\n# HELP rpc_duration_seconds A summary of the RPC duration in seconds.\n# TYPE rpc_duration_seconds summary\nrpc_duration_seconds{quantile=\"0.01\"} 3102\nrpc_duration_seconds{quantile=\"0.05\"} 3272\nrpc_duration_seconds{quantile=\"0.5\"} 4773\nrpc_duration_seconds{quantile=\"0.9\"} 9001\nrpc_duration_seconds{quantile=\"0.99\"} 76656\nrpc_duration_seconds_sum 1.7560473e+07\nrpc_duration_seconds_count 2693\n```\n\n\n\n## 6 导出器exporter\n\n### 6.1 概述\n\n​\t指标名，一般为导出程序名称作为前缀，例如， haproxy_up；\n\n​\t度量标准必须使用基本单位（例如秒，字节），并保留将其转换为更具可读性的图形工具；\n\n \t指标有效字符：`[a-zA-Z0-9:_]` ，其他任何字符都要用下划线_代替；\n\n​\t指标后缀`_sum`, `_count`, `_bucket` and `_total` 只可用在Summaries、 Histograms 、 Counters\n\n\n\n## 7 告警alertmanager程序\n\n​\t概述：\n\n​\t\tpromethues：根据配置文件prometheus.yml的rule_files告警规则，将告警信息存到promethues的磁盘，供promethues的前台页面查看；根据配置文件prometheus.yml的alerting（配置altermanager进程的ip 端口信息），将告警信息发送altermanager进程上。\n\n​\t\taltermanager：接收promethues发来的告警信息，存在磁盘中供altermanager进程的前台查看；同时根据altermanager的告警配置文件simple.yml发送邮件等提醒。\n\n\n\n### 7.1 promethues告警配置举例\n\n![](/img/promethues-alarm.png)\n\n### 7.2 promethues告警规则配置文件举例\n\n![](/img/rule_file.png)\n\nalert：自定义的告警含义简写\n\nexpor：告警条件，如上图的node_forks为具体mertics里的指标\n\nfor：周期\n\nlabels：severity，在alertmanager前台页面可以根据severity条件来查询告警信息\n\nannotations：summary写些较详细的告警信息\n\n### 7.3 alertmanager告警发送邮件提示\n\n![](/img/email_1.png)\n\n![](/img/email_2.png)\n\n\n\n### 7.4 启动alertmanager\n\n nohup ./alertmanager --config.file=simple.yml &\n\nalertmanager前台：http://192.168.7.176:9093/\n\n### 7.5通过其他方式告警\n\n#### hipchat_config：\n\n​\t是一款能够在苹果mac平台上运行的社交聊天软件，HipChat的功能和QQ相似，集聊天、视频、语音等功能于一身，不同之处在于HipChat界面更加的简洁、操作更加的流畅。\n\n#### pagerduty_config：\n\n​\t是一款能够在服务器出问题时发送提醒的软件。在发生问题时，提醒的方式包括屏幕显示、电话呼叫、短信通知、电邮通知等，而且在无人应答时还会自动将提醒级别提高。PagerDuty 不是免费的。\n\n#### pushover_config：\n\n​\t是一款网络通知推送服务，类似ifttt或脚本服务，你可以将需要推送的服务设置好后，遇到情况将把通知自动推送到你的[安卓手机](http://www.onlinedown.net/soft/222292.htm)。\n\n#### slack_config：\n\n​\tslack是聊天群组 + 大规模工具集成 + 文件整合 + 统一搜索。截至2014年底，Slack 已经整合了电子邮件、短信、[Google](https://baike.baidu.com/item/Google) Drives、[Twitter](https://baike.baidu.com/item/Twitter)、Trello、Asana、[GitHub](https://baike.baidu.com/item/GitHub) 等 65 种工具和服务，可以把各种碎片化的企业沟通和协作集中到一起。\n\n```\n# Whether or not to notify about resolved alerts.\n[ send_resolved: <boolean> | default = false ]\n\n# The Slack webhook URL.\n[ api_url: <secret> | default = global.slack_api_url ]\n\n# The channel or user to send notifications to.\nchannel: <tmpl_string>\n\n# API request data as defined by the Slack webhook API.\n[ color: <tmpl_string> | default = '{{ if eq .Status \"firing\" }}danger{{ else }}good{{ end }}' ]\n[ username: <tmpl_string> | default = '{{ template \"slack.default.username\" . }}' ]\n[ title: <tmpl_string> | default = '{{ template \"slack.default.title\" . }}' ]\n[ title_link: <tmpl_string> | default = '{{ template \"slack.default.titlelink\" . }}' ]\n[ icon_emoji: <tmpl_string> ]\n[ icon_url: <tmpl_string> ]\n[ pretext: <tmpl_string> | default = '{{ template \"slack.default.pretext\" . }}' ]\n[ text: <tmpl_string> | default = '{{ template \"slack.default.text\" . }}' ]\n[ fallback: <tmpl_string> | default = '{{ template \"slack.default.fallback\" . }}' ]\n\n# The HTTP client's configuration.\n[ http_config: <http_config> | default = global.http_config ]\n```\n\n\n\n#### opsgenie_config ：\n\n​\t集成电话短信邮件等等\n\n#### victorops_config：\n\n​\t聊天应用\n\n#### http_config：\n\n​\tA `http_config` allows configuring the HTTP client that the receiver uses to communicate with HTTP-based API services.\n\n\n\n\n\n## 8 问题笔记\n\n### 8.1已解决\n\n1. prometheus浏览器查询不到exporter指标数据，但是浏览器exporter的mertic有指标数据。原因是时间不同步\n\n\n\n### 8.2 未解决\n\n","source":"_posts/prometheus.md","raw":"---\ntitle: Prometheus 监控体系\ndate: 2018-03-25 15:48:29\ntags: \n - Prometheus\n - Go\ncategories: 技术\n---\n## 1 概述\n\n### 1.1 主要功能\n\n- 多维 [数据模型](https://prometheus.io/docs/concepts/data_model/)（时序由 metric 名字和 k/v 的 labels 构成）。\n- 灵活的查询语句（[PromQL](https://prometheus.io/docs/querying/basics/)）。\n- 无依赖存储，支持 local 和 remote 不同模型。\n- 采用 http 协议，使用 pull 模式，拉取数据，简单易懂。\n- 监控目标，可以采用服务发现或静态配置的方式。\n- 支持多种统计数据模型，图形化友好。\n\n### 1.2 核心组件\n\n- [Prometheus Server](https://github.com/prometheus/prometheus)， 主要用于抓取数据和存储时序数据，另外还提供查询和 Alert Rule 配置管理。\n- [client libraries](https://prometheus.io/docs/instrumenting/clientlibs/)，用于对接 Prometheus Server, 可以查询和上报数据。\n- [push gateway](https://github.com/prometheus/pushgateway) ，用于批量，短期的监控数据的汇总节点，主要用于业务数据汇报等。\n- 各种汇报数据的 [exporters](https://prometheus.io/docs/instrumenting/exporters/) ，例如汇报机器数据的 node_exporter,  汇报 MongoDB 信息的 [MongoDB exporter](https://github.com/dcu/mongodb_exporter) 等等。\n- 用于告警通知管理的 [alertmanager](https://github.com/prometheus/alertmanager) 。\n\n### 1.3 基础架构\n\n一图胜千言，先来张官方的架构图\n\n![img](/img/prometheus1.png)\n\n<!-- more -->\n\n从这个架构图，也可以看出 Prometheus 的主要模块包含， Server,  Exporters, Pushgateway, PromQL, Alertmanager, WebUI 等。\n\n它大致使用逻辑是这样：\n\n1. Prometheus server 定期从静态配置的 targets 或者服务发现的 targets 拉取数据。\n2. 当新拉取的数据大于配置内存缓存区的时候，Prometheus 会将数据持久化到磁盘（如果使用 remote storage 将持久化到云端）。\n3. Prometheus 可以配置 rules，然后定时查询数据，当条件触发的时候，会将 alert 推送到配置的 Alertmanager。\n4. Alertmanager 收到警告的时候，可以根据配置，聚合，去重，降噪，最后发送警告。\n5. 可以使用 API， Prometheus Console 或者 Grafana 查询和聚合数据。\n\n### 1.4 注意\n\n- Prometheus 的数据是基于时序的 float64 的值，如果你的数据值有更多类型，无法满足。\n- Prometheus 不适合做审计计费，因为它的数据是按一定时间采集的，关注的更多是系统的运行瞬时状态以及趋势，即使有少量数据没有采集也能容忍，但是审计计费需要记录每个请求，并且数据长期存储，这个和 Prometheus 无法满足，可能需要采用专门的审计系统。\n\n\n\n## 2 BO关注项\n\n### 2.1 数据收集方式\n\n使用 pull 模式，拉取数据。\n\n\n\n### 2.2 数据格式\n\nPrometheus 时序格式与 [OpenTSDB](http://opentsdb.net/) 相似：\n\n```\n<metric name>{<label name>=<label value>, ...}\n\n```\n\n其中包含时序名字以及时序的标签。\n\n\n\n#### 2.2.1 时序 4 种类型\n\nPrometheus 时序数据分为 [Counter](https://prometheus.io/docs/concepts/metric_types/#counter), [Gauge](https://prometheus.io/docs/concepts/metric_types/#gauge), [Histogram](https://prometheus.io/docs/concepts/metric_types/#histogram), [Summary](https://prometheus.io/docs/concepts/metric_types/#summary) 四种类型。\n\n##### 2.2.1.1 Counter\n\nCounter 表示收集的数据是按照某个趋势（增加／减少）一直变化的，我们往往用它记录服务请求总量，错误总数等。\n\n例如 Prometheus server 中 `http_requests_total`,  表示 Prometheus 处理的 http 请求总数，我们可以使用 `delta`, 很容易得到任意区间数据的增量，这个会在 PromQL 一节中细讲。\n\n```\n# HELP http_requests_total Total number of HTTP requests made.\n# TYPE http_requests_total counter\nhttp_requests_total{code=\"200\",handler=\"alerts\",method=\"get\"} 2\nhttp_requests_total{code=\"200\",handler=\"config\",method=\"get\"} 1\nhttp_requests_total{code=\"200\",handler=\"flags\",method=\"get\"} 2\nhttp_requests_total{code=\"200\",handler=\"graph\",method=\"get\"} 6\nhttp_requests_total{code=\"200\",handler=\"label_values\",method=\"get\"} 6\nhttp_requests_total{code=\"200\",handler=\"prometheus\",method=\"get\"} 24755\nhttp_requests_total{code=\"200\",handler=\"query\",method=\"get\"} 6\nhttp_requests_total{code=\"200\",handler=\"static\",method=\"get\"} 6\nhttp_requests_total{code=\"200\",handler=\"status\",method=\"get\"} 2\nhttp_requests_total{code=\"200\",handler=\"targets\",method=\"get\"} 4\nhttp_requests_total{code=\"304\",handler=\"static\",method=\"get\"} 4\n\n```\n\n##### 2.2.1.2 Gauge\n\nGauge 表示搜集的数据是一个瞬时的，与时间没有关系，可以任意变高变低，往往可以用来记录内存使用率、磁盘使用率等。\n\n例如 Prometheus server 中 `go_goroutines`,  表示 Prometheus 当前 goroutines 的数量。\n\n```\n# HELP go_goroutines Number of goroutines that currently exist.\n# TYPE go_goroutines gauge\ngo_goroutines 100\n\n```\n\n##### 2.2.1.3 Histogram\n\nHistogram 由 `<basename>_bucket{le=\"<upper inclusive bound>\"}`，`<basename>_bucket{le=\"+Inf\"}`, `<basename>_sum`，`<basename>_count` 组成，主要用于表示一段时间范围内对数据进行采样，（通常是请求持续时间或响应大小），并能够对其指定区间以及总数进行统计，通常我们用它计算分位数的直方图。\n\n例如 Prometheus server 中 `prometheus_local_storage_series_chunks_persisted`,  表示 Prometheus 中每个时序需要存储的 chunks 数量，我们可以用它计算待持久化的数据的分位数。\n\n```\n# HELP prometheus_tsdb_compaction_chunk_range Final time range of chunks on their first compaction\n# TYPE prometheus_tsdb_compaction_chunk_range histogram\nprometheus_tsdb_compaction_chunk_range_bucket{le=\"100\"} 0\nprometheus_tsdb_compaction_chunk_range_bucket{le=\"400\"} 0\nprometheus_tsdb_compaction_chunk_range_bucket{le=\"1600\"} 0\nprometheus_tsdb_compaction_chunk_range_bucket{le=\"6400\"} 0\nprometheus_tsdb_compaction_chunk_range_bucket{le=\"25600\"} 0\nprometheus_tsdb_compaction_chunk_range_bucket{le=\"102400\"} 0\nprometheus_tsdb_compaction_chunk_range_bucket{le=\"409600\"} 605\nprometheus_tsdb_compaction_chunk_range_bucket{le=\"1.6384e+06\"} 612\nprometheus_tsdb_compaction_chunk_range_bucket{le=\"6.5536e+06\"} 126358\nprometheus_tsdb_compaction_chunk_range_bucket{le=\"2.62144e+07\"} 126358\nprometheus_tsdb_compaction_chunk_range_bucket{le=\"+Inf\"} 126358\nprometheus_tsdb_compaction_chunk_range_sum 2.25313627417e+11\nprometheus_tsdb_compaction_chunk_range_count 126358\n\n```\n\n##### 2.2.1.4 Summary\n\nSummary 和 Histogram 类似，由 `<basename>{quantile=\"<φ>\"}`，`<basename>_sum`，`<basename>_count` 组成，主要用于表示一段时间内数据采样结果，（通常是请求持续时间或响应大小），它直接存储了 quantile 数据，而不是根据统计区间计算出来的。\n\n\n\n例如 Prometheus server 中 `prometheus_target_interval_length_seconds`。\n\n```\n# HELP prometheus_target_interval_length_seconds Actual intervals between scrapes.\n# TYPE prometheus_target_interval_length_seconds summary\nprometheus_target_interval_length_seconds{interval=\"15s\",quantile=\"0.01\"} 14.999987534\nprometheus_target_interval_length_seconds{interval=\"15s\",quantile=\"0.05\"} 14.999987534\nprometheus_target_interval_length_seconds{interval=\"15s\",quantile=\"0.5\"} 15.000020575\nprometheus_target_interval_length_seconds{interval=\"15s\",quantile=\"0.9\"} 15.000045415\nprometheus_target_interval_length_seconds{interval=\"15s\",quantile=\"0.99\"} 15.000050555\nprometheus_target_interval_length_seconds_sum{interval=\"15s\"} 371280.61110144516\nprometheus_target_interval_length_seconds_count{interval=\"15s\"} 24752\n\n```\n\n##### 2.2.1.5 Histogram vs Summary\n\n- 都包含 `<basename>_sum`，`<basename>_count`\n- Histogram 需要通过 `<basename>_bucket` 计算 quantile, 而 Summary 直接存储了 quantile 的值。\n\n\n\n### 2.2.3 数据存储方式\n\n​\t数据存在promethues自身的数据库，以数据文件的形式存储，有自身的查询方式：promql；详见https://songjiayang.gitbooks.io/prometheus/content/promql/summary.html\n\n### 2.2.4 数据输出方式\n\n​\tagent：被动拉取；\n\n​\tpromethues server：主动拉取客户端的数据。promethues将拉取到的数据存到data/目录。（除了 promethues 前台的PromQ查询页面，应该有某种工具可以直接在命令行查询promethues的历史数据（暂未找到）；多种导出工具，可以支持Prometheus存储数据转化为HAProxy、StatsD、Graphite等工具所需要的数据存储格式（工具未研究过）。）\n\n### 2.2.5 agent部署方式\n\n​\tpromethues未提供自动部署agent的功能。\n\n### 2.2.6 任务下发方式\n\n​\tagent每个周期固定采集设备的指定指标，若要自定义采集某些指标则需要修改agent源码。\n\n​\tpromethues server拉取数据的任务在prometheus.yml配置。\n\n## 3 promethues组件及部署\n\n### 3.1 promethues server\n\n#### 3.1.1 部署\n\n​\ttar包解压即可用\n\n#### 3.1.2 prometheus.yml配置举例\n\n```\nglobal:\n  scrape_interval:     15s # By default, scrape targets every 15 seconds.\n  evaluation_interval: 15s # By default, scrape targets every 15 seconds.\n\nrule_files:\n  - \"rules/node.rules\"\n\nscrape_configs:\n  - job_name: 'prometheus'\n    scrape_interval: 5s\n    static_configs:\n      - targets: ['localhost:9090']\n\n  - job_name: 'node'\n    scrape_interval: 8s\n    static_configs:\n      - targets: ['127.0.0.1:9100', '127.0.0.12:9100']\n\n  - job_name: 'mysqld'\n    static_configs:\n      - targets: ['127.0.0.1:9104']\n  - job_name: 'memcached'\n    static_configs:\n      - targets: ['127.0.0.1:9150']\n\n```\n\n#### 3.1.3 命令\n\n[chenrj@kfapp01 prometheus-2.0.0.linux-amd64]$ ./prometheus -h\nusage: prometheus [<flags>]\n\nThe Prometheus monitoring server\n\nFlags:\n  -h, --help                     Show context-sensitive help (also try --help-long and --help-man).\n#### 3.1.4 前台地址\n\n​\thttp://192.168.7.40:9090/graph\n\n​\t默认9090端口\n\n### 3.2 grafana\n\n​       [http://192.168.7.40:3000](http://192.168.7.40:3000/)\n\n​       端口默认3000，\n\n​       用户密码： admin/admin\n\n​\t./grafana-server\n\n### 3.3 主机节点\n\n​\thttp://10.140.20.142:9100/metrics\n\n### 3.4 redis节点\n\n​\thttp://10.140.20.143:9121/metrics \n\n### 3.5 elasticsearch节点\n\n​\thttp://10.140.20.146:9108/metrics\n\n## 4 数据查询\n\n### 4.1 http方式查询promethues数据\n\n​\thttps://prometheus.io/docs/prometheus/latest/querying/api/#querying-metadata\n\n#### 4.1.1 即时查询\n\n```\nGET /api/v1/query\n\n```\n\n**URL查询参数：**\n\n- `query=<string>`：普罗米修斯表达查询字符串。\n- `time=<rfc3339 | unix_timestamp>`：评估时间戳。可选的。\n- `timeout=<duration>`：评价超时。可选的。默认为，并通过价值上限`-query.timeout`标志。\n\n若省略时间time测试，则默认使用服务器时间\n\n\n\n**例：**查询2018-01-16T03:12:51.781这个时刻go_memstats_frees_total的值\n\n[logstash@CP-ITSM-OMC-ZSC05 supervisor]$ curl 'http://10.140.20.146:9090/api/v1/query?query=go_memstats_frees_total&time=2018-01-16T03:12:51.781Z'\n{\"status\":\"success\",\"data\":{\"resultType\":\"vector\",\"result\":[{\"metric\":{\"__name__\":\"go_memstats_frees_total\",\"instance\":\"10.140.20.142:9100\",\"job\":\"node\"},\"value\":[1516072371.781,\"5599415948\"]},{\"metric\":{\"__name__\":\"go_memstats_frees_total\",\"instance\":\"10.140.20.143:9100\",\"job\":\"node\"},\"value\":[1516072371.781,\"5152870637\"]},{\"metric\":{\"__name__\":\"go_memstats_frees_total\",\"instance\":\"10.140.20.143:9108\",\"job\":\"elasticsearch_exporter\"},\"value\":[1516072371.781,\"1385642849\"]},{\"metric\":{\"__name__\":\"go_memstats_frees_total\",\"instance\":\"10.140.20.143:9121\",\"job\":\"redis_exporter_143\"},\"value\":[1516072371.781,\"159639669\"]},{\"metric\":{\"__name__\":\"go_memstats_frees_total\",\"instance\":\"10.140.20.144:9100\",\"job\":\"node\"},\"value\":[1516072371.781,\"5167404030\"]},{\"metric\":{\"__name__\":\"go_memstats_frees_total\",\"instance\":\"10.140.20.144:9108\",\"job\":\"elasticsearch_exporter\"},\"value\":[1516072371.781,\"1383957758\"]},{\"metric\":{\"__name__\":\"go_memstats_frees_total\",\"instance\":\"10.140.20.144:9121\",\"job\":\"redis_exporter_144\"},\"value\":[1516072371.781,\"373190465\"]},{\"metric\":{\"__name__\":\"go_memstats_frees_total\",\"instance\":\"10.140.20.145:9100\",\"job\":\"node\"},\"value\":[1516072371.781,\"5124941908\"]},{\"metric\":{\"__name__\":\"go_memstats_frees_total\",\"instance\":\"10.140.20.145:9108\",\"job\":\"elasticsearch_exporter\"},\"value\":[1516072371.781,\"1370943258\"]},{\"metric\":{\"__name__\":\"go_memstats_frees_total\",\"instance\":\"10.140.20.146:9100\",\"job\":\"node\"},\"value\":[1516072371.781,\"4850755799\"]},{\"metric\":{\"__name__\":\"go_memstats_frees_total\",\"instance\":\"10.140.20.146:9108\",\"job\":\"elasticsearch_exporter\"},\"value\":[1516072371.781,\"1370683906\"]},{\"metric\":{\"__name__\":\"go_memstats_frees_total\",\"instance\":\"localhost:9090\",\"job\":\"prometheus\"},\"value\":[1516072371.781,\"2299674805\"]}]}}\n\n**注：\"value\":[时间戳,\"对应值\"]**\n\n\n\n#### 4.1.2 范围查询\n\n```\nGET /api/v1/query_range\n\n```\n\n**URL查询参数：**\n\n- `query=<string>`：普罗米修斯表达查询字符串。\n- `start=<rfc3339 | unix_timestamp>`：开始时间戳。\n- `end=<rfc3339 | unix_timestamp>`：结束时间戳。\n- `step=<duration>`：查询分辨率步的宽度。\n- `timeout=<duration>`：评价超时。可选的。默认为，并通过价值上限`-query.timeout`标志。\n\n\n\n例：时间在2018-01-01T20:10:30.781到2018-01-01T20:11:00.781范围内，间隔15秒，up的数据\n\n[logstash@CP-ITSM-OMC-ZSC05 supervisor]$ curl 'http://10.140.20.146:9090/api/v1/query_range?query=up&start=2018-01-01T20:10:30.781Z&end=2018-01-01T20:11:00.781Z&step=15s'\n{\"status\":\"success\",\"data\":{\"resultType\":\"matrix\",\"result\":[{\"metric\":{\"__name__\":\"up\",\"instance\":\"localhost:9090\",\"job\":\"prometheus\"},\"values\":[[1514837430.781,\"1\"],[1514837445.781,\"1\"],[1514837460.781,\"1\"]]},{\"metric\":{\"__name__\":\"up\",\"instance\":\"localhost:9100\",\"job\":\"node\"},\"values\":[[1514837430.781,\"0\"],[1514837445.781,\"0\"],[1514837460.781,\"0\"]]}]}}\n\n\n\n### 4.2 http查询方式作用未知系列？？\n\n#### Querying metadata\n\n##### Finding series by label matchers\n\nThe following endpoint returns the list of time series that match a certain label set.\n\n```\nGET /api/v1/series\n\n```\n\nURL query parameters:\n\n- `match[]=<series_selector>`: Repeated series selector argument that selects the series to return. At least one `match[]` argument must be provided.\n- `start=<rfc3339 | unix_timestamp>`: Start timestamp.\n- `end=<rfc3339 | unix_timestamp>`: End timestamp.\n\nThe `data` section of the query result consists of a list of objects that contain the label name/value pairs which identify each series.\n\nThe following example returns all series that match either of the selectors `up` or `process_start_time_seconds{job=\"prometheus\"}`:\n\n```\n$ curl -g 'http://localhost:9090/api/v1/series?match[]=up&match[]=process_start_time_seconds{job=\"prometheus\"}'\n{\n   \"status\" : \"success\",\n   \"data\" : [\n      {\n         \"__name__\" : \"up\",\n         \"job\" : \"prometheus\",\n         \"instance\" : \"localhost:9090\"\n      },\n      {\n         \"__name__\" : \"up\",\n         \"job\" : \"node\",\n         \"instance\" : \"localhost:9091\"\n      },\n      {\n         \"__name__\" : \"process_start_time_seconds\",\n         \"job\" : \"prometheus\",\n         \"instance\" : \"localhost:9090\"\n      }\n   ]\n}\n\n```\n\n##### Querying label values\n\nThe following endpoint returns a list of label values for a provided label name:\n\n```\nGET /api/v1/label/<label_name>/values\n\n```\n\nThe `data` section of the JSON response is a list of string label names.\n\nThis example queries for all label values for the `job` label:\n\n```\n$ curl http://localhost:9090/api/v1/label/job/values\n{\n   \"status\" : \"success\",\n   \"data\" : [\n      \"node\",\n      \"prometheus\"\n   ]\n}\n\n```\n\n#### Expression query result formats\n\nExpression queries may return the following response values in the `result` property of the `data` section. `<sample_value>` placeholders are numeric sample values. JSON does not support special float values such as `NaN`, `Inf`, and `-Inf`, so sample values are transferred as quoted JSON strings rather than raw numbers.\n\n##### Range vectors\n\nRange vectors are returned as result type `matrix`. The corresponding `result` property has the following format:\n\n```\n[\n  {\n    \"metric\": { \"<label_name>\": \"<label_value>\", ... },\n    \"values\": [ [ <unix_time>, \"<sample_value>\" ], ... ]\n  },\n  ...\n]\n\n```\n\n##### Instant vectors\n\nInstant vectors are returned as result type `vector`. The corresponding `result` property has the following format:\n\n```\n[\n  {\n    \"metric\": { \"<label_name>\": \"<label_value>\", ... },\n    \"value\": [ <unix_time>, \"<sample_value>\" ]\n  },\n  ...\n]\n\n```\n\n##### Scalars\n\nScalar results are returned as result type `scalar`. The corresponding `result` property has the following format:\n\n```\n[ <unix_time>, \"<scalar_value>\" ]\n\n```\n\n##### Strings\n\nString results are returned as result type `string`. The corresponding `result` property has the following format:\n\n```\n[ <unix_time>, \"<string_value>\" ]\n\n```\n\n#### Targets\n\n> This API is experimental as it is intended to be extended with targets dropped due to relabelling in the future.\n\nThe following endpoint returns an overview of the current state of the Prometheus target discovery:\n\n```\nGET /api/v1/targets\n\n```\n\nCurrently only the active targets are part of the response.\n\n```\n$ curl http://localhost:9090/api/v1/targets\n{\n  \"status\": \"success\",                                                                                                                                [3/11]\n  \"data\": {\n    \"activeTargets\": [\n      {\n        \"discoveredLabels\": {\n          \"__address__\": \"127.0.0.1:9090\",\n          \"__metrics_path__\": \"/metrics\",\n          \"__scheme__\": \"http\",\n          \"job\": \"prometheus\"\n        },\n        \"labels\": {\n          \"instance\": \"127.0.0.1:9090\",\n          \"job\": \"prometheus\"\n        },\n        \"scrapeUrl\": \"http://127.0.0.1:9090/metrics\",\n        \"lastError\": \"\",\n        \"lastScrape\": \"2017-01-17T15:07:44.723715405+01:00\",\n        \"health\": \"up\"\n      }\n    ]\n  }\n}\n\n```\n\n#### Alertmanagers\n\n> This API is experimental as it is intended to be extended with Alertmanagers dropped due to relabelling in the future.\n\nThe following endpoint returns an overview of the current state of the Prometheus alertmanager discovery:\n\n```\nGET /api/v1/alertmanagers\n\n```\n\nCurrently only the active Alertmanagers are part of the response.\n\n```\n$ curl http://localhost:9090/api/v1/alertmanagers\n{\n  \"status\": \"success\",\n  \"data\": {\n    \"activeAlertmanagers\": [\n      {\n        \"url\": \"http://127.0.0.1:9090/api/v1/alerts\"\n      }\n    ]\n  }\n}\n```\n\n\n\n\n\n查询指标标签：\tcurl -g 'http://192.168.7.40:9090/api/v1/series?match[]=up&match[]=process_start_time_seconds{job=\"prometheus\"}'\n\n\n\n查询标签 {\"status\":\"success\",\"data\":[]}[logstash@CP-ITSM-OMC-ZSC05 supervisor]$ curl http://10.140.20.146:9090/api/v1/label/job/values\n{\"status\":\"success\",\"data\":[\"elasticsearch_exporter\",\"node\",\"prometheus\",\"redis_exporter\",\"redis_exporter_143\",\"redis_exporter_144\"]}\n\n\n\n\n\n### 4.3 查询节点数据\n\n​\t查询节点exporter的所有数据：curl -s http://192.168.7.40:9100/metrics\n\n\n\n## 5 exporter格式\n\n基于协议缓冲区格式 和 文本格式\n\n客户端可以暴露promethues无法解析的其他格式\n\n\n\n### 5.1 基于协议缓冲区格式 和 文本格式 的区别\n\n|                                    | Protocol buffer format                   | Text format                              |\n| ---------------------------------- | ---------------------------------------- | ---------------------------------------- |\n| **Inception**                      | April 2014                               | April 2014                               |\n| **Supported in**                   | Prometheus version `>=0.4.0`             | Prometheus version `>=0.4.0`             |\n| **Transmission**                   | HTTP                                     | HTTP                                     |\n| **Encoding**                       | [32-bit varint-encoded record length-delimited](https://developers.google.com/protocol-buffers/docs/reference/java/com/google/protobuf/AbstractMessageLite#writeDelimitedTo(java.io.OutputStream)) Protocol Buffer messages of type [io.prometheus.client.MetricFamily](https://github.com/prometheus/client_model/blob/086fe7ca28bde6cec2acd5223423c1475a362858/metrics.proto#L76-%20%20L81) | UTF-8, `\\n` line endings                 |\n| **HTTP Content-Type**              | `application/vnd.google.protobuf; proto=io.prometheus.client.MetricFamily; encoding=delimited` | `text/plain; version=0.0.4` (A missing `version` value will lead to a fall-back to the most recent text format version.) |\n| **Optional HTTP Content-Encoding** | `gzip`                                   | `gzip`                                   |\n| **Advantages**                     | Cross-platformSizeEncoding and decoding costsStrict schemaSupports concatenation and theoretically streaming (only server-side behavior would need to change) | Human-readableEasy to assemble, especially for minimalistic cases (no nesting required)Readable line by line (with the exception of type hints and docstrings) |\n| **Limitations**                    | Not human-readable                       | VerboseTypes and docstrings not integral part of the syntax, meaning little-to-nonexistent metric contract validationParsing cost |\n| **Supported metric primitives**    | CounterGaugeHistogramSummaryUntyped      | CounterGaugeHistogramSummaryUntyped      |\n| **Compatibility**                  | Version `0.0.3` protocol buffers are also valid version `0.0.4` protocol buffers. | none                                     |\n\n\n\n### 5.2 基于协议缓冲区格式\n\n​\tReproducible sorting of the protocol buffer fields in repeated expositions is preferred but not required, i.e. do not sort if the computational cost is prohibitive.\n\n​\tEach `MetricFamily` within the same exposition must have a unique name. Each `Metric` within the same `MetricFamily` must have a unique set of `LabelPair` fields. Otherwise, the ingestion behavior is undefined.\n\n### 5.3 文本类型格式\n\n​\t＃打头的是注释行（除非＃之后的第一个标记是HELP或TYPE）。\n\n​\tHELP行：可能包含任何UTF-8字符序列（在指标名称之后），但反斜杠和换行字符必须分别转义为`\\\\`和`\\ n`。对于相同的指标名称，只能有一条HELP行，一个指标只能有一个HELP行。\n\nTYPE行：TPYE后的第一个参数是指标名，第二个参数是数据类型（可以是counter, gauge, histogram, summary,  untyped）。相同的指标名称，只能有一个TYPE行。如果指标名称没有TYPE行，则该类型设置为无类型。\n\n格式：\n\n```\nmetric_name [\n  \"{\" label_name \"=\" `\"` label_value `\"` { \",\" label_name \"=\" `\"` label_value `\"` } [ \",\" ] \"}\"\n] value [ timestamp ]\n```\n\nlabel_value可以是任何UTF-8格式的内容，但反斜杠、双引号、换行符 必须转义成\n\n```\n\\\\  \\\"  \\\\n\n```\n\nhistogram（直方图）  summary（汇总）类型的特别格式：\n\n1. 需要单独一行xxx_sum；\n2. 需要单独一行xxx_count；\n3. Each quantile of a summary named x is given as a separate sample line with the same name x and a label {quantile=\"y\"}；\n4. A histogram must have a bucket with {le=\"+Inf\"}. Its value must be identical to the value of x_count；\n5. histogram类型必须要有{le=\"+Inf\"}，并且值要和xxx_count一致；\n\n\n\n### 5.4 文本类型格式举例\n\n```\n# HELP http_requests_total The total number of HTTP requests.\n# TYPE http_requests_total counter\nhttp_requests_total{method=\"post\",code=\"200\"} 1027 1395066363000\nhttp_requests_total{method=\"post\",code=\"400\"}    3 1395066363000\n\n# Escaping in label values:\nmsdos_file_access_time_seconds{path=\"C:\\\\DIR\\\\FILE.TXT\",error=\"Cannot find file:\\n\\\"FILE.TXT\\\"\"} 1.458255915e9\n\n# Minimalistic line:\nmetric_without_timestamp_and_labels 12.47\n\n# A weird metric from before the epoch:\nsomething_weird{problem=\"division by zero\"} +Inf -3982045\n\n# A histogram, which has a pretty complex representation in the text format:\n# HELP http_request_duration_seconds A histogram of the request duration.\n# TYPE http_request_duration_seconds histogram\nhttp_request_duration_seconds_bucket{le=\"0.05\"} 24054\nhttp_request_duration_seconds_bucket{le=\"0.1\"} 33444\nhttp_request_duration_seconds_bucket{le=\"0.2\"} 100392\nhttp_request_duration_seconds_bucket{le=\"0.5\"} 129389\nhttp_request_duration_seconds_bucket{le=\"1\"} 133988\nhttp_request_duration_seconds_bucket{le=\"+Inf\"} 144320\nhttp_request_duration_seconds_sum 53423\nhttp_request_duration_seconds_count 144320\n\n# Finally a summary, which has a complex representation, too:\n# HELP rpc_duration_seconds A summary of the RPC duration in seconds.\n# TYPE rpc_duration_seconds summary\nrpc_duration_seconds{quantile=\"0.01\"} 3102\nrpc_duration_seconds{quantile=\"0.05\"} 3272\nrpc_duration_seconds{quantile=\"0.5\"} 4773\nrpc_duration_seconds{quantile=\"0.9\"} 9001\nrpc_duration_seconds{quantile=\"0.99\"} 76656\nrpc_duration_seconds_sum 1.7560473e+07\nrpc_duration_seconds_count 2693\n```\n\n\n\n## 6 导出器exporter\n\n### 6.1 概述\n\n​\t指标名，一般为导出程序名称作为前缀，例如， haproxy_up；\n\n​\t度量标准必须使用基本单位（例如秒，字节），并保留将其转换为更具可读性的图形工具；\n\n \t指标有效字符：`[a-zA-Z0-9:_]` ，其他任何字符都要用下划线_代替；\n\n​\t指标后缀`_sum`, `_count`, `_bucket` and `_total` 只可用在Summaries、 Histograms 、 Counters\n\n\n\n## 7 告警alertmanager程序\n\n​\t概述：\n\n​\t\tpromethues：根据配置文件prometheus.yml的rule_files告警规则，将告警信息存到promethues的磁盘，供promethues的前台页面查看；根据配置文件prometheus.yml的alerting（配置altermanager进程的ip 端口信息），将告警信息发送altermanager进程上。\n\n​\t\taltermanager：接收promethues发来的告警信息，存在磁盘中供altermanager进程的前台查看；同时根据altermanager的告警配置文件simple.yml发送邮件等提醒。\n\n\n\n### 7.1 promethues告警配置举例\n\n![](/img/promethues-alarm.png)\n\n### 7.2 promethues告警规则配置文件举例\n\n![](/img/rule_file.png)\n\nalert：自定义的告警含义简写\n\nexpor：告警条件，如上图的node_forks为具体mertics里的指标\n\nfor：周期\n\nlabels：severity，在alertmanager前台页面可以根据severity条件来查询告警信息\n\nannotations：summary写些较详细的告警信息\n\n### 7.3 alertmanager告警发送邮件提示\n\n![](/img/email_1.png)\n\n![](/img/email_2.png)\n\n\n\n### 7.4 启动alertmanager\n\n nohup ./alertmanager --config.file=simple.yml &\n\nalertmanager前台：http://192.168.7.176:9093/\n\n### 7.5通过其他方式告警\n\n#### hipchat_config：\n\n​\t是一款能够在苹果mac平台上运行的社交聊天软件，HipChat的功能和QQ相似，集聊天、视频、语音等功能于一身，不同之处在于HipChat界面更加的简洁、操作更加的流畅。\n\n#### pagerduty_config：\n\n​\t是一款能够在服务器出问题时发送提醒的软件。在发生问题时，提醒的方式包括屏幕显示、电话呼叫、短信通知、电邮通知等，而且在无人应答时还会自动将提醒级别提高。PagerDuty 不是免费的。\n\n#### pushover_config：\n\n​\t是一款网络通知推送服务，类似ifttt或脚本服务，你可以将需要推送的服务设置好后，遇到情况将把通知自动推送到你的[安卓手机](http://www.onlinedown.net/soft/222292.htm)。\n\n#### slack_config：\n\n​\tslack是聊天群组 + 大规模工具集成 + 文件整合 + 统一搜索。截至2014年底，Slack 已经整合了电子邮件、短信、[Google](https://baike.baidu.com/item/Google) Drives、[Twitter](https://baike.baidu.com/item/Twitter)、Trello、Asana、[GitHub](https://baike.baidu.com/item/GitHub) 等 65 种工具和服务，可以把各种碎片化的企业沟通和协作集中到一起。\n\n```\n# Whether or not to notify about resolved alerts.\n[ send_resolved: <boolean> | default = false ]\n\n# The Slack webhook URL.\n[ api_url: <secret> | default = global.slack_api_url ]\n\n# The channel or user to send notifications to.\nchannel: <tmpl_string>\n\n# API request data as defined by the Slack webhook API.\n[ color: <tmpl_string> | default = '{{ if eq .Status \"firing\" }}danger{{ else }}good{{ end }}' ]\n[ username: <tmpl_string> | default = '{{ template \"slack.default.username\" . }}' ]\n[ title: <tmpl_string> | default = '{{ template \"slack.default.title\" . }}' ]\n[ title_link: <tmpl_string> | default = '{{ template \"slack.default.titlelink\" . }}' ]\n[ icon_emoji: <tmpl_string> ]\n[ icon_url: <tmpl_string> ]\n[ pretext: <tmpl_string> | default = '{{ template \"slack.default.pretext\" . }}' ]\n[ text: <tmpl_string> | default = '{{ template \"slack.default.text\" . }}' ]\n[ fallback: <tmpl_string> | default = '{{ template \"slack.default.fallback\" . }}' ]\n\n# The HTTP client's configuration.\n[ http_config: <http_config> | default = global.http_config ]\n```\n\n\n\n#### opsgenie_config ：\n\n​\t集成电话短信邮件等等\n\n#### victorops_config：\n\n​\t聊天应用\n\n#### http_config：\n\n​\tA `http_config` allows configuring the HTTP client that the receiver uses to communicate with HTTP-based API services.\n\n\n\n\n\n## 8 问题笔记\n\n### 8.1已解决\n\n1. prometheus浏览器查询不到exporter指标数据，但是浏览器exporter的mertic有指标数据。原因是时间不同步\n\n\n\n### 8.2 未解决\n\n","slug":"prometheus","published":1,"updated":"2018-10-20T08:15:43.514Z","_id":"cjhajczp50011e4rwtcresxyd","comments":1,"layout":"post","photos":[],"link":"","content":"<h2 id=\"1-概述\"><a href=\"#1-概述\" class=\"headerlink\" title=\"1 概述\"></a>1 概述</h2><h3 id=\"1-1-主要功能\"><a href=\"#1-1-主要功能\" class=\"headerlink\" title=\"1.1 主要功能\"></a>1.1 主要功能</h3><ul>\n<li>多维 <a href=\"https://prometheus.io/docs/concepts/data_model/\" target=\"_blank\" rel=\"noopener\">数据模型</a>（时序由 metric 名字和 k/v 的 labels 构成）。</li>\n<li>灵活的查询语句（<a href=\"https://prometheus.io/docs/querying/basics/\" target=\"_blank\" rel=\"noopener\">PromQL</a>）。</li>\n<li>无依赖存储，支持 local 和 remote 不同模型。</li>\n<li>采用 http 协议，使用 pull 模式，拉取数据，简单易懂。</li>\n<li>监控目标，可以采用服务发现或静态配置的方式。</li>\n<li>支持多种统计数据模型，图形化友好。</li>\n</ul>\n<h3 id=\"1-2-核心组件\"><a href=\"#1-2-核心组件\" class=\"headerlink\" title=\"1.2 核心组件\"></a>1.2 核心组件</h3><ul>\n<li><a href=\"https://github.com/prometheus/prometheus\" target=\"_blank\" rel=\"noopener\">Prometheus Server</a>， 主要用于抓取数据和存储时序数据，另外还提供查询和 Alert Rule 配置管理。</li>\n<li><a href=\"https://prometheus.io/docs/instrumenting/clientlibs/\" target=\"_blank\" rel=\"noopener\">client libraries</a>，用于对接 Prometheus Server, 可以查询和上报数据。</li>\n<li><a href=\"https://github.com/prometheus/pushgateway\" target=\"_blank\" rel=\"noopener\">push gateway</a> ，用于批量，短期的监控数据的汇总节点，主要用于业务数据汇报等。</li>\n<li>各种汇报数据的 <a href=\"https://prometheus.io/docs/instrumenting/exporters/\" target=\"_blank\" rel=\"noopener\">exporters</a> ，例如汇报机器数据的 node_exporter,  汇报 MongoDB 信息的 <a href=\"https://github.com/dcu/mongodb_exporter\" target=\"_blank\" rel=\"noopener\">MongoDB exporter</a> 等等。</li>\n<li>用于告警通知管理的 <a href=\"https://github.com/prometheus/alertmanager\" target=\"_blank\" rel=\"noopener\">alertmanager</a> 。</li>\n</ul>\n<h3 id=\"1-3-基础架构\"><a href=\"#1-3-基础架构\" class=\"headerlink\" title=\"1.3 基础架构\"></a>1.3 基础架构</h3><p>一图胜千言，先来张官方的架构图</p>\n<p><img src=\"/img/prometheus1.png\" alt=\"img\"></p>\n<a id=\"more\"></a>\n<p>从这个架构图，也可以看出 Prometheus 的主要模块包含， Server,  Exporters, Pushgateway, PromQL, Alertmanager, WebUI 等。</p>\n<p>它大致使用逻辑是这样：</p>\n<ol>\n<li>Prometheus server 定期从静态配置的 targets 或者服务发现的 targets 拉取数据。</li>\n<li>当新拉取的数据大于配置内存缓存区的时候，Prometheus 会将数据持久化到磁盘（如果使用 remote storage 将持久化到云端）。</li>\n<li>Prometheus 可以配置 rules，然后定时查询数据，当条件触发的时候，会将 alert 推送到配置的 Alertmanager。</li>\n<li>Alertmanager 收到警告的时候，可以根据配置，聚合，去重，降噪，最后发送警告。</li>\n<li>可以使用 API， Prometheus Console 或者 Grafana 查询和聚合数据。</li>\n</ol>\n<h3 id=\"1-4-注意\"><a href=\"#1-4-注意\" class=\"headerlink\" title=\"1.4 注意\"></a>1.4 注意</h3><ul>\n<li>Prometheus 的数据是基于时序的 float64 的值，如果你的数据值有更多类型，无法满足。</li>\n<li>Prometheus 不适合做审计计费，因为它的数据是按一定时间采集的，关注的更多是系统的运行瞬时状态以及趋势，即使有少量数据没有采集也能容忍，但是审计计费需要记录每个请求，并且数据长期存储，这个和 Prometheus 无法满足，可能需要采用专门的审计系统。</li>\n</ul>\n<h2 id=\"2-BO关注项\"><a href=\"#2-BO关注项\" class=\"headerlink\" title=\"2 BO关注项\"></a>2 BO关注项</h2><h3 id=\"2-1-数据收集方式\"><a href=\"#2-1-数据收集方式\" class=\"headerlink\" title=\"2.1 数据收集方式\"></a>2.1 数据收集方式</h3><p>使用 pull 模式，拉取数据。</p>\n<h3 id=\"2-2-数据格式\"><a href=\"#2-2-数据格式\" class=\"headerlink\" title=\"2.2 数据格式\"></a>2.2 数据格式</h3><p>Prometheus 时序格式与 <a href=\"http://opentsdb.net/\" target=\"_blank\" rel=\"noopener\">OpenTSDB</a> 相似：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;metric name&gt;&#123;&lt;label name&gt;=&lt;label value&gt;, ...&#125;</span><br></pre></td></tr></table></figure>\n<p>其中包含时序名字以及时序的标签。</p>\n<h4 id=\"2-2-1-时序-4-种类型\"><a href=\"#2-2-1-时序-4-种类型\" class=\"headerlink\" title=\"2.2.1 时序 4 种类型\"></a>2.2.1 时序 4 种类型</h4><p>Prometheus 时序数据分为 <a href=\"https://prometheus.io/docs/concepts/metric_types/#counter\" target=\"_blank\" rel=\"noopener\">Counter</a>, <a href=\"https://prometheus.io/docs/concepts/metric_types/#gauge\" target=\"_blank\" rel=\"noopener\">Gauge</a>, <a href=\"https://prometheus.io/docs/concepts/metric_types/#histogram\" target=\"_blank\" rel=\"noopener\">Histogram</a>, <a href=\"https://prometheus.io/docs/concepts/metric_types/#summary\" target=\"_blank\" rel=\"noopener\">Summary</a> 四种类型。</p>\n<h5 id=\"2-2-1-1-Counter\"><a href=\"#2-2-1-1-Counter\" class=\"headerlink\" title=\"2.2.1.1 Counter\"></a>2.2.1.1 Counter</h5><p>Counter 表示收集的数据是按照某个趋势（增加／减少）一直变化的，我们往往用它记录服务请求总量，错误总数等。</p>\n<p>例如 Prometheus server 中 <code>http_requests_total</code>,  表示 Prometheus 处理的 http 请求总数，我们可以使用 <code>delta</code>, 很容易得到任意区间数据的增量，这个会在 PromQL 一节中细讲。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># HELP http_requests_total Total number of HTTP requests made.</span><br><span class=\"line\"># TYPE http_requests_total counter</span><br><span class=\"line\">http_requests_total&#123;code=&quot;200&quot;,handler=&quot;alerts&quot;,method=&quot;get&quot;&#125; 2</span><br><span class=\"line\">http_requests_total&#123;code=&quot;200&quot;,handler=&quot;config&quot;,method=&quot;get&quot;&#125; 1</span><br><span class=\"line\">http_requests_total&#123;code=&quot;200&quot;,handler=&quot;flags&quot;,method=&quot;get&quot;&#125; 2</span><br><span class=\"line\">http_requests_total&#123;code=&quot;200&quot;,handler=&quot;graph&quot;,method=&quot;get&quot;&#125; 6</span><br><span class=\"line\">http_requests_total&#123;code=&quot;200&quot;,handler=&quot;label_values&quot;,method=&quot;get&quot;&#125; 6</span><br><span class=\"line\">http_requests_total&#123;code=&quot;200&quot;,handler=&quot;prometheus&quot;,method=&quot;get&quot;&#125; 24755</span><br><span class=\"line\">http_requests_total&#123;code=&quot;200&quot;,handler=&quot;query&quot;,method=&quot;get&quot;&#125; 6</span><br><span class=\"line\">http_requests_total&#123;code=&quot;200&quot;,handler=&quot;static&quot;,method=&quot;get&quot;&#125; 6</span><br><span class=\"line\">http_requests_total&#123;code=&quot;200&quot;,handler=&quot;status&quot;,method=&quot;get&quot;&#125; 2</span><br><span class=\"line\">http_requests_total&#123;code=&quot;200&quot;,handler=&quot;targets&quot;,method=&quot;get&quot;&#125; 4</span><br><span class=\"line\">http_requests_total&#123;code=&quot;304&quot;,handler=&quot;static&quot;,method=&quot;get&quot;&#125; 4</span><br></pre></td></tr></table></figure>\n<h5 id=\"2-2-1-2-Gauge\"><a href=\"#2-2-1-2-Gauge\" class=\"headerlink\" title=\"2.2.1.2 Gauge\"></a>2.2.1.2 Gauge</h5><p>Gauge 表示搜集的数据是一个瞬时的，与时间没有关系，可以任意变高变低，往往可以用来记录内存使用率、磁盘使用率等。</p>\n<p>例如 Prometheus server 中 <code>go_goroutines</code>,  表示 Prometheus 当前 goroutines 的数量。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># HELP go_goroutines Number of goroutines that currently exist.</span><br><span class=\"line\"># TYPE go_goroutines gauge</span><br><span class=\"line\">go_goroutines 100</span><br></pre></td></tr></table></figure>\n<h5 id=\"2-2-1-3-Histogram\"><a href=\"#2-2-1-3-Histogram\" class=\"headerlink\" title=\"2.2.1.3 Histogram\"></a>2.2.1.3 Histogram</h5><p>Histogram 由 <code>&lt;basename&gt;_bucket{le=&quot;&lt;upper inclusive bound&gt;&quot;}</code>，<code>&lt;basename&gt;_bucket{le=&quot;+Inf&quot;}</code>, <code>&lt;basename&gt;_sum</code>，<code>&lt;basename&gt;_count</code> 组成，主要用于表示一段时间范围内对数据进行采样，（通常是请求持续时间或响应大小），并能够对其指定区间以及总数进行统计，通常我们用它计算分位数的直方图。</p>\n<p>例如 Prometheus server 中 <code>prometheus_local_storage_series_chunks_persisted</code>,  表示 Prometheus 中每个时序需要存储的 chunks 数量，我们可以用它计算待持久化的数据的分位数。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># HELP prometheus_tsdb_compaction_chunk_range Final time range of chunks on their first compaction</span><br><span class=\"line\"># TYPE prometheus_tsdb_compaction_chunk_range histogram</span><br><span class=\"line\">prometheus_tsdb_compaction_chunk_range_bucket&#123;le=&quot;100&quot;&#125; 0</span><br><span class=\"line\">prometheus_tsdb_compaction_chunk_range_bucket&#123;le=&quot;400&quot;&#125; 0</span><br><span class=\"line\">prometheus_tsdb_compaction_chunk_range_bucket&#123;le=&quot;1600&quot;&#125; 0</span><br><span class=\"line\">prometheus_tsdb_compaction_chunk_range_bucket&#123;le=&quot;6400&quot;&#125; 0</span><br><span class=\"line\">prometheus_tsdb_compaction_chunk_range_bucket&#123;le=&quot;25600&quot;&#125; 0</span><br><span class=\"line\">prometheus_tsdb_compaction_chunk_range_bucket&#123;le=&quot;102400&quot;&#125; 0</span><br><span class=\"line\">prometheus_tsdb_compaction_chunk_range_bucket&#123;le=&quot;409600&quot;&#125; 605</span><br><span class=\"line\">prometheus_tsdb_compaction_chunk_range_bucket&#123;le=&quot;1.6384e+06&quot;&#125; 612</span><br><span class=\"line\">prometheus_tsdb_compaction_chunk_range_bucket&#123;le=&quot;6.5536e+06&quot;&#125; 126358</span><br><span class=\"line\">prometheus_tsdb_compaction_chunk_range_bucket&#123;le=&quot;2.62144e+07&quot;&#125; 126358</span><br><span class=\"line\">prometheus_tsdb_compaction_chunk_range_bucket&#123;le=&quot;+Inf&quot;&#125; 126358</span><br><span class=\"line\">prometheus_tsdb_compaction_chunk_range_sum 2.25313627417e+11</span><br><span class=\"line\">prometheus_tsdb_compaction_chunk_range_count 126358</span><br></pre></td></tr></table></figure>\n<h5 id=\"2-2-1-4-Summary\"><a href=\"#2-2-1-4-Summary\" class=\"headerlink\" title=\"2.2.1.4 Summary\"></a>2.2.1.4 Summary</h5><p>Summary 和 Histogram 类似，由 <code>&lt;basename&gt;{quantile=&quot;&lt;φ&gt;&quot;}</code>，<code>&lt;basename&gt;_sum</code>，<code>&lt;basename&gt;_count</code> 组成，主要用于表示一段时间内数据采样结果，（通常是请求持续时间或响应大小），它直接存储了 quantile 数据，而不是根据统计区间计算出来的。</p>\n<p>例如 Prometheus server 中 <code>prometheus_target_interval_length_seconds</code>。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># HELP prometheus_target_interval_length_seconds Actual intervals between scrapes.</span><br><span class=\"line\"># TYPE prometheus_target_interval_length_seconds summary</span><br><span class=\"line\">prometheus_target_interval_length_seconds&#123;interval=&quot;15s&quot;,quantile=&quot;0.01&quot;&#125; 14.999987534</span><br><span class=\"line\">prometheus_target_interval_length_seconds&#123;interval=&quot;15s&quot;,quantile=&quot;0.05&quot;&#125; 14.999987534</span><br><span class=\"line\">prometheus_target_interval_length_seconds&#123;interval=&quot;15s&quot;,quantile=&quot;0.5&quot;&#125; 15.000020575</span><br><span class=\"line\">prometheus_target_interval_length_seconds&#123;interval=&quot;15s&quot;,quantile=&quot;0.9&quot;&#125; 15.000045415</span><br><span class=\"line\">prometheus_target_interval_length_seconds&#123;interval=&quot;15s&quot;,quantile=&quot;0.99&quot;&#125; 15.000050555</span><br><span class=\"line\">prometheus_target_interval_length_seconds_sum&#123;interval=&quot;15s&quot;&#125; 371280.61110144516</span><br><span class=\"line\">prometheus_target_interval_length_seconds_count&#123;interval=&quot;15s&quot;&#125; 24752</span><br></pre></td></tr></table></figure>\n<h5 id=\"2-2-1-5-Histogram-vs-Summary\"><a href=\"#2-2-1-5-Histogram-vs-Summary\" class=\"headerlink\" title=\"2.2.1.5 Histogram vs Summary\"></a>2.2.1.5 Histogram vs Summary</h5><ul>\n<li>都包含 <code>&lt;basename&gt;_sum</code>，<code>&lt;basename&gt;_count</code></li>\n<li>Histogram 需要通过 <code>&lt;basename&gt;_bucket</code> 计算 quantile, 而 Summary 直接存储了 quantile 的值。</li>\n</ul>\n<h3 id=\"2-2-3-数据存储方式\"><a href=\"#2-2-3-数据存储方式\" class=\"headerlink\" title=\"2.2.3 数据存储方式\"></a>2.2.3 数据存储方式</h3><p>​    数据存在promethues自身的数据库，以数据文件的形式存储，有自身的查询方式：promql；详见<a href=\"https://songjiayang.gitbooks.io/prometheus/content/promql/summary.html\" target=\"_blank\" rel=\"noopener\">https://songjiayang.gitbooks.io/prometheus/content/promql/summary.html</a></p>\n<h3 id=\"2-2-4-数据输出方式\"><a href=\"#2-2-4-数据输出方式\" class=\"headerlink\" title=\"2.2.4 数据输出方式\"></a>2.2.4 数据输出方式</h3><p>​    agent：被动拉取；</p>\n<p>​    promethues server：主动拉取客户端的数据。promethues将拉取到的数据存到data/目录。（除了 promethues 前台的PromQ查询页面，应该有某种工具可以直接在命令行查询promethues的历史数据（暂未找到）；多种导出工具，可以支持Prometheus存储数据转化为HAProxy、StatsD、Graphite等工具所需要的数据存储格式（工具未研究过）。）</p>\n<h3 id=\"2-2-5-agent部署方式\"><a href=\"#2-2-5-agent部署方式\" class=\"headerlink\" title=\"2.2.5 agent部署方式\"></a>2.2.5 agent部署方式</h3><p>​    promethues未提供自动部署agent的功能。</p>\n<h3 id=\"2-2-6-任务下发方式\"><a href=\"#2-2-6-任务下发方式\" class=\"headerlink\" title=\"2.2.6 任务下发方式\"></a>2.2.6 任务下发方式</h3><p>​    agent每个周期固定采集设备的指定指标，若要自定义采集某些指标则需要修改agent源码。</p>\n<p>​    promethues server拉取数据的任务在prometheus.yml配置。</p>\n<h2 id=\"3-promethues组件及部署\"><a href=\"#3-promethues组件及部署\" class=\"headerlink\" title=\"3 promethues组件及部署\"></a>3 promethues组件及部署</h2><h3 id=\"3-1-promethues-server\"><a href=\"#3-1-promethues-server\" class=\"headerlink\" title=\"3.1 promethues server\"></a>3.1 promethues server</h3><h4 id=\"3-1-1-部署\"><a href=\"#3-1-1-部署\" class=\"headerlink\" title=\"3.1.1 部署\"></a>3.1.1 部署</h4><p>​    tar包解压即可用</p>\n<h4 id=\"3-1-2-prometheus-yml配置举例\"><a href=\"#3-1-2-prometheus-yml配置举例\" class=\"headerlink\" title=\"3.1.2 prometheus.yml配置举例\"></a>3.1.2 prometheus.yml配置举例</h4><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">global:</span><br><span class=\"line\">  scrape_interval:     15s # By default, scrape targets every 15 seconds.</span><br><span class=\"line\">  evaluation_interval: 15s # By default, scrape targets every 15 seconds.</span><br><span class=\"line\"></span><br><span class=\"line\">rule_files:</span><br><span class=\"line\">  - &quot;rules/node.rules&quot;</span><br><span class=\"line\"></span><br><span class=\"line\">scrape_configs:</span><br><span class=\"line\">  - job_name: &apos;prometheus&apos;</span><br><span class=\"line\">    scrape_interval: 5s</span><br><span class=\"line\">    static_configs:</span><br><span class=\"line\">      - targets: [&apos;localhost:9090&apos;]</span><br><span class=\"line\"></span><br><span class=\"line\">  - job_name: &apos;node&apos;</span><br><span class=\"line\">    scrape_interval: 8s</span><br><span class=\"line\">    static_configs:</span><br><span class=\"line\">      - targets: [&apos;127.0.0.1:9100&apos;, &apos;127.0.0.12:9100&apos;]</span><br><span class=\"line\"></span><br><span class=\"line\">  - job_name: &apos;mysqld&apos;</span><br><span class=\"line\">    static_configs:</span><br><span class=\"line\">      - targets: [&apos;127.0.0.1:9104&apos;]</span><br><span class=\"line\">  - job_name: &apos;memcached&apos;</span><br><span class=\"line\">    static_configs:</span><br><span class=\"line\">      - targets: [&apos;127.0.0.1:9150&apos;]</span><br></pre></td></tr></table></figure>\n<h4 id=\"3-1-3-命令\"><a href=\"#3-1-3-命令\" class=\"headerlink\" title=\"3.1.3 命令\"></a>3.1.3 命令</h4><p>[chenrj@kfapp01 prometheus-2.0.0.linux-amd64]$ ./prometheus -h<br>usage: prometheus [<flags>]</flags></p>\n<p>The Prometheus monitoring server</p>\n<p>Flags:<br>  -h, –help                     Show context-sensitive help (also try –help-long and –help-man).</p>\n<h4 id=\"3-1-4-前台地址\"><a href=\"#3-1-4-前台地址\" class=\"headerlink\" title=\"3.1.4 前台地址\"></a>3.1.4 前台地址</h4><p>​    <a href=\"http://192.168.7.40:9090/graph\" target=\"_blank\" rel=\"noopener\">http://192.168.7.40:9090/graph</a></p>\n<p>​    默认9090端口</p>\n<h3 id=\"3-2-grafana\"><a href=\"#3-2-grafana\" class=\"headerlink\" title=\"3.2 grafana\"></a>3.2 grafana</h3><p>​       <a href=\"http://192.168.7.40:3000/\" target=\"_blank\" rel=\"noopener\">http://192.168.7.40:3000</a></p>\n<p>​       端口默认3000，</p>\n<p>​       用户密码： admin/admin</p>\n<p>​    ./grafana-server</p>\n<h3 id=\"3-3-主机节点\"><a href=\"#3-3-主机节点\" class=\"headerlink\" title=\"3.3 主机节点\"></a>3.3 主机节点</h3><p>​    <a href=\"http://10.140.20.142:9100/metrics\" target=\"_blank\" rel=\"noopener\">http://10.140.20.142:9100/metrics</a></p>\n<h3 id=\"3-4-redis节点\"><a href=\"#3-4-redis节点\" class=\"headerlink\" title=\"3.4 redis节点\"></a>3.4 redis节点</h3><p>​    <a href=\"http://10.140.20.143:9121/metrics\" target=\"_blank\" rel=\"noopener\">http://10.140.20.143:9121/metrics</a> </p>\n<h3 id=\"3-5-elasticsearch节点\"><a href=\"#3-5-elasticsearch节点\" class=\"headerlink\" title=\"3.5 elasticsearch节点\"></a>3.5 elasticsearch节点</h3><p>​    <a href=\"http://10.140.20.146:9108/metrics\" target=\"_blank\" rel=\"noopener\">http://10.140.20.146:9108/metrics</a></p>\n<h2 id=\"4-数据查询\"><a href=\"#4-数据查询\" class=\"headerlink\" title=\"4 数据查询\"></a>4 数据查询</h2><h3 id=\"4-1-http方式查询promethues数据\"><a href=\"#4-1-http方式查询promethues数据\" class=\"headerlink\" title=\"4.1 http方式查询promethues数据\"></a>4.1 http方式查询promethues数据</h3><p>​    <a href=\"https://prometheus.io/docs/prometheus/latest/querying/api/#querying-metadata\" target=\"_blank\" rel=\"noopener\">https://prometheus.io/docs/prometheus/latest/querying/api/#querying-metadata</a></p>\n<h4 id=\"4-1-1-即时查询\"><a href=\"#4-1-1-即时查询\" class=\"headerlink\" title=\"4.1.1 即时查询\"></a>4.1.1 即时查询</h4><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">GET /api/v1/query</span><br></pre></td></tr></table></figure>\n<p><strong>URL查询参数：</strong></p>\n<ul>\n<li><code>query=&lt;string&gt;</code>：普罗米修斯表达查询字符串。</li>\n<li><code>time=&lt;rfc3339 | unix_timestamp&gt;</code>：评估时间戳。可选的。</li>\n<li><code>timeout=&lt;duration&gt;</code>：评价超时。可选的。默认为，并通过价值上限<code>-query.timeout</code>标志。</li>\n</ul>\n<p>若省略时间time测试，则默认使用服务器时间</p>\n<p><strong>例：</strong>查询2018-01-16T03:12:51.781这个时刻go_memstats_frees_total的值</p>\n<p>[logstash@CP-ITSM-OMC-ZSC05 supervisor]$ curl ‘<a href=\"http://10.140.20.146:9090/api/v1/query?query=go_memstats_frees_total&amp;time=2018-01-16T03:12:51.781Z\" target=\"_blank\" rel=\"noopener\">http://10.140.20.146:9090/api/v1/query?query=go_memstats_frees_total&amp;time=2018-01-16T03:12:51.781Z</a>‘<br>{“status”:”success”,”data”:{“resultType”:”vector”,”result”:[{“metric”:{“<strong>name</strong>“:”go_memstats_frees_total”,”instance”:”10.140.20.142:9100”,”job”:”node”},”value”:[1516072371.781,”5599415948”]},{“metric”:{“<strong>name</strong>“:”go_memstats_frees_total”,”instance”:”10.140.20.143:9100”,”job”:”node”},”value”:[1516072371.781,”5152870637”]},{“metric”:{“<strong>name</strong>“:”go_memstats_frees_total”,”instance”:”10.140.20.143:9108”,”job”:”elasticsearch_exporter”},”value”:[1516072371.781,”1385642849”]},{“metric”:{“<strong>name</strong>“:”go_memstats_frees_total”,”instance”:”10.140.20.143:9121”,”job”:”redis_exporter_143”},”value”:[1516072371.781,”159639669”]},{“metric”:{“<strong>name</strong>“:”go_memstats_frees_total”,”instance”:”10.140.20.144:9100”,”job”:”node”},”value”:[1516072371.781,”5167404030”]},{“metric”:{“<strong>name</strong>“:”go_memstats_frees_total”,”instance”:”10.140.20.144:9108”,”job”:”elasticsearch_exporter”},”value”:[1516072371.781,”1383957758”]},{“metric”:{“<strong>name</strong>“:”go_memstats_frees_total”,”instance”:”10.140.20.144:9121”,”job”:”redis_exporter_144”},”value”:[1516072371.781,”373190465”]},{“metric”:{“<strong>name</strong>“:”go_memstats_frees_total”,”instance”:”10.140.20.145:9100”,”job”:”node”},”value”:[1516072371.781,”5124941908”]},{“metric”:{“<strong>name</strong>“:”go_memstats_frees_total”,”instance”:”10.140.20.145:9108”,”job”:”elasticsearch_exporter”},”value”:[1516072371.781,”1370943258”]},{“metric”:{“<strong>name</strong>“:”go_memstats_frees_total”,”instance”:”10.140.20.146:9100”,”job”:”node”},”value”:[1516072371.781,”4850755799”]},{“metric”:{“<strong>name</strong>“:”go_memstats_frees_total”,”instance”:”10.140.20.146:9108”,”job”:”elasticsearch_exporter”},”value”:[1516072371.781,”1370683906”]},{“metric”:{“<strong>name</strong>“:”go_memstats_frees_total”,”instance”:”localhost:9090”,”job”:”prometheus”},”value”:[1516072371.781,”2299674805”]}]}}</p>\n<p><strong>注：”value”:[时间戳,”对应值”]</strong></p>\n<h4 id=\"4-1-2-范围查询\"><a href=\"#4-1-2-范围查询\" class=\"headerlink\" title=\"4.1.2 范围查询\"></a>4.1.2 范围查询</h4><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">GET /api/v1/query_range</span><br></pre></td></tr></table></figure>\n<p><strong>URL查询参数：</strong></p>\n<ul>\n<li><code>query=&lt;string&gt;</code>：普罗米修斯表达查询字符串。</li>\n<li><code>start=&lt;rfc3339 | unix_timestamp&gt;</code>：开始时间戳。</li>\n<li><code>end=&lt;rfc3339 | unix_timestamp&gt;</code>：结束时间戳。</li>\n<li><code>step=&lt;duration&gt;</code>：查询分辨率步的宽度。</li>\n<li><code>timeout=&lt;duration&gt;</code>：评价超时。可选的。默认为，并通过价值上限<code>-query.timeout</code>标志。</li>\n</ul>\n<p>例：时间在2018-01-01T20:10:30.781到2018-01-01T20:11:00.781范围内，间隔15秒，up的数据</p>\n<p>[logstash@CP-ITSM-OMC-ZSC05 supervisor]$ curl ‘<a href=\"http://10.140.20.146:9090/api/v1/query_range?query=up&amp;start=2018-01-01T20:10:30.781Z&amp;end=2018-01-01T20:11:00.781Z&amp;step=15s\" target=\"_blank\" rel=\"noopener\">http://10.140.20.146:9090/api/v1/query_range?query=up&amp;start=2018-01-01T20:10:30.781Z&amp;end=2018-01-01T20:11:00.781Z&amp;step=15s</a>‘<br>{“status”:”success”,”data”:{“resultType”:”matrix”,”result”:[{“metric”:{“<strong>name</strong>“:”up”,”instance”:”localhost:9090”,”job”:”prometheus”},”values”:[[1514837430.781,”1”],[1514837445.781,”1”],[1514837460.781,”1”]]},{“metric”:{“<strong>name</strong>“:”up”,”instance”:”localhost:9100”,”job”:”node”},”values”:[[1514837430.781,”0”],[1514837445.781,”0”],[1514837460.781,”0”]]}]}}</p>\n<h3 id=\"4-2-http查询方式作用未知系列？？\"><a href=\"#4-2-http查询方式作用未知系列？？\" class=\"headerlink\" title=\"4.2 http查询方式作用未知系列？？\"></a>4.2 http查询方式作用未知系列？？</h3><h4 id=\"Querying-metadata\"><a href=\"#Querying-metadata\" class=\"headerlink\" title=\"Querying metadata\"></a>Querying metadata</h4><h5 id=\"Finding-series-by-label-matchers\"><a href=\"#Finding-series-by-label-matchers\" class=\"headerlink\" title=\"Finding series by label matchers\"></a>Finding series by label matchers</h5><p>The following endpoint returns the list of time series that match a certain label set.</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">GET /api/v1/series</span><br></pre></td></tr></table></figure>\n<p>URL query parameters:</p>\n<ul>\n<li><code>match[]=&lt;series_selector&gt;</code>: Repeated series selector argument that selects the series to return. At least one <code>match[]</code> argument must be provided.</li>\n<li><code>start=&lt;rfc3339 | unix_timestamp&gt;</code>: Start timestamp.</li>\n<li><code>end=&lt;rfc3339 | unix_timestamp&gt;</code>: End timestamp.</li>\n</ul>\n<p>The <code>data</code> section of the query result consists of a list of objects that contain the label name/value pairs which identify each series.</p>\n<p>The following example returns all series that match either of the selectors <code>up</code> or <code>process_start_time_seconds{job=&quot;prometheus&quot;}</code>:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ curl -g &apos;http://localhost:9090/api/v1/series?match[]=up&amp;match[]=process_start_time_seconds&#123;job=&quot;prometheus&quot;&#125;&apos;</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">   &quot;status&quot; : &quot;success&quot;,</span><br><span class=\"line\">   &quot;data&quot; : [</span><br><span class=\"line\">      &#123;</span><br><span class=\"line\">         &quot;__name__&quot; : &quot;up&quot;,</span><br><span class=\"line\">         &quot;job&quot; : &quot;prometheus&quot;,</span><br><span class=\"line\">         &quot;instance&quot; : &quot;localhost:9090&quot;</span><br><span class=\"line\">      &#125;,</span><br><span class=\"line\">      &#123;</span><br><span class=\"line\">         &quot;__name__&quot; : &quot;up&quot;,</span><br><span class=\"line\">         &quot;job&quot; : &quot;node&quot;,</span><br><span class=\"line\">         &quot;instance&quot; : &quot;localhost:9091&quot;</span><br><span class=\"line\">      &#125;,</span><br><span class=\"line\">      &#123;</span><br><span class=\"line\">         &quot;__name__&quot; : &quot;process_start_time_seconds&quot;,</span><br><span class=\"line\">         &quot;job&quot; : &quot;prometheus&quot;,</span><br><span class=\"line\">         &quot;instance&quot; : &quot;localhost:9090&quot;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">   ]</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h5 id=\"Querying-label-values\"><a href=\"#Querying-label-values\" class=\"headerlink\" title=\"Querying label values\"></a>Querying label values</h5><p>The following endpoint returns a list of label values for a provided label name:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">GET /api/v1/label/&lt;label_name&gt;/values</span><br></pre></td></tr></table></figure>\n<p>The <code>data</code> section of the JSON response is a list of string label names.</p>\n<p>This example queries for all label values for the <code>job</code> label:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ curl http://localhost:9090/api/v1/label/job/values</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">   &quot;status&quot; : &quot;success&quot;,</span><br><span class=\"line\">   &quot;data&quot; : [</span><br><span class=\"line\">      &quot;node&quot;,</span><br><span class=\"line\">      &quot;prometheus&quot;</span><br><span class=\"line\">   ]</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h4 id=\"Expression-query-result-formats\"><a href=\"#Expression-query-result-formats\" class=\"headerlink\" title=\"Expression query result formats\"></a>Expression query result formats</h4><p>Expression queries may return the following response values in the <code>result</code> property of the <code>data</code> section. <code>&lt;sample_value&gt;</code> placeholders are numeric sample values. JSON does not support special float values such as <code>NaN</code>, <code>Inf</code>, and <code>-Inf</code>, so sample values are transferred as quoted JSON strings rather than raw numbers.</p>\n<h5 id=\"Range-vectors\"><a href=\"#Range-vectors\" class=\"headerlink\" title=\"Range vectors\"></a>Range vectors</h5><p>Range vectors are returned as result type <code>matrix</code>. The corresponding <code>result</code> property has the following format:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[</span><br><span class=\"line\">  &#123;</span><br><span class=\"line\">    &quot;metric&quot;: &#123; &quot;&lt;label_name&gt;&quot;: &quot;&lt;label_value&gt;&quot;, ... &#125;,</span><br><span class=\"line\">    &quot;values&quot;: [ [ &lt;unix_time&gt;, &quot;&lt;sample_value&gt;&quot; ], ... ]</span><br><span class=\"line\">  &#125;,</span><br><span class=\"line\">  ...</span><br><span class=\"line\">]</span><br></pre></td></tr></table></figure>\n<h5 id=\"Instant-vectors\"><a href=\"#Instant-vectors\" class=\"headerlink\" title=\"Instant vectors\"></a>Instant vectors</h5><p>Instant vectors are returned as result type <code>vector</code>. The corresponding <code>result</code> property has the following format:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[</span><br><span class=\"line\">  &#123;</span><br><span class=\"line\">    &quot;metric&quot;: &#123; &quot;&lt;label_name&gt;&quot;: &quot;&lt;label_value&gt;&quot;, ... &#125;,</span><br><span class=\"line\">    &quot;value&quot;: [ &lt;unix_time&gt;, &quot;&lt;sample_value&gt;&quot; ]</span><br><span class=\"line\">  &#125;,</span><br><span class=\"line\">  ...</span><br><span class=\"line\">]</span><br></pre></td></tr></table></figure>\n<h5 id=\"Scalars\"><a href=\"#Scalars\" class=\"headerlink\" title=\"Scalars\"></a>Scalars</h5><p>Scalar results are returned as result type <code>scalar</code>. The corresponding <code>result</code> property has the following format:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[ &lt;unix_time&gt;, &quot;&lt;scalar_value&gt;&quot; ]</span><br></pre></td></tr></table></figure>\n<h5 id=\"Strings\"><a href=\"#Strings\" class=\"headerlink\" title=\"Strings\"></a>Strings</h5><p>String results are returned as result type <code>string</code>. The corresponding <code>result</code> property has the following format:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[ &lt;unix_time&gt;, &quot;&lt;string_value&gt;&quot; ]</span><br></pre></td></tr></table></figure>\n<h4 id=\"Targets\"><a href=\"#Targets\" class=\"headerlink\" title=\"Targets\"></a>Targets</h4><blockquote>\n<p>This API is experimental as it is intended to be extended with targets dropped due to relabelling in the future.</p>\n</blockquote>\n<p>The following endpoint returns an overview of the current state of the Prometheus target discovery:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">GET /api/v1/targets</span><br></pre></td></tr></table></figure>\n<p>Currently only the active targets are part of the response.</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ curl http://localhost:9090/api/v1/targets</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;status&quot;: &quot;success&quot;,                                                                                                                                [3/11]</span><br><span class=\"line\">  &quot;data&quot;: &#123;</span><br><span class=\"line\">    &quot;activeTargets&quot;: [</span><br><span class=\"line\">      &#123;</span><br><span class=\"line\">        &quot;discoveredLabels&quot;: &#123;</span><br><span class=\"line\">          &quot;__address__&quot;: &quot;127.0.0.1:9090&quot;,</span><br><span class=\"line\">          &quot;__metrics_path__&quot;: &quot;/metrics&quot;,</span><br><span class=\"line\">          &quot;__scheme__&quot;: &quot;http&quot;,</span><br><span class=\"line\">          &quot;job&quot;: &quot;prometheus&quot;</span><br><span class=\"line\">        &#125;,</span><br><span class=\"line\">        &quot;labels&quot;: &#123;</span><br><span class=\"line\">          &quot;instance&quot;: &quot;127.0.0.1:9090&quot;,</span><br><span class=\"line\">          &quot;job&quot;: &quot;prometheus&quot;</span><br><span class=\"line\">        &#125;,</span><br><span class=\"line\">        &quot;scrapeUrl&quot;: &quot;http://127.0.0.1:9090/metrics&quot;,</span><br><span class=\"line\">        &quot;lastError&quot;: &quot;&quot;,</span><br><span class=\"line\">        &quot;lastScrape&quot;: &quot;2017-01-17T15:07:44.723715405+01:00&quot;,</span><br><span class=\"line\">        &quot;health&quot;: &quot;up&quot;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    ]</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h4 id=\"Alertmanagers\"><a href=\"#Alertmanagers\" class=\"headerlink\" title=\"Alertmanagers\"></a>Alertmanagers</h4><blockquote>\n<p>This API is experimental as it is intended to be extended with Alertmanagers dropped due to relabelling in the future.</p>\n</blockquote>\n<p>The following endpoint returns an overview of the current state of the Prometheus alertmanager discovery:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">GET /api/v1/alertmanagers</span><br></pre></td></tr></table></figure>\n<p>Currently only the active Alertmanagers are part of the response.</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ curl http://localhost:9090/api/v1/alertmanagers</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;status&quot;: &quot;success&quot;,</span><br><span class=\"line\">  &quot;data&quot;: &#123;</span><br><span class=\"line\">    &quot;activeAlertmanagers&quot;: [</span><br><span class=\"line\">      &#123;</span><br><span class=\"line\">        &quot;url&quot;: &quot;http://127.0.0.1:9090/api/v1/alerts&quot;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    ]</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>查询指标标签：    curl -g ‘<a href=\"http://192.168.7.40:9090/api/v1/series?match[]=up&amp;match[]=process_start_time_seconds{job=&quot;prometheus&quot;}\" target=\"_blank\" rel=\"noopener\">http://192.168.7.40:9090/api/v1/series?match[]=up&amp;match[]=process_start_time_seconds{job=&quot;prometheus&quot;}</a>‘</p>\n<p>查询标签 {“status”:”success”,”data”:[]}[logstash@CP-ITSM-OMC-ZSC05 supervisor]$ curl <a href=\"http://10.140.20.146:9090/api/v1/label/job/values\" target=\"_blank\" rel=\"noopener\">http://10.140.20.146:9090/api/v1/label/job/values</a><br>{“status”:”success”,”data”:[“elasticsearch_exporter”,”node”,”prometheus”,”redis_exporter”,”redis_exporter_143”,”redis_exporter_144”]}</p>\n<h3 id=\"4-3-查询节点数据\"><a href=\"#4-3-查询节点数据\" class=\"headerlink\" title=\"4.3 查询节点数据\"></a>4.3 查询节点数据</h3><p>​    查询节点exporter的所有数据：curl -s <a href=\"http://192.168.7.40:9100/metrics\" target=\"_blank\" rel=\"noopener\">http://192.168.7.40:9100/metrics</a></p>\n<h2 id=\"5-exporter格式\"><a href=\"#5-exporter格式\" class=\"headerlink\" title=\"5 exporter格式\"></a>5 exporter格式</h2><p>基于协议缓冲区格式 和 文本格式</p>\n<p>客户端可以暴露promethues无法解析的其他格式</p>\n<h3 id=\"5-1-基于协议缓冲区格式-和-文本格式-的区别\"><a href=\"#5-1-基于协议缓冲区格式-和-文本格式-的区别\" class=\"headerlink\" title=\"5.1 基于协议缓冲区格式 和 文本格式 的区别\"></a>5.1 基于协议缓冲区格式 和 文本格式 的区别</h3><table>\n<thead>\n<tr>\n<th></th>\n<th>Protocol buffer format</th>\n<th>Text format</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>Inception</strong></td>\n<td>April 2014</td>\n<td>April 2014</td>\n</tr>\n<tr>\n<td><strong>Supported in</strong></td>\n<td>Prometheus version <code>&gt;=0.4.0</code></td>\n<td>Prometheus version <code>&gt;=0.4.0</code></td>\n</tr>\n<tr>\n<td><strong>Transmission</strong></td>\n<td>HTTP</td>\n<td>HTTP</td>\n</tr>\n<tr>\n<td><strong>Encoding</strong></td>\n<td><a href=\"https://developers.google.com/protocol-buffers/docs/reference/java/com/google/protobuf/AbstractMessageLite#writeDelimitedTo(java.io.OutputStream\" target=\"_blank\" rel=\"noopener\">32-bit varint-encoded record length-delimited</a>) Protocol Buffer messages of type <a href=\"https://github.com/prometheus/client_model/blob/086fe7ca28bde6cec2acd5223423c1475a362858/metrics.proto#L76-%20%20L81\" target=\"_blank\" rel=\"noopener\">io.prometheus.client.MetricFamily</a></td>\n<td>UTF-8, <code>\\n</code> line endings</td>\n</tr>\n<tr>\n<td><strong>HTTP Content-Type</strong></td>\n<td><code>application/vnd.google.protobuf; proto=io.prometheus.client.MetricFamily; encoding=delimited</code></td>\n<td><code>text/plain; version=0.0.4</code> (A missing <code>version</code> value will lead to a fall-back to the most recent text format version.)</td>\n</tr>\n<tr>\n<td><strong>Optional HTTP Content-Encoding</strong></td>\n<td><code>gzip</code></td>\n<td><code>gzip</code></td>\n</tr>\n<tr>\n<td><strong>Advantages</strong></td>\n<td>Cross-platformSizeEncoding and decoding costsStrict schemaSupports concatenation and theoretically streaming (only server-side behavior would need to change)</td>\n<td>Human-readableEasy to assemble, especially for minimalistic cases (no nesting required)Readable line by line (with the exception of type hints and docstrings)</td>\n</tr>\n<tr>\n<td><strong>Limitations</strong></td>\n<td>Not human-readable</td>\n<td>VerboseTypes and docstrings not integral part of the syntax, meaning little-to-nonexistent metric contract validationParsing cost</td>\n</tr>\n<tr>\n<td><strong>Supported metric primitives</strong></td>\n<td>CounterGaugeHistogramSummaryUntyped</td>\n<td>CounterGaugeHistogramSummaryUntyped</td>\n</tr>\n<tr>\n<td><strong>Compatibility</strong></td>\n<td>Version <code>0.0.3</code> protocol buffers are also valid version <code>0.0.4</code> protocol buffers.</td>\n<td>none</td>\n</tr>\n</tbody>\n</table>\n<h3 id=\"5-2-基于协议缓冲区格式\"><a href=\"#5-2-基于协议缓冲区格式\" class=\"headerlink\" title=\"5.2 基于协议缓冲区格式\"></a>5.2 基于协议缓冲区格式</h3><p>​    Reproducible sorting of the protocol buffer fields in repeated expositions is preferred but not required, i.e. do not sort if the computational cost is prohibitive.</p>\n<p>​    Each <code>MetricFamily</code> within the same exposition must have a unique name. Each <code>Metric</code> within the same <code>MetricFamily</code> must have a unique set of <code>LabelPair</code> fields. Otherwise, the ingestion behavior is undefined.</p>\n<h3 id=\"5-3-文本类型格式\"><a href=\"#5-3-文本类型格式\" class=\"headerlink\" title=\"5.3 文本类型格式\"></a>5.3 文本类型格式</h3><p>​    ＃打头的是注释行（除非＃之后的第一个标记是HELP或TYPE）。</p>\n<p>​    HELP行：可能包含任何UTF-8字符序列（在指标名称之后），但反斜杠和换行字符必须分别转义为<code>\\\\</code>和<code>\\ n</code>。对于相同的指标名称，只能有一条HELP行，一个指标只能有一个HELP行。</p>\n<p>TYPE行：TPYE后的第一个参数是指标名，第二个参数是数据类型（可以是counter, gauge, histogram, summary,  untyped）。相同的指标名称，只能有一个TYPE行。如果指标名称没有TYPE行，则该类型设置为无类型。</p>\n<p>格式：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">metric_name [</span><br><span class=\"line\">  &quot;&#123;&quot; label_name &quot;=&quot; `&quot;` label_value `&quot;` &#123; &quot;,&quot; label_name &quot;=&quot; `&quot;` label_value `&quot;` &#125; [ &quot;,&quot; ] &quot;&#125;&quot;</span><br><span class=\"line\">] value [ timestamp ]</span><br></pre></td></tr></table></figure>\n<p>label_value可以是任何UTF-8格式的内容，但反斜杠、双引号、换行符 必须转义成</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">\\\\  \\&quot;  \\\\n</span><br></pre></td></tr></table></figure>\n<p>histogram（直方图）  summary（汇总）类型的特别格式：</p>\n<ol>\n<li>需要单独一行xxx_sum；</li>\n<li>需要单独一行xxx_count；</li>\n<li>Each quantile of a summary named x is given as a separate sample line with the same name x and a label {quantile=”y”}；</li>\n<li>A histogram must have a bucket with {le=”+Inf”}. Its value must be identical to the value of x_count；</li>\n<li>histogram类型必须要有{le=”+Inf”}，并且值要和xxx_count一致；</li>\n</ol>\n<h3 id=\"5-4-文本类型格式举例\"><a href=\"#5-4-文本类型格式举例\" class=\"headerlink\" title=\"5.4 文本类型格式举例\"></a>5.4 文本类型格式举例</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># HELP http_requests_total The total number of HTTP requests.</span><br><span class=\"line\"># TYPE http_requests_total counter</span><br><span class=\"line\">http_requests_total&#123;method=&quot;post&quot;,code=&quot;200&quot;&#125; 1027 1395066363000</span><br><span class=\"line\">http_requests_total&#123;method=&quot;post&quot;,code=&quot;400&quot;&#125;    3 1395066363000</span><br><span class=\"line\"></span><br><span class=\"line\"># Escaping in label values:</span><br><span class=\"line\">msdos_file_access_time_seconds&#123;path=&quot;C:\\\\DIR\\\\FILE.TXT&quot;,error=&quot;Cannot find file:\\n\\&quot;FILE.TXT\\&quot;&quot;&#125; 1.458255915e9</span><br><span class=\"line\"></span><br><span class=\"line\"># Minimalistic line:</span><br><span class=\"line\">metric_without_timestamp_and_labels 12.47</span><br><span class=\"line\"></span><br><span class=\"line\"># A weird metric from before the epoch:</span><br><span class=\"line\">something_weird&#123;problem=&quot;division by zero&quot;&#125; +Inf -3982045</span><br><span class=\"line\"></span><br><span class=\"line\"># A histogram, which has a pretty complex representation in the text format:</span><br><span class=\"line\"># HELP http_request_duration_seconds A histogram of the request duration.</span><br><span class=\"line\"># TYPE http_request_duration_seconds histogram</span><br><span class=\"line\">http_request_duration_seconds_bucket&#123;le=&quot;0.05&quot;&#125; 24054</span><br><span class=\"line\">http_request_duration_seconds_bucket&#123;le=&quot;0.1&quot;&#125; 33444</span><br><span class=\"line\">http_request_duration_seconds_bucket&#123;le=&quot;0.2&quot;&#125; 100392</span><br><span class=\"line\">http_request_duration_seconds_bucket&#123;le=&quot;0.5&quot;&#125; 129389</span><br><span class=\"line\">http_request_duration_seconds_bucket&#123;le=&quot;1&quot;&#125; 133988</span><br><span class=\"line\">http_request_duration_seconds_bucket&#123;le=&quot;+Inf&quot;&#125; 144320</span><br><span class=\"line\">http_request_duration_seconds_sum 53423</span><br><span class=\"line\">http_request_duration_seconds_count 144320</span><br><span class=\"line\"></span><br><span class=\"line\"># Finally a summary, which has a complex representation, too:</span><br><span class=\"line\"># HELP rpc_duration_seconds A summary of the RPC duration in seconds.</span><br><span class=\"line\"># TYPE rpc_duration_seconds summary</span><br><span class=\"line\">rpc_duration_seconds&#123;quantile=&quot;0.01&quot;&#125; 3102</span><br><span class=\"line\">rpc_duration_seconds&#123;quantile=&quot;0.05&quot;&#125; 3272</span><br><span class=\"line\">rpc_duration_seconds&#123;quantile=&quot;0.5&quot;&#125; 4773</span><br><span class=\"line\">rpc_duration_seconds&#123;quantile=&quot;0.9&quot;&#125; 9001</span><br><span class=\"line\">rpc_duration_seconds&#123;quantile=&quot;0.99&quot;&#125; 76656</span><br><span class=\"line\">rpc_duration_seconds_sum 1.7560473e+07</span><br><span class=\"line\">rpc_duration_seconds_count 2693</span><br></pre></td></tr></table></figure>\n<h2 id=\"6-导出器exporter\"><a href=\"#6-导出器exporter\" class=\"headerlink\" title=\"6 导出器exporter\"></a>6 导出器exporter</h2><h3 id=\"6-1-概述\"><a href=\"#6-1-概述\" class=\"headerlink\" title=\"6.1 概述\"></a>6.1 概述</h3><p>​    指标名，一般为导出程序名称作为前缀，例如， haproxy_up；</p>\n<p>​    度量标准必须使用基本单位（例如秒，字节），并保留将其转换为更具可读性的图形工具；</p>\n<pre><code>指标有效字符：`[a-zA-Z0-9:_]` ，其他任何字符都要用下划线_代替；\n</code></pre><p>​    指标后缀<code>_sum</code>, <code>_count</code>, <code>_bucket</code> and <code>_total</code> 只可用在Summaries、 Histograms 、 Counters</p>\n<h2 id=\"7-告警alertmanager程序\"><a href=\"#7-告警alertmanager程序\" class=\"headerlink\" title=\"7 告警alertmanager程序\"></a>7 告警alertmanager程序</h2><p>​    概述：</p>\n<p>​        promethues：根据配置文件prometheus.yml的rule_files告警规则，将告警信息存到promethues的磁盘，供promethues的前台页面查看；根据配置文件prometheus.yml的alerting（配置altermanager进程的ip 端口信息），将告警信息发送altermanager进程上。</p>\n<p>​        altermanager：接收promethues发来的告警信息，存在磁盘中供altermanager进程的前台查看；同时根据altermanager的告警配置文件simple.yml发送邮件等提醒。</p>\n<h3 id=\"7-1-promethues告警配置举例\"><a href=\"#7-1-promethues告警配置举例\" class=\"headerlink\" title=\"7.1 promethues告警配置举例\"></a>7.1 promethues告警配置举例</h3><p><img src=\"/img/promethues-alarm.png\" alt=\"\"></p>\n<h3 id=\"7-2-promethues告警规则配置文件举例\"><a href=\"#7-2-promethues告警规则配置文件举例\" class=\"headerlink\" title=\"7.2 promethues告警规则配置文件举例\"></a>7.2 promethues告警规则配置文件举例</h3><p><img src=\"/img/rule_file.png\" alt=\"\"></p>\n<p>alert：自定义的告警含义简写</p>\n<p>expor：告警条件，如上图的node_forks为具体mertics里的指标</p>\n<p>for：周期</p>\n<p>labels：severity，在alertmanager前台页面可以根据severity条件来查询告警信息</p>\n<p>annotations：summary写些较详细的告警信息</p>\n<h3 id=\"7-3-alertmanager告警发送邮件提示\"><a href=\"#7-3-alertmanager告警发送邮件提示\" class=\"headerlink\" title=\"7.3 alertmanager告警发送邮件提示\"></a>7.3 alertmanager告警发送邮件提示</h3><p><img src=\"/img/email_1.png\" alt=\"\"></p>\n<p><img src=\"/img/email_2.png\" alt=\"\"></p>\n<h3 id=\"7-4-启动alertmanager\"><a href=\"#7-4-启动alertmanager\" class=\"headerlink\" title=\"7.4 启动alertmanager\"></a>7.4 启动alertmanager</h3><p> nohup ./alertmanager –config.file=simple.yml &amp;</p>\n<p>alertmanager前台：<a href=\"http://192.168.7.176:9093/\" target=\"_blank\" rel=\"noopener\">http://192.168.7.176:9093/</a></p>\n<h3 id=\"7-5通过其他方式告警\"><a href=\"#7-5通过其他方式告警\" class=\"headerlink\" title=\"7.5通过其他方式告警\"></a>7.5通过其他方式告警</h3><h4 id=\"hipchat-config：\"><a href=\"#hipchat-config：\" class=\"headerlink\" title=\"hipchat_config：\"></a>hipchat_config：</h4><p>​    是一款能够在苹果mac平台上运行的社交聊天软件，HipChat的功能和QQ相似，集聊天、视频、语音等功能于一身，不同之处在于HipChat界面更加的简洁、操作更加的流畅。</p>\n<h4 id=\"pagerduty-config：\"><a href=\"#pagerduty-config：\" class=\"headerlink\" title=\"pagerduty_config：\"></a>pagerduty_config：</h4><p>​    是一款能够在服务器出问题时发送提醒的软件。在发生问题时，提醒的方式包括屏幕显示、电话呼叫、短信通知、电邮通知等，而且在无人应答时还会自动将提醒级别提高。PagerDuty 不是免费的。</p>\n<h4 id=\"pushover-config：\"><a href=\"#pushover-config：\" class=\"headerlink\" title=\"pushover_config：\"></a>pushover_config：</h4><p>​    是一款网络通知推送服务，类似ifttt或脚本服务，你可以将需要推送的服务设置好后，遇到情况将把通知自动推送到你的<a href=\"http://www.onlinedown.net/soft/222292.htm\" target=\"_blank\" rel=\"noopener\">安卓手机</a>。</p>\n<h4 id=\"slack-config：\"><a href=\"#slack-config：\" class=\"headerlink\" title=\"slack_config：\"></a>slack_config：</h4><p>​    slack是聊天群组 + 大规模工具集成 + 文件整合 + 统一搜索。截至2014年底，Slack 已经整合了电子邮件、短信、<a href=\"https://baike.baidu.com/item/Google\" target=\"_blank\" rel=\"noopener\">Google</a> Drives、<a href=\"https://baike.baidu.com/item/Twitter\" target=\"_blank\" rel=\"noopener\">Twitter</a>、Trello、Asana、<a href=\"https://baike.baidu.com/item/GitHub\" target=\"_blank\" rel=\"noopener\">GitHub</a> 等 65 种工具和服务，可以把各种碎片化的企业沟通和协作集中到一起。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># Whether or not to notify about resolved alerts.</span><br><span class=\"line\">[ send_resolved: &lt;boolean&gt; | default = false ]</span><br><span class=\"line\"></span><br><span class=\"line\"># The Slack webhook URL.</span><br><span class=\"line\">[ api_url: &lt;secret&gt; | default = global.slack_api_url ]</span><br><span class=\"line\"></span><br><span class=\"line\"># The channel or user to send notifications to.</span><br><span class=\"line\">channel: &lt;tmpl_string&gt;</span><br><span class=\"line\"></span><br><span class=\"line\"># API request data as defined by the Slack webhook API.</span><br><span class=\"line\">[ color: &lt;tmpl_string&gt; | default = &apos;&#123;&#123; if eq .Status &quot;firing&quot; &#125;&#125;danger&#123;&#123; else &#125;&#125;good&#123;&#123; end &#125;&#125;&apos; ]</span><br><span class=\"line\">[ username: &lt;tmpl_string&gt; | default = &apos;&#123;&#123; template &quot;slack.default.username&quot; . &#125;&#125;&apos; ]</span><br><span class=\"line\">[ title: &lt;tmpl_string&gt; | default = &apos;&#123;&#123; template &quot;slack.default.title&quot; . &#125;&#125;&apos; ]</span><br><span class=\"line\">[ title_link: &lt;tmpl_string&gt; | default = &apos;&#123;&#123; template &quot;slack.default.titlelink&quot; . &#125;&#125;&apos; ]</span><br><span class=\"line\">[ icon_emoji: &lt;tmpl_string&gt; ]</span><br><span class=\"line\">[ icon_url: &lt;tmpl_string&gt; ]</span><br><span class=\"line\">[ pretext: &lt;tmpl_string&gt; | default = &apos;&#123;&#123; template &quot;slack.default.pretext&quot; . &#125;&#125;&apos; ]</span><br><span class=\"line\">[ text: &lt;tmpl_string&gt; | default = &apos;&#123;&#123; template &quot;slack.default.text&quot; . &#125;&#125;&apos; ]</span><br><span class=\"line\">[ fallback: &lt;tmpl_string&gt; | default = &apos;&#123;&#123; template &quot;slack.default.fallback&quot; . &#125;&#125;&apos; ]</span><br><span class=\"line\"></span><br><span class=\"line\"># The HTTP client&apos;s configuration.</span><br><span class=\"line\">[ http_config: &lt;http_config&gt; | default = global.http_config ]</span><br></pre></td></tr></table></figure>\n<h4 id=\"opsgenie-config-：\"><a href=\"#opsgenie-config-：\" class=\"headerlink\" title=\"opsgenie_config ：\"></a>opsgenie_config ：</h4><p>​    集成电话短信邮件等等</p>\n<h4 id=\"victorops-config：\"><a href=\"#victorops-config：\" class=\"headerlink\" title=\"victorops_config：\"></a>victorops_config：</h4><p>​    聊天应用</p>\n<h4 id=\"http-config：\"><a href=\"#http-config：\" class=\"headerlink\" title=\"http_config：\"></a>http_config：</h4><p>​    A <code>http_config</code> allows configuring the HTTP client that the receiver uses to communicate with HTTP-based API services.</p>\n<h2 id=\"8-问题笔记\"><a href=\"#8-问题笔记\" class=\"headerlink\" title=\"8 问题笔记\"></a>8 问题笔记</h2><h3 id=\"8-1已解决\"><a href=\"#8-1已解决\" class=\"headerlink\" title=\"8.1已解决\"></a>8.1已解决</h3><ol>\n<li>prometheus浏览器查询不到exporter指标数据，但是浏览器exporter的mertic有指标数据。原因是时间不同步</li>\n</ol>\n<h3 id=\"8-2-未解决\"><a href=\"#8-2-未解决\" class=\"headerlink\" title=\"8.2 未解决\"></a>8.2 未解决</h3>","site":{"data":{}},"excerpt":"<h2 id=\"1-概述\"><a href=\"#1-概述\" class=\"headerlink\" title=\"1 概述\"></a>1 概述</h2><h3 id=\"1-1-主要功能\"><a href=\"#1-1-主要功能\" class=\"headerlink\" title=\"1.1 主要功能\"></a>1.1 主要功能</h3><ul>\n<li>多维 <a href=\"https://prometheus.io/docs/concepts/data_model/\" target=\"_blank\" rel=\"noopener\">数据模型</a>（时序由 metric 名字和 k/v 的 labels 构成）。</li>\n<li>灵活的查询语句（<a href=\"https://prometheus.io/docs/querying/basics/\" target=\"_blank\" rel=\"noopener\">PromQL</a>）。</li>\n<li>无依赖存储，支持 local 和 remote 不同模型。</li>\n<li>采用 http 协议，使用 pull 模式，拉取数据，简单易懂。</li>\n<li>监控目标，可以采用服务发现或静态配置的方式。</li>\n<li>支持多种统计数据模型，图形化友好。</li>\n</ul>\n<h3 id=\"1-2-核心组件\"><a href=\"#1-2-核心组件\" class=\"headerlink\" title=\"1.2 核心组件\"></a>1.2 核心组件</h3><ul>\n<li><a href=\"https://github.com/prometheus/prometheus\" target=\"_blank\" rel=\"noopener\">Prometheus Server</a>， 主要用于抓取数据和存储时序数据，另外还提供查询和 Alert Rule 配置管理。</li>\n<li><a href=\"https://prometheus.io/docs/instrumenting/clientlibs/\" target=\"_blank\" rel=\"noopener\">client libraries</a>，用于对接 Prometheus Server, 可以查询和上报数据。</li>\n<li><a href=\"https://github.com/prometheus/pushgateway\" target=\"_blank\" rel=\"noopener\">push gateway</a> ，用于批量，短期的监控数据的汇总节点，主要用于业务数据汇报等。</li>\n<li>各种汇报数据的 <a href=\"https://prometheus.io/docs/instrumenting/exporters/\" target=\"_blank\" rel=\"noopener\">exporters</a> ，例如汇报机器数据的 node_exporter,  汇报 MongoDB 信息的 <a href=\"https://github.com/dcu/mongodb_exporter\" target=\"_blank\" rel=\"noopener\">MongoDB exporter</a> 等等。</li>\n<li>用于告警通知管理的 <a href=\"https://github.com/prometheus/alertmanager\" target=\"_blank\" rel=\"noopener\">alertmanager</a> 。</li>\n</ul>\n<h3 id=\"1-3-基础架构\"><a href=\"#1-3-基础架构\" class=\"headerlink\" title=\"1.3 基础架构\"></a>1.3 基础架构</h3><p>一图胜千言，先来张官方的架构图</p>\n<p><img src=\"/img/prometheus1.png\" alt=\"img\"></p>","more":"<p>从这个架构图，也可以看出 Prometheus 的主要模块包含， Server,  Exporters, Pushgateway, PromQL, Alertmanager, WebUI 等。</p>\n<p>它大致使用逻辑是这样：</p>\n<ol>\n<li>Prometheus server 定期从静态配置的 targets 或者服务发现的 targets 拉取数据。</li>\n<li>当新拉取的数据大于配置内存缓存区的时候，Prometheus 会将数据持久化到磁盘（如果使用 remote storage 将持久化到云端）。</li>\n<li>Prometheus 可以配置 rules，然后定时查询数据，当条件触发的时候，会将 alert 推送到配置的 Alertmanager。</li>\n<li>Alertmanager 收到警告的时候，可以根据配置，聚合，去重，降噪，最后发送警告。</li>\n<li>可以使用 API， Prometheus Console 或者 Grafana 查询和聚合数据。</li>\n</ol>\n<h3 id=\"1-4-注意\"><a href=\"#1-4-注意\" class=\"headerlink\" title=\"1.4 注意\"></a>1.4 注意</h3><ul>\n<li>Prometheus 的数据是基于时序的 float64 的值，如果你的数据值有更多类型，无法满足。</li>\n<li>Prometheus 不适合做审计计费，因为它的数据是按一定时间采集的，关注的更多是系统的运行瞬时状态以及趋势，即使有少量数据没有采集也能容忍，但是审计计费需要记录每个请求，并且数据长期存储，这个和 Prometheus 无法满足，可能需要采用专门的审计系统。</li>\n</ul>\n<h2 id=\"2-BO关注项\"><a href=\"#2-BO关注项\" class=\"headerlink\" title=\"2 BO关注项\"></a>2 BO关注项</h2><h3 id=\"2-1-数据收集方式\"><a href=\"#2-1-数据收集方式\" class=\"headerlink\" title=\"2.1 数据收集方式\"></a>2.1 数据收集方式</h3><p>使用 pull 模式，拉取数据。</p>\n<h3 id=\"2-2-数据格式\"><a href=\"#2-2-数据格式\" class=\"headerlink\" title=\"2.2 数据格式\"></a>2.2 数据格式</h3><p>Prometheus 时序格式与 <a href=\"http://opentsdb.net/\" target=\"_blank\" rel=\"noopener\">OpenTSDB</a> 相似：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;metric name&gt;&#123;&lt;label name&gt;=&lt;label value&gt;, ...&#125;</span><br></pre></td></tr></table></figure>\n<p>其中包含时序名字以及时序的标签。</p>\n<h4 id=\"2-2-1-时序-4-种类型\"><a href=\"#2-2-1-时序-4-种类型\" class=\"headerlink\" title=\"2.2.1 时序 4 种类型\"></a>2.2.1 时序 4 种类型</h4><p>Prometheus 时序数据分为 <a href=\"https://prometheus.io/docs/concepts/metric_types/#counter\" target=\"_blank\" rel=\"noopener\">Counter</a>, <a href=\"https://prometheus.io/docs/concepts/metric_types/#gauge\" target=\"_blank\" rel=\"noopener\">Gauge</a>, <a href=\"https://prometheus.io/docs/concepts/metric_types/#histogram\" target=\"_blank\" rel=\"noopener\">Histogram</a>, <a href=\"https://prometheus.io/docs/concepts/metric_types/#summary\" target=\"_blank\" rel=\"noopener\">Summary</a> 四种类型。</p>\n<h5 id=\"2-2-1-1-Counter\"><a href=\"#2-2-1-1-Counter\" class=\"headerlink\" title=\"2.2.1.1 Counter\"></a>2.2.1.1 Counter</h5><p>Counter 表示收集的数据是按照某个趋势（增加／减少）一直变化的，我们往往用它记录服务请求总量，错误总数等。</p>\n<p>例如 Prometheus server 中 <code>http_requests_total</code>,  表示 Prometheus 处理的 http 请求总数，我们可以使用 <code>delta</code>, 很容易得到任意区间数据的增量，这个会在 PromQL 一节中细讲。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># HELP http_requests_total Total number of HTTP requests made.</span><br><span class=\"line\"># TYPE http_requests_total counter</span><br><span class=\"line\">http_requests_total&#123;code=&quot;200&quot;,handler=&quot;alerts&quot;,method=&quot;get&quot;&#125; 2</span><br><span class=\"line\">http_requests_total&#123;code=&quot;200&quot;,handler=&quot;config&quot;,method=&quot;get&quot;&#125; 1</span><br><span class=\"line\">http_requests_total&#123;code=&quot;200&quot;,handler=&quot;flags&quot;,method=&quot;get&quot;&#125; 2</span><br><span class=\"line\">http_requests_total&#123;code=&quot;200&quot;,handler=&quot;graph&quot;,method=&quot;get&quot;&#125; 6</span><br><span class=\"line\">http_requests_total&#123;code=&quot;200&quot;,handler=&quot;label_values&quot;,method=&quot;get&quot;&#125; 6</span><br><span class=\"line\">http_requests_total&#123;code=&quot;200&quot;,handler=&quot;prometheus&quot;,method=&quot;get&quot;&#125; 24755</span><br><span class=\"line\">http_requests_total&#123;code=&quot;200&quot;,handler=&quot;query&quot;,method=&quot;get&quot;&#125; 6</span><br><span class=\"line\">http_requests_total&#123;code=&quot;200&quot;,handler=&quot;static&quot;,method=&quot;get&quot;&#125; 6</span><br><span class=\"line\">http_requests_total&#123;code=&quot;200&quot;,handler=&quot;status&quot;,method=&quot;get&quot;&#125; 2</span><br><span class=\"line\">http_requests_total&#123;code=&quot;200&quot;,handler=&quot;targets&quot;,method=&quot;get&quot;&#125; 4</span><br><span class=\"line\">http_requests_total&#123;code=&quot;304&quot;,handler=&quot;static&quot;,method=&quot;get&quot;&#125; 4</span><br></pre></td></tr></table></figure>\n<h5 id=\"2-2-1-2-Gauge\"><a href=\"#2-2-1-2-Gauge\" class=\"headerlink\" title=\"2.2.1.2 Gauge\"></a>2.2.1.2 Gauge</h5><p>Gauge 表示搜集的数据是一个瞬时的，与时间没有关系，可以任意变高变低，往往可以用来记录内存使用率、磁盘使用率等。</p>\n<p>例如 Prometheus server 中 <code>go_goroutines</code>,  表示 Prometheus 当前 goroutines 的数量。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># HELP go_goroutines Number of goroutines that currently exist.</span><br><span class=\"line\"># TYPE go_goroutines gauge</span><br><span class=\"line\">go_goroutines 100</span><br></pre></td></tr></table></figure>\n<h5 id=\"2-2-1-3-Histogram\"><a href=\"#2-2-1-3-Histogram\" class=\"headerlink\" title=\"2.2.1.3 Histogram\"></a>2.2.1.3 Histogram</h5><p>Histogram 由 <code>&lt;basename&gt;_bucket{le=&quot;&lt;upper inclusive bound&gt;&quot;}</code>，<code>&lt;basename&gt;_bucket{le=&quot;+Inf&quot;}</code>, <code>&lt;basename&gt;_sum</code>，<code>&lt;basename&gt;_count</code> 组成，主要用于表示一段时间范围内对数据进行采样，（通常是请求持续时间或响应大小），并能够对其指定区间以及总数进行统计，通常我们用它计算分位数的直方图。</p>\n<p>例如 Prometheus server 中 <code>prometheus_local_storage_series_chunks_persisted</code>,  表示 Prometheus 中每个时序需要存储的 chunks 数量，我们可以用它计算待持久化的数据的分位数。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># HELP prometheus_tsdb_compaction_chunk_range Final time range of chunks on their first compaction</span><br><span class=\"line\"># TYPE prometheus_tsdb_compaction_chunk_range histogram</span><br><span class=\"line\">prometheus_tsdb_compaction_chunk_range_bucket&#123;le=&quot;100&quot;&#125; 0</span><br><span class=\"line\">prometheus_tsdb_compaction_chunk_range_bucket&#123;le=&quot;400&quot;&#125; 0</span><br><span class=\"line\">prometheus_tsdb_compaction_chunk_range_bucket&#123;le=&quot;1600&quot;&#125; 0</span><br><span class=\"line\">prometheus_tsdb_compaction_chunk_range_bucket&#123;le=&quot;6400&quot;&#125; 0</span><br><span class=\"line\">prometheus_tsdb_compaction_chunk_range_bucket&#123;le=&quot;25600&quot;&#125; 0</span><br><span class=\"line\">prometheus_tsdb_compaction_chunk_range_bucket&#123;le=&quot;102400&quot;&#125; 0</span><br><span class=\"line\">prometheus_tsdb_compaction_chunk_range_bucket&#123;le=&quot;409600&quot;&#125; 605</span><br><span class=\"line\">prometheus_tsdb_compaction_chunk_range_bucket&#123;le=&quot;1.6384e+06&quot;&#125; 612</span><br><span class=\"line\">prometheus_tsdb_compaction_chunk_range_bucket&#123;le=&quot;6.5536e+06&quot;&#125; 126358</span><br><span class=\"line\">prometheus_tsdb_compaction_chunk_range_bucket&#123;le=&quot;2.62144e+07&quot;&#125; 126358</span><br><span class=\"line\">prometheus_tsdb_compaction_chunk_range_bucket&#123;le=&quot;+Inf&quot;&#125; 126358</span><br><span class=\"line\">prometheus_tsdb_compaction_chunk_range_sum 2.25313627417e+11</span><br><span class=\"line\">prometheus_tsdb_compaction_chunk_range_count 126358</span><br></pre></td></tr></table></figure>\n<h5 id=\"2-2-1-4-Summary\"><a href=\"#2-2-1-4-Summary\" class=\"headerlink\" title=\"2.2.1.4 Summary\"></a>2.2.1.4 Summary</h5><p>Summary 和 Histogram 类似，由 <code>&lt;basename&gt;{quantile=&quot;&lt;φ&gt;&quot;}</code>，<code>&lt;basename&gt;_sum</code>，<code>&lt;basename&gt;_count</code> 组成，主要用于表示一段时间内数据采样结果，（通常是请求持续时间或响应大小），它直接存储了 quantile 数据，而不是根据统计区间计算出来的。</p>\n<p>例如 Prometheus server 中 <code>prometheus_target_interval_length_seconds</code>。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># HELP prometheus_target_interval_length_seconds Actual intervals between scrapes.</span><br><span class=\"line\"># TYPE prometheus_target_interval_length_seconds summary</span><br><span class=\"line\">prometheus_target_interval_length_seconds&#123;interval=&quot;15s&quot;,quantile=&quot;0.01&quot;&#125; 14.999987534</span><br><span class=\"line\">prometheus_target_interval_length_seconds&#123;interval=&quot;15s&quot;,quantile=&quot;0.05&quot;&#125; 14.999987534</span><br><span class=\"line\">prometheus_target_interval_length_seconds&#123;interval=&quot;15s&quot;,quantile=&quot;0.5&quot;&#125; 15.000020575</span><br><span class=\"line\">prometheus_target_interval_length_seconds&#123;interval=&quot;15s&quot;,quantile=&quot;0.9&quot;&#125; 15.000045415</span><br><span class=\"line\">prometheus_target_interval_length_seconds&#123;interval=&quot;15s&quot;,quantile=&quot;0.99&quot;&#125; 15.000050555</span><br><span class=\"line\">prometheus_target_interval_length_seconds_sum&#123;interval=&quot;15s&quot;&#125; 371280.61110144516</span><br><span class=\"line\">prometheus_target_interval_length_seconds_count&#123;interval=&quot;15s&quot;&#125; 24752</span><br></pre></td></tr></table></figure>\n<h5 id=\"2-2-1-5-Histogram-vs-Summary\"><a href=\"#2-2-1-5-Histogram-vs-Summary\" class=\"headerlink\" title=\"2.2.1.5 Histogram vs Summary\"></a>2.2.1.5 Histogram vs Summary</h5><ul>\n<li>都包含 <code>&lt;basename&gt;_sum</code>，<code>&lt;basename&gt;_count</code></li>\n<li>Histogram 需要通过 <code>&lt;basename&gt;_bucket</code> 计算 quantile, 而 Summary 直接存储了 quantile 的值。</li>\n</ul>\n<h3 id=\"2-2-3-数据存储方式\"><a href=\"#2-2-3-数据存储方式\" class=\"headerlink\" title=\"2.2.3 数据存储方式\"></a>2.2.3 数据存储方式</h3><p>​    数据存在promethues自身的数据库，以数据文件的形式存储，有自身的查询方式：promql；详见<a href=\"https://songjiayang.gitbooks.io/prometheus/content/promql/summary.html\" target=\"_blank\" rel=\"noopener\">https://songjiayang.gitbooks.io/prometheus/content/promql/summary.html</a></p>\n<h3 id=\"2-2-4-数据输出方式\"><a href=\"#2-2-4-数据输出方式\" class=\"headerlink\" title=\"2.2.4 数据输出方式\"></a>2.2.4 数据输出方式</h3><p>​    agent：被动拉取；</p>\n<p>​    promethues server：主动拉取客户端的数据。promethues将拉取到的数据存到data/目录。（除了 promethues 前台的PromQ查询页面，应该有某种工具可以直接在命令行查询promethues的历史数据（暂未找到）；多种导出工具，可以支持Prometheus存储数据转化为HAProxy、StatsD、Graphite等工具所需要的数据存储格式（工具未研究过）。）</p>\n<h3 id=\"2-2-5-agent部署方式\"><a href=\"#2-2-5-agent部署方式\" class=\"headerlink\" title=\"2.2.5 agent部署方式\"></a>2.2.5 agent部署方式</h3><p>​    promethues未提供自动部署agent的功能。</p>\n<h3 id=\"2-2-6-任务下发方式\"><a href=\"#2-2-6-任务下发方式\" class=\"headerlink\" title=\"2.2.6 任务下发方式\"></a>2.2.6 任务下发方式</h3><p>​    agent每个周期固定采集设备的指定指标，若要自定义采集某些指标则需要修改agent源码。</p>\n<p>​    promethues server拉取数据的任务在prometheus.yml配置。</p>\n<h2 id=\"3-promethues组件及部署\"><a href=\"#3-promethues组件及部署\" class=\"headerlink\" title=\"3 promethues组件及部署\"></a>3 promethues组件及部署</h2><h3 id=\"3-1-promethues-server\"><a href=\"#3-1-promethues-server\" class=\"headerlink\" title=\"3.1 promethues server\"></a>3.1 promethues server</h3><h4 id=\"3-1-1-部署\"><a href=\"#3-1-1-部署\" class=\"headerlink\" title=\"3.1.1 部署\"></a>3.1.1 部署</h4><p>​    tar包解压即可用</p>\n<h4 id=\"3-1-2-prometheus-yml配置举例\"><a href=\"#3-1-2-prometheus-yml配置举例\" class=\"headerlink\" title=\"3.1.2 prometheus.yml配置举例\"></a>3.1.2 prometheus.yml配置举例</h4><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">global:</span><br><span class=\"line\">  scrape_interval:     15s # By default, scrape targets every 15 seconds.</span><br><span class=\"line\">  evaluation_interval: 15s # By default, scrape targets every 15 seconds.</span><br><span class=\"line\"></span><br><span class=\"line\">rule_files:</span><br><span class=\"line\">  - &quot;rules/node.rules&quot;</span><br><span class=\"line\"></span><br><span class=\"line\">scrape_configs:</span><br><span class=\"line\">  - job_name: &apos;prometheus&apos;</span><br><span class=\"line\">    scrape_interval: 5s</span><br><span class=\"line\">    static_configs:</span><br><span class=\"line\">      - targets: [&apos;localhost:9090&apos;]</span><br><span class=\"line\"></span><br><span class=\"line\">  - job_name: &apos;node&apos;</span><br><span class=\"line\">    scrape_interval: 8s</span><br><span class=\"line\">    static_configs:</span><br><span class=\"line\">      - targets: [&apos;127.0.0.1:9100&apos;, &apos;127.0.0.12:9100&apos;]</span><br><span class=\"line\"></span><br><span class=\"line\">  - job_name: &apos;mysqld&apos;</span><br><span class=\"line\">    static_configs:</span><br><span class=\"line\">      - targets: [&apos;127.0.0.1:9104&apos;]</span><br><span class=\"line\">  - job_name: &apos;memcached&apos;</span><br><span class=\"line\">    static_configs:</span><br><span class=\"line\">      - targets: [&apos;127.0.0.1:9150&apos;]</span><br></pre></td></tr></table></figure>\n<h4 id=\"3-1-3-命令\"><a href=\"#3-1-3-命令\" class=\"headerlink\" title=\"3.1.3 命令\"></a>3.1.3 命令</h4><p>[chenrj@kfapp01 prometheus-2.0.0.linux-amd64]$ ./prometheus -h<br>usage: prometheus [<flags>]</flags></p>\n<p>The Prometheus monitoring server</p>\n<p>Flags:<br>  -h, –help                     Show context-sensitive help (also try –help-long and –help-man).</p>\n<h4 id=\"3-1-4-前台地址\"><a href=\"#3-1-4-前台地址\" class=\"headerlink\" title=\"3.1.4 前台地址\"></a>3.1.4 前台地址</h4><p>​    <a href=\"http://192.168.7.40:9090/graph\" target=\"_blank\" rel=\"noopener\">http://192.168.7.40:9090/graph</a></p>\n<p>​    默认9090端口</p>\n<h3 id=\"3-2-grafana\"><a href=\"#3-2-grafana\" class=\"headerlink\" title=\"3.2 grafana\"></a>3.2 grafana</h3><p>​       <a href=\"http://192.168.7.40:3000/\" target=\"_blank\" rel=\"noopener\">http://192.168.7.40:3000</a></p>\n<p>​       端口默认3000，</p>\n<p>​       用户密码： admin/admin</p>\n<p>​    ./grafana-server</p>\n<h3 id=\"3-3-主机节点\"><a href=\"#3-3-主机节点\" class=\"headerlink\" title=\"3.3 主机节点\"></a>3.3 主机节点</h3><p>​    <a href=\"http://10.140.20.142:9100/metrics\" target=\"_blank\" rel=\"noopener\">http://10.140.20.142:9100/metrics</a></p>\n<h3 id=\"3-4-redis节点\"><a href=\"#3-4-redis节点\" class=\"headerlink\" title=\"3.4 redis节点\"></a>3.4 redis节点</h3><p>​    <a href=\"http://10.140.20.143:9121/metrics\" target=\"_blank\" rel=\"noopener\">http://10.140.20.143:9121/metrics</a> </p>\n<h3 id=\"3-5-elasticsearch节点\"><a href=\"#3-5-elasticsearch节点\" class=\"headerlink\" title=\"3.5 elasticsearch节点\"></a>3.5 elasticsearch节点</h3><p>​    <a href=\"http://10.140.20.146:9108/metrics\" target=\"_blank\" rel=\"noopener\">http://10.140.20.146:9108/metrics</a></p>\n<h2 id=\"4-数据查询\"><a href=\"#4-数据查询\" class=\"headerlink\" title=\"4 数据查询\"></a>4 数据查询</h2><h3 id=\"4-1-http方式查询promethues数据\"><a href=\"#4-1-http方式查询promethues数据\" class=\"headerlink\" title=\"4.1 http方式查询promethues数据\"></a>4.1 http方式查询promethues数据</h3><p>​    <a href=\"https://prometheus.io/docs/prometheus/latest/querying/api/#querying-metadata\" target=\"_blank\" rel=\"noopener\">https://prometheus.io/docs/prometheus/latest/querying/api/#querying-metadata</a></p>\n<h4 id=\"4-1-1-即时查询\"><a href=\"#4-1-1-即时查询\" class=\"headerlink\" title=\"4.1.1 即时查询\"></a>4.1.1 即时查询</h4><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">GET /api/v1/query</span><br></pre></td></tr></table></figure>\n<p><strong>URL查询参数：</strong></p>\n<ul>\n<li><code>query=&lt;string&gt;</code>：普罗米修斯表达查询字符串。</li>\n<li><code>time=&lt;rfc3339 | unix_timestamp&gt;</code>：评估时间戳。可选的。</li>\n<li><code>timeout=&lt;duration&gt;</code>：评价超时。可选的。默认为，并通过价值上限<code>-query.timeout</code>标志。</li>\n</ul>\n<p>若省略时间time测试，则默认使用服务器时间</p>\n<p><strong>例：</strong>查询2018-01-16T03:12:51.781这个时刻go_memstats_frees_total的值</p>\n<p>[logstash@CP-ITSM-OMC-ZSC05 supervisor]$ curl ‘<a href=\"http://10.140.20.146:9090/api/v1/query?query=go_memstats_frees_total&amp;time=2018-01-16T03:12:51.781Z\" target=\"_blank\" rel=\"noopener\">http://10.140.20.146:9090/api/v1/query?query=go_memstats_frees_total&amp;time=2018-01-16T03:12:51.781Z</a>‘<br>{“status”:”success”,”data”:{“resultType”:”vector”,”result”:[{“metric”:{“<strong>name</strong>“:”go_memstats_frees_total”,”instance”:”10.140.20.142:9100”,”job”:”node”},”value”:[1516072371.781,”5599415948”]},{“metric”:{“<strong>name</strong>“:”go_memstats_frees_total”,”instance”:”10.140.20.143:9100”,”job”:”node”},”value”:[1516072371.781,”5152870637”]},{“metric”:{“<strong>name</strong>“:”go_memstats_frees_total”,”instance”:”10.140.20.143:9108”,”job”:”elasticsearch_exporter”},”value”:[1516072371.781,”1385642849”]},{“metric”:{“<strong>name</strong>“:”go_memstats_frees_total”,”instance”:”10.140.20.143:9121”,”job”:”redis_exporter_143”},”value”:[1516072371.781,”159639669”]},{“metric”:{“<strong>name</strong>“:”go_memstats_frees_total”,”instance”:”10.140.20.144:9100”,”job”:”node”},”value”:[1516072371.781,”5167404030”]},{“metric”:{“<strong>name</strong>“:”go_memstats_frees_total”,”instance”:”10.140.20.144:9108”,”job”:”elasticsearch_exporter”},”value”:[1516072371.781,”1383957758”]},{“metric”:{“<strong>name</strong>“:”go_memstats_frees_total”,”instance”:”10.140.20.144:9121”,”job”:”redis_exporter_144”},”value”:[1516072371.781,”373190465”]},{“metric”:{“<strong>name</strong>“:”go_memstats_frees_total”,”instance”:”10.140.20.145:9100”,”job”:”node”},”value”:[1516072371.781,”5124941908”]},{“metric”:{“<strong>name</strong>“:”go_memstats_frees_total”,”instance”:”10.140.20.145:9108”,”job”:”elasticsearch_exporter”},”value”:[1516072371.781,”1370943258”]},{“metric”:{“<strong>name</strong>“:”go_memstats_frees_total”,”instance”:”10.140.20.146:9100”,”job”:”node”},”value”:[1516072371.781,”4850755799”]},{“metric”:{“<strong>name</strong>“:”go_memstats_frees_total”,”instance”:”10.140.20.146:9108”,”job”:”elasticsearch_exporter”},”value”:[1516072371.781,”1370683906”]},{“metric”:{“<strong>name</strong>“:”go_memstats_frees_total”,”instance”:”localhost:9090”,”job”:”prometheus”},”value”:[1516072371.781,”2299674805”]}]}}</p>\n<p><strong>注：”value”:[时间戳,”对应值”]</strong></p>\n<h4 id=\"4-1-2-范围查询\"><a href=\"#4-1-2-范围查询\" class=\"headerlink\" title=\"4.1.2 范围查询\"></a>4.1.2 范围查询</h4><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">GET /api/v1/query_range</span><br></pre></td></tr></table></figure>\n<p><strong>URL查询参数：</strong></p>\n<ul>\n<li><code>query=&lt;string&gt;</code>：普罗米修斯表达查询字符串。</li>\n<li><code>start=&lt;rfc3339 | unix_timestamp&gt;</code>：开始时间戳。</li>\n<li><code>end=&lt;rfc3339 | unix_timestamp&gt;</code>：结束时间戳。</li>\n<li><code>step=&lt;duration&gt;</code>：查询分辨率步的宽度。</li>\n<li><code>timeout=&lt;duration&gt;</code>：评价超时。可选的。默认为，并通过价值上限<code>-query.timeout</code>标志。</li>\n</ul>\n<p>例：时间在2018-01-01T20:10:30.781到2018-01-01T20:11:00.781范围内，间隔15秒，up的数据</p>\n<p>[logstash@CP-ITSM-OMC-ZSC05 supervisor]$ curl ‘<a href=\"http://10.140.20.146:9090/api/v1/query_range?query=up&amp;start=2018-01-01T20:10:30.781Z&amp;end=2018-01-01T20:11:00.781Z&amp;step=15s\" target=\"_blank\" rel=\"noopener\">http://10.140.20.146:9090/api/v1/query_range?query=up&amp;start=2018-01-01T20:10:30.781Z&amp;end=2018-01-01T20:11:00.781Z&amp;step=15s</a>‘<br>{“status”:”success”,”data”:{“resultType”:”matrix”,”result”:[{“metric”:{“<strong>name</strong>“:”up”,”instance”:”localhost:9090”,”job”:”prometheus”},”values”:[[1514837430.781,”1”],[1514837445.781,”1”],[1514837460.781,”1”]]},{“metric”:{“<strong>name</strong>“:”up”,”instance”:”localhost:9100”,”job”:”node”},”values”:[[1514837430.781,”0”],[1514837445.781,”0”],[1514837460.781,”0”]]}]}}</p>\n<h3 id=\"4-2-http查询方式作用未知系列？？\"><a href=\"#4-2-http查询方式作用未知系列？？\" class=\"headerlink\" title=\"4.2 http查询方式作用未知系列？？\"></a>4.2 http查询方式作用未知系列？？</h3><h4 id=\"Querying-metadata\"><a href=\"#Querying-metadata\" class=\"headerlink\" title=\"Querying metadata\"></a>Querying metadata</h4><h5 id=\"Finding-series-by-label-matchers\"><a href=\"#Finding-series-by-label-matchers\" class=\"headerlink\" title=\"Finding series by label matchers\"></a>Finding series by label matchers</h5><p>The following endpoint returns the list of time series that match a certain label set.</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">GET /api/v1/series</span><br></pre></td></tr></table></figure>\n<p>URL query parameters:</p>\n<ul>\n<li><code>match[]=&lt;series_selector&gt;</code>: Repeated series selector argument that selects the series to return. At least one <code>match[]</code> argument must be provided.</li>\n<li><code>start=&lt;rfc3339 | unix_timestamp&gt;</code>: Start timestamp.</li>\n<li><code>end=&lt;rfc3339 | unix_timestamp&gt;</code>: End timestamp.</li>\n</ul>\n<p>The <code>data</code> section of the query result consists of a list of objects that contain the label name/value pairs which identify each series.</p>\n<p>The following example returns all series that match either of the selectors <code>up</code> or <code>process_start_time_seconds{job=&quot;prometheus&quot;}</code>:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ curl -g &apos;http://localhost:9090/api/v1/series?match[]=up&amp;match[]=process_start_time_seconds&#123;job=&quot;prometheus&quot;&#125;&apos;</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">   &quot;status&quot; : &quot;success&quot;,</span><br><span class=\"line\">   &quot;data&quot; : [</span><br><span class=\"line\">      &#123;</span><br><span class=\"line\">         &quot;__name__&quot; : &quot;up&quot;,</span><br><span class=\"line\">         &quot;job&quot; : &quot;prometheus&quot;,</span><br><span class=\"line\">         &quot;instance&quot; : &quot;localhost:9090&quot;</span><br><span class=\"line\">      &#125;,</span><br><span class=\"line\">      &#123;</span><br><span class=\"line\">         &quot;__name__&quot; : &quot;up&quot;,</span><br><span class=\"line\">         &quot;job&quot; : &quot;node&quot;,</span><br><span class=\"line\">         &quot;instance&quot; : &quot;localhost:9091&quot;</span><br><span class=\"line\">      &#125;,</span><br><span class=\"line\">      &#123;</span><br><span class=\"line\">         &quot;__name__&quot; : &quot;process_start_time_seconds&quot;,</span><br><span class=\"line\">         &quot;job&quot; : &quot;prometheus&quot;,</span><br><span class=\"line\">         &quot;instance&quot; : &quot;localhost:9090&quot;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">   ]</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h5 id=\"Querying-label-values\"><a href=\"#Querying-label-values\" class=\"headerlink\" title=\"Querying label values\"></a>Querying label values</h5><p>The following endpoint returns a list of label values for a provided label name:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">GET /api/v1/label/&lt;label_name&gt;/values</span><br></pre></td></tr></table></figure>\n<p>The <code>data</code> section of the JSON response is a list of string label names.</p>\n<p>This example queries for all label values for the <code>job</code> label:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ curl http://localhost:9090/api/v1/label/job/values</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">   &quot;status&quot; : &quot;success&quot;,</span><br><span class=\"line\">   &quot;data&quot; : [</span><br><span class=\"line\">      &quot;node&quot;,</span><br><span class=\"line\">      &quot;prometheus&quot;</span><br><span class=\"line\">   ]</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h4 id=\"Expression-query-result-formats\"><a href=\"#Expression-query-result-formats\" class=\"headerlink\" title=\"Expression query result formats\"></a>Expression query result formats</h4><p>Expression queries may return the following response values in the <code>result</code> property of the <code>data</code> section. <code>&lt;sample_value&gt;</code> placeholders are numeric sample values. JSON does not support special float values such as <code>NaN</code>, <code>Inf</code>, and <code>-Inf</code>, so sample values are transferred as quoted JSON strings rather than raw numbers.</p>\n<h5 id=\"Range-vectors\"><a href=\"#Range-vectors\" class=\"headerlink\" title=\"Range vectors\"></a>Range vectors</h5><p>Range vectors are returned as result type <code>matrix</code>. The corresponding <code>result</code> property has the following format:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[</span><br><span class=\"line\">  &#123;</span><br><span class=\"line\">    &quot;metric&quot;: &#123; &quot;&lt;label_name&gt;&quot;: &quot;&lt;label_value&gt;&quot;, ... &#125;,</span><br><span class=\"line\">    &quot;values&quot;: [ [ &lt;unix_time&gt;, &quot;&lt;sample_value&gt;&quot; ], ... ]</span><br><span class=\"line\">  &#125;,</span><br><span class=\"line\">  ...</span><br><span class=\"line\">]</span><br></pre></td></tr></table></figure>\n<h5 id=\"Instant-vectors\"><a href=\"#Instant-vectors\" class=\"headerlink\" title=\"Instant vectors\"></a>Instant vectors</h5><p>Instant vectors are returned as result type <code>vector</code>. The corresponding <code>result</code> property has the following format:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[</span><br><span class=\"line\">  &#123;</span><br><span class=\"line\">    &quot;metric&quot;: &#123; &quot;&lt;label_name&gt;&quot;: &quot;&lt;label_value&gt;&quot;, ... &#125;,</span><br><span class=\"line\">    &quot;value&quot;: [ &lt;unix_time&gt;, &quot;&lt;sample_value&gt;&quot; ]</span><br><span class=\"line\">  &#125;,</span><br><span class=\"line\">  ...</span><br><span class=\"line\">]</span><br></pre></td></tr></table></figure>\n<h5 id=\"Scalars\"><a href=\"#Scalars\" class=\"headerlink\" title=\"Scalars\"></a>Scalars</h5><p>Scalar results are returned as result type <code>scalar</code>. The corresponding <code>result</code> property has the following format:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[ &lt;unix_time&gt;, &quot;&lt;scalar_value&gt;&quot; ]</span><br></pre></td></tr></table></figure>\n<h5 id=\"Strings\"><a href=\"#Strings\" class=\"headerlink\" title=\"Strings\"></a>Strings</h5><p>String results are returned as result type <code>string</code>. The corresponding <code>result</code> property has the following format:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[ &lt;unix_time&gt;, &quot;&lt;string_value&gt;&quot; ]</span><br></pre></td></tr></table></figure>\n<h4 id=\"Targets\"><a href=\"#Targets\" class=\"headerlink\" title=\"Targets\"></a>Targets</h4><blockquote>\n<p>This API is experimental as it is intended to be extended with targets dropped due to relabelling in the future.</p>\n</blockquote>\n<p>The following endpoint returns an overview of the current state of the Prometheus target discovery:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">GET /api/v1/targets</span><br></pre></td></tr></table></figure>\n<p>Currently only the active targets are part of the response.</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ curl http://localhost:9090/api/v1/targets</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;status&quot;: &quot;success&quot;,                                                                                                                                [3/11]</span><br><span class=\"line\">  &quot;data&quot;: &#123;</span><br><span class=\"line\">    &quot;activeTargets&quot;: [</span><br><span class=\"line\">      &#123;</span><br><span class=\"line\">        &quot;discoveredLabels&quot;: &#123;</span><br><span class=\"line\">          &quot;__address__&quot;: &quot;127.0.0.1:9090&quot;,</span><br><span class=\"line\">          &quot;__metrics_path__&quot;: &quot;/metrics&quot;,</span><br><span class=\"line\">          &quot;__scheme__&quot;: &quot;http&quot;,</span><br><span class=\"line\">          &quot;job&quot;: &quot;prometheus&quot;</span><br><span class=\"line\">        &#125;,</span><br><span class=\"line\">        &quot;labels&quot;: &#123;</span><br><span class=\"line\">          &quot;instance&quot;: &quot;127.0.0.1:9090&quot;,</span><br><span class=\"line\">          &quot;job&quot;: &quot;prometheus&quot;</span><br><span class=\"line\">        &#125;,</span><br><span class=\"line\">        &quot;scrapeUrl&quot;: &quot;http://127.0.0.1:9090/metrics&quot;,</span><br><span class=\"line\">        &quot;lastError&quot;: &quot;&quot;,</span><br><span class=\"line\">        &quot;lastScrape&quot;: &quot;2017-01-17T15:07:44.723715405+01:00&quot;,</span><br><span class=\"line\">        &quot;health&quot;: &quot;up&quot;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    ]</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h4 id=\"Alertmanagers\"><a href=\"#Alertmanagers\" class=\"headerlink\" title=\"Alertmanagers\"></a>Alertmanagers</h4><blockquote>\n<p>This API is experimental as it is intended to be extended with Alertmanagers dropped due to relabelling in the future.</p>\n</blockquote>\n<p>The following endpoint returns an overview of the current state of the Prometheus alertmanager discovery:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">GET /api/v1/alertmanagers</span><br></pre></td></tr></table></figure>\n<p>Currently only the active Alertmanagers are part of the response.</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ curl http://localhost:9090/api/v1/alertmanagers</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;status&quot;: &quot;success&quot;,</span><br><span class=\"line\">  &quot;data&quot;: &#123;</span><br><span class=\"line\">    &quot;activeAlertmanagers&quot;: [</span><br><span class=\"line\">      &#123;</span><br><span class=\"line\">        &quot;url&quot;: &quot;http://127.0.0.1:9090/api/v1/alerts&quot;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    ]</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>查询指标标签：    curl -g ‘<a href=\"http://192.168.7.40:9090/api/v1/series?match[]=up&amp;match[]=process_start_time_seconds{job=&quot;prometheus&quot;}\" target=\"_blank\" rel=\"noopener\">http://192.168.7.40:9090/api/v1/series?match[]=up&amp;match[]=process_start_time_seconds{job=&quot;prometheus&quot;}</a>‘</p>\n<p>查询标签 {“status”:”success”,”data”:[]}[logstash@CP-ITSM-OMC-ZSC05 supervisor]$ curl <a href=\"http://10.140.20.146:9090/api/v1/label/job/values\" target=\"_blank\" rel=\"noopener\">http://10.140.20.146:9090/api/v1/label/job/values</a><br>{“status”:”success”,”data”:[“elasticsearch_exporter”,”node”,”prometheus”,”redis_exporter”,”redis_exporter_143”,”redis_exporter_144”]}</p>\n<h3 id=\"4-3-查询节点数据\"><a href=\"#4-3-查询节点数据\" class=\"headerlink\" title=\"4.3 查询节点数据\"></a>4.3 查询节点数据</h3><p>​    查询节点exporter的所有数据：curl -s <a href=\"http://192.168.7.40:9100/metrics\" target=\"_blank\" rel=\"noopener\">http://192.168.7.40:9100/metrics</a></p>\n<h2 id=\"5-exporter格式\"><a href=\"#5-exporter格式\" class=\"headerlink\" title=\"5 exporter格式\"></a>5 exporter格式</h2><p>基于协议缓冲区格式 和 文本格式</p>\n<p>客户端可以暴露promethues无法解析的其他格式</p>\n<h3 id=\"5-1-基于协议缓冲区格式-和-文本格式-的区别\"><a href=\"#5-1-基于协议缓冲区格式-和-文本格式-的区别\" class=\"headerlink\" title=\"5.1 基于协议缓冲区格式 和 文本格式 的区别\"></a>5.1 基于协议缓冲区格式 和 文本格式 的区别</h3><table>\n<thead>\n<tr>\n<th></th>\n<th>Protocol buffer format</th>\n<th>Text format</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>Inception</strong></td>\n<td>April 2014</td>\n<td>April 2014</td>\n</tr>\n<tr>\n<td><strong>Supported in</strong></td>\n<td>Prometheus version <code>&gt;=0.4.0</code></td>\n<td>Prometheus version <code>&gt;=0.4.0</code></td>\n</tr>\n<tr>\n<td><strong>Transmission</strong></td>\n<td>HTTP</td>\n<td>HTTP</td>\n</tr>\n<tr>\n<td><strong>Encoding</strong></td>\n<td><a href=\"https://developers.google.com/protocol-buffers/docs/reference/java/com/google/protobuf/AbstractMessageLite#writeDelimitedTo(java.io.OutputStream\" target=\"_blank\" rel=\"noopener\">32-bit varint-encoded record length-delimited</a>) Protocol Buffer messages of type <a href=\"https://github.com/prometheus/client_model/blob/086fe7ca28bde6cec2acd5223423c1475a362858/metrics.proto#L76-%20%20L81\" target=\"_blank\" rel=\"noopener\">io.prometheus.client.MetricFamily</a></td>\n<td>UTF-8, <code>\\n</code> line endings</td>\n</tr>\n<tr>\n<td><strong>HTTP Content-Type</strong></td>\n<td><code>application/vnd.google.protobuf; proto=io.prometheus.client.MetricFamily; encoding=delimited</code></td>\n<td><code>text/plain; version=0.0.4</code> (A missing <code>version</code> value will lead to a fall-back to the most recent text format version.)</td>\n</tr>\n<tr>\n<td><strong>Optional HTTP Content-Encoding</strong></td>\n<td><code>gzip</code></td>\n<td><code>gzip</code></td>\n</tr>\n<tr>\n<td><strong>Advantages</strong></td>\n<td>Cross-platformSizeEncoding and decoding costsStrict schemaSupports concatenation and theoretically streaming (only server-side behavior would need to change)</td>\n<td>Human-readableEasy to assemble, especially for minimalistic cases (no nesting required)Readable line by line (with the exception of type hints and docstrings)</td>\n</tr>\n<tr>\n<td><strong>Limitations</strong></td>\n<td>Not human-readable</td>\n<td>VerboseTypes and docstrings not integral part of the syntax, meaning little-to-nonexistent metric contract validationParsing cost</td>\n</tr>\n<tr>\n<td><strong>Supported metric primitives</strong></td>\n<td>CounterGaugeHistogramSummaryUntyped</td>\n<td>CounterGaugeHistogramSummaryUntyped</td>\n</tr>\n<tr>\n<td><strong>Compatibility</strong></td>\n<td>Version <code>0.0.3</code> protocol buffers are also valid version <code>0.0.4</code> protocol buffers.</td>\n<td>none</td>\n</tr>\n</tbody>\n</table>\n<h3 id=\"5-2-基于协议缓冲区格式\"><a href=\"#5-2-基于协议缓冲区格式\" class=\"headerlink\" title=\"5.2 基于协议缓冲区格式\"></a>5.2 基于协议缓冲区格式</h3><p>​    Reproducible sorting of the protocol buffer fields in repeated expositions is preferred but not required, i.e. do not sort if the computational cost is prohibitive.</p>\n<p>​    Each <code>MetricFamily</code> within the same exposition must have a unique name. Each <code>Metric</code> within the same <code>MetricFamily</code> must have a unique set of <code>LabelPair</code> fields. Otherwise, the ingestion behavior is undefined.</p>\n<h3 id=\"5-3-文本类型格式\"><a href=\"#5-3-文本类型格式\" class=\"headerlink\" title=\"5.3 文本类型格式\"></a>5.3 文本类型格式</h3><p>​    ＃打头的是注释行（除非＃之后的第一个标记是HELP或TYPE）。</p>\n<p>​    HELP行：可能包含任何UTF-8字符序列（在指标名称之后），但反斜杠和换行字符必须分别转义为<code>\\\\</code>和<code>\\ n</code>。对于相同的指标名称，只能有一条HELP行，一个指标只能有一个HELP行。</p>\n<p>TYPE行：TPYE后的第一个参数是指标名，第二个参数是数据类型（可以是counter, gauge, histogram, summary,  untyped）。相同的指标名称，只能有一个TYPE行。如果指标名称没有TYPE行，则该类型设置为无类型。</p>\n<p>格式：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">metric_name [</span><br><span class=\"line\">  &quot;&#123;&quot; label_name &quot;=&quot; `&quot;` label_value `&quot;` &#123; &quot;,&quot; label_name &quot;=&quot; `&quot;` label_value `&quot;` &#125; [ &quot;,&quot; ] &quot;&#125;&quot;</span><br><span class=\"line\">] value [ timestamp ]</span><br></pre></td></tr></table></figure>\n<p>label_value可以是任何UTF-8格式的内容，但反斜杠、双引号、换行符 必须转义成</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">\\\\  \\&quot;  \\\\n</span><br></pre></td></tr></table></figure>\n<p>histogram（直方图）  summary（汇总）类型的特别格式：</p>\n<ol>\n<li>需要单独一行xxx_sum；</li>\n<li>需要单独一行xxx_count；</li>\n<li>Each quantile of a summary named x is given as a separate sample line with the same name x and a label {quantile=”y”}；</li>\n<li>A histogram must have a bucket with {le=”+Inf”}. Its value must be identical to the value of x_count；</li>\n<li>histogram类型必须要有{le=”+Inf”}，并且值要和xxx_count一致；</li>\n</ol>\n<h3 id=\"5-4-文本类型格式举例\"><a href=\"#5-4-文本类型格式举例\" class=\"headerlink\" title=\"5.4 文本类型格式举例\"></a>5.4 文本类型格式举例</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># HELP http_requests_total The total number of HTTP requests.</span><br><span class=\"line\"># TYPE http_requests_total counter</span><br><span class=\"line\">http_requests_total&#123;method=&quot;post&quot;,code=&quot;200&quot;&#125; 1027 1395066363000</span><br><span class=\"line\">http_requests_total&#123;method=&quot;post&quot;,code=&quot;400&quot;&#125;    3 1395066363000</span><br><span class=\"line\"></span><br><span class=\"line\"># Escaping in label values:</span><br><span class=\"line\">msdos_file_access_time_seconds&#123;path=&quot;C:\\\\DIR\\\\FILE.TXT&quot;,error=&quot;Cannot find file:\\n\\&quot;FILE.TXT\\&quot;&quot;&#125; 1.458255915e9</span><br><span class=\"line\"></span><br><span class=\"line\"># Minimalistic line:</span><br><span class=\"line\">metric_without_timestamp_and_labels 12.47</span><br><span class=\"line\"></span><br><span class=\"line\"># A weird metric from before the epoch:</span><br><span class=\"line\">something_weird&#123;problem=&quot;division by zero&quot;&#125; +Inf -3982045</span><br><span class=\"line\"></span><br><span class=\"line\"># A histogram, which has a pretty complex representation in the text format:</span><br><span class=\"line\"># HELP http_request_duration_seconds A histogram of the request duration.</span><br><span class=\"line\"># TYPE http_request_duration_seconds histogram</span><br><span class=\"line\">http_request_duration_seconds_bucket&#123;le=&quot;0.05&quot;&#125; 24054</span><br><span class=\"line\">http_request_duration_seconds_bucket&#123;le=&quot;0.1&quot;&#125; 33444</span><br><span class=\"line\">http_request_duration_seconds_bucket&#123;le=&quot;0.2&quot;&#125; 100392</span><br><span class=\"line\">http_request_duration_seconds_bucket&#123;le=&quot;0.5&quot;&#125; 129389</span><br><span class=\"line\">http_request_duration_seconds_bucket&#123;le=&quot;1&quot;&#125; 133988</span><br><span class=\"line\">http_request_duration_seconds_bucket&#123;le=&quot;+Inf&quot;&#125; 144320</span><br><span class=\"line\">http_request_duration_seconds_sum 53423</span><br><span class=\"line\">http_request_duration_seconds_count 144320</span><br><span class=\"line\"></span><br><span class=\"line\"># Finally a summary, which has a complex representation, too:</span><br><span class=\"line\"># HELP rpc_duration_seconds A summary of the RPC duration in seconds.</span><br><span class=\"line\"># TYPE rpc_duration_seconds summary</span><br><span class=\"line\">rpc_duration_seconds&#123;quantile=&quot;0.01&quot;&#125; 3102</span><br><span class=\"line\">rpc_duration_seconds&#123;quantile=&quot;0.05&quot;&#125; 3272</span><br><span class=\"line\">rpc_duration_seconds&#123;quantile=&quot;0.5&quot;&#125; 4773</span><br><span class=\"line\">rpc_duration_seconds&#123;quantile=&quot;0.9&quot;&#125; 9001</span><br><span class=\"line\">rpc_duration_seconds&#123;quantile=&quot;0.99&quot;&#125; 76656</span><br><span class=\"line\">rpc_duration_seconds_sum 1.7560473e+07</span><br><span class=\"line\">rpc_duration_seconds_count 2693</span><br></pre></td></tr></table></figure>\n<h2 id=\"6-导出器exporter\"><a href=\"#6-导出器exporter\" class=\"headerlink\" title=\"6 导出器exporter\"></a>6 导出器exporter</h2><h3 id=\"6-1-概述\"><a href=\"#6-1-概述\" class=\"headerlink\" title=\"6.1 概述\"></a>6.1 概述</h3><p>​    指标名，一般为导出程序名称作为前缀，例如， haproxy_up；</p>\n<p>​    度量标准必须使用基本单位（例如秒，字节），并保留将其转换为更具可读性的图形工具；</p>\n<pre><code>指标有效字符：`[a-zA-Z0-9:_]` ，其他任何字符都要用下划线_代替；\n</code></pre><p>​    指标后缀<code>_sum</code>, <code>_count</code>, <code>_bucket</code> and <code>_total</code> 只可用在Summaries、 Histograms 、 Counters</p>\n<h2 id=\"7-告警alertmanager程序\"><a href=\"#7-告警alertmanager程序\" class=\"headerlink\" title=\"7 告警alertmanager程序\"></a>7 告警alertmanager程序</h2><p>​    概述：</p>\n<p>​        promethues：根据配置文件prometheus.yml的rule_files告警规则，将告警信息存到promethues的磁盘，供promethues的前台页面查看；根据配置文件prometheus.yml的alerting（配置altermanager进程的ip 端口信息），将告警信息发送altermanager进程上。</p>\n<p>​        altermanager：接收promethues发来的告警信息，存在磁盘中供altermanager进程的前台查看；同时根据altermanager的告警配置文件simple.yml发送邮件等提醒。</p>\n<h3 id=\"7-1-promethues告警配置举例\"><a href=\"#7-1-promethues告警配置举例\" class=\"headerlink\" title=\"7.1 promethues告警配置举例\"></a>7.1 promethues告警配置举例</h3><p><img src=\"/img/promethues-alarm.png\" alt=\"\"></p>\n<h3 id=\"7-2-promethues告警规则配置文件举例\"><a href=\"#7-2-promethues告警规则配置文件举例\" class=\"headerlink\" title=\"7.2 promethues告警规则配置文件举例\"></a>7.2 promethues告警规则配置文件举例</h3><p><img src=\"/img/rule_file.png\" alt=\"\"></p>\n<p>alert：自定义的告警含义简写</p>\n<p>expor：告警条件，如上图的node_forks为具体mertics里的指标</p>\n<p>for：周期</p>\n<p>labels：severity，在alertmanager前台页面可以根据severity条件来查询告警信息</p>\n<p>annotations：summary写些较详细的告警信息</p>\n<h3 id=\"7-3-alertmanager告警发送邮件提示\"><a href=\"#7-3-alertmanager告警发送邮件提示\" class=\"headerlink\" title=\"7.3 alertmanager告警发送邮件提示\"></a>7.3 alertmanager告警发送邮件提示</h3><p><img src=\"/img/email_1.png\" alt=\"\"></p>\n<p><img src=\"/img/email_2.png\" alt=\"\"></p>\n<h3 id=\"7-4-启动alertmanager\"><a href=\"#7-4-启动alertmanager\" class=\"headerlink\" title=\"7.4 启动alertmanager\"></a>7.4 启动alertmanager</h3><p> nohup ./alertmanager –config.file=simple.yml &amp;</p>\n<p>alertmanager前台：<a href=\"http://192.168.7.176:9093/\" target=\"_blank\" rel=\"noopener\">http://192.168.7.176:9093/</a></p>\n<h3 id=\"7-5通过其他方式告警\"><a href=\"#7-5通过其他方式告警\" class=\"headerlink\" title=\"7.5通过其他方式告警\"></a>7.5通过其他方式告警</h3><h4 id=\"hipchat-config：\"><a href=\"#hipchat-config：\" class=\"headerlink\" title=\"hipchat_config：\"></a>hipchat_config：</h4><p>​    是一款能够在苹果mac平台上运行的社交聊天软件，HipChat的功能和QQ相似，集聊天、视频、语音等功能于一身，不同之处在于HipChat界面更加的简洁、操作更加的流畅。</p>\n<h4 id=\"pagerduty-config：\"><a href=\"#pagerduty-config：\" class=\"headerlink\" title=\"pagerduty_config：\"></a>pagerduty_config：</h4><p>​    是一款能够在服务器出问题时发送提醒的软件。在发生问题时，提醒的方式包括屏幕显示、电话呼叫、短信通知、电邮通知等，而且在无人应答时还会自动将提醒级别提高。PagerDuty 不是免费的。</p>\n<h4 id=\"pushover-config：\"><a href=\"#pushover-config：\" class=\"headerlink\" title=\"pushover_config：\"></a>pushover_config：</h4><p>​    是一款网络通知推送服务，类似ifttt或脚本服务，你可以将需要推送的服务设置好后，遇到情况将把通知自动推送到你的<a href=\"http://www.onlinedown.net/soft/222292.htm\" target=\"_blank\" rel=\"noopener\">安卓手机</a>。</p>\n<h4 id=\"slack-config：\"><a href=\"#slack-config：\" class=\"headerlink\" title=\"slack_config：\"></a>slack_config：</h4><p>​    slack是聊天群组 + 大规模工具集成 + 文件整合 + 统一搜索。截至2014年底，Slack 已经整合了电子邮件、短信、<a href=\"https://baike.baidu.com/item/Google\" target=\"_blank\" rel=\"noopener\">Google</a> Drives、<a href=\"https://baike.baidu.com/item/Twitter\" target=\"_blank\" rel=\"noopener\">Twitter</a>、Trello、Asana、<a href=\"https://baike.baidu.com/item/GitHub\" target=\"_blank\" rel=\"noopener\">GitHub</a> 等 65 种工具和服务，可以把各种碎片化的企业沟通和协作集中到一起。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># Whether or not to notify about resolved alerts.</span><br><span class=\"line\">[ send_resolved: &lt;boolean&gt; | default = false ]</span><br><span class=\"line\"></span><br><span class=\"line\"># The Slack webhook URL.</span><br><span class=\"line\">[ api_url: &lt;secret&gt; | default = global.slack_api_url ]</span><br><span class=\"line\"></span><br><span class=\"line\"># The channel or user to send notifications to.</span><br><span class=\"line\">channel: &lt;tmpl_string&gt;</span><br><span class=\"line\"></span><br><span class=\"line\"># API request data as defined by the Slack webhook API.</span><br><span class=\"line\">[ color: &lt;tmpl_string&gt; | default = &apos;&#123;&#123; if eq .Status &quot;firing&quot; &#125;&#125;danger&#123;&#123; else &#125;&#125;good&#123;&#123; end &#125;&#125;&apos; ]</span><br><span class=\"line\">[ username: &lt;tmpl_string&gt; | default = &apos;&#123;&#123; template &quot;slack.default.username&quot; . &#125;&#125;&apos; ]</span><br><span class=\"line\">[ title: &lt;tmpl_string&gt; | default = &apos;&#123;&#123; template &quot;slack.default.title&quot; . &#125;&#125;&apos; ]</span><br><span class=\"line\">[ title_link: &lt;tmpl_string&gt; | default = &apos;&#123;&#123; template &quot;slack.default.titlelink&quot; . &#125;&#125;&apos; ]</span><br><span class=\"line\">[ icon_emoji: &lt;tmpl_string&gt; ]</span><br><span class=\"line\">[ icon_url: &lt;tmpl_string&gt; ]</span><br><span class=\"line\">[ pretext: &lt;tmpl_string&gt; | default = &apos;&#123;&#123; template &quot;slack.default.pretext&quot; . &#125;&#125;&apos; ]</span><br><span class=\"line\">[ text: &lt;tmpl_string&gt; | default = &apos;&#123;&#123; template &quot;slack.default.text&quot; . &#125;&#125;&apos; ]</span><br><span class=\"line\">[ fallback: &lt;tmpl_string&gt; | default = &apos;&#123;&#123; template &quot;slack.default.fallback&quot; . &#125;&#125;&apos; ]</span><br><span class=\"line\"></span><br><span class=\"line\"># The HTTP client&apos;s configuration.</span><br><span class=\"line\">[ http_config: &lt;http_config&gt; | default = global.http_config ]</span><br></pre></td></tr></table></figure>\n<h4 id=\"opsgenie-config-：\"><a href=\"#opsgenie-config-：\" class=\"headerlink\" title=\"opsgenie_config ：\"></a>opsgenie_config ：</h4><p>​    集成电话短信邮件等等</p>\n<h4 id=\"victorops-config：\"><a href=\"#victorops-config：\" class=\"headerlink\" title=\"victorops_config：\"></a>victorops_config：</h4><p>​    聊天应用</p>\n<h4 id=\"http-config：\"><a href=\"#http-config：\" class=\"headerlink\" title=\"http_config：\"></a>http_config：</h4><p>​    A <code>http_config</code> allows configuring the HTTP client that the receiver uses to communicate with HTTP-based API services.</p>\n<h2 id=\"8-问题笔记\"><a href=\"#8-问题笔记\" class=\"headerlink\" title=\"8 问题笔记\"></a>8 问题笔记</h2><h3 id=\"8-1已解决\"><a href=\"#8-1已解决\" class=\"headerlink\" title=\"8.1已解决\"></a>8.1已解决</h3><ol>\n<li>prometheus浏览器查询不到exporter指标数据，但是浏览器exporter的mertic有指标数据。原因是时间不同步</li>\n</ol>\n<h3 id=\"8-2-未解决\"><a href=\"#8-2-未解决\" class=\"headerlink\" title=\"8.2 未解决\"></a>8.2 未解决</h3>"},{"title":"Python编程环境搭建","date":"2016-08-19T14:13:30.000Z","_content":"\n# 简介  #\n假如新建了一台CentOs虚拟机，作为纯净的操作系统，我们需要搭建基本的开发环境。CentOs6.x一般都是java1.5版本，python2.6，并且很多开发工具都是没有默认安装的，比如gcc。下面重点介绍Python环境的搭建，主要涉及Python升级，pip安装，virtualenv安装。\n# yum #\nyum 源更新，可选操作，可替换国内的yum源。\nyum groupinstall 。必选操作，编译python需要编译工具。还有一些必要的库文件需要安装，例如openssl,zlib,这些都需要额外安装。pip要安装python2.7版本，因此需要zlib需要更新到最新版本1.2.8，而CentOs默认是1.2.3。否则会报错，错误信息后面提示。\n```\n[root@jinqiu opt]# yum -y update\n[root@jinqiu opt]# yum groupinstall \"Development tools\"\n[root@jinqiu opt]# yum install zlib-devel bzip2-devel openssl-devel ncurses-devel sqlite-devel readline-devel tk-devel\n```\n顺便简单提下，快速免验证安装jdk\n```\n1. wget --no-check-certificate --no-cookies --header \"Cookie: oraclelicense=accept-securebackup-cookie\" http://download.oracle.com/otn-pub/java/jdk/8u131-b11/d54c1d3a095b4ff2b6607d096fa80163/jdk-8u131-linux-x64.rpm  \n2. rpm -ivh jdk-8u131-linux-x64.rpm\n```\n# python #\n保存2.6版本的python\n下载2.7版本的python源码文件，解压，然后配置，编译，安装\n```\n[root@jinqiu~]$ cd /opt\n[root@jinqiuopt]$ sudo wget --no-check-certificate https://www.python.org/ftp/python/2.7.6/Python-2.7.6.tar.xz\n[root@jinqiuopt]$ sudo tar xf Python-2.7.6.tar.xz \n[root@jinqiuopt]$ cd Python-2.7.6\n[root@jinqiuPython-2.7.6]$ sudo ./configure --prefix=/usr/local\n[root@jinqiuPython-2.7.6]$ sudo make && sudo make altinstall\n```\n<!-- more -->\n\n创建软链接到2.7版本的python\n修改yum文件，由于yum只支持2.6版本的python，因此需要将/usr/bin/yum中的默认python解释器修改为2.6版本。\n```\n[root@jinqiu opt]# mv /usr/bin/python /usr/bin/python2.6\n[root@jinqiu opt]# ln -s /usr/bin/python2.7  /usr/bin/python\n```\n\n从一开始，如果要做一些实际Python开发，你一定会用到一些第三方包。\n在Linux系统上至少有3种安装第三方包的方法。\n* 使用系统自带的包管理系统(deb, rpm, 等)\n* 通过社区开发的各种工具，例如 pip ， easy_install 等\n* 从源文件安装\n这三个方面，几乎完成同样的事情。即：安装依赖，编译代码（如果需要的话），将一个包含模块的包复制的标准软件包搜索位置。\n\n# 安装pip #\n```\n[root@jinqiu opt]# yum install python-pip\n[root@jinqiu opt]# pip install --upgrade pip\n```\n通过yum安装，可能安装的pip版本比较低。因此也可以通过网络上最新的pip源进行安装\n```\ncurl https://raw.githubusercontent.com/pypa/pip/master/contrib/get-pip.py | python2.7 -\n```\npip用法这里不再赘述，man pip 即可。\n\n# virtualenv #\n安装virtualenv，virtualenv作为python环境的隔离器，可以保证在这个环境下安装的第三方python库，都是在这个环境下的，而不会安装到系统路径上面去。\n我们需要处理的基本问题是包的依赖、版本和间接权限问题。想象一下，你有两个应用，一个应用需要libfoo的版本1，而另一应用需要版本2。如何才能同时使用这些应用程序？如果您安装到的/usr/lib/python2.7/site-packages（或任何平台的标准位置）的一切，在这种情况下，您可能会不小心升级不应该升级的应用程序。\n```\nsudo pip install virtualenv\nmkdir my_project_venv\nvirtualenv --distribute my_project_venv\n```\n** The output will something like:**\nNew python executable in my_project_venv/bin/python\nInstalling distribute.............................................done.\nInstalling pip.....................done.\n\n这里只列出了将被讨论的目录和文件\n```\n|-- bin\n|   |-- activate  # <-- 这个virtualenv的激活文件\n|   |-- pip       # <-- 这个virtualenv的独立pip\n|   |-- python    # <-- python解释器的一个副本\n|-- lib\n|-- python2.7 # <-- 所有的新包会被存在这\n```\n通过下面的命令激活这个virtualenv：\n```\ncd my_project_venv\nsource bin/activate\n```\n执行完毕后，提示可能是这个样子的：\n```\n(my_project_venv)$ # the virtualenv name prepended to the prompt\n通过 deactivate 命令离开virtualenv环境\n(my_project_venv)$ deactivate\n```\n注意：一定要先进行yum步骤的操作，安装必要的openssl库和zlib库，否则在编译之后的python2.7，会有问题。例如pip2.7会报错，zlib异常。\n\n# 异常处理：\npythonImportError: No module named zlib2.7，报错如下：\nTraceback (most recent call last):\nFile \"/usr/local/bin/pip\", line 9, in <module>\n   load_entry_point('pip==1.4.1', 'console_scripts', 'pip')()\nFile \"build/bdist.linux-x86_64/egg/pkg_resources.py\", line 378, in load_entry_point\nFile \"build/bdist.linux-x86_64/egg/pkg_resources.py\", line 2566, in load_entry_point\nFile \"build/bdist.linux-x86_64/egg/pkg_resources.py\", line 2260, in load\nFile \"/usr/local/lib/python2.7/site-packages/pip-1.4.1-py2.7.egg/pip/__init__.py\", line 10, in <module>\n   from pip.util import get_installed_distributions, get_prog\nFile \"/usr/local/lib/python2.7/site-packages/pip-1.4.1-py2.7.egg/pip/util.py\", line 17, in <module>\n   from pip.vendor.distlib import version\nFile \"/usr/local/lib/python2.7/site-packages/pip-1.4.1-py2.7.egg/pip/vendor/distlib/version.py\", line 13, in <module>\n   from .compat import string_types\nFile \"/usr/local/lib/python2.7/site-packages/pip-1.4.1-py2.7.egg/pip/vendor/distlib/compat.py\", line 31, in <module>\n   from urllib2 import (Request, urlopen, URLError, HTTPError,\nImportError: cannot import name HTTPSHandle\n类似的错误还有：\nImportError: No module named zlib\nTraceback (most recent call last):\n  File \"/usr/bin/pip\", line 5, in <module>\n    from pkg_resources import load_entry_point\nImportError: No module named pkg_resources\n都是因为前面最早没有对系统进行升级，要进行Yum install openssl 和zlib 的安装升级。然后再重新安装python2.7，这样高版本的python 才不会有问题。\n","source":"_posts/python-build.md","raw":"---\ntitle: Python编程环境搭建\ndate: 2016-08-19 22:13:30\ntags: Python\ncategories: 技术\n---\n\n# 简介  #\n假如新建了一台CentOs虚拟机，作为纯净的操作系统，我们需要搭建基本的开发环境。CentOs6.x一般都是java1.5版本，python2.6，并且很多开发工具都是没有默认安装的，比如gcc。下面重点介绍Python环境的搭建，主要涉及Python升级，pip安装，virtualenv安装。\n# yum #\nyum 源更新，可选操作，可替换国内的yum源。\nyum groupinstall 。必选操作，编译python需要编译工具。还有一些必要的库文件需要安装，例如openssl,zlib,这些都需要额外安装。pip要安装python2.7版本，因此需要zlib需要更新到最新版本1.2.8，而CentOs默认是1.2.3。否则会报错，错误信息后面提示。\n```\n[root@jinqiu opt]# yum -y update\n[root@jinqiu opt]# yum groupinstall \"Development tools\"\n[root@jinqiu opt]# yum install zlib-devel bzip2-devel openssl-devel ncurses-devel sqlite-devel readline-devel tk-devel\n```\n顺便简单提下，快速免验证安装jdk\n```\n1. wget --no-check-certificate --no-cookies --header \"Cookie: oraclelicense=accept-securebackup-cookie\" http://download.oracle.com/otn-pub/java/jdk/8u131-b11/d54c1d3a095b4ff2b6607d096fa80163/jdk-8u131-linux-x64.rpm  \n2. rpm -ivh jdk-8u131-linux-x64.rpm\n```\n# python #\n保存2.6版本的python\n下载2.7版本的python源码文件，解压，然后配置，编译，安装\n```\n[root@jinqiu~]$ cd /opt\n[root@jinqiuopt]$ sudo wget --no-check-certificate https://www.python.org/ftp/python/2.7.6/Python-2.7.6.tar.xz\n[root@jinqiuopt]$ sudo tar xf Python-2.7.6.tar.xz \n[root@jinqiuopt]$ cd Python-2.7.6\n[root@jinqiuPython-2.7.6]$ sudo ./configure --prefix=/usr/local\n[root@jinqiuPython-2.7.6]$ sudo make && sudo make altinstall\n```\n<!-- more -->\n\n创建软链接到2.7版本的python\n修改yum文件，由于yum只支持2.6版本的python，因此需要将/usr/bin/yum中的默认python解释器修改为2.6版本。\n```\n[root@jinqiu opt]# mv /usr/bin/python /usr/bin/python2.6\n[root@jinqiu opt]# ln -s /usr/bin/python2.7  /usr/bin/python\n```\n\n从一开始，如果要做一些实际Python开发，你一定会用到一些第三方包。\n在Linux系统上至少有3种安装第三方包的方法。\n* 使用系统自带的包管理系统(deb, rpm, 等)\n* 通过社区开发的各种工具，例如 pip ， easy_install 等\n* 从源文件安装\n这三个方面，几乎完成同样的事情。即：安装依赖，编译代码（如果需要的话），将一个包含模块的包复制的标准软件包搜索位置。\n\n# 安装pip #\n```\n[root@jinqiu opt]# yum install python-pip\n[root@jinqiu opt]# pip install --upgrade pip\n```\n通过yum安装，可能安装的pip版本比较低。因此也可以通过网络上最新的pip源进行安装\n```\ncurl https://raw.githubusercontent.com/pypa/pip/master/contrib/get-pip.py | python2.7 -\n```\npip用法这里不再赘述，man pip 即可。\n\n# virtualenv #\n安装virtualenv，virtualenv作为python环境的隔离器，可以保证在这个环境下安装的第三方python库，都是在这个环境下的，而不会安装到系统路径上面去。\n我们需要处理的基本问题是包的依赖、版本和间接权限问题。想象一下，你有两个应用，一个应用需要libfoo的版本1，而另一应用需要版本2。如何才能同时使用这些应用程序？如果您安装到的/usr/lib/python2.7/site-packages（或任何平台的标准位置）的一切，在这种情况下，您可能会不小心升级不应该升级的应用程序。\n```\nsudo pip install virtualenv\nmkdir my_project_venv\nvirtualenv --distribute my_project_venv\n```\n** The output will something like:**\nNew python executable in my_project_venv/bin/python\nInstalling distribute.............................................done.\nInstalling pip.....................done.\n\n这里只列出了将被讨论的目录和文件\n```\n|-- bin\n|   |-- activate  # <-- 这个virtualenv的激活文件\n|   |-- pip       # <-- 这个virtualenv的独立pip\n|   |-- python    # <-- python解释器的一个副本\n|-- lib\n|-- python2.7 # <-- 所有的新包会被存在这\n```\n通过下面的命令激活这个virtualenv：\n```\ncd my_project_venv\nsource bin/activate\n```\n执行完毕后，提示可能是这个样子的：\n```\n(my_project_venv)$ # the virtualenv name prepended to the prompt\n通过 deactivate 命令离开virtualenv环境\n(my_project_venv)$ deactivate\n```\n注意：一定要先进行yum步骤的操作，安装必要的openssl库和zlib库，否则在编译之后的python2.7，会有问题。例如pip2.7会报错，zlib异常。\n\n# 异常处理：\npythonImportError: No module named zlib2.7，报错如下：\nTraceback (most recent call last):\nFile \"/usr/local/bin/pip\", line 9, in <module>\n   load_entry_point('pip==1.4.1', 'console_scripts', 'pip')()\nFile \"build/bdist.linux-x86_64/egg/pkg_resources.py\", line 378, in load_entry_point\nFile \"build/bdist.linux-x86_64/egg/pkg_resources.py\", line 2566, in load_entry_point\nFile \"build/bdist.linux-x86_64/egg/pkg_resources.py\", line 2260, in load\nFile \"/usr/local/lib/python2.7/site-packages/pip-1.4.1-py2.7.egg/pip/__init__.py\", line 10, in <module>\n   from pip.util import get_installed_distributions, get_prog\nFile \"/usr/local/lib/python2.7/site-packages/pip-1.4.1-py2.7.egg/pip/util.py\", line 17, in <module>\n   from pip.vendor.distlib import version\nFile \"/usr/local/lib/python2.7/site-packages/pip-1.4.1-py2.7.egg/pip/vendor/distlib/version.py\", line 13, in <module>\n   from .compat import string_types\nFile \"/usr/local/lib/python2.7/site-packages/pip-1.4.1-py2.7.egg/pip/vendor/distlib/compat.py\", line 31, in <module>\n   from urllib2 import (Request, urlopen, URLError, HTTPError,\nImportError: cannot import name HTTPSHandle\n类似的错误还有：\nImportError: No module named zlib\nTraceback (most recent call last):\n  File \"/usr/bin/pip\", line 5, in <module>\n    from pkg_resources import load_entry_point\nImportError: No module named pkg_resources\n都是因为前面最早没有对系统进行升级，要进行Yum install openssl 和zlib 的安装升级。然后再重新安装python2.7，这样高版本的python 才不会有问题。\n","slug":"python-build","published":1,"updated":"2018-04-27T04:11:03.367Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjhajczp70014e4rwpju9ryqs","content":"<h1 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h1><p>假如新建了一台CentOs虚拟机，作为纯净的操作系统，我们需要搭建基本的开发环境。CentOs6.x一般都是java1.5版本，python2.6，并且很多开发工具都是没有默认安装的，比如gcc。下面重点介绍Python环境的搭建，主要涉及Python升级，pip安装，virtualenv安装。</p>\n<h1 id=\"yum\"><a href=\"#yum\" class=\"headerlink\" title=\"yum\"></a>yum</h1><p>yum 源更新，可选操作，可替换国内的yum源。<br>yum groupinstall 。必选操作，编译python需要编译工具。还有一些必要的库文件需要安装，例如openssl,zlib,这些都需要额外安装。pip要安装python2.7版本，因此需要zlib需要更新到最新版本1.2.8，而CentOs默认是1.2.3。否则会报错，错误信息后面提示。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">[root@jinqiu opt]# yum -y update</div><div class=\"line\">[root@jinqiu opt]# yum groupinstall &quot;Development tools&quot;</div><div class=\"line\">[root@jinqiu opt]# yum install zlib-devel bzip2-devel openssl-devel ncurses-devel sqlite-devel readline-devel tk-devel</div></pre></td></tr></table></figure></p>\n<p>顺便简单提下，快速免验证安装jdk<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">1. wget --no-check-certificate --no-cookies --header &quot;Cookie: oraclelicense=accept-securebackup-cookie&quot; http://download.oracle.com/otn-pub/java/jdk/8u131-b11/d54c1d3a095b4ff2b6607d096fa80163/jdk-8u131-linux-x64.rpm  </div><div class=\"line\">2. rpm -ivh jdk-8u131-linux-x64.rpm</div></pre></td></tr></table></figure></p>\n<h1 id=\"python\"><a href=\"#python\" class=\"headerlink\" title=\"python\"></a>python</h1><p>保存2.6版本的python<br>下载2.7版本的python源码文件，解压，然后配置，编译，安装<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\">[root@jinqiu~]$ cd /opt</div><div class=\"line\">[root@jinqiuopt]$ sudo wget --no-check-certificate https://www.python.org/ftp/python/2.7.6/Python-2.7.6.tar.xz</div><div class=\"line\">[root@jinqiuopt]$ sudo tar xf Python-2.7.6.tar.xz </div><div class=\"line\">[root@jinqiuopt]$ cd Python-2.7.6</div><div class=\"line\">[root@jinqiuPython-2.7.6]$ sudo ./configure --prefix=/usr/local</div><div class=\"line\">[root@jinqiuPython-2.7.6]$ sudo make &amp;&amp; sudo make altinstall</div></pre></td></tr></table></figure></p>\n<a id=\"more\"></a>\n<p>创建软链接到2.7版本的python<br>修改yum文件，由于yum只支持2.6版本的python，因此需要将/usr/bin/yum中的默认python解释器修改为2.6版本。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">[root@jinqiu opt]# mv /usr/bin/python /usr/bin/python2.6</div><div class=\"line\">[root@jinqiu opt]# ln -s /usr/bin/python2.7  /usr/bin/python</div></pre></td></tr></table></figure></p>\n<p>从一开始，如果要做一些实际Python开发，你一定会用到一些第三方包。<br>在Linux系统上至少有3种安装第三方包的方法。</p>\n<ul>\n<li>使用系统自带的包管理系统(deb, rpm, 等)</li>\n<li>通过社区开发的各种工具，例如 pip ， easy_install 等</li>\n<li>从源文件安装<br>这三个方面，几乎完成同样的事情。即：安装依赖，编译代码（如果需要的话），将一个包含模块的包复制的标准软件包搜索位置。</li>\n</ul>\n<h1 id=\"安装pip\"><a href=\"#安装pip\" class=\"headerlink\" title=\"安装pip\"></a>安装pip</h1><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">[root@jinqiu opt]# yum install python-pip</div><div class=\"line\">[root@jinqiu opt]# pip install --upgrade pip</div></pre></td></tr></table></figure>\n<p>通过yum安装，可能安装的pip版本比较低。因此也可以通过网络上最新的pip源进行安装<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">curl https://raw.githubusercontent.com/pypa/pip/master/contrib/get-pip.py | python2.7 -</div></pre></td></tr></table></figure></p>\n<p>pip用法这里不再赘述，man pip 即可。</p>\n<h1 id=\"virtualenv\"><a href=\"#virtualenv\" class=\"headerlink\" title=\"virtualenv\"></a>virtualenv</h1><p>安装virtualenv，virtualenv作为python环境的隔离器，可以保证在这个环境下安装的第三方python库，都是在这个环境下的，而不会安装到系统路径上面去。<br>我们需要处理的基本问题是包的依赖、版本和间接权限问题。想象一下，你有两个应用，一个应用需要libfoo的版本1，而另一应用需要版本2。如何才能同时使用这些应用程序？如果您安装到的/usr/lib/python2.7/site-packages（或任何平台的标准位置）的一切，在这种情况下，您可能会不小心升级不应该升级的应用程序。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">sudo pip install virtualenv</div><div class=\"line\">mkdir my_project_venv</div><div class=\"line\">virtualenv --distribute my_project_venv</div></pre></td></tr></table></figure></p>\n<p><strong> The output will something like:</strong><br>New python executable in my_project_venv/bin/python<br>Installing distribute………………………………………done.<br>Installing pip…………………done.</p>\n<p>这里只列出了将被讨论的目录和文件<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\">|-- bin</div><div class=\"line\">|   |-- activate  # &lt;-- 这个virtualenv的激活文件</div><div class=\"line\">|   |-- pip       # &lt;-- 这个virtualenv的独立pip</div><div class=\"line\">|   |-- python    # &lt;-- python解释器的一个副本</div><div class=\"line\">|-- lib</div><div class=\"line\">|-- python2.7 # &lt;-- 所有的新包会被存在这</div></pre></td></tr></table></figure></p>\n<p>通过下面的命令激活这个virtualenv：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">cd my_project_venv</div><div class=\"line\">source bin/activate</div></pre></td></tr></table></figure></p>\n<p>执行完毕后，提示可能是这个样子的：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">(my_project_venv)$ # the virtualenv name prepended to the prompt</div><div class=\"line\">通过 deactivate 命令离开virtualenv环境</div><div class=\"line\">(my_project_venv)$ deactivate</div></pre></td></tr></table></figure></p>\n<p>注意：一定要先进行yum步骤的操作，安装必要的openssl库和zlib库，否则在编译之后的python2.7，会有问题。例如pip2.7会报错，zlib异常。</p>\n<h1 id=\"异常处理：\"><a href=\"#异常处理：\" class=\"headerlink\" title=\"异常处理：\"></a>异常处理：</h1><p>pythonImportError: No module named zlib2.7，报错如下：<br>Traceback (most recent call last):<br>File “/usr/local/bin/pip”, line 9, in <module><br>   load_entry_point(‘pip==1.4.1’, ‘console_scripts’, ‘pip’)()<br>File “build/bdist.linux-x86_64/egg/pkg_resources.py”, line 378, in load_entry_point<br>File “build/bdist.linux-x86_64/egg/pkg_resources.py”, line 2566, in load_entry_point<br>File “build/bdist.linux-x86_64/egg/pkg_resources.py”, line 2260, in load<br>File “/usr/local/lib/python2.7/site-packages/pip-1.4.1-py2.7.egg/pip/<strong>init</strong>.py”, line 10, in <module><br>   from pip.util import get_installed_distributions, get_prog<br>File “/usr/local/lib/python2.7/site-packages/pip-1.4.1-py2.7.egg/pip/util.py”, line 17, in <module><br>   from pip.vendor.distlib import version<br>File “/usr/local/lib/python2.7/site-packages/pip-1.4.1-py2.7.egg/pip/vendor/distlib/version.py”, line 13, in <module><br>   from .compat import string_types<br>File “/usr/local/lib/python2.7/site-packages/pip-1.4.1-py2.7.egg/pip/vendor/distlib/compat.py”, line 31, in <module><br>   from urllib2 import (Request, urlopen, URLError, HTTPError,<br>ImportError: cannot import name HTTPSHandle<br>类似的错误还有：<br>ImportError: No module named zlib<br>Traceback (most recent call last):<br>  File “/usr/bin/pip”, line 5, in <module><br>    from pkg_resources import load_entry_point<br>ImportError: No module named pkg_resources<br>都是因为前面最早没有对系统进行升级，要进行Yum install openssl 和zlib 的安装升级。然后再重新安装python2.7，这样高版本的python 才不会有问题。</module></module></module></module></module></module></p>\n","site":{"data":{}},"excerpt":"<h1 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h1><p>假如新建了一台CentOs虚拟机，作为纯净的操作系统，我们需要搭建基本的开发环境。CentOs6.x一般都是java1.5版本，python2.6，并且很多开发工具都是没有默认安装的，比如gcc。下面重点介绍Python环境的搭建，主要涉及Python升级，pip安装，virtualenv安装。</p>\n<h1 id=\"yum\"><a href=\"#yum\" class=\"headerlink\" title=\"yum\"></a>yum</h1><p>yum 源更新，可选操作，可替换国内的yum源。<br>yum groupinstall 。必选操作，编译python需要编译工具。还有一些必要的库文件需要安装，例如openssl,zlib,这些都需要额外安装。pip要安装python2.7版本，因此需要zlib需要更新到最新版本1.2.8，而CentOs默认是1.2.3。否则会报错，错误信息后面提示。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">[root@jinqiu opt]# yum -y update</div><div class=\"line\">[root@jinqiu opt]# yum groupinstall &quot;Development tools&quot;</div><div class=\"line\">[root@jinqiu opt]# yum install zlib-devel bzip2-devel openssl-devel ncurses-devel sqlite-devel readline-devel tk-devel</div></pre></td></tr></table></figure></p>\n<p>顺便简单提下，快速免验证安装jdk<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">1. wget --no-check-certificate --no-cookies --header &quot;Cookie: oraclelicense=accept-securebackup-cookie&quot; http://download.oracle.com/otn-pub/java/jdk/8u131-b11/d54c1d3a095b4ff2b6607d096fa80163/jdk-8u131-linux-x64.rpm  </div><div class=\"line\">2. rpm -ivh jdk-8u131-linux-x64.rpm</div></pre></td></tr></table></figure></p>\n<h1 id=\"python\"><a href=\"#python\" class=\"headerlink\" title=\"python\"></a>python</h1><p>保存2.6版本的python<br>下载2.7版本的python源码文件，解压，然后配置，编译，安装<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\">[root@jinqiu~]$ cd /opt</div><div class=\"line\">[root@jinqiuopt]$ sudo wget --no-check-certificate https://www.python.org/ftp/python/2.7.6/Python-2.7.6.tar.xz</div><div class=\"line\">[root@jinqiuopt]$ sudo tar xf Python-2.7.6.tar.xz </div><div class=\"line\">[root@jinqiuopt]$ cd Python-2.7.6</div><div class=\"line\">[root@jinqiuPython-2.7.6]$ sudo ./configure --prefix=/usr/local</div><div class=\"line\">[root@jinqiuPython-2.7.6]$ sudo make &amp;&amp; sudo make altinstall</div></pre></td></tr></table></figure></p>","more":"<p>创建软链接到2.7版本的python<br>修改yum文件，由于yum只支持2.6版本的python，因此需要将/usr/bin/yum中的默认python解释器修改为2.6版本。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">[root@jinqiu opt]# mv /usr/bin/python /usr/bin/python2.6</div><div class=\"line\">[root@jinqiu opt]# ln -s /usr/bin/python2.7  /usr/bin/python</div></pre></td></tr></table></figure></p>\n<p>从一开始，如果要做一些实际Python开发，你一定会用到一些第三方包。<br>在Linux系统上至少有3种安装第三方包的方法。</p>\n<ul>\n<li>使用系统自带的包管理系统(deb, rpm, 等)</li>\n<li>通过社区开发的各种工具，例如 pip ， easy_install 等</li>\n<li>从源文件安装<br>这三个方面，几乎完成同样的事情。即：安装依赖，编译代码（如果需要的话），将一个包含模块的包复制的标准软件包搜索位置。</li>\n</ul>\n<h1 id=\"安装pip\"><a href=\"#安装pip\" class=\"headerlink\" title=\"安装pip\"></a>安装pip</h1><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">[root@jinqiu opt]# yum install python-pip</div><div class=\"line\">[root@jinqiu opt]# pip install --upgrade pip</div></pre></td></tr></table></figure>\n<p>通过yum安装，可能安装的pip版本比较低。因此也可以通过网络上最新的pip源进行安装<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">curl https://raw.githubusercontent.com/pypa/pip/master/contrib/get-pip.py | python2.7 -</div></pre></td></tr></table></figure></p>\n<p>pip用法这里不再赘述，man pip 即可。</p>\n<h1 id=\"virtualenv\"><a href=\"#virtualenv\" class=\"headerlink\" title=\"virtualenv\"></a>virtualenv</h1><p>安装virtualenv，virtualenv作为python环境的隔离器，可以保证在这个环境下安装的第三方python库，都是在这个环境下的，而不会安装到系统路径上面去。<br>我们需要处理的基本问题是包的依赖、版本和间接权限问题。想象一下，你有两个应用，一个应用需要libfoo的版本1，而另一应用需要版本2。如何才能同时使用这些应用程序？如果您安装到的/usr/lib/python2.7/site-packages（或任何平台的标准位置）的一切，在这种情况下，您可能会不小心升级不应该升级的应用程序。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">sudo pip install virtualenv</div><div class=\"line\">mkdir my_project_venv</div><div class=\"line\">virtualenv --distribute my_project_venv</div></pre></td></tr></table></figure></p>\n<p><strong> The output will something like:</strong><br>New python executable in my_project_venv/bin/python<br>Installing distribute………………………………………done.<br>Installing pip…………………done.</p>\n<p>这里只列出了将被讨论的目录和文件<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\">|-- bin</div><div class=\"line\">|   |-- activate  # &lt;-- 这个virtualenv的激活文件</div><div class=\"line\">|   |-- pip       # &lt;-- 这个virtualenv的独立pip</div><div class=\"line\">|   |-- python    # &lt;-- python解释器的一个副本</div><div class=\"line\">|-- lib</div><div class=\"line\">|-- python2.7 # &lt;-- 所有的新包会被存在这</div></pre></td></tr></table></figure></p>\n<p>通过下面的命令激活这个virtualenv：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">cd my_project_venv</div><div class=\"line\">source bin/activate</div></pre></td></tr></table></figure></p>\n<p>执行完毕后，提示可能是这个样子的：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">(my_project_venv)$ # the virtualenv name prepended to the prompt</div><div class=\"line\">通过 deactivate 命令离开virtualenv环境</div><div class=\"line\">(my_project_venv)$ deactivate</div></pre></td></tr></table></figure></p>\n<p>注意：一定要先进行yum步骤的操作，安装必要的openssl库和zlib库，否则在编译之后的python2.7，会有问题。例如pip2.7会报错，zlib异常。</p>\n<h1 id=\"异常处理：\"><a href=\"#异常处理：\" class=\"headerlink\" title=\"异常处理：\"></a>异常处理：</h1><p>pythonImportError: No module named zlib2.7，报错如下：<br>Traceback (most recent call last):<br>File “/usr/local/bin/pip”, line 9, in <module><br>   load_entry_point(‘pip==1.4.1’, ‘console_scripts’, ‘pip’)()<br>File “build/bdist.linux-x86_64/egg/pkg_resources.py”, line 378, in load_entry_point<br>File “build/bdist.linux-x86_64/egg/pkg_resources.py”, line 2566, in load_entry_point<br>File “build/bdist.linux-x86_64/egg/pkg_resources.py”, line 2260, in load<br>File “/usr/local/lib/python2.7/site-packages/pip-1.4.1-py2.7.egg/pip/<strong>init</strong>.py”, line 10, in <module><br>   from pip.util import get_installed_distributions, get_prog<br>File “/usr/local/lib/python2.7/site-packages/pip-1.4.1-py2.7.egg/pip/util.py”, line 17, in <module><br>   from pip.vendor.distlib import version<br>File “/usr/local/lib/python2.7/site-packages/pip-1.4.1-py2.7.egg/pip/vendor/distlib/version.py”, line 13, in <module><br>   from .compat import string_types<br>File “/usr/local/lib/python2.7/site-packages/pip-1.4.1-py2.7.egg/pip/vendor/distlib/compat.py”, line 31, in <module><br>   from urllib2 import (Request, urlopen, URLError, HTTPError,<br>ImportError: cannot import name HTTPSHandle<br>类似的错误还有：<br>ImportError: No module named zlib<br>Traceback (most recent call last):<br>  File “/usr/bin/pip”, line 5, in <module><br>    from pkg_resources import load_entry_point<br>ImportError: No module named pkg_resources<br>都是因为前面最早没有对系统进行升级，要进行Yum install openssl 和zlib 的安装升级。然后再重新安装python2.7，这样高版本的python 才不会有问题。</module></module></module></module></module></module></p>"},{"title":"TICK 监控体系","date":"2018-02-27T04:33:38.000Z","_content":"\n## 1.Tick安装部署\n\n### Tick简介\n\nTICK 是由 InfluxData 开发的一套运维工具栈，由 Telegraf, InfluxDB, Chronograf, Kapacitor 四个工具的首字母组成。\n这一套组件将收集数据和入库、数据库存储、展示、告警四者囊括了。\n\n![TICK框架图](/img/tick.png \"TICK\") \n\n<!-- more -->\n\nTelegraf\n\n是一个数据收集和入库的工具。提供了很多 input 和 output 插件，比如收集本地的 cpu、load、网络流量等数据，然后写入 InfluxDB 、Kafka或者OpenTSDB等。相当于ELK栈中的 logstash 功能。\n\nInfluxDB\n\nInfluxDB 是一个开源的GO语言为基础的数据库, 用来处理时间序列数据,提供了较高的可用性。与opentsdb类似，支持HTTP API方式，写入和读取数据。相当于ELK栈中的elasticsearch功能。\n\nChronograf\n\n从InfluxDB时间序列数据的数据可视化工具，负责从InfluxDB收集数据，并将数据图表以web的形式发布。它使用简单,包括模板和库可以快速构建实时数据的可视化仪表板，轻松地创建报警和自动化的规则。相当于ELK栈中的kibana功能。\n\nKapacitor\n\nKapacitor是InfluxDB的数据处理引擎，主要作用是时间序列数据处理、监视和警报。\n\n### TICK安装（DOCKER版）\n\n[官方安装介绍](https://portal.influxdata.com/downloads)\n\n本地安装版本分别是telegraf1.5,influxdb1.42,chronograf1.4,kapacitor1.4\n\n```\ndocker pull telegraf\ndocker pull influxdb\ndocker pull quay.io/influxdb/chronograf\ndocker pull kapacitor\n```\n\n\n\n### TICK配置与运行\n\n[TICK Docker file](https://github.com/influxdata/influxdata-docker)\n\n\n\n#### InfluxDB\n\n[How to run](https://github.com/docker-library/docs/blob/master/influxdb/README.md)\n\n参数介绍：\n\n- 8086 HTTP API port\n- 8083 Administrator interface port, if it is enabled\n- 2003 Graphite support, if it is enabled\n\n启动server 可修改默认配置文件等，详见[How to run]\n\n```bash\ndocker run --name influxdb -d -p 8086:8086 -p 8083:8083  -e INFLUXDB_ADMIN_ENABLED=true  -v /var/lib/influxdb:/var/lib/influxdb  influxdb\n```\n\n初始化数据库\n\n```bash\ndocker run --rm \\\n      -e INFLUXDB_DB=db0 -e INFLUXDB_ADMIN_ENABLED=true \\\n      -e INFLUXDB_ADMIN_USER=admin -e INFLUXDB_ADMIN_USER=admin \\\n      -e INFLUXDB_USER=telegraf -e INFLUXDB_USER_PASSWORD=telegraf \\\n      -v /var/lib/influxdb:/var/lib/influxdb \\\n      influxdb /init-influxdb.sh\n```\n\n启动client\n\n```bash\ndocker run --rm --link=influxdb -it influxdb influx -host influxdb\n```\n\n其他命令\n\n创建数据库\n\n```shell\ncurl -G http://localhost:8086/query --data-urlencode \"q=CREATE DATABASE mydb\"\n```\n\n插入数据\n\n```shell\ncurl -XPOST \"http://localhost:8086/write?db=mydb\" \\\n-d 'cpu,host=server01,region=uswest load=42 1434055562000000000'\n\ncurl -XPOST \"http://localhost:8086/write?db=mydb\" \\\n-d 'cpu,host=server02,region=uswest load=78 1434055562000000000'\n\ncurl -XPOST \"http://localhost:8086/write?db=mydb\" \\\n-d 'cpu,host=server03,region=useast load=15.4 1434055562000000000'\n```\n\n查询数据\n\n```shell\ncurl -G \"http://localhost:8086/query?pretty=true\" --data-urlencode \"db=mydb\" \\\n--data-urlencode \"q=SELECT * FROM cpu WHERE host='server01' AND time < now() - 1d\"\n```\n\n分析数据\n\n```shell\ncurl -G \"http://localhost:8086/query?pretty=true\" --data-urlencode \"db=mydb\" \\\n--data-urlencode \"q=SELECT mean(load) FROM cpu WHERE region='uswest'\"\n```\n\n[InfluxDB概念介绍](https://docs.influxdata.com/influxdb/v1.4/concepts/key_concepts/#field-key)\n\n#### Telegraf\n\n[Telegraf Docker file](https://github.com/influxdata/influxdata-docker/blob/ae56003075c8f39b6c265294ca8235c2c5235cc6/telegraf/1.5/alpine/Dockerfile)\n[How to run](https://github.com/docker-library/docs/blob/master/telegraf/content.md)\n\n参数介绍：\n\n- 8125 StatsD\n- 8092 UDP\n- 8094 TCP\n\n```bash\ndocker run --name telegraf -d -p 8125:8125 -p 8092:8092 -p 8094:8094  -v ~/telegraf.conf:/etc/telegraf/telegraf.conf telegraf\n```\n\n#### Chronograf\n\nExposed Ports\n\n- 8888TCP -- Listen  endpoint\n\n```shell\ndocker run -d --name chronograf -p 8888:8888 -v /var/lib/chronograf:/var/lib/chronograf quay.io/influxdb/chronograf chronograf\n```\n\n\n\n#### Kapacitor\n\nExposed Ports\n\n- 9092 TCP -- HTTP API endpoint\n\n需要修改默认的kapacitor.conf url地址，默认是配置localhost，而容器进程肯定不是localhost.\n\n```shell\ndocker run -d --name kapacitor -p 9092:9092 -v /var/lib/kapacitor:/var/lib/kapacitor  -v ~/kapacitor.conf:/etc/kapacitor/kapacitor.conf  kapacitor \n```\n\n查看帮助\n\n```shell\ndocker exec  kapacitor kapacitor help\ndocker exec  kapacitor kapacitor list tasks \ndocker exec  kapacitor kapacitor show CPURule\n\n```\n\n常见问题：\n\n容器告警因为ip原因无法，注册到influxdb时，ip是容器id，如下，导致Influxdb 无法识别该地址，性能数据无法发送到kapacitor。因此临时解决方案是修改容器的hostname\n\n```\n> show SUBSCRIPTIONS\nname: telegraf\nretention_policy name                                           mode destinations\n---------------- ----                                           ---- ------------\nautogen          kapacitor-8ca0780c-5208-41d5-91ec-05b28d13b2b8 ANY  [http://c9816b89f375:9092]\n\nname: _internal\nretention_policy name                                           mode destinations\n---------------- ----                                           ---- ------------\nmonitor          kapacitor-8ca0780c-5208-41d5-91ec-05b28d13b2b8 ANY  [http://c9816b89f375:9092]\n\nname: chronograf\nretention_policy name                                           mode destinations\n---------------- ----                                           ---- ------------\nautogen          kapacitor-8ca0780c-5208-41d5-91ec-05b28d13b2b8 ANY  [http://c9816b89f375:9092]\n\n```\n\n\n\n```\n docker run -d --name kapacitor -e KAPACITOR_HOSTNAME=192.168.14.165 -p 9092:9092 -v /var/lib/kapacitor:/var/lib/kapacitor  -v ~/kapacitor.conf:/etc/kapacitor/kapacitor.conf  kapacitor\n```\n\n\n\n## 2.Telgraf重点介绍\n\nTelegraf是一个用Go编写的代理程序，用于收集，处理，汇总和编写度量标准。\n\nTelegraf是插件驱动，并有4个不同的插件的概念：\n\n1. [输入插件](https://github.com/influxdata/telegraf#input-plugins)从系统，服务或第三方API收集指标。\n2. [处理器插件](https://github.com/influxdata/telegraf#processor-plugins)转换，修饰和/或过滤度量标准\n3. [聚合器插件](https://github.com/influxdata/telegraf#aggregator-plugins)创建聚合度量（例如平均值，最小值，最大值，分位数等）\n4. [输出插件](https://github.com/influxdata/telegraf#output-plugins)将指标写入各个目标\n\nTelegraf 二进制文件包含对所有支持的采集对象的采集功能，只是可以通过配置选择性的开启。如果更新某个对象的采集指标，则需要重新编译整个二进制文件。\n\n\n\n### 测试数据/帮助命令\n\n~/telegraf.conf 测试用配置文件，可定义输入插件\n\n```shell\ndocker run -v ~/telegraf.conf:/etc/telegraf/telegraf.conf --rm telegraf telegraf --config /etc/telegraf/telegraf.conf  --test\n```\n\n```\ndocker run --rm telegraf telegraf --help\n```\n\n### 数据收集方式\n\npush 模式，由telegraf 根据input配置主动采集并发送到output配置的后端。一般是InfluxDB数据库。TICK并无服务端的概念，在TICK监控部署文档的架构图中可知。\n\n[input-output-plugins](https://github.com/influxdata/telegraf/blob/master/README.md)\n\n\n\n### InfluxDB Line Protocol 数据格式\n\nInfluxDB Line Protocol 格式\n\nTelegraf metrics, like InfluxDB [points](https://docs.influxdata.com/influxdb/v0.10/write_protocols/line/), are a combination of four basic parts:\n\n```shell\nmeasurement_name[,tag1=val1,...]  field1=val1,field2=val2,...\n```\n\n1. Measurement Name\n\n2. Tags\n\n3. Fields\n\n4. Timestamp\n\n   ​\n\n### 输入数据格式\n\n[telegraf有六种采集数据输入格式](https://docs.influxdata.com/telegraf/v1.5/concepts/data_formats_input/)\n\nTelegraf is able to parse the following input data formats into metrics:\n\n1. [InfluxDB Line Protocol](https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md#influx)\n2. [JSON](https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md#json)\n3. [Graphite](https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md#graphite)\n4. [Value](https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md#value), ie: 45 or \"booyah\"\n5. [Nagios](https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md#nagios) (exec input only)\n6. [Collectd](https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md#collectd)\n\n默认是第一种：InfluxDB Line Protocol。Graphite、Nagios、Collected都是其他开源监控项目，其中Nagios是比较完整的监控系统，其格式只能用在exec输入插件中；Collected一般用作系统守护进程，采集系统统计信息，并且通过网络开放数据；Graphite则是另外一套监控系统，也是C-S模式。\n\n如果是配置成其他格式，那么采集回来的数据（受限于采集方可吐出的数据），将会被Telegraf  转换为其可识别的InfluxDB Line Protocol。\n\n\n\n### 输出数据格式\n\n[telegraf有三种采集数据输出格式](https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md)\n\nTelegraf is able to serialize metrics into the following output data formats:\n\n1. [InfluxDB Line Protocol](https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md#influx)\n2. [JSON](https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md#json)\n3. [Graphite](https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md#graphite)\n\n一般而言，输出格式如果是其他格式，例如Json，那么输出的对象则一般为file类型。从实际应用中来看，即通过telegraf采集回数据，生成json文件，发送至其他系统进行处理。\n\n以下是采集结果示例\n\n```shell\n* Plugin: inputs.cpu, Collection 1\n* Plugin: inputs.cpu, Collection 2\n> cpu,cpu=cpu0,host=0336dcb23579 usage_system=0,usage_idle=100,usage_nice=0,usage_iowait=0,usage_irq=0,usage_guest=0,usage_user=0,usage_softirq=0,usage_steal=0,usage_guest_nice=0 1515338025000000000\n> cpu,cpu=cpu1,host=0336dcb23579 usage_user=0,usage_guest_nice=0,usage_steal=0,usage_guest=0,usage_system=0,usage_idle=100,usage_nice=0,usage_iowait=0,usage_irq=0,usage_softirq=0 1515338025000000000\n> cpu,cpu=cpu2,host=0336dcb23579 usage_idle=100,usage_nice=0,usage_steal=0,usage_guest=0,usage_user=0,usage_system=0,usage_iowait=0,usage_irq=0,usage_softirq=0,usage_guest_nice=0 1515338025000000000\n> cpu,host=0336dcb23579,cpu=cpu3 usage_softirq=0,usage_guest_nice=0,usage_idle=100,usage_nice=0,usage_iowait=0,usage_irq=0,usage_steal=0,usage_guest=0,usage_user=0,usage_system=0 1515338025000000000\n> cpu,cpu=cpu-total,host=0336dcb23579 usage_guest_nice=0,usage_idle=100,usage_nice=0,usage_irq=0,usage_steal=0,usage_guest=0,usage_user=0,usage_system=0,usage_iowait=0,usage_softirq=0 1515338025000000000\n\n```\n\ncpu：measurements ，类似oracle的表名\n\ncpu=cpu-total,host=0336dcb23579 ： cpu、host为tag，索引；\n\nusage_guest_nice=0,usage_idle=100,usage_nice=0... : usage_guest_nice等为field，即被采集的指标字段。field在measurements是不可或缺的，tag可无， tag与field用空格隔开，field放置在后面\n\n1515338025000000000： time 时间戳\n\n\n\n### 数据过滤/聚合\n\nMeasurement Filtering\n\n![数据处理](/img/agg-pro.jpg)\n\nFilters can be configured per input, output, processor, or aggregator, see below for examples.\n\n- **namepass**: An array of glob pattern strings. Only points whose measurement name matches a pattern in this list are emitted.\n- **namedrop**: The inverse of `namepass`. If a match is found the point is discarded. This is tested on points after they have passed the `namepass` test.\n- **fieldpass**: An array of glob pattern strings. Only fields whose field key matches a pattern in this list are emitted. Not available for outputs.\n- **fielddrop**: The inverse of `fieldpass`. Fields with a field key matching one of the patterns will be discarded from the point. This is tested on points after they have passed the `fieldpass` test. Not available for outputs.\n- **tagpass**: A table mapping tag keys to arrays of glob pattern strings. Only points that contain a tag key in the table and a tag value matching one of its patterns is emitted.\n- **tagdrop**: The inverse of `tagpass`. If a match is found the point is discarded. This is tested on points after they have passed the `tagpass` test.\n- **taginclude**: An array of glob pattern strings. Only tags with a tag key matching one of the patterns are emitted. In contrast to `tagpass`, which will pass an entire point based on its tag, `taginclude` removes all non matching tags from the point. This filter can be used on both inputs & outputs, but it is *recommended* to be used on inputs, as it is more efficient to filter out tags at the ingestion point.\n- **tagexclude**: The inverse of `taginclude`. Tags with a tag key matching one of the patterns will be discarded from the point.\n\n输入数据示例\n\n```shell\n[[inputs.cpu]]\n  percpu = true\n  totalcpu = false\n  fielddrop = [\"cpu_time\"]\n  # Don't collect CPU data for cpu6 & cpu7\n  [inputs.cpu.tagdrop]\n    cpu = [ \"cpu6\", \"cpu7\" ]\n\n[[inputs.disk]]\n  [inputs.disk.tagpass]\n    # tagpass conditions are OR, not AND.\n    # If the (filesystem is ext4 or xfs) OR (the path is /opt or /home)\n    # then the metric passes\n    fstype = [ \"ext4\", \"xfs\" ]\n    # Globs can also be used on the tag values\n    path = [ \"/opt\", \"/home*\" ]\n```\n\n输出数据示例\n\n与输入（采集）数据过滤类似，可定义多个输出源\n\n```shell\n[[outputs.influxdb]]\n  urls = [ \"http://localhost:8086\" ]\n  database = \"telegraf\"\n  # Drop all measurements that start with \"aerospike\"\n  namedrop = [\"aerospike*\"]\n\n[[outputs.influxdb]]\n  urls = [ \"http://localhost:8086\" ]\n  database = \"telegraf-aerospike-data\"\n  # Only accept aerospike data:\n  namepass = [\"aerospike*\"]\n\n[[outputs.influxdb]]\n  urls = [ \"http://localhost:8086\" ]\n  database = \"telegraf-cpu0-data\"\n  # Only store measurements where the tag \"cpu\" matches the value \"cpu0\"\n  [outputs.influxdb.tagpass]\n    cpu = [\"cpu0\"]\n```\n\nAggregator Plugins\n\n- [basicstats](https://github.com/influxdata/telegraf/blob/master/plugins/aggregators/basicstats)\n- [minmax](https://github.com/influxdata/telegraf/blob/master/plugins/aggregators/minmax)\n- [histogram](https://github.com/influxdata/telegraf/blob/master/plugins/aggregators/histogram)\n\n### 部署方式\n\n客户端部署方式，可结合Ansible 进行部署，以及配置管理。官方说明并未看到。\n\n\n\n### 任务配置\n\n客户端配置文件配置 /etc/telegraf/telegraf.conf，可配置指标采集周期、过滤指标等。\n\n\n\n### 告警处理\n\n与Telegraf配合使用的告警处理程序是[kapacitor](https://github.com/influxdata/kapacitor) \n\nOpen source framework for processing, monitoring, and alerting on time series data。","source":"_posts/tick.md","raw":"---\ntitle: TICK 监控体系\ndate: 2018-02-27 12:33:38\ntags: \n - TICK\n - InfluxDB \n - Telegraf\n - Go\ncategories: 技术\n---\n\n## 1.Tick安装部署\n\n### Tick简介\n\nTICK 是由 InfluxData 开发的一套运维工具栈，由 Telegraf, InfluxDB, Chronograf, Kapacitor 四个工具的首字母组成。\n这一套组件将收集数据和入库、数据库存储、展示、告警四者囊括了。\n\n![TICK框架图](/img/tick.png \"TICK\") \n\n<!-- more -->\n\nTelegraf\n\n是一个数据收集和入库的工具。提供了很多 input 和 output 插件，比如收集本地的 cpu、load、网络流量等数据，然后写入 InfluxDB 、Kafka或者OpenTSDB等。相当于ELK栈中的 logstash 功能。\n\nInfluxDB\n\nInfluxDB 是一个开源的GO语言为基础的数据库, 用来处理时间序列数据,提供了较高的可用性。与opentsdb类似，支持HTTP API方式，写入和读取数据。相当于ELK栈中的elasticsearch功能。\n\nChronograf\n\n从InfluxDB时间序列数据的数据可视化工具，负责从InfluxDB收集数据，并将数据图表以web的形式发布。它使用简单,包括模板和库可以快速构建实时数据的可视化仪表板，轻松地创建报警和自动化的规则。相当于ELK栈中的kibana功能。\n\nKapacitor\n\nKapacitor是InfluxDB的数据处理引擎，主要作用是时间序列数据处理、监视和警报。\n\n### TICK安装（DOCKER版）\n\n[官方安装介绍](https://portal.influxdata.com/downloads)\n\n本地安装版本分别是telegraf1.5,influxdb1.42,chronograf1.4,kapacitor1.4\n\n```\ndocker pull telegraf\ndocker pull influxdb\ndocker pull quay.io/influxdb/chronograf\ndocker pull kapacitor\n```\n\n\n\n### TICK配置与运行\n\n[TICK Docker file](https://github.com/influxdata/influxdata-docker)\n\n\n\n#### InfluxDB\n\n[How to run](https://github.com/docker-library/docs/blob/master/influxdb/README.md)\n\n参数介绍：\n\n- 8086 HTTP API port\n- 8083 Administrator interface port, if it is enabled\n- 2003 Graphite support, if it is enabled\n\n启动server 可修改默认配置文件等，详见[How to run]\n\n```bash\ndocker run --name influxdb -d -p 8086:8086 -p 8083:8083  -e INFLUXDB_ADMIN_ENABLED=true  -v /var/lib/influxdb:/var/lib/influxdb  influxdb\n```\n\n初始化数据库\n\n```bash\ndocker run --rm \\\n      -e INFLUXDB_DB=db0 -e INFLUXDB_ADMIN_ENABLED=true \\\n      -e INFLUXDB_ADMIN_USER=admin -e INFLUXDB_ADMIN_USER=admin \\\n      -e INFLUXDB_USER=telegraf -e INFLUXDB_USER_PASSWORD=telegraf \\\n      -v /var/lib/influxdb:/var/lib/influxdb \\\n      influxdb /init-influxdb.sh\n```\n\n启动client\n\n```bash\ndocker run --rm --link=influxdb -it influxdb influx -host influxdb\n```\n\n其他命令\n\n创建数据库\n\n```shell\ncurl -G http://localhost:8086/query --data-urlencode \"q=CREATE DATABASE mydb\"\n```\n\n插入数据\n\n```shell\ncurl -XPOST \"http://localhost:8086/write?db=mydb\" \\\n-d 'cpu,host=server01,region=uswest load=42 1434055562000000000'\n\ncurl -XPOST \"http://localhost:8086/write?db=mydb\" \\\n-d 'cpu,host=server02,region=uswest load=78 1434055562000000000'\n\ncurl -XPOST \"http://localhost:8086/write?db=mydb\" \\\n-d 'cpu,host=server03,region=useast load=15.4 1434055562000000000'\n```\n\n查询数据\n\n```shell\ncurl -G \"http://localhost:8086/query?pretty=true\" --data-urlencode \"db=mydb\" \\\n--data-urlencode \"q=SELECT * FROM cpu WHERE host='server01' AND time < now() - 1d\"\n```\n\n分析数据\n\n```shell\ncurl -G \"http://localhost:8086/query?pretty=true\" --data-urlencode \"db=mydb\" \\\n--data-urlencode \"q=SELECT mean(load) FROM cpu WHERE region='uswest'\"\n```\n\n[InfluxDB概念介绍](https://docs.influxdata.com/influxdb/v1.4/concepts/key_concepts/#field-key)\n\n#### Telegraf\n\n[Telegraf Docker file](https://github.com/influxdata/influxdata-docker/blob/ae56003075c8f39b6c265294ca8235c2c5235cc6/telegraf/1.5/alpine/Dockerfile)\n[How to run](https://github.com/docker-library/docs/blob/master/telegraf/content.md)\n\n参数介绍：\n\n- 8125 StatsD\n- 8092 UDP\n- 8094 TCP\n\n```bash\ndocker run --name telegraf -d -p 8125:8125 -p 8092:8092 -p 8094:8094  -v ~/telegraf.conf:/etc/telegraf/telegraf.conf telegraf\n```\n\n#### Chronograf\n\nExposed Ports\n\n- 8888TCP -- Listen  endpoint\n\n```shell\ndocker run -d --name chronograf -p 8888:8888 -v /var/lib/chronograf:/var/lib/chronograf quay.io/influxdb/chronograf chronograf\n```\n\n\n\n#### Kapacitor\n\nExposed Ports\n\n- 9092 TCP -- HTTP API endpoint\n\n需要修改默认的kapacitor.conf url地址，默认是配置localhost，而容器进程肯定不是localhost.\n\n```shell\ndocker run -d --name kapacitor -p 9092:9092 -v /var/lib/kapacitor:/var/lib/kapacitor  -v ~/kapacitor.conf:/etc/kapacitor/kapacitor.conf  kapacitor \n```\n\n查看帮助\n\n```shell\ndocker exec  kapacitor kapacitor help\ndocker exec  kapacitor kapacitor list tasks \ndocker exec  kapacitor kapacitor show CPURule\n\n```\n\n常见问题：\n\n容器告警因为ip原因无法，注册到influxdb时，ip是容器id，如下，导致Influxdb 无法识别该地址，性能数据无法发送到kapacitor。因此临时解决方案是修改容器的hostname\n\n```\n> show SUBSCRIPTIONS\nname: telegraf\nretention_policy name                                           mode destinations\n---------------- ----                                           ---- ------------\nautogen          kapacitor-8ca0780c-5208-41d5-91ec-05b28d13b2b8 ANY  [http://c9816b89f375:9092]\n\nname: _internal\nretention_policy name                                           mode destinations\n---------------- ----                                           ---- ------------\nmonitor          kapacitor-8ca0780c-5208-41d5-91ec-05b28d13b2b8 ANY  [http://c9816b89f375:9092]\n\nname: chronograf\nretention_policy name                                           mode destinations\n---------------- ----                                           ---- ------------\nautogen          kapacitor-8ca0780c-5208-41d5-91ec-05b28d13b2b8 ANY  [http://c9816b89f375:9092]\n\n```\n\n\n\n```\n docker run -d --name kapacitor -e KAPACITOR_HOSTNAME=192.168.14.165 -p 9092:9092 -v /var/lib/kapacitor:/var/lib/kapacitor  -v ~/kapacitor.conf:/etc/kapacitor/kapacitor.conf  kapacitor\n```\n\n\n\n## 2.Telgraf重点介绍\n\nTelegraf是一个用Go编写的代理程序，用于收集，处理，汇总和编写度量标准。\n\nTelegraf是插件驱动，并有4个不同的插件的概念：\n\n1. [输入插件](https://github.com/influxdata/telegraf#input-plugins)从系统，服务或第三方API收集指标。\n2. [处理器插件](https://github.com/influxdata/telegraf#processor-plugins)转换，修饰和/或过滤度量标准\n3. [聚合器插件](https://github.com/influxdata/telegraf#aggregator-plugins)创建聚合度量（例如平均值，最小值，最大值，分位数等）\n4. [输出插件](https://github.com/influxdata/telegraf#output-plugins)将指标写入各个目标\n\nTelegraf 二进制文件包含对所有支持的采集对象的采集功能，只是可以通过配置选择性的开启。如果更新某个对象的采集指标，则需要重新编译整个二进制文件。\n\n\n\n### 测试数据/帮助命令\n\n~/telegraf.conf 测试用配置文件，可定义输入插件\n\n```shell\ndocker run -v ~/telegraf.conf:/etc/telegraf/telegraf.conf --rm telegraf telegraf --config /etc/telegraf/telegraf.conf  --test\n```\n\n```\ndocker run --rm telegraf telegraf --help\n```\n\n### 数据收集方式\n\npush 模式，由telegraf 根据input配置主动采集并发送到output配置的后端。一般是InfluxDB数据库。TICK并无服务端的概念，在TICK监控部署文档的架构图中可知。\n\n[input-output-plugins](https://github.com/influxdata/telegraf/blob/master/README.md)\n\n\n\n### InfluxDB Line Protocol 数据格式\n\nInfluxDB Line Protocol 格式\n\nTelegraf metrics, like InfluxDB [points](https://docs.influxdata.com/influxdb/v0.10/write_protocols/line/), are a combination of four basic parts:\n\n```shell\nmeasurement_name[,tag1=val1,...]  field1=val1,field2=val2,...\n```\n\n1. Measurement Name\n\n2. Tags\n\n3. Fields\n\n4. Timestamp\n\n   ​\n\n### 输入数据格式\n\n[telegraf有六种采集数据输入格式](https://docs.influxdata.com/telegraf/v1.5/concepts/data_formats_input/)\n\nTelegraf is able to parse the following input data formats into metrics:\n\n1. [InfluxDB Line Protocol](https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md#influx)\n2. [JSON](https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md#json)\n3. [Graphite](https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md#graphite)\n4. [Value](https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md#value), ie: 45 or \"booyah\"\n5. [Nagios](https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md#nagios) (exec input only)\n6. [Collectd](https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md#collectd)\n\n默认是第一种：InfluxDB Line Protocol。Graphite、Nagios、Collected都是其他开源监控项目，其中Nagios是比较完整的监控系统，其格式只能用在exec输入插件中；Collected一般用作系统守护进程，采集系统统计信息，并且通过网络开放数据；Graphite则是另外一套监控系统，也是C-S模式。\n\n如果是配置成其他格式，那么采集回来的数据（受限于采集方可吐出的数据），将会被Telegraf  转换为其可识别的InfluxDB Line Protocol。\n\n\n\n### 输出数据格式\n\n[telegraf有三种采集数据输出格式](https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md)\n\nTelegraf is able to serialize metrics into the following output data formats:\n\n1. [InfluxDB Line Protocol](https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md#influx)\n2. [JSON](https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md#json)\n3. [Graphite](https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md#graphite)\n\n一般而言，输出格式如果是其他格式，例如Json，那么输出的对象则一般为file类型。从实际应用中来看，即通过telegraf采集回数据，生成json文件，发送至其他系统进行处理。\n\n以下是采集结果示例\n\n```shell\n* Plugin: inputs.cpu, Collection 1\n* Plugin: inputs.cpu, Collection 2\n> cpu,cpu=cpu0,host=0336dcb23579 usage_system=0,usage_idle=100,usage_nice=0,usage_iowait=0,usage_irq=0,usage_guest=0,usage_user=0,usage_softirq=0,usage_steal=0,usage_guest_nice=0 1515338025000000000\n> cpu,cpu=cpu1,host=0336dcb23579 usage_user=0,usage_guest_nice=0,usage_steal=0,usage_guest=0,usage_system=0,usage_idle=100,usage_nice=0,usage_iowait=0,usage_irq=0,usage_softirq=0 1515338025000000000\n> cpu,cpu=cpu2,host=0336dcb23579 usage_idle=100,usage_nice=0,usage_steal=0,usage_guest=0,usage_user=0,usage_system=0,usage_iowait=0,usage_irq=0,usage_softirq=0,usage_guest_nice=0 1515338025000000000\n> cpu,host=0336dcb23579,cpu=cpu3 usage_softirq=0,usage_guest_nice=0,usage_idle=100,usage_nice=0,usage_iowait=0,usage_irq=0,usage_steal=0,usage_guest=0,usage_user=0,usage_system=0 1515338025000000000\n> cpu,cpu=cpu-total,host=0336dcb23579 usage_guest_nice=0,usage_idle=100,usage_nice=0,usage_irq=0,usage_steal=0,usage_guest=0,usage_user=0,usage_system=0,usage_iowait=0,usage_softirq=0 1515338025000000000\n\n```\n\ncpu：measurements ，类似oracle的表名\n\ncpu=cpu-total,host=0336dcb23579 ： cpu、host为tag，索引；\n\nusage_guest_nice=0,usage_idle=100,usage_nice=0... : usage_guest_nice等为field，即被采集的指标字段。field在measurements是不可或缺的，tag可无， tag与field用空格隔开，field放置在后面\n\n1515338025000000000： time 时间戳\n\n\n\n### 数据过滤/聚合\n\nMeasurement Filtering\n\n![数据处理](/img/agg-pro.jpg)\n\nFilters can be configured per input, output, processor, or aggregator, see below for examples.\n\n- **namepass**: An array of glob pattern strings. Only points whose measurement name matches a pattern in this list are emitted.\n- **namedrop**: The inverse of `namepass`. If a match is found the point is discarded. This is tested on points after they have passed the `namepass` test.\n- **fieldpass**: An array of glob pattern strings. Only fields whose field key matches a pattern in this list are emitted. Not available for outputs.\n- **fielddrop**: The inverse of `fieldpass`. Fields with a field key matching one of the patterns will be discarded from the point. This is tested on points after they have passed the `fieldpass` test. Not available for outputs.\n- **tagpass**: A table mapping tag keys to arrays of glob pattern strings. Only points that contain a tag key in the table and a tag value matching one of its patterns is emitted.\n- **tagdrop**: The inverse of `tagpass`. If a match is found the point is discarded. This is tested on points after they have passed the `tagpass` test.\n- **taginclude**: An array of glob pattern strings. Only tags with a tag key matching one of the patterns are emitted. In contrast to `tagpass`, which will pass an entire point based on its tag, `taginclude` removes all non matching tags from the point. This filter can be used on both inputs & outputs, but it is *recommended* to be used on inputs, as it is more efficient to filter out tags at the ingestion point.\n- **tagexclude**: The inverse of `taginclude`. Tags with a tag key matching one of the patterns will be discarded from the point.\n\n输入数据示例\n\n```shell\n[[inputs.cpu]]\n  percpu = true\n  totalcpu = false\n  fielddrop = [\"cpu_time\"]\n  # Don't collect CPU data for cpu6 & cpu7\n  [inputs.cpu.tagdrop]\n    cpu = [ \"cpu6\", \"cpu7\" ]\n\n[[inputs.disk]]\n  [inputs.disk.tagpass]\n    # tagpass conditions are OR, not AND.\n    # If the (filesystem is ext4 or xfs) OR (the path is /opt or /home)\n    # then the metric passes\n    fstype = [ \"ext4\", \"xfs\" ]\n    # Globs can also be used on the tag values\n    path = [ \"/opt\", \"/home*\" ]\n```\n\n输出数据示例\n\n与输入（采集）数据过滤类似，可定义多个输出源\n\n```shell\n[[outputs.influxdb]]\n  urls = [ \"http://localhost:8086\" ]\n  database = \"telegraf\"\n  # Drop all measurements that start with \"aerospike\"\n  namedrop = [\"aerospike*\"]\n\n[[outputs.influxdb]]\n  urls = [ \"http://localhost:8086\" ]\n  database = \"telegraf-aerospike-data\"\n  # Only accept aerospike data:\n  namepass = [\"aerospike*\"]\n\n[[outputs.influxdb]]\n  urls = [ \"http://localhost:8086\" ]\n  database = \"telegraf-cpu0-data\"\n  # Only store measurements where the tag \"cpu\" matches the value \"cpu0\"\n  [outputs.influxdb.tagpass]\n    cpu = [\"cpu0\"]\n```\n\nAggregator Plugins\n\n- [basicstats](https://github.com/influxdata/telegraf/blob/master/plugins/aggregators/basicstats)\n- [minmax](https://github.com/influxdata/telegraf/blob/master/plugins/aggregators/minmax)\n- [histogram](https://github.com/influxdata/telegraf/blob/master/plugins/aggregators/histogram)\n\n### 部署方式\n\n客户端部署方式，可结合Ansible 进行部署，以及配置管理。官方说明并未看到。\n\n\n\n### 任务配置\n\n客户端配置文件配置 /etc/telegraf/telegraf.conf，可配置指标采集周期、过滤指标等。\n\n\n\n### 告警处理\n\n与Telegraf配合使用的告警处理程序是[kapacitor](https://github.com/influxdata/kapacitor) \n\nOpen source framework for processing, monitoring, and alerting on time series data。","slug":"tick","published":1,"updated":"2018-10-20T08:15:43.515Z","_id":"cjhajczp90016e4rwbgwkji48","comments":1,"layout":"post","photos":[],"link":"","content":"<h2 id=\"1-Tick安装部署\"><a href=\"#1-Tick安装部署\" class=\"headerlink\" title=\"1.Tick安装部署\"></a>1.Tick安装部署</h2><h3 id=\"Tick简介\"><a href=\"#Tick简介\" class=\"headerlink\" title=\"Tick简介\"></a>Tick简介</h3><p>TICK 是由 InfluxData 开发的一套运维工具栈，由 Telegraf, InfluxDB, Chronograf, Kapacitor 四个工具的首字母组成。<br>这一套组件将收集数据和入库、数据库存储、展示、告警四者囊括了。</p>\n<p><img src=\"/img/tick.png\" alt=\"TICK框架图\" title=\"TICK\"> </p>\n<a id=\"more\"></a>\n<p>Telegraf</p>\n<p>是一个数据收集和入库的工具。提供了很多 input 和 output 插件，比如收集本地的 cpu、load、网络流量等数据，然后写入 InfluxDB 、Kafka或者OpenTSDB等。相当于ELK栈中的 logstash 功能。</p>\n<p>InfluxDB</p>\n<p>InfluxDB 是一个开源的GO语言为基础的数据库, 用来处理时间序列数据,提供了较高的可用性。与opentsdb类似，支持HTTP API方式，写入和读取数据。相当于ELK栈中的elasticsearch功能。</p>\n<p>Chronograf</p>\n<p>从InfluxDB时间序列数据的数据可视化工具，负责从InfluxDB收集数据，并将数据图表以web的形式发布。它使用简单,包括模板和库可以快速构建实时数据的可视化仪表板，轻松地创建报警和自动化的规则。相当于ELK栈中的kibana功能。</p>\n<p>Kapacitor</p>\n<p>Kapacitor是InfluxDB的数据处理引擎，主要作用是时间序列数据处理、监视和警报。</p>\n<h3 id=\"TICK安装（DOCKER版）\"><a href=\"#TICK安装（DOCKER版）\" class=\"headerlink\" title=\"TICK安装（DOCKER版）\"></a>TICK安装（DOCKER版）</h3><p><a href=\"https://portal.influxdata.com/downloads\" target=\"_blank\" rel=\"noopener\">官方安装介绍</a></p>\n<p>本地安装版本分别是telegraf1.5,influxdb1.42,chronograf1.4,kapacitor1.4</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">docker pull telegraf</span><br><span class=\"line\">docker pull influxdb</span><br><span class=\"line\">docker pull quay.io/influxdb/chronograf</span><br><span class=\"line\">docker pull kapacitor</span><br></pre></td></tr></table></figure>\n<h3 id=\"TICK配置与运行\"><a href=\"#TICK配置与运行\" class=\"headerlink\" title=\"TICK配置与运行\"></a>TICK配置与运行</h3><p><a href=\"https://github.com/influxdata/influxdata-docker\" target=\"_blank\" rel=\"noopener\">TICK Docker file</a></p>\n<h4 id=\"InfluxDB\"><a href=\"#InfluxDB\" class=\"headerlink\" title=\"InfluxDB\"></a>InfluxDB</h4><p><a href=\"https://github.com/docker-library/docs/blob/master/influxdb/README.md\" target=\"_blank\" rel=\"noopener\">How to run</a></p>\n<p>参数介绍：</p>\n<ul>\n<li>8086 HTTP API port</li>\n<li>8083 Administrator interface port, if it is enabled</li>\n<li>2003 Graphite support, if it is enabled</li>\n</ul>\n<p>启动server 可修改默认配置文件等，详见[How to run]</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">docker run --name influxdb -d -p 8086:8086 -p 8083:8083  -e INFLUXDB_ADMIN_ENABLED=<span class=\"literal\">true</span>  -v /var/lib/influxdb:/var/lib/influxdb  influxdb</span><br></pre></td></tr></table></figure>\n<p>初始化数据库</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">docker run --rm \\</span><br><span class=\"line\">      -e INFLUXDB_DB=db0 -e INFLUXDB_ADMIN_ENABLED=<span class=\"literal\">true</span> \\</span><br><span class=\"line\">      -e INFLUXDB_ADMIN_USER=admin -e INFLUXDB_ADMIN_USER=admin \\</span><br><span class=\"line\">      -e INFLUXDB_USER=telegraf -e INFLUXDB_USER_PASSWORD=telegraf \\</span><br><span class=\"line\">      -v /var/lib/influxdb:/var/lib/influxdb \\</span><br><span class=\"line\">      influxdb /init-influxdb.sh</span><br></pre></td></tr></table></figure>\n<p>启动client</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">docker run --rm --link=influxdb -it influxdb influx -host influxdb</span><br></pre></td></tr></table></figure>\n<p>其他命令</p>\n<p>创建数据库</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">curl -G http://localhost:8086/query --data-urlencode \"q=CREATE DATABASE mydb\"</span><br></pre></td></tr></table></figure>\n<p>插入数据</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">curl -XPOST \"http://localhost:8086/write?db=mydb\" \\</span><br><span class=\"line\">-d 'cpu,host=server01,region=uswest load=42 1434055562000000000'</span><br><span class=\"line\"></span><br><span class=\"line\">curl -XPOST \"http://localhost:8086/write?db=mydb\" \\</span><br><span class=\"line\">-d 'cpu,host=server02,region=uswest load=78 1434055562000000000'</span><br><span class=\"line\"></span><br><span class=\"line\">curl -XPOST \"http://localhost:8086/write?db=mydb\" \\</span><br><span class=\"line\">-d 'cpu,host=server03,region=useast load=15.4 1434055562000000000'</span><br></pre></td></tr></table></figure>\n<p>查询数据</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">curl -G \"http://localhost:8086/query?pretty=true\" --data-urlencode \"db=mydb\" \\</span><br><span class=\"line\">--data-urlencode \"q=SELECT * FROM cpu WHERE host='server01' AND time &lt; now() - 1d\"</span><br></pre></td></tr></table></figure>\n<p>分析数据</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">curl -G \"http://localhost:8086/query?pretty=true\" --data-urlencode \"db=mydb\" \\</span><br><span class=\"line\">--data-urlencode \"q=SELECT mean(load) FROM cpu WHERE region='uswest'\"</span><br></pre></td></tr></table></figure>\n<p><a href=\"https://docs.influxdata.com/influxdb/v1.4/concepts/key_concepts/#field-key\" target=\"_blank\" rel=\"noopener\">InfluxDB概念介绍</a></p>\n<h4 id=\"Telegraf\"><a href=\"#Telegraf\" class=\"headerlink\" title=\"Telegraf\"></a>Telegraf</h4><p><a href=\"https://github.com/influxdata/influxdata-docker/blob/ae56003075c8f39b6c265294ca8235c2c5235cc6/telegraf/1.5/alpine/Dockerfile\" target=\"_blank\" rel=\"noopener\">Telegraf Docker file</a><br><a href=\"https://github.com/docker-library/docs/blob/master/telegraf/content.md\" target=\"_blank\" rel=\"noopener\">How to run</a></p>\n<p>参数介绍：</p>\n<ul>\n<li>8125 StatsD</li>\n<li>8092 UDP</li>\n<li>8094 TCP</li>\n</ul>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">docker run --name telegraf -d -p 8125:8125 -p 8092:8092 -p 8094:8094  -v ~/telegraf.conf:/etc/telegraf/telegraf.conf telegraf</span><br></pre></td></tr></table></figure>\n<h4 id=\"Chronograf\"><a href=\"#Chronograf\" class=\"headerlink\" title=\"Chronograf\"></a>Chronograf</h4><p>Exposed Ports</p>\n<ul>\n<li>8888TCP – Listen  endpoint</li>\n</ul>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">docker run -d --name chronograf -p 8888:8888 -v /var/lib/chronograf:/var/lib/chronograf quay.io/influxdb/chronograf chronograf</span><br></pre></td></tr></table></figure>\n<h4 id=\"Kapacitor\"><a href=\"#Kapacitor\" class=\"headerlink\" title=\"Kapacitor\"></a>Kapacitor</h4><p>Exposed Ports</p>\n<ul>\n<li>9092 TCP – HTTP API endpoint</li>\n</ul>\n<p>需要修改默认的kapacitor.conf url地址，默认是配置localhost，而容器进程肯定不是localhost.</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">docker run -d --name kapacitor -p 9092:9092 -v /var/lib/kapacitor:/var/lib/kapacitor  -v ~/kapacitor.conf:/etc/kapacitor/kapacitor.conf  kapacitor</span><br></pre></td></tr></table></figure>\n<p>查看帮助</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">docker exec  kapacitor kapacitor help</span><br><span class=\"line\">docker exec  kapacitor kapacitor list tasks </span><br><span class=\"line\">docker exec  kapacitor kapacitor show CPURule</span><br></pre></td></tr></table></figure>\n<p>常见问题：</p>\n<p>容器告警因为ip原因无法，注册到influxdb时，ip是容器id，如下，导致Influxdb 无法识别该地址，性能数据无法发送到kapacitor。因此临时解决方案是修改容器的hostname</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&gt; show SUBSCRIPTIONS</span><br><span class=\"line\">name: telegraf</span><br><span class=\"line\">retention_policy name                                           mode destinations</span><br><span class=\"line\">---------------- ----                                           ---- ------------</span><br><span class=\"line\">autogen          kapacitor-8ca0780c-5208-41d5-91ec-05b28d13b2b8 ANY  [http://c9816b89f375:9092]</span><br><span class=\"line\"></span><br><span class=\"line\">name: _internal</span><br><span class=\"line\">retention_policy name                                           mode destinations</span><br><span class=\"line\">---------------- ----                                           ---- ------------</span><br><span class=\"line\">monitor          kapacitor-8ca0780c-5208-41d5-91ec-05b28d13b2b8 ANY  [http://c9816b89f375:9092]</span><br><span class=\"line\"></span><br><span class=\"line\">name: chronograf</span><br><span class=\"line\">retention_policy name                                           mode destinations</span><br><span class=\"line\">---------------- ----                                           ---- ------------</span><br><span class=\"line\">autogen          kapacitor-8ca0780c-5208-41d5-91ec-05b28d13b2b8 ANY  [http://c9816b89f375:9092]</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">docker run -d --name kapacitor -e KAPACITOR_HOSTNAME=192.168.14.165 -p 9092:9092 -v /var/lib/kapacitor:/var/lib/kapacitor  -v ~/kapacitor.conf:/etc/kapacitor/kapacitor.conf  kapacitor</span><br></pre></td></tr></table></figure>\n<h2 id=\"2-Telgraf重点介绍\"><a href=\"#2-Telgraf重点介绍\" class=\"headerlink\" title=\"2.Telgraf重点介绍\"></a>2.Telgraf重点介绍</h2><p>Telegraf是一个用Go编写的代理程序，用于收集，处理，汇总和编写度量标准。</p>\n<p>Telegraf是插件驱动，并有4个不同的插件的概念：</p>\n<ol>\n<li><a href=\"https://github.com/influxdata/telegraf#input-plugins\" target=\"_blank\" rel=\"noopener\">输入插件</a>从系统，服务或第三方API收集指标。</li>\n<li><a href=\"https://github.com/influxdata/telegraf#processor-plugins\" target=\"_blank\" rel=\"noopener\">处理器插件</a>转换，修饰和/或过滤度量标准</li>\n<li><a href=\"https://github.com/influxdata/telegraf#aggregator-plugins\" target=\"_blank\" rel=\"noopener\">聚合器插件</a>创建聚合度量（例如平均值，最小值，最大值，分位数等）</li>\n<li><a href=\"https://github.com/influxdata/telegraf#output-plugins\" target=\"_blank\" rel=\"noopener\">输出插件</a>将指标写入各个目标</li>\n</ol>\n<p>Telegraf 二进制文件包含对所有支持的采集对象的采集功能，只是可以通过配置选择性的开启。如果更新某个对象的采集指标，则需要重新编译整个二进制文件。</p>\n<h3 id=\"测试数据-帮助命令\"><a href=\"#测试数据-帮助命令\" class=\"headerlink\" title=\"测试数据/帮助命令\"></a>测试数据/帮助命令</h3><p>~/telegraf.conf 测试用配置文件，可定义输入插件</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">docker run -v ~/telegraf.conf:/etc/telegraf/telegraf.conf --rm telegraf telegraf --config /etc/telegraf/telegraf.conf  --test</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">docker run --rm telegraf telegraf --help</span><br></pre></td></tr></table></figure>\n<h3 id=\"数据收集方式\"><a href=\"#数据收集方式\" class=\"headerlink\" title=\"数据收集方式\"></a>数据收集方式</h3><p>push 模式，由telegraf 根据input配置主动采集并发送到output配置的后端。一般是InfluxDB数据库。TICK并无服务端的概念，在TICK监控部署文档的架构图中可知。</p>\n<p><a href=\"https://github.com/influxdata/telegraf/blob/master/README.md\" target=\"_blank\" rel=\"noopener\">input-output-plugins</a></p>\n<h3 id=\"InfluxDB-Line-Protocol-数据格式\"><a href=\"#InfluxDB-Line-Protocol-数据格式\" class=\"headerlink\" title=\"InfluxDB Line Protocol 数据格式\"></a>InfluxDB Line Protocol 数据格式</h3><p>InfluxDB Line Protocol 格式</p>\n<p>Telegraf metrics, like InfluxDB <a href=\"https://docs.influxdata.com/influxdb/v0.10/write_protocols/line/\" target=\"_blank\" rel=\"noopener\">points</a>, are a combination of four basic parts:</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">measurement_name[,tag1=val1,...]  field1=val1,field2=val2,...</span><br></pre></td></tr></table></figure>\n<ol>\n<li><p>Measurement Name</p>\n</li>\n<li><p>Tags</p>\n</li>\n<li><p>Fields</p>\n</li>\n<li><p>Timestamp</p>\n<p>​</p>\n</li>\n</ol>\n<h3 id=\"输入数据格式\"><a href=\"#输入数据格式\" class=\"headerlink\" title=\"输入数据格式\"></a>输入数据格式</h3><p><a href=\"https://docs.influxdata.com/telegraf/v1.5/concepts/data_formats_input/\" target=\"_blank\" rel=\"noopener\">telegraf有六种采集数据输入格式</a></p>\n<p>Telegraf is able to parse the following input data formats into metrics:</p>\n<ol>\n<li><a href=\"https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md#influx\" target=\"_blank\" rel=\"noopener\">InfluxDB Line Protocol</a></li>\n<li><a href=\"https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md#json\" target=\"_blank\" rel=\"noopener\">JSON</a></li>\n<li><a href=\"https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md#graphite\" target=\"_blank\" rel=\"noopener\">Graphite</a></li>\n<li><a href=\"https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md#value\" target=\"_blank\" rel=\"noopener\">Value</a>, ie: 45 or “booyah”</li>\n<li><a href=\"https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md#nagios\" target=\"_blank\" rel=\"noopener\">Nagios</a> (exec input only)</li>\n<li><a href=\"https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md#collectd\" target=\"_blank\" rel=\"noopener\">Collectd</a></li>\n</ol>\n<p>默认是第一种：InfluxDB Line Protocol。Graphite、Nagios、Collected都是其他开源监控项目，其中Nagios是比较完整的监控系统，其格式只能用在exec输入插件中；Collected一般用作系统守护进程，采集系统统计信息，并且通过网络开放数据；Graphite则是另外一套监控系统，也是C-S模式。</p>\n<p>如果是配置成其他格式，那么采集回来的数据（受限于采集方可吐出的数据），将会被Telegraf  转换为其可识别的InfluxDB Line Protocol。</p>\n<h3 id=\"输出数据格式\"><a href=\"#输出数据格式\" class=\"headerlink\" title=\"输出数据格式\"></a>输出数据格式</h3><p><a href=\"https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md\" target=\"_blank\" rel=\"noopener\">telegraf有三种采集数据输出格式</a></p>\n<p>Telegraf is able to serialize metrics into the following output data formats:</p>\n<ol>\n<li><a href=\"https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md#influx\" target=\"_blank\" rel=\"noopener\">InfluxDB Line Protocol</a></li>\n<li><a href=\"https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md#json\" target=\"_blank\" rel=\"noopener\">JSON</a></li>\n<li><a href=\"https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md#graphite\" target=\"_blank\" rel=\"noopener\">Graphite</a></li>\n</ol>\n<p>一般而言，输出格式如果是其他格式，例如Json，那么输出的对象则一般为file类型。从实际应用中来看，即通过telegraf采集回数据，生成json文件，发送至其他系统进行处理。</p>\n<p>以下是采集结果示例</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">* Plugin: inputs.cpu, Collection 1</span><br><span class=\"line\">* Plugin: inputs.cpu, Collection 2</span><br><span class=\"line\"><span class=\"meta\">&gt;</span><span class=\"bash\"> cpu,cpu=cpu0,host=0336dcb23579 usage_system=0,usage_idle=100,usage_nice=0,usage_iowait=0,usage_irq=0,usage_guest=0,usage_user=0,usage_softirq=0,usage_steal=0,usage_guest_nice=0 1515338025000000000</span></span><br><span class=\"line\"><span class=\"meta\">&gt;</span><span class=\"bash\"> cpu,cpu=cpu1,host=0336dcb23579 usage_user=0,usage_guest_nice=0,usage_steal=0,usage_guest=0,usage_system=0,usage_idle=100,usage_nice=0,usage_iowait=0,usage_irq=0,usage_softirq=0 1515338025000000000</span></span><br><span class=\"line\"><span class=\"meta\">&gt;</span><span class=\"bash\"> cpu,cpu=cpu2,host=0336dcb23579 usage_idle=100,usage_nice=0,usage_steal=0,usage_guest=0,usage_user=0,usage_system=0,usage_iowait=0,usage_irq=0,usage_softirq=0,usage_guest_nice=0 1515338025000000000</span></span><br><span class=\"line\"><span class=\"meta\">&gt;</span><span class=\"bash\"> cpu,host=0336dcb23579,cpu=cpu3 usage_softirq=0,usage_guest_nice=0,usage_idle=100,usage_nice=0,usage_iowait=0,usage_irq=0,usage_steal=0,usage_guest=0,usage_user=0,usage_system=0 1515338025000000000</span></span><br><span class=\"line\"><span class=\"meta\">&gt;</span><span class=\"bash\"> cpu,cpu=cpu-total,host=0336dcb23579 usage_guest_nice=0,usage_idle=100,usage_nice=0,usage_irq=0,usage_steal=0,usage_guest=0,usage_user=0,usage_system=0,usage_iowait=0,usage_softirq=0 1515338025000000000</span></span><br></pre></td></tr></table></figure>\n<p>cpu：measurements ，类似oracle的表名</p>\n<p>cpu=cpu-total,host=0336dcb23579 ： cpu、host为tag，索引；</p>\n<p>usage_guest_nice=0,usage_idle=100,usage_nice=0… : usage_guest_nice等为field，即被采集的指标字段。field在measurements是不可或缺的，tag可无， tag与field用空格隔开，field放置在后面</p>\n<p>1515338025000000000： time 时间戳</p>\n<h3 id=\"数据过滤-聚合\"><a href=\"#数据过滤-聚合\" class=\"headerlink\" title=\"数据过滤/聚合\"></a>数据过滤/聚合</h3><p>Measurement Filtering</p>\n<p><img src=\"/img/agg-pro.jpg\" alt=\"数据处理\"></p>\n<p>Filters can be configured per input, output, processor, or aggregator, see below for examples.</p>\n<ul>\n<li><strong>namepass</strong>: An array of glob pattern strings. Only points whose measurement name matches a pattern in this list are emitted.</li>\n<li><strong>namedrop</strong>: The inverse of <code>namepass</code>. If a match is found the point is discarded. This is tested on points after they have passed the <code>namepass</code> test.</li>\n<li><strong>fieldpass</strong>: An array of glob pattern strings. Only fields whose field key matches a pattern in this list are emitted. Not available for outputs.</li>\n<li><strong>fielddrop</strong>: The inverse of <code>fieldpass</code>. Fields with a field key matching one of the patterns will be discarded from the point. This is tested on points after they have passed the <code>fieldpass</code> test. Not available for outputs.</li>\n<li><strong>tagpass</strong>: A table mapping tag keys to arrays of glob pattern strings. Only points that contain a tag key in the table and a tag value matching one of its patterns is emitted.</li>\n<li><strong>tagdrop</strong>: The inverse of <code>tagpass</code>. If a match is found the point is discarded. This is tested on points after they have passed the <code>tagpass</code> test.</li>\n<li><strong>taginclude</strong>: An array of glob pattern strings. Only tags with a tag key matching one of the patterns are emitted. In contrast to <code>tagpass</code>, which will pass an entire point based on its tag, <code>taginclude</code> removes all non matching tags from the point. This filter can be used on both inputs &amp; outputs, but it is <em>recommended</em> to be used on inputs, as it is more efficient to filter out tags at the ingestion point.</li>\n<li><strong>tagexclude</strong>: The inverse of <code>taginclude</code>. Tags with a tag key matching one of the patterns will be discarded from the point.</li>\n</ul>\n<p>输入数据示例</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[[inputs.cpu]]</span><br><span class=\"line\">  percpu = true</span><br><span class=\"line\">  totalcpu = false</span><br><span class=\"line\">  fielddrop = [\"cpu_time\"]</span><br><span class=\"line\"><span class=\"meta\">  #</span><span class=\"bash\"> Don<span class=\"string\">'t collect CPU data for cpu6 &amp; cpu7</span></span></span><br><span class=\"line\">  [inputs.cpu.tagdrop]</span><br><span class=\"line\">    cpu = [ \"cpu6\", \"cpu7\" ]</span><br><span class=\"line\"></span><br><span class=\"line\">[[inputs.disk]]</span><br><span class=\"line\">  [inputs.disk.tagpass]</span><br><span class=\"line\">    # tagpass conditions are OR, not AND.</span><br><span class=\"line\">    # If the (filesystem is ext4 or xfs) OR (the path is /opt or /home)</span><br><span class=\"line\">    # then the metric passes</span><br><span class=\"line\">    fstype = [ \"ext4\", \"xfs\" ]</span><br><span class=\"line\">    # Globs can also be used on the tag values</span><br><span class=\"line\">    path = [ \"/opt\", \"/home*\" ]</span><br></pre></td></tr></table></figure>\n<p>输出数据示例</p>\n<p>与输入（采集）数据过滤类似，可定义多个输出源</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[[outputs.influxdb]]</span><br><span class=\"line\">  urls = [ \"http://localhost:8086\" ]</span><br><span class=\"line\">  database = \"telegraf\"</span><br><span class=\"line\"><span class=\"meta\">  #</span><span class=\"bash\"> Drop all measurements that start with <span class=\"string\">\"aerospike\"</span></span></span><br><span class=\"line\">  namedrop = [\"aerospike*\"]</span><br><span class=\"line\"></span><br><span class=\"line\">[[outputs.influxdb]]</span><br><span class=\"line\">  urls = [ \"http://localhost:8086\" ]</span><br><span class=\"line\">  database = \"telegraf-aerospike-data\"</span><br><span class=\"line\"><span class=\"meta\">  #</span><span class=\"bash\"> Only accept aerospike data:</span></span><br><span class=\"line\">  namepass = [\"aerospike*\"]</span><br><span class=\"line\"></span><br><span class=\"line\">[[outputs.influxdb]]</span><br><span class=\"line\">  urls = [ \"http://localhost:8086\" ]</span><br><span class=\"line\">  database = \"telegraf-cpu0-data\"</span><br><span class=\"line\"><span class=\"meta\">  #</span><span class=\"bash\"> Only store measurements <span class=\"built_in\">where</span> the tag <span class=\"string\">\"cpu\"</span> matches the value <span class=\"string\">\"cpu0\"</span></span></span><br><span class=\"line\">  [outputs.influxdb.tagpass]</span><br><span class=\"line\">    cpu = [\"cpu0\"]</span><br></pre></td></tr></table></figure>\n<p>Aggregator Plugins</p>\n<ul>\n<li><a href=\"https://github.com/influxdata/telegraf/blob/master/plugins/aggregators/basicstats\" target=\"_blank\" rel=\"noopener\">basicstats</a></li>\n<li><a href=\"https://github.com/influxdata/telegraf/blob/master/plugins/aggregators/minmax\" target=\"_blank\" rel=\"noopener\">minmax</a></li>\n<li><a href=\"https://github.com/influxdata/telegraf/blob/master/plugins/aggregators/histogram\" target=\"_blank\" rel=\"noopener\">histogram</a></li>\n</ul>\n<h3 id=\"部署方式\"><a href=\"#部署方式\" class=\"headerlink\" title=\"部署方式\"></a>部署方式</h3><p>客户端部署方式，可结合Ansible 进行部署，以及配置管理。官方说明并未看到。</p>\n<h3 id=\"任务配置\"><a href=\"#任务配置\" class=\"headerlink\" title=\"任务配置\"></a>任务配置</h3><p>客户端配置文件配置 /etc/telegraf/telegraf.conf，可配置指标采集周期、过滤指标等。</p>\n<h3 id=\"告警处理\"><a href=\"#告警处理\" class=\"headerlink\" title=\"告警处理\"></a>告警处理</h3><p>与Telegraf配合使用的告警处理程序是<a href=\"https://github.com/influxdata/kapacitor\" target=\"_blank\" rel=\"noopener\">kapacitor</a> </p>\n<p>Open source framework for processing, monitoring, and alerting on time series data。</p>\n","site":{"data":{}},"excerpt":"<h2 id=\"1-Tick安装部署\"><a href=\"#1-Tick安装部署\" class=\"headerlink\" title=\"1.Tick安装部署\"></a>1.Tick安装部署</h2><h3 id=\"Tick简介\"><a href=\"#Tick简介\" class=\"headerlink\" title=\"Tick简介\"></a>Tick简介</h3><p>TICK 是由 InfluxData 开发的一套运维工具栈，由 Telegraf, InfluxDB, Chronograf, Kapacitor 四个工具的首字母组成。<br>这一套组件将收集数据和入库、数据库存储、展示、告警四者囊括了。</p>\n<p><img src=\"/img/tick.png\" alt=\"TICK框架图\" title=\"TICK\"> </p>","more":"<p>Telegraf</p>\n<p>是一个数据收集和入库的工具。提供了很多 input 和 output 插件，比如收集本地的 cpu、load、网络流量等数据，然后写入 InfluxDB 、Kafka或者OpenTSDB等。相当于ELK栈中的 logstash 功能。</p>\n<p>InfluxDB</p>\n<p>InfluxDB 是一个开源的GO语言为基础的数据库, 用来处理时间序列数据,提供了较高的可用性。与opentsdb类似，支持HTTP API方式，写入和读取数据。相当于ELK栈中的elasticsearch功能。</p>\n<p>Chronograf</p>\n<p>从InfluxDB时间序列数据的数据可视化工具，负责从InfluxDB收集数据，并将数据图表以web的形式发布。它使用简单,包括模板和库可以快速构建实时数据的可视化仪表板，轻松地创建报警和自动化的规则。相当于ELK栈中的kibana功能。</p>\n<p>Kapacitor</p>\n<p>Kapacitor是InfluxDB的数据处理引擎，主要作用是时间序列数据处理、监视和警报。</p>\n<h3 id=\"TICK安装（DOCKER版）\"><a href=\"#TICK安装（DOCKER版）\" class=\"headerlink\" title=\"TICK安装（DOCKER版）\"></a>TICK安装（DOCKER版）</h3><p><a href=\"https://portal.influxdata.com/downloads\" target=\"_blank\" rel=\"noopener\">官方安装介绍</a></p>\n<p>本地安装版本分别是telegraf1.5,influxdb1.42,chronograf1.4,kapacitor1.4</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">docker pull telegraf</span><br><span class=\"line\">docker pull influxdb</span><br><span class=\"line\">docker pull quay.io/influxdb/chronograf</span><br><span class=\"line\">docker pull kapacitor</span><br></pre></td></tr></table></figure>\n<h3 id=\"TICK配置与运行\"><a href=\"#TICK配置与运行\" class=\"headerlink\" title=\"TICK配置与运行\"></a>TICK配置与运行</h3><p><a href=\"https://github.com/influxdata/influxdata-docker\" target=\"_blank\" rel=\"noopener\">TICK Docker file</a></p>\n<h4 id=\"InfluxDB\"><a href=\"#InfluxDB\" class=\"headerlink\" title=\"InfluxDB\"></a>InfluxDB</h4><p><a href=\"https://github.com/docker-library/docs/blob/master/influxdb/README.md\" target=\"_blank\" rel=\"noopener\">How to run</a></p>\n<p>参数介绍：</p>\n<ul>\n<li>8086 HTTP API port</li>\n<li>8083 Administrator interface port, if it is enabled</li>\n<li>2003 Graphite support, if it is enabled</li>\n</ul>\n<p>启动server 可修改默认配置文件等，详见[How to run]</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">docker run --name influxdb -d -p 8086:8086 -p 8083:8083  -e INFLUXDB_ADMIN_ENABLED=<span class=\"literal\">true</span>  -v /var/lib/influxdb:/var/lib/influxdb  influxdb</span><br></pre></td></tr></table></figure>\n<p>初始化数据库</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">docker run --rm \\</span><br><span class=\"line\">      -e INFLUXDB_DB=db0 -e INFLUXDB_ADMIN_ENABLED=<span class=\"literal\">true</span> \\</span><br><span class=\"line\">      -e INFLUXDB_ADMIN_USER=admin -e INFLUXDB_ADMIN_USER=admin \\</span><br><span class=\"line\">      -e INFLUXDB_USER=telegraf -e INFLUXDB_USER_PASSWORD=telegraf \\</span><br><span class=\"line\">      -v /var/lib/influxdb:/var/lib/influxdb \\</span><br><span class=\"line\">      influxdb /init-influxdb.sh</span><br></pre></td></tr></table></figure>\n<p>启动client</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">docker run --rm --link=influxdb -it influxdb influx -host influxdb</span><br></pre></td></tr></table></figure>\n<p>其他命令</p>\n<p>创建数据库</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">curl -G http://localhost:8086/query --data-urlencode \"q=CREATE DATABASE mydb\"</span><br></pre></td></tr></table></figure>\n<p>插入数据</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">curl -XPOST \"http://localhost:8086/write?db=mydb\" \\</span><br><span class=\"line\">-d 'cpu,host=server01,region=uswest load=42 1434055562000000000'</span><br><span class=\"line\"></span><br><span class=\"line\">curl -XPOST \"http://localhost:8086/write?db=mydb\" \\</span><br><span class=\"line\">-d 'cpu,host=server02,region=uswest load=78 1434055562000000000'</span><br><span class=\"line\"></span><br><span class=\"line\">curl -XPOST \"http://localhost:8086/write?db=mydb\" \\</span><br><span class=\"line\">-d 'cpu,host=server03,region=useast load=15.4 1434055562000000000'</span><br></pre></td></tr></table></figure>\n<p>查询数据</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">curl -G \"http://localhost:8086/query?pretty=true\" --data-urlencode \"db=mydb\" \\</span><br><span class=\"line\">--data-urlencode \"q=SELECT * FROM cpu WHERE host='server01' AND time &lt; now() - 1d\"</span><br></pre></td></tr></table></figure>\n<p>分析数据</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">curl -G \"http://localhost:8086/query?pretty=true\" --data-urlencode \"db=mydb\" \\</span><br><span class=\"line\">--data-urlencode \"q=SELECT mean(load) FROM cpu WHERE region='uswest'\"</span><br></pre></td></tr></table></figure>\n<p><a href=\"https://docs.influxdata.com/influxdb/v1.4/concepts/key_concepts/#field-key\" target=\"_blank\" rel=\"noopener\">InfluxDB概念介绍</a></p>\n<h4 id=\"Telegraf\"><a href=\"#Telegraf\" class=\"headerlink\" title=\"Telegraf\"></a>Telegraf</h4><p><a href=\"https://github.com/influxdata/influxdata-docker/blob/ae56003075c8f39b6c265294ca8235c2c5235cc6/telegraf/1.5/alpine/Dockerfile\" target=\"_blank\" rel=\"noopener\">Telegraf Docker file</a><br><a href=\"https://github.com/docker-library/docs/blob/master/telegraf/content.md\" target=\"_blank\" rel=\"noopener\">How to run</a></p>\n<p>参数介绍：</p>\n<ul>\n<li>8125 StatsD</li>\n<li>8092 UDP</li>\n<li>8094 TCP</li>\n</ul>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">docker run --name telegraf -d -p 8125:8125 -p 8092:8092 -p 8094:8094  -v ~/telegraf.conf:/etc/telegraf/telegraf.conf telegraf</span><br></pre></td></tr></table></figure>\n<h4 id=\"Chronograf\"><a href=\"#Chronograf\" class=\"headerlink\" title=\"Chronograf\"></a>Chronograf</h4><p>Exposed Ports</p>\n<ul>\n<li>8888TCP – Listen  endpoint</li>\n</ul>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">docker run -d --name chronograf -p 8888:8888 -v /var/lib/chronograf:/var/lib/chronograf quay.io/influxdb/chronograf chronograf</span><br></pre></td></tr></table></figure>\n<h4 id=\"Kapacitor\"><a href=\"#Kapacitor\" class=\"headerlink\" title=\"Kapacitor\"></a>Kapacitor</h4><p>Exposed Ports</p>\n<ul>\n<li>9092 TCP – HTTP API endpoint</li>\n</ul>\n<p>需要修改默认的kapacitor.conf url地址，默认是配置localhost，而容器进程肯定不是localhost.</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">docker run -d --name kapacitor -p 9092:9092 -v /var/lib/kapacitor:/var/lib/kapacitor  -v ~/kapacitor.conf:/etc/kapacitor/kapacitor.conf  kapacitor</span><br></pre></td></tr></table></figure>\n<p>查看帮助</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">docker exec  kapacitor kapacitor help</span><br><span class=\"line\">docker exec  kapacitor kapacitor list tasks </span><br><span class=\"line\">docker exec  kapacitor kapacitor show CPURule</span><br></pre></td></tr></table></figure>\n<p>常见问题：</p>\n<p>容器告警因为ip原因无法，注册到influxdb时，ip是容器id，如下，导致Influxdb 无法识别该地址，性能数据无法发送到kapacitor。因此临时解决方案是修改容器的hostname</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&gt; show SUBSCRIPTIONS</span><br><span class=\"line\">name: telegraf</span><br><span class=\"line\">retention_policy name                                           mode destinations</span><br><span class=\"line\">---------------- ----                                           ---- ------------</span><br><span class=\"line\">autogen          kapacitor-8ca0780c-5208-41d5-91ec-05b28d13b2b8 ANY  [http://c9816b89f375:9092]</span><br><span class=\"line\"></span><br><span class=\"line\">name: _internal</span><br><span class=\"line\">retention_policy name                                           mode destinations</span><br><span class=\"line\">---------------- ----                                           ---- ------------</span><br><span class=\"line\">monitor          kapacitor-8ca0780c-5208-41d5-91ec-05b28d13b2b8 ANY  [http://c9816b89f375:9092]</span><br><span class=\"line\"></span><br><span class=\"line\">name: chronograf</span><br><span class=\"line\">retention_policy name                                           mode destinations</span><br><span class=\"line\">---------------- ----                                           ---- ------------</span><br><span class=\"line\">autogen          kapacitor-8ca0780c-5208-41d5-91ec-05b28d13b2b8 ANY  [http://c9816b89f375:9092]</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">docker run -d --name kapacitor -e KAPACITOR_HOSTNAME=192.168.14.165 -p 9092:9092 -v /var/lib/kapacitor:/var/lib/kapacitor  -v ~/kapacitor.conf:/etc/kapacitor/kapacitor.conf  kapacitor</span><br></pre></td></tr></table></figure>\n<h2 id=\"2-Telgraf重点介绍\"><a href=\"#2-Telgraf重点介绍\" class=\"headerlink\" title=\"2.Telgraf重点介绍\"></a>2.Telgraf重点介绍</h2><p>Telegraf是一个用Go编写的代理程序，用于收集，处理，汇总和编写度量标准。</p>\n<p>Telegraf是插件驱动，并有4个不同的插件的概念：</p>\n<ol>\n<li><a href=\"https://github.com/influxdata/telegraf#input-plugins\" target=\"_blank\" rel=\"noopener\">输入插件</a>从系统，服务或第三方API收集指标。</li>\n<li><a href=\"https://github.com/influxdata/telegraf#processor-plugins\" target=\"_blank\" rel=\"noopener\">处理器插件</a>转换，修饰和/或过滤度量标准</li>\n<li><a href=\"https://github.com/influxdata/telegraf#aggregator-plugins\" target=\"_blank\" rel=\"noopener\">聚合器插件</a>创建聚合度量（例如平均值，最小值，最大值，分位数等）</li>\n<li><a href=\"https://github.com/influxdata/telegraf#output-plugins\" target=\"_blank\" rel=\"noopener\">输出插件</a>将指标写入各个目标</li>\n</ol>\n<p>Telegraf 二进制文件包含对所有支持的采集对象的采集功能，只是可以通过配置选择性的开启。如果更新某个对象的采集指标，则需要重新编译整个二进制文件。</p>\n<h3 id=\"测试数据-帮助命令\"><a href=\"#测试数据-帮助命令\" class=\"headerlink\" title=\"测试数据/帮助命令\"></a>测试数据/帮助命令</h3><p>~/telegraf.conf 测试用配置文件，可定义输入插件</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">docker run -v ~/telegraf.conf:/etc/telegraf/telegraf.conf --rm telegraf telegraf --config /etc/telegraf/telegraf.conf  --test</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">docker run --rm telegraf telegraf --help</span><br></pre></td></tr></table></figure>\n<h3 id=\"数据收集方式\"><a href=\"#数据收集方式\" class=\"headerlink\" title=\"数据收集方式\"></a>数据收集方式</h3><p>push 模式，由telegraf 根据input配置主动采集并发送到output配置的后端。一般是InfluxDB数据库。TICK并无服务端的概念，在TICK监控部署文档的架构图中可知。</p>\n<p><a href=\"https://github.com/influxdata/telegraf/blob/master/README.md\" target=\"_blank\" rel=\"noopener\">input-output-plugins</a></p>\n<h3 id=\"InfluxDB-Line-Protocol-数据格式\"><a href=\"#InfluxDB-Line-Protocol-数据格式\" class=\"headerlink\" title=\"InfluxDB Line Protocol 数据格式\"></a>InfluxDB Line Protocol 数据格式</h3><p>InfluxDB Line Protocol 格式</p>\n<p>Telegraf metrics, like InfluxDB <a href=\"https://docs.influxdata.com/influxdb/v0.10/write_protocols/line/\" target=\"_blank\" rel=\"noopener\">points</a>, are a combination of four basic parts:</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">measurement_name[,tag1=val1,...]  field1=val1,field2=val2,...</span><br></pre></td></tr></table></figure>\n<ol>\n<li><p>Measurement Name</p>\n</li>\n<li><p>Tags</p>\n</li>\n<li><p>Fields</p>\n</li>\n<li><p>Timestamp</p>\n<p>​</p>\n</li>\n</ol>\n<h3 id=\"输入数据格式\"><a href=\"#输入数据格式\" class=\"headerlink\" title=\"输入数据格式\"></a>输入数据格式</h3><p><a href=\"https://docs.influxdata.com/telegraf/v1.5/concepts/data_formats_input/\" target=\"_blank\" rel=\"noopener\">telegraf有六种采集数据输入格式</a></p>\n<p>Telegraf is able to parse the following input data formats into metrics:</p>\n<ol>\n<li><a href=\"https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md#influx\" target=\"_blank\" rel=\"noopener\">InfluxDB Line Protocol</a></li>\n<li><a href=\"https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md#json\" target=\"_blank\" rel=\"noopener\">JSON</a></li>\n<li><a href=\"https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md#graphite\" target=\"_blank\" rel=\"noopener\">Graphite</a></li>\n<li><a href=\"https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md#value\" target=\"_blank\" rel=\"noopener\">Value</a>, ie: 45 or “booyah”</li>\n<li><a href=\"https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md#nagios\" target=\"_blank\" rel=\"noopener\">Nagios</a> (exec input only)</li>\n<li><a href=\"https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md#collectd\" target=\"_blank\" rel=\"noopener\">Collectd</a></li>\n</ol>\n<p>默认是第一种：InfluxDB Line Protocol。Graphite、Nagios、Collected都是其他开源监控项目，其中Nagios是比较完整的监控系统，其格式只能用在exec输入插件中；Collected一般用作系统守护进程，采集系统统计信息，并且通过网络开放数据；Graphite则是另外一套监控系统，也是C-S模式。</p>\n<p>如果是配置成其他格式，那么采集回来的数据（受限于采集方可吐出的数据），将会被Telegraf  转换为其可识别的InfluxDB Line Protocol。</p>\n<h3 id=\"输出数据格式\"><a href=\"#输出数据格式\" class=\"headerlink\" title=\"输出数据格式\"></a>输出数据格式</h3><p><a href=\"https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md\" target=\"_blank\" rel=\"noopener\">telegraf有三种采集数据输出格式</a></p>\n<p>Telegraf is able to serialize metrics into the following output data formats:</p>\n<ol>\n<li><a href=\"https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md#influx\" target=\"_blank\" rel=\"noopener\">InfluxDB Line Protocol</a></li>\n<li><a href=\"https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md#json\" target=\"_blank\" rel=\"noopener\">JSON</a></li>\n<li><a href=\"https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md#graphite\" target=\"_blank\" rel=\"noopener\">Graphite</a></li>\n</ol>\n<p>一般而言，输出格式如果是其他格式，例如Json，那么输出的对象则一般为file类型。从实际应用中来看，即通过telegraf采集回数据，生成json文件，发送至其他系统进行处理。</p>\n<p>以下是采集结果示例</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">* Plugin: inputs.cpu, Collection 1</span><br><span class=\"line\">* Plugin: inputs.cpu, Collection 2</span><br><span class=\"line\"><span class=\"meta\">&gt;</span><span class=\"bash\"> cpu,cpu=cpu0,host=0336dcb23579 usage_system=0,usage_idle=100,usage_nice=0,usage_iowait=0,usage_irq=0,usage_guest=0,usage_user=0,usage_softirq=0,usage_steal=0,usage_guest_nice=0 1515338025000000000</span></span><br><span class=\"line\"><span class=\"meta\">&gt;</span><span class=\"bash\"> cpu,cpu=cpu1,host=0336dcb23579 usage_user=0,usage_guest_nice=0,usage_steal=0,usage_guest=0,usage_system=0,usage_idle=100,usage_nice=0,usage_iowait=0,usage_irq=0,usage_softirq=0 1515338025000000000</span></span><br><span class=\"line\"><span class=\"meta\">&gt;</span><span class=\"bash\"> cpu,cpu=cpu2,host=0336dcb23579 usage_idle=100,usage_nice=0,usage_steal=0,usage_guest=0,usage_user=0,usage_system=0,usage_iowait=0,usage_irq=0,usage_softirq=0,usage_guest_nice=0 1515338025000000000</span></span><br><span class=\"line\"><span class=\"meta\">&gt;</span><span class=\"bash\"> cpu,host=0336dcb23579,cpu=cpu3 usage_softirq=0,usage_guest_nice=0,usage_idle=100,usage_nice=0,usage_iowait=0,usage_irq=0,usage_steal=0,usage_guest=0,usage_user=0,usage_system=0 1515338025000000000</span></span><br><span class=\"line\"><span class=\"meta\">&gt;</span><span class=\"bash\"> cpu,cpu=cpu-total,host=0336dcb23579 usage_guest_nice=0,usage_idle=100,usage_nice=0,usage_irq=0,usage_steal=0,usage_guest=0,usage_user=0,usage_system=0,usage_iowait=0,usage_softirq=0 1515338025000000000</span></span><br></pre></td></tr></table></figure>\n<p>cpu：measurements ，类似oracle的表名</p>\n<p>cpu=cpu-total,host=0336dcb23579 ： cpu、host为tag，索引；</p>\n<p>usage_guest_nice=0,usage_idle=100,usage_nice=0… : usage_guest_nice等为field，即被采集的指标字段。field在measurements是不可或缺的，tag可无， tag与field用空格隔开，field放置在后面</p>\n<p>1515338025000000000： time 时间戳</p>\n<h3 id=\"数据过滤-聚合\"><a href=\"#数据过滤-聚合\" class=\"headerlink\" title=\"数据过滤/聚合\"></a>数据过滤/聚合</h3><p>Measurement Filtering</p>\n<p><img src=\"/img/agg-pro.jpg\" alt=\"数据处理\"></p>\n<p>Filters can be configured per input, output, processor, or aggregator, see below for examples.</p>\n<ul>\n<li><strong>namepass</strong>: An array of glob pattern strings. Only points whose measurement name matches a pattern in this list are emitted.</li>\n<li><strong>namedrop</strong>: The inverse of <code>namepass</code>. If a match is found the point is discarded. This is tested on points after they have passed the <code>namepass</code> test.</li>\n<li><strong>fieldpass</strong>: An array of glob pattern strings. Only fields whose field key matches a pattern in this list are emitted. Not available for outputs.</li>\n<li><strong>fielddrop</strong>: The inverse of <code>fieldpass</code>. Fields with a field key matching one of the patterns will be discarded from the point. This is tested on points after they have passed the <code>fieldpass</code> test. Not available for outputs.</li>\n<li><strong>tagpass</strong>: A table mapping tag keys to arrays of glob pattern strings. Only points that contain a tag key in the table and a tag value matching one of its patterns is emitted.</li>\n<li><strong>tagdrop</strong>: The inverse of <code>tagpass</code>. If a match is found the point is discarded. This is tested on points after they have passed the <code>tagpass</code> test.</li>\n<li><strong>taginclude</strong>: An array of glob pattern strings. Only tags with a tag key matching one of the patterns are emitted. In contrast to <code>tagpass</code>, which will pass an entire point based on its tag, <code>taginclude</code> removes all non matching tags from the point. This filter can be used on both inputs &amp; outputs, but it is <em>recommended</em> to be used on inputs, as it is more efficient to filter out tags at the ingestion point.</li>\n<li><strong>tagexclude</strong>: The inverse of <code>taginclude</code>. Tags with a tag key matching one of the patterns will be discarded from the point.</li>\n</ul>\n<p>输入数据示例</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[[inputs.cpu]]</span><br><span class=\"line\">  percpu = true</span><br><span class=\"line\">  totalcpu = false</span><br><span class=\"line\">  fielddrop = [\"cpu_time\"]</span><br><span class=\"line\"><span class=\"meta\">  #</span><span class=\"bash\"> Don<span class=\"string\">'t collect CPU data for cpu6 &amp; cpu7</span></span></span><br><span class=\"line\">  [inputs.cpu.tagdrop]</span><br><span class=\"line\">    cpu = [ \"cpu6\", \"cpu7\" ]</span><br><span class=\"line\"></span><br><span class=\"line\">[[inputs.disk]]</span><br><span class=\"line\">  [inputs.disk.tagpass]</span><br><span class=\"line\">    # tagpass conditions are OR, not AND.</span><br><span class=\"line\">    # If the (filesystem is ext4 or xfs) OR (the path is /opt or /home)</span><br><span class=\"line\">    # then the metric passes</span><br><span class=\"line\">    fstype = [ \"ext4\", \"xfs\" ]</span><br><span class=\"line\">    # Globs can also be used on the tag values</span><br><span class=\"line\">    path = [ \"/opt\", \"/home*\" ]</span><br></pre></td></tr></table></figure>\n<p>输出数据示例</p>\n<p>与输入（采集）数据过滤类似，可定义多个输出源</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[[outputs.influxdb]]</span><br><span class=\"line\">  urls = [ \"http://localhost:8086\" ]</span><br><span class=\"line\">  database = \"telegraf\"</span><br><span class=\"line\"><span class=\"meta\">  #</span><span class=\"bash\"> Drop all measurements that start with <span class=\"string\">\"aerospike\"</span></span></span><br><span class=\"line\">  namedrop = [\"aerospike*\"]</span><br><span class=\"line\"></span><br><span class=\"line\">[[outputs.influxdb]]</span><br><span class=\"line\">  urls = [ \"http://localhost:8086\" ]</span><br><span class=\"line\">  database = \"telegraf-aerospike-data\"</span><br><span class=\"line\"><span class=\"meta\">  #</span><span class=\"bash\"> Only accept aerospike data:</span></span><br><span class=\"line\">  namepass = [\"aerospike*\"]</span><br><span class=\"line\"></span><br><span class=\"line\">[[outputs.influxdb]]</span><br><span class=\"line\">  urls = [ \"http://localhost:8086\" ]</span><br><span class=\"line\">  database = \"telegraf-cpu0-data\"</span><br><span class=\"line\"><span class=\"meta\">  #</span><span class=\"bash\"> Only store measurements <span class=\"built_in\">where</span> the tag <span class=\"string\">\"cpu\"</span> matches the value <span class=\"string\">\"cpu0\"</span></span></span><br><span class=\"line\">  [outputs.influxdb.tagpass]</span><br><span class=\"line\">    cpu = [\"cpu0\"]</span><br></pre></td></tr></table></figure>\n<p>Aggregator Plugins</p>\n<ul>\n<li><a href=\"https://github.com/influxdata/telegraf/blob/master/plugins/aggregators/basicstats\" target=\"_blank\" rel=\"noopener\">basicstats</a></li>\n<li><a href=\"https://github.com/influxdata/telegraf/blob/master/plugins/aggregators/minmax\" target=\"_blank\" rel=\"noopener\">minmax</a></li>\n<li><a href=\"https://github.com/influxdata/telegraf/blob/master/plugins/aggregators/histogram\" target=\"_blank\" rel=\"noopener\">histogram</a></li>\n</ul>\n<h3 id=\"部署方式\"><a href=\"#部署方式\" class=\"headerlink\" title=\"部署方式\"></a>部署方式</h3><p>客户端部署方式，可结合Ansible 进行部署，以及配置管理。官方说明并未看到。</p>\n<h3 id=\"任务配置\"><a href=\"#任务配置\" class=\"headerlink\" title=\"任务配置\"></a>任务配置</h3><p>客户端配置文件配置 /etc/telegraf/telegraf.conf，可配置指标采集周期、过滤指标等。</p>\n<h3 id=\"告警处理\"><a href=\"#告警处理\" class=\"headerlink\" title=\"告警处理\"></a>告警处理</h3><p>与Telegraf配合使用的告警处理程序是<a href=\"https://github.com/influxdata/kapacitor\" target=\"_blank\" rel=\"noopener\">kapacitor</a> </p>\n<p>Open source framework for processing, monitoring, and alerting on time series data。</p>"},{"title":"Sensu 监控体系介绍","date":"2018-03-27T04:51:23.000Z","_content":"\n## 1.Sensu 安装部署\n\n### 主要特性\n\n[首页介绍](https://sensuapp.org/docs/1.2/overview/what-is-sensu.html)\n\nSensu was designed to provide a comprehensive monitoring platform for monitoring infrastructure (servers), services, application health, and business KPIs — and to collect and analyze custom metrics. \n\n主要包含以下功能与特性：\n\n- 检查系统、服务和程序的运行状态。\n- 基于分布式的设计，能够轻松的动态伸缩规模。\n- 支持通过插件的形式自定义检查的内容，拥有丰富的插件库。\n- 收集信息，获取被监控节点上的各项数据指标等。\n- 可视化的操作界面，提供实时的 GUI 用于显示和操作相关信息。\n- 内置的集成工具，可用于和其它系统集成，如 PagerDuty、Graphite、Email 等。\n- 提供丰富的 API 接口，支持通过 API 调用访问事件和客户端信息，触发检测等。\n- 加密的安全通信，支持各种复杂的网络拓扑。\n\n包括有类似我们拨测的功能，可对服务可用性进行检查，例如指定页面进行访问。\n\n结合其他软件：Sensu 从监控节点获取数据，将数据格式化成 Graphite 要求的格式，然后通过调用 Graphite 的接口将数据发给 Graphite。Graphite 将数据存储在时序数据中供 Grafana 绘图使用。最终用户通过在 Grafana 中[定制](http://www.liuhaihua.cn/archives/tag/private)需要显示的数据及显示的方式，获得最终的可视化图表。\n\n------\n\n### 架构\n\nSensu 由服务器、客户端、RabbitMQ、Redis 和 API 这五个部分构成。图 1 展示了这些组件之间的关系以及通信的数据流。如图所示，RabbitMQ 用于组件之间的通信，Redis 用于持久化 Sensu 服务器和 Sensu API 的数据。因为客户端都是通过文件进行配置，并且不需要在服务器端配置客户端的信息，所以可以很轻易的增加和减少客户端的数量。由于 Sensu 的服务器和 API 原生支持多节点部署，所以不存在效率的瓶颈问题。从图中可以看到，为了解耦服务器和客户端，通信都是通过 RabbitMQ 进行的。\n\n![组成部分](/img/sensu组成部分.png)\n\n核心代码由Ruby编写。[Github地址](https://github.com/sensu/sensu)\n\n实际安装中，不一定要使用RabbitMq作为消息中间件。直接使用redis即可。以下安装使用redis。\n\n<!-- more -->\n\n### 安装部署\n\nserver/clent 支持平台有\n\n![支持平台](/img/sensu-support-platforms.png)\n\n\n\nServer安装部署：\n\n安装配置yum源 /etc/yum.repos.d/sensu.repo\n\n```shell\n[sensu]\nname=sensu\nbaseurl=https://sensu.global.ssl.fastly.net/yum/$releasever/$basearch/\ngpgcheck=0\nenabled=1\n```\n\n```shell\nyum install redis -y\nyum install sensu -y\n```\n\nsensu server 配置 /etc/sensu/config.json\n\n```\n{\n  \"transport\": {\n    \"name\": \"redis\"\n  },\n  \"api\": {\n    \"host\": \"127.0.0.1\",\n    \"port\": 4567\n  }\n}\n```\n\nsensu client/server配置transport.redis\n\n```shell\n{\n  \"redis\":\n    {\n       \"host\": \"127.0.0.1\",\n       \"port\": 6379\n    }\n}\n```\n\nsensu client 配置 /etc/sensu/conf.d/client.json\n\n```\n{\n  \"client\": {\n    \"name\": \"rhel-client\",\n    \"address\": \"127.0.0.1\",\n    \"environment\": \"development\",\n    \"subscriptions\": [\n      \"dev\",\n      \"rhel-hosts\"\n    ],\n    \"socket\": {\n      \"bind\": \"127.0.0.1\",\n      \"port\": 3030\n    }\n  }\n｝\n```\n\nsensu dashboard配置 /etc/sensu/dashboard.json\n\n```\n{\n  \"sensu\": [\n    {\n      \"name\": \"sensu\",\n      \"host\": \"127.0.0.1\",\n      \"port\": 4567\n    }\n  ],\n  \"dashboard\": {\n    \"host\": \"0.0.0.0\",\n    \"port\": 3000\n  }\n}\n```\n\n启动进程\n\n```shell\nsystemctl enable sensu-{server,api,client} \nsystemctl start sensu-{server,api,client}\nsystemctl restart sensu-{server,api,client}\n```\n\nuchiwa 图形展示界面，需要额外安装。sensu提供了两个版本的展示界面，一个是企业版，一个就是uchiwa 。这里使用uchiwa，[下载链接](https://uchiwa.io/#/download)。 下载完成后，使用yum命令安装\n\n```\nyum -y localinstall uchiwa-1.1.1-1.x86_64.rpm\nsystemctl enable uchiwa\nsystemctl start uchiwa\n```\n\n配置好 uchiwa  /etc/sensu/uchiwa.json\n\n```shell\n{\n  \"sensu\": [\n    {\n      \"name\": \"Sensu-Server-1\",\n      \"host\": \"127.0.0.1\",\n      \"port\": 4567,\n      \"timeout\": 10\n    }\n  ],\n  \"uchiwa\": {\n    \"host\": \"0.0.0.0\",\n    \"port\": 3000,\n    \"refresh\": 10\n  }\n}\n```\n\n[URL](http://192.168.14.165:3000)\n\n![图例](/img/uchiwa.png)\n\n\n\nuchiwa 只是简单的展示功能，并不能对client等进行增删操作，即相当于对Sensu-API 只实现了Get相关的功能\n\n[HTTP API用例](https://sensuapp.org/docs/1.2/api/overview.html)\n\n### 部署过程中遇到的问题\n\n- uchiwa无法展示client和server，大部分都是json配置文件没有配置清楚导致。可查看日志/var/log/uchiwa，/var/log/sensu/sensu-client.log\n\n\n\n## 2.Sensu 使用\n\n### Sensu Client\n\nSensu Client 就是采集客户端agent。\n\n[介绍博文](https://blog.csdn.net/houzhe_adore/article/details/48518825)\n\n### Proxy Clients\n\n\n\n### Sensu Check\n\nSensu checks allow you to monitor server resources, services, and application health, as well as collect & analyze metrics; they are executed on servers running the Sensu client. Sensu checks allow you to monitor server resources, services, and application health, as well as collect & analyze metrics; they are executed on servers running the Sensu client. \n\n首先安装check插件\n\n```shell\nsensu-install -p process-checks\t\nsensu-install -p cpu-checks\n```\n\n插件是ruby脚本，安装后会在/opt/sensu/embedded/bin/ 路径下下载安装rubu解释器和check插件。process-checks 是进程检测插件，cpu-checks 是cpu检测指标插件。\n\n[github插件脚本](https://github.com/sensu-plugins/sensu-plugins-process-checks)\n\n配置check 服务端路径 /etc/sensu/conf.d/\n\n检查sensu进程个数 check_cron.json\n\n```shell\n{\n  \"checks\": {\n    \"cron\": {\n      \"command\": \"check-process.rb -p cron\",\n      \"standalone\": true,\n      \"subscribers\": [\n        \"production\"\n      ],\n      \"interval\": 60\n    }\n  }\n}\n```\n\n收集cpu指标 metrics_cpu.json\n\n```\n{\n  \"checks\": {\n    \"cpu_metrics\": {\n      \"type\": \"metric\",\n      \"command\": \"metrics-cpu.rb\",\n      \"subscribers\": [\n        \"production\"\n      ],\n      \"interval\": 10,\n      \"handler\": \"tcp_socket\"\n    }\n  }\n}\n\n```\n\n其中 \"standalone\": true 表示允许client 单独调度运行，不需要server 调度。\"type\": \"metric\" 表明这是指标类型的采集。\n\n在/var/log/sensu/sensu-client.log 、sensu-server.log、HTTP API 可以看到执行结果。后续可再对结果进行处理，需借助另外的插件[**sensu-plugins-graphite**](https://github.com/sensu-plugins/sensu-plugins-graphite) . \n\n*采集一个指标展示，真累。。。*\n\n接着通过redis查看cpu指标数据\n\n```shell\nredis-cli\nkeys * \nget result:tick-client:cpu_metrics\n\"{\\\"type\\\":\\\"metric\\\",\\\"command\\\":\\\"metrics-cpu.rb\\\",\\\"subscribers\\\":[\\\"production\\\"],\\\"interval\\\":10,\\\"handler\\\":\\\"tcp_socket\\\",\\\"standalone\\\":true,\\\"name\\\":\\\"cpu_metrics\\\",\\\"issued\\\":1524824239,\\\"executed\\\":1524824239,\\\"duration\\\":0.073,\\\"output\\\":\\\"tick.cpu.total.user 8057 1524824240\\\\n...\\\",\\\"status\\\":0}\"\n```\n\n\n\n也可以直接到脚本安装路径，运行ruby脚本，直接看看采集结果\n\n```shell\n[root@tick bin]# /opt/sensu/embedded/bin/ruby  check-cpu.rb\nCheckCPU TOTAL OK: total=0.25 user=0.0 nice=0.0 system=0.25 idle=99.75 iowait=0.0 irq=0.0 softirq=0.0 steal=0.0 guest=0.0 guest_nice=0.0\n```\n\n\n\n### Sensu data output\n\nMetric collection checks are used to collect measurements from server resources, services, and applications. Metric collection checks can output metric data in a variety of metric formats:\n\n- [Graphite plaintext](http://graphite.readthedocs.org/en/latest/feeding-carbon.html#the-plaintext-protocol)\n- [Nagios Performance Data](http://nagios.sourceforge.net/docs/3_0/perfdata.html)\n- [OpenTSDB](http://opentsdb.net/docs/build/html/user_guide/writing.html)\n- [Metrics 2.0 wire format](http://metrics20.org/spec/)\n\nThe output produced by the check `command` ，貌似不同的插件返回不同的结果集。\n\n\n\n### Sensu event processor\n\nThe Sensu server provides a scalable event processor. Event processing involves conversion of [check results](https://sensuapp.org/docs/latest/reference/checks.html#check-results) into Sensu events, and then applying any defined [event filters](https://sensuapp.org/docs/latest/reference/filters.html), [event data mutators](https://sensuapp.org/docs/latest/reference/mutators.html), and [event handlers](https://sensuapp.org/docs/latest/reference/handlers.html). All event processing happens on a Sensu server system.\n\n\n\n### Sensu event handlers\n\nSensu event handlers are for taking action on [events](https://sensuapp.org/docs/latest/reference/events.html) (produced by check results), such as sending an email alert, creating or resolving a PagerDuty incident, or storing metrics in Graphite. There are several types of handlers: `pipe`, `tcp`, `udp`, `transport`, and `set`.\n\n- **Pipe handlers** execute a command and pass the event data to the created process via `STDIN`.\n- **TCP and UDP handlers** send the event data to a remote socket.\n- **Transport handlers** publish the event data to the Sensu transport (by default, this is RabbitMQ).\n- **Set handlers** are used to group event handlers, making it easier to manage many event handlers.\n\n类似于告警。\n\n这里使用TCP handlers 作为验证，需安装nc 命令。开启两个终端，分别运行以下命令，一个是作为server端，监听6000端口，一个是客户端，往监听端口发送数据。\n\n```shell\n nc -l -k -4 -p 6000      \n echo \"testing\" | nc localhost 6000\n```\n\n新建TCP 配置文件 /etc/sensu/conf.d/tcp_socket.json\n\n```sh\n{\n  \"handlers\": {\n    \"tcp_socket\": {\n      \"type\": \"tcp\",\n      \"socket\": {\n        \"host\": \"localhost\",\n        \"port\": 6000\n      }\n    }\n  }\n}\n```\n\n注意，需修改对应的check 配置，把handler 修改为 *tcp_socket*  \n\n例如，cron 进程检测的异常情况，即进程不存在![Check Error Event](/img/check-error-event.png)\n\n\n\n### Sensu API\n\n常用的API \n\n```shell\ncurl -s http://127.0.0.1:4567/clients | jq .\ncurl -s http://localhost:4567/events | jq .\ncurl -s http://localhost:4567/results | jq .\ncurl -s http://localhost:4567/client | jq .\n```\n\n\n\n## 3.关注事项\n\n1. 数据收集方式（Pull、Push）   \n\n   默认都是Push，由Client发送采集数据到transport，服务端再去transport 读取数据。\n\n2. 数据格式\n\n   check-result-cpu.json\n\n3. 数据存储方式\n\n   数据存储在transport 中，可以固化到文件系统\n\n4. 数据输出方式\n\n   checks 返回的结果是有一定格式的，其中，output字段存储的是对应的实际插件执行的命令输出结果。不同插件返回的命令结果不同。\n\n5. agent 部署方式\n\n   并未提供部署方案。\n\n6. 任务下发方式\n\n   客户端配置任务。无法从通过sensu-server直接下发到客户端。","source":"_posts/sensu.md","raw":"---\ntitle: Sensu 监控体系介绍\ndate: 2018-03-27 12:51:23\ntags: \n - Sensu\n - Ruby\ncategories: 技术\n---\n\n## 1.Sensu 安装部署\n\n### 主要特性\n\n[首页介绍](https://sensuapp.org/docs/1.2/overview/what-is-sensu.html)\n\nSensu was designed to provide a comprehensive monitoring platform for monitoring infrastructure (servers), services, application health, and business KPIs — and to collect and analyze custom metrics. \n\n主要包含以下功能与特性：\n\n- 检查系统、服务和程序的运行状态。\n- 基于分布式的设计，能够轻松的动态伸缩规模。\n- 支持通过插件的形式自定义检查的内容，拥有丰富的插件库。\n- 收集信息，获取被监控节点上的各项数据指标等。\n- 可视化的操作界面，提供实时的 GUI 用于显示和操作相关信息。\n- 内置的集成工具，可用于和其它系统集成，如 PagerDuty、Graphite、Email 等。\n- 提供丰富的 API 接口，支持通过 API 调用访问事件和客户端信息，触发检测等。\n- 加密的安全通信，支持各种复杂的网络拓扑。\n\n包括有类似我们拨测的功能，可对服务可用性进行检查，例如指定页面进行访问。\n\n结合其他软件：Sensu 从监控节点获取数据，将数据格式化成 Graphite 要求的格式，然后通过调用 Graphite 的接口将数据发给 Graphite。Graphite 将数据存储在时序数据中供 Grafana 绘图使用。最终用户通过在 Grafana 中[定制](http://www.liuhaihua.cn/archives/tag/private)需要显示的数据及显示的方式，获得最终的可视化图表。\n\n------\n\n### 架构\n\nSensu 由服务器、客户端、RabbitMQ、Redis 和 API 这五个部分构成。图 1 展示了这些组件之间的关系以及通信的数据流。如图所示，RabbitMQ 用于组件之间的通信，Redis 用于持久化 Sensu 服务器和 Sensu API 的数据。因为客户端都是通过文件进行配置，并且不需要在服务器端配置客户端的信息，所以可以很轻易的增加和减少客户端的数量。由于 Sensu 的服务器和 API 原生支持多节点部署，所以不存在效率的瓶颈问题。从图中可以看到，为了解耦服务器和客户端，通信都是通过 RabbitMQ 进行的。\n\n![组成部分](/img/sensu组成部分.png)\n\n核心代码由Ruby编写。[Github地址](https://github.com/sensu/sensu)\n\n实际安装中，不一定要使用RabbitMq作为消息中间件。直接使用redis即可。以下安装使用redis。\n\n<!-- more -->\n\n### 安装部署\n\nserver/clent 支持平台有\n\n![支持平台](/img/sensu-support-platforms.png)\n\n\n\nServer安装部署：\n\n安装配置yum源 /etc/yum.repos.d/sensu.repo\n\n```shell\n[sensu]\nname=sensu\nbaseurl=https://sensu.global.ssl.fastly.net/yum/$releasever/$basearch/\ngpgcheck=0\nenabled=1\n```\n\n```shell\nyum install redis -y\nyum install sensu -y\n```\n\nsensu server 配置 /etc/sensu/config.json\n\n```\n{\n  \"transport\": {\n    \"name\": \"redis\"\n  },\n  \"api\": {\n    \"host\": \"127.0.0.1\",\n    \"port\": 4567\n  }\n}\n```\n\nsensu client/server配置transport.redis\n\n```shell\n{\n  \"redis\":\n    {\n       \"host\": \"127.0.0.1\",\n       \"port\": 6379\n    }\n}\n```\n\nsensu client 配置 /etc/sensu/conf.d/client.json\n\n```\n{\n  \"client\": {\n    \"name\": \"rhel-client\",\n    \"address\": \"127.0.0.1\",\n    \"environment\": \"development\",\n    \"subscriptions\": [\n      \"dev\",\n      \"rhel-hosts\"\n    ],\n    \"socket\": {\n      \"bind\": \"127.0.0.1\",\n      \"port\": 3030\n    }\n  }\n｝\n```\n\nsensu dashboard配置 /etc/sensu/dashboard.json\n\n```\n{\n  \"sensu\": [\n    {\n      \"name\": \"sensu\",\n      \"host\": \"127.0.0.1\",\n      \"port\": 4567\n    }\n  ],\n  \"dashboard\": {\n    \"host\": \"0.0.0.0\",\n    \"port\": 3000\n  }\n}\n```\n\n启动进程\n\n```shell\nsystemctl enable sensu-{server,api,client} \nsystemctl start sensu-{server,api,client}\nsystemctl restart sensu-{server,api,client}\n```\n\nuchiwa 图形展示界面，需要额外安装。sensu提供了两个版本的展示界面，一个是企业版，一个就是uchiwa 。这里使用uchiwa，[下载链接](https://uchiwa.io/#/download)。 下载完成后，使用yum命令安装\n\n```\nyum -y localinstall uchiwa-1.1.1-1.x86_64.rpm\nsystemctl enable uchiwa\nsystemctl start uchiwa\n```\n\n配置好 uchiwa  /etc/sensu/uchiwa.json\n\n```shell\n{\n  \"sensu\": [\n    {\n      \"name\": \"Sensu-Server-1\",\n      \"host\": \"127.0.0.1\",\n      \"port\": 4567,\n      \"timeout\": 10\n    }\n  ],\n  \"uchiwa\": {\n    \"host\": \"0.0.0.0\",\n    \"port\": 3000,\n    \"refresh\": 10\n  }\n}\n```\n\n[URL](http://192.168.14.165:3000)\n\n![图例](/img/uchiwa.png)\n\n\n\nuchiwa 只是简单的展示功能，并不能对client等进行增删操作，即相当于对Sensu-API 只实现了Get相关的功能\n\n[HTTP API用例](https://sensuapp.org/docs/1.2/api/overview.html)\n\n### 部署过程中遇到的问题\n\n- uchiwa无法展示client和server，大部分都是json配置文件没有配置清楚导致。可查看日志/var/log/uchiwa，/var/log/sensu/sensu-client.log\n\n\n\n## 2.Sensu 使用\n\n### Sensu Client\n\nSensu Client 就是采集客户端agent。\n\n[介绍博文](https://blog.csdn.net/houzhe_adore/article/details/48518825)\n\n### Proxy Clients\n\n\n\n### Sensu Check\n\nSensu checks allow you to monitor server resources, services, and application health, as well as collect & analyze metrics; they are executed on servers running the Sensu client. Sensu checks allow you to monitor server resources, services, and application health, as well as collect & analyze metrics; they are executed on servers running the Sensu client. \n\n首先安装check插件\n\n```shell\nsensu-install -p process-checks\t\nsensu-install -p cpu-checks\n```\n\n插件是ruby脚本，安装后会在/opt/sensu/embedded/bin/ 路径下下载安装rubu解释器和check插件。process-checks 是进程检测插件，cpu-checks 是cpu检测指标插件。\n\n[github插件脚本](https://github.com/sensu-plugins/sensu-plugins-process-checks)\n\n配置check 服务端路径 /etc/sensu/conf.d/\n\n检查sensu进程个数 check_cron.json\n\n```shell\n{\n  \"checks\": {\n    \"cron\": {\n      \"command\": \"check-process.rb -p cron\",\n      \"standalone\": true,\n      \"subscribers\": [\n        \"production\"\n      ],\n      \"interval\": 60\n    }\n  }\n}\n```\n\n收集cpu指标 metrics_cpu.json\n\n```\n{\n  \"checks\": {\n    \"cpu_metrics\": {\n      \"type\": \"metric\",\n      \"command\": \"metrics-cpu.rb\",\n      \"subscribers\": [\n        \"production\"\n      ],\n      \"interval\": 10,\n      \"handler\": \"tcp_socket\"\n    }\n  }\n}\n\n```\n\n其中 \"standalone\": true 表示允许client 单独调度运行，不需要server 调度。\"type\": \"metric\" 表明这是指标类型的采集。\n\n在/var/log/sensu/sensu-client.log 、sensu-server.log、HTTP API 可以看到执行结果。后续可再对结果进行处理，需借助另外的插件[**sensu-plugins-graphite**](https://github.com/sensu-plugins/sensu-plugins-graphite) . \n\n*采集一个指标展示，真累。。。*\n\n接着通过redis查看cpu指标数据\n\n```shell\nredis-cli\nkeys * \nget result:tick-client:cpu_metrics\n\"{\\\"type\\\":\\\"metric\\\",\\\"command\\\":\\\"metrics-cpu.rb\\\",\\\"subscribers\\\":[\\\"production\\\"],\\\"interval\\\":10,\\\"handler\\\":\\\"tcp_socket\\\",\\\"standalone\\\":true,\\\"name\\\":\\\"cpu_metrics\\\",\\\"issued\\\":1524824239,\\\"executed\\\":1524824239,\\\"duration\\\":0.073,\\\"output\\\":\\\"tick.cpu.total.user 8057 1524824240\\\\n...\\\",\\\"status\\\":0}\"\n```\n\n\n\n也可以直接到脚本安装路径，运行ruby脚本，直接看看采集结果\n\n```shell\n[root@tick bin]# /opt/sensu/embedded/bin/ruby  check-cpu.rb\nCheckCPU TOTAL OK: total=0.25 user=0.0 nice=0.0 system=0.25 idle=99.75 iowait=0.0 irq=0.0 softirq=0.0 steal=0.0 guest=0.0 guest_nice=0.0\n```\n\n\n\n### Sensu data output\n\nMetric collection checks are used to collect measurements from server resources, services, and applications. Metric collection checks can output metric data in a variety of metric formats:\n\n- [Graphite plaintext](http://graphite.readthedocs.org/en/latest/feeding-carbon.html#the-plaintext-protocol)\n- [Nagios Performance Data](http://nagios.sourceforge.net/docs/3_0/perfdata.html)\n- [OpenTSDB](http://opentsdb.net/docs/build/html/user_guide/writing.html)\n- [Metrics 2.0 wire format](http://metrics20.org/spec/)\n\nThe output produced by the check `command` ，貌似不同的插件返回不同的结果集。\n\n\n\n### Sensu event processor\n\nThe Sensu server provides a scalable event processor. Event processing involves conversion of [check results](https://sensuapp.org/docs/latest/reference/checks.html#check-results) into Sensu events, and then applying any defined [event filters](https://sensuapp.org/docs/latest/reference/filters.html), [event data mutators](https://sensuapp.org/docs/latest/reference/mutators.html), and [event handlers](https://sensuapp.org/docs/latest/reference/handlers.html). All event processing happens on a Sensu server system.\n\n\n\n### Sensu event handlers\n\nSensu event handlers are for taking action on [events](https://sensuapp.org/docs/latest/reference/events.html) (produced by check results), such as sending an email alert, creating or resolving a PagerDuty incident, or storing metrics in Graphite. There are several types of handlers: `pipe`, `tcp`, `udp`, `transport`, and `set`.\n\n- **Pipe handlers** execute a command and pass the event data to the created process via `STDIN`.\n- **TCP and UDP handlers** send the event data to a remote socket.\n- **Transport handlers** publish the event data to the Sensu transport (by default, this is RabbitMQ).\n- **Set handlers** are used to group event handlers, making it easier to manage many event handlers.\n\n类似于告警。\n\n这里使用TCP handlers 作为验证，需安装nc 命令。开启两个终端，分别运行以下命令，一个是作为server端，监听6000端口，一个是客户端，往监听端口发送数据。\n\n```shell\n nc -l -k -4 -p 6000      \n echo \"testing\" | nc localhost 6000\n```\n\n新建TCP 配置文件 /etc/sensu/conf.d/tcp_socket.json\n\n```sh\n{\n  \"handlers\": {\n    \"tcp_socket\": {\n      \"type\": \"tcp\",\n      \"socket\": {\n        \"host\": \"localhost\",\n        \"port\": 6000\n      }\n    }\n  }\n}\n```\n\n注意，需修改对应的check 配置，把handler 修改为 *tcp_socket*  \n\n例如，cron 进程检测的异常情况，即进程不存在![Check Error Event](/img/check-error-event.png)\n\n\n\n### Sensu API\n\n常用的API \n\n```shell\ncurl -s http://127.0.0.1:4567/clients | jq .\ncurl -s http://localhost:4567/events | jq .\ncurl -s http://localhost:4567/results | jq .\ncurl -s http://localhost:4567/client | jq .\n```\n\n\n\n## 3.关注事项\n\n1. 数据收集方式（Pull、Push）   \n\n   默认都是Push，由Client发送采集数据到transport，服务端再去transport 读取数据。\n\n2. 数据格式\n\n   check-result-cpu.json\n\n3. 数据存储方式\n\n   数据存储在transport 中，可以固化到文件系统\n\n4. 数据输出方式\n\n   checks 返回的结果是有一定格式的，其中，output字段存储的是对应的实际插件执行的命令输出结果。不同插件返回的命令结果不同。\n\n5. agent 部署方式\n\n   并未提供部署方案。\n\n6. 任务下发方式\n\n   客户端配置任务。无法从通过sensu-server直接下发到客户端。","slug":"sensu","published":1,"updated":"2018-10-20T08:15:43.515Z","_id":"cjhajczpc0019e4rwe273vqqw","comments":1,"layout":"post","photos":[],"link":"","content":"<h2 id=\"1-Sensu-安装部署\"><a href=\"#1-Sensu-安装部署\" class=\"headerlink\" title=\"1.Sensu 安装部署\"></a>1.Sensu 安装部署</h2><h3 id=\"主要特性\"><a href=\"#主要特性\" class=\"headerlink\" title=\"主要特性\"></a>主要特性</h3><p><a href=\"https://sensuapp.org/docs/1.2/overview/what-is-sensu.html\" target=\"_blank\" rel=\"noopener\">首页介绍</a></p>\n<p>Sensu was designed to provide a comprehensive monitoring platform for monitoring infrastructure (servers), services, application health, and business KPIs — and to collect and analyze custom metrics. </p>\n<p>主要包含以下功能与特性：</p>\n<ul>\n<li>检查系统、服务和程序的运行状态。</li>\n<li>基于分布式的设计，能够轻松的动态伸缩规模。</li>\n<li>支持通过插件的形式自定义检查的内容，拥有丰富的插件库。</li>\n<li>收集信息，获取被监控节点上的各项数据指标等。</li>\n<li>可视化的操作界面，提供实时的 GUI 用于显示和操作相关信息。</li>\n<li>内置的集成工具，可用于和其它系统集成，如 PagerDuty、Graphite、Email 等。</li>\n<li>提供丰富的 API 接口，支持通过 API 调用访问事件和客户端信息，触发检测等。</li>\n<li>加密的安全通信，支持各种复杂的网络拓扑。</li>\n</ul>\n<p>包括有类似我们拨测的功能，可对服务可用性进行检查，例如指定页面进行访问。</p>\n<p>结合其他软件：Sensu 从监控节点获取数据，将数据格式化成 Graphite 要求的格式，然后通过调用 Graphite 的接口将数据发给 Graphite。Graphite 将数据存储在时序数据中供 Grafana 绘图使用。最终用户通过在 Grafana 中<a href=\"http://www.liuhaihua.cn/archives/tag/private\" target=\"_blank\" rel=\"noopener\">定制</a>需要显示的数据及显示的方式，获得最终的可视化图表。</p>\n<hr>\n<h3 id=\"架构\"><a href=\"#架构\" class=\"headerlink\" title=\"架构\"></a>架构</h3><p>Sensu 由服务器、客户端、RabbitMQ、Redis 和 API 这五个部分构成。图 1 展示了这些组件之间的关系以及通信的数据流。如图所示，RabbitMQ 用于组件之间的通信，Redis 用于持久化 Sensu 服务器和 Sensu API 的数据。因为客户端都是通过文件进行配置，并且不需要在服务器端配置客户端的信息，所以可以很轻易的增加和减少客户端的数量。由于 Sensu 的服务器和 API 原生支持多节点部署，所以不存在效率的瓶颈问题。从图中可以看到，为了解耦服务器和客户端，通信都是通过 RabbitMQ 进行的。</p>\n<p><img src=\"/img/sensu组成部分.png\" alt=\"组成部分\"></p>\n<p>核心代码由Ruby编写。<a href=\"https://github.com/sensu/sensu\" target=\"_blank\" rel=\"noopener\">Github地址</a></p>\n<p>实际安装中，不一定要使用RabbitMq作为消息中间件。直接使用redis即可。以下安装使用redis。</p>\n<a id=\"more\"></a>\n<h3 id=\"安装部署\"><a href=\"#安装部署\" class=\"headerlink\" title=\"安装部署\"></a>安装部署</h3><p>server/clent 支持平台有</p>\n<p><img src=\"/img/sensu-support-platforms.png\" alt=\"支持平台\"></p>\n<p>Server安装部署：</p>\n<p>安装配置yum源 /etc/yum.repos.d/sensu.repo</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[sensu]</span><br><span class=\"line\">name=sensu</span><br><span class=\"line\">baseurl=https://sensu.global.ssl.fastly.net/yum/$releasever/$basearch/</span><br><span class=\"line\">gpgcheck=0</span><br><span class=\"line\">enabled=1</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">yum install redis -y</span><br><span class=\"line\">yum install sensu -y</span><br></pre></td></tr></table></figure>\n<p>sensu server 配置 /etc/sensu/config.json</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;transport&quot;: &#123;</span><br><span class=\"line\">    &quot;name&quot;: &quot;redis&quot;</span><br><span class=\"line\">  &#125;,</span><br><span class=\"line\">  &quot;api&quot;: &#123;</span><br><span class=\"line\">    &quot;host&quot;: &quot;127.0.0.1&quot;,</span><br><span class=\"line\">    &quot;port&quot;: 4567</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>sensu client/server配置transport.redis</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#123;</span><br><span class=\"line\">  \"redis\":</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">       \"host\": \"127.0.0.1\",</span><br><span class=\"line\">       \"port\": 6379</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>sensu client 配置 /etc/sensu/conf.d/client.json</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;client&quot;: &#123;</span><br><span class=\"line\">    &quot;name&quot;: &quot;rhel-client&quot;,</span><br><span class=\"line\">    &quot;address&quot;: &quot;127.0.0.1&quot;,</span><br><span class=\"line\">    &quot;environment&quot;: &quot;development&quot;,</span><br><span class=\"line\">    &quot;subscriptions&quot;: [</span><br><span class=\"line\">      &quot;dev&quot;,</span><br><span class=\"line\">      &quot;rhel-hosts&quot;</span><br><span class=\"line\">    ],</span><br><span class=\"line\">    &quot;socket&quot;: &#123;</span><br><span class=\"line\">      &quot;bind&quot;: &quot;127.0.0.1&quot;,</span><br><span class=\"line\">      &quot;port&quot;: 3030</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">｝</span><br></pre></td></tr></table></figure>\n<p>sensu dashboard配置 /etc/sensu/dashboard.json</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;sensu&quot;: [</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">      &quot;name&quot;: &quot;sensu&quot;,</span><br><span class=\"line\">      &quot;host&quot;: &quot;127.0.0.1&quot;,</span><br><span class=\"line\">      &quot;port&quot;: 4567</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  ],</span><br><span class=\"line\">  &quot;dashboard&quot;: &#123;</span><br><span class=\"line\">    &quot;host&quot;: &quot;0.0.0.0&quot;,</span><br><span class=\"line\">    &quot;port&quot;: 3000</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>启动进程</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">systemctl enable sensu-&#123;server,api,client&#125; </span><br><span class=\"line\">systemctl start sensu-&#123;server,api,client&#125;</span><br><span class=\"line\">systemctl restart sensu-&#123;server,api,client&#125;</span><br></pre></td></tr></table></figure>\n<p>uchiwa 图形展示界面，需要额外安装。sensu提供了两个版本的展示界面，一个是企业版，一个就是uchiwa 。这里使用uchiwa，<a href=\"https://uchiwa.io/#/download\" target=\"_blank\" rel=\"noopener\">下载链接</a>。 下载完成后，使用yum命令安装</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">yum -y localinstall uchiwa-1.1.1-1.x86_64.rpm</span><br><span class=\"line\">systemctl enable uchiwa</span><br><span class=\"line\">systemctl start uchiwa</span><br></pre></td></tr></table></figure>\n<p>配置好 uchiwa  /etc/sensu/uchiwa.json</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#123;</span><br><span class=\"line\">  \"sensu\": [</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">      \"name\": \"Sensu-Server-1\",</span><br><span class=\"line\">      \"host\": \"127.0.0.1\",</span><br><span class=\"line\">      \"port\": 4567,</span><br><span class=\"line\">      \"timeout\": 10</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  ],</span><br><span class=\"line\">  \"uchiwa\": &#123;</span><br><span class=\"line\">    \"host\": \"0.0.0.0\",</span><br><span class=\"line\">    \"port\": 3000,</span><br><span class=\"line\">    \"refresh\": 10</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p><a href=\"http://192.168.14.165:3000\" target=\"_blank\" rel=\"noopener\">URL</a></p>\n<p><img src=\"/img/uchiwa.png\" alt=\"图例\"></p>\n<p>uchiwa 只是简单的展示功能，并不能对client等进行增删操作，即相当于对Sensu-API 只实现了Get相关的功能</p>\n<p><a href=\"https://sensuapp.org/docs/1.2/api/overview.html\" target=\"_blank\" rel=\"noopener\">HTTP API用例</a></p>\n<h3 id=\"部署过程中遇到的问题\"><a href=\"#部署过程中遇到的问题\" class=\"headerlink\" title=\"部署过程中遇到的问题\"></a>部署过程中遇到的问题</h3><ul>\n<li>uchiwa无法展示client和server，大部分都是json配置文件没有配置清楚导致。可查看日志/var/log/uchiwa，/var/log/sensu/sensu-client.log</li>\n</ul>\n<h2 id=\"2-Sensu-使用\"><a href=\"#2-Sensu-使用\" class=\"headerlink\" title=\"2.Sensu 使用\"></a>2.Sensu 使用</h2><h3 id=\"Sensu-Client\"><a href=\"#Sensu-Client\" class=\"headerlink\" title=\"Sensu Client\"></a>Sensu Client</h3><p>Sensu Client 就是采集客户端agent。</p>\n<p><a href=\"https://blog.csdn.net/houzhe_adore/article/details/48518825\" target=\"_blank\" rel=\"noopener\">介绍博文</a></p>\n<h3 id=\"Proxy-Clients\"><a href=\"#Proxy-Clients\" class=\"headerlink\" title=\"Proxy Clients\"></a>Proxy Clients</h3><h3 id=\"Sensu-Check\"><a href=\"#Sensu-Check\" class=\"headerlink\" title=\"Sensu Check\"></a>Sensu Check</h3><p>Sensu checks allow you to monitor server resources, services, and application health, as well as collect &amp; analyze metrics; they are executed on servers running the Sensu client. Sensu checks allow you to monitor server resources, services, and application health, as well as collect &amp; analyze metrics; they are executed on servers running the Sensu client. </p>\n<p>首先安装check插件</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sensu-install -p process-checks\t</span><br><span class=\"line\">sensu-install -p cpu-checks</span><br></pre></td></tr></table></figure>\n<p>插件是ruby脚本，安装后会在/opt/sensu/embedded/bin/ 路径下下载安装rubu解释器和check插件。process-checks 是进程检测插件，cpu-checks 是cpu检测指标插件。</p>\n<p><a href=\"https://github.com/sensu-plugins/sensu-plugins-process-checks\" target=\"_blank\" rel=\"noopener\">github插件脚本</a></p>\n<p>配置check 服务端路径 /etc/sensu/conf.d/</p>\n<p>检查sensu进程个数 check_cron.json</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#123;</span><br><span class=\"line\">  \"checks\": &#123;</span><br><span class=\"line\">    \"cron\": &#123;</span><br><span class=\"line\">      \"command\": \"check-process.rb -p cron\",</span><br><span class=\"line\">      \"standalone\": true,</span><br><span class=\"line\">      \"subscribers\": [</span><br><span class=\"line\">        \"production\"</span><br><span class=\"line\">      ],</span><br><span class=\"line\">      \"interval\": 60</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>收集cpu指标 metrics_cpu.json</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;checks&quot;: &#123;</span><br><span class=\"line\">    &quot;cpu_metrics&quot;: &#123;</span><br><span class=\"line\">      &quot;type&quot;: &quot;metric&quot;,</span><br><span class=\"line\">      &quot;command&quot;: &quot;metrics-cpu.rb&quot;,</span><br><span class=\"line\">      &quot;subscribers&quot;: [</span><br><span class=\"line\">        &quot;production&quot;</span><br><span class=\"line\">      ],</span><br><span class=\"line\">      &quot;interval&quot;: 10,</span><br><span class=\"line\">      &quot;handler&quot;: &quot;tcp_socket&quot;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>其中 “standalone”: true 表示允许client 单独调度运行，不需要server 调度。”type”: “metric” 表明这是指标类型的采集。</p>\n<p>在/var/log/sensu/sensu-client.log 、sensu-server.log、HTTP API 可以看到执行结果。后续可再对结果进行处理，需借助另外的插件<a href=\"https://github.com/sensu-plugins/sensu-plugins-graphite\" target=\"_blank\" rel=\"noopener\"><strong>sensu-plugins-graphite</strong></a> . </p>\n<p><em>采集一个指标展示，真累。。。</em></p>\n<p>接着通过redis查看cpu指标数据</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">redis-cli</span><br><span class=\"line\">keys * </span><br><span class=\"line\">get result:tick-client:cpu_metrics</span><br><span class=\"line\">\"&#123;\\\"type\\\":\\\"metric\\\",\\\"command\\\":\\\"metrics-cpu.rb\\\",\\\"subscribers\\\":[\\\"production\\\"],\\\"interval\\\":10,\\\"handler\\\":\\\"tcp_socket\\\",\\\"standalone\\\":true,\\\"name\\\":\\\"cpu_metrics\\\",\\\"issued\\\":1524824239,\\\"executed\\\":1524824239,\\\"duration\\\":0.073,\\\"output\\\":\\\"tick.cpu.total.user 8057 1524824240\\\\n...\\\",\\\"status\\\":0&#125;\"</span><br></pre></td></tr></table></figure>\n<p>也可以直接到脚本安装路径，运行ruby脚本，直接看看采集结果</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[root@tick bin]# /opt/sensu/embedded/bin/ruby  check-cpu.rb</span><br><span class=\"line\">CheckCPU TOTAL OK: total=0.25 user=0.0 nice=0.0 system=0.25 idle=99.75 iowait=0.0 irq=0.0 softirq=0.0 steal=0.0 guest=0.0 guest_nice=0.0</span><br></pre></td></tr></table></figure>\n<h3 id=\"Sensu-data-output\"><a href=\"#Sensu-data-output\" class=\"headerlink\" title=\"Sensu data output\"></a>Sensu data output</h3><p>Metric collection checks are used to collect measurements from server resources, services, and applications. Metric collection checks can output metric data in a variety of metric formats:</p>\n<ul>\n<li><a href=\"http://graphite.readthedocs.org/en/latest/feeding-carbon.html#the-plaintext-protocol\" target=\"_blank\" rel=\"noopener\">Graphite plaintext</a></li>\n<li><a href=\"http://nagios.sourceforge.net/docs/3_0/perfdata.html\" target=\"_blank\" rel=\"noopener\">Nagios Performance Data</a></li>\n<li><a href=\"http://opentsdb.net/docs/build/html/user_guide/writing.html\" target=\"_blank\" rel=\"noopener\">OpenTSDB</a></li>\n<li><a href=\"http://metrics20.org/spec/\" target=\"_blank\" rel=\"noopener\">Metrics 2.0 wire format</a></li>\n</ul>\n<p>The output produced by the check <code>command</code> ，貌似不同的插件返回不同的结果集。</p>\n<h3 id=\"Sensu-event-processor\"><a href=\"#Sensu-event-processor\" class=\"headerlink\" title=\"Sensu event processor\"></a>Sensu event processor</h3><p>The Sensu server provides a scalable event processor. Event processing involves conversion of <a href=\"https://sensuapp.org/docs/latest/reference/checks.html#check-results\" target=\"_blank\" rel=\"noopener\">check results</a> into Sensu events, and then applying any defined <a href=\"https://sensuapp.org/docs/latest/reference/filters.html\" target=\"_blank\" rel=\"noopener\">event filters</a>, <a href=\"https://sensuapp.org/docs/latest/reference/mutators.html\" target=\"_blank\" rel=\"noopener\">event data mutators</a>, and <a href=\"https://sensuapp.org/docs/latest/reference/handlers.html\" target=\"_blank\" rel=\"noopener\">event handlers</a>. All event processing happens on a Sensu server system.</p>\n<h3 id=\"Sensu-event-handlers\"><a href=\"#Sensu-event-handlers\" class=\"headerlink\" title=\"Sensu event handlers\"></a>Sensu event handlers</h3><p>Sensu event handlers are for taking action on <a href=\"https://sensuapp.org/docs/latest/reference/events.html\" target=\"_blank\" rel=\"noopener\">events</a> (produced by check results), such as sending an email alert, creating or resolving a PagerDuty incident, or storing metrics in Graphite. There are several types of handlers: <code>pipe</code>, <code>tcp</code>, <code>udp</code>, <code>transport</code>, and <code>set</code>.</p>\n<ul>\n<li><strong>Pipe handlers</strong> execute a command and pass the event data to the created process via <code>STDIN</code>.</li>\n<li><strong>TCP and UDP handlers</strong> send the event data to a remote socket.</li>\n<li><strong>Transport handlers</strong> publish the event data to the Sensu transport (by default, this is RabbitMQ).</li>\n<li><strong>Set handlers</strong> are used to group event handlers, making it easier to manage many event handlers.</li>\n</ul>\n<p>类似于告警。</p>\n<p>这里使用TCP handlers 作为验证，需安装nc 命令。开启两个终端，分别运行以下命令，一个是作为server端，监听6000端口，一个是客户端，往监听端口发送数据。</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">nc -l -k -4 -p 6000      </span><br><span class=\"line\">echo \"testing\" | nc localhost 6000</span><br></pre></td></tr></table></figure>\n<p>新建TCP 配置文件 /etc/sensu/conf.d/tcp_socket.json</p>\n<figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#123;</span><br><span class=\"line\">  <span class=\"string\">\"handlers\"</span>: &#123;</span><br><span class=\"line\">    <span class=\"string\">\"tcp_socket\"</span>: &#123;</span><br><span class=\"line\">      <span class=\"string\">\"type\"</span>: <span class=\"string\">\"tcp\"</span>,</span><br><span class=\"line\">      <span class=\"string\">\"socket\"</span>: &#123;</span><br><span class=\"line\">        <span class=\"string\">\"host\"</span>: <span class=\"string\">\"localhost\"</span>,</span><br><span class=\"line\">        <span class=\"string\">\"port\"</span>: 6000</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>注意，需修改对应的check 配置，把handler 修改为 <em>tcp_socket</em>  </p>\n<p>例如，cron 进程检测的异常情况，即进程不存在<img src=\"/img/check-error-event.png\" alt=\"Check Error Event\"></p>\n<h3 id=\"Sensu-API\"><a href=\"#Sensu-API\" class=\"headerlink\" title=\"Sensu API\"></a>Sensu API</h3><p>常用的API </p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">curl -s http://127.0.0.1:4567/clients | jq .</span><br><span class=\"line\">curl -s http://localhost:4567/events | jq .</span><br><span class=\"line\">curl -s http://localhost:4567/results | jq .</span><br><span class=\"line\">curl -s http://localhost:4567/client | jq .</span><br></pre></td></tr></table></figure>\n<h2 id=\"3-关注事项\"><a href=\"#3-关注事项\" class=\"headerlink\" title=\"3.关注事项\"></a>3.关注事项</h2><ol>\n<li><p>数据收集方式（Pull、Push）   </p>\n<p>默认都是Push，由Client发送采集数据到transport，服务端再去transport 读取数据。</p>\n</li>\n<li><p>数据格式</p>\n<p>check-result-cpu.json</p>\n</li>\n<li><p>数据存储方式</p>\n<p>数据存储在transport 中，可以固化到文件系统</p>\n</li>\n<li><p>数据输出方式</p>\n<p>checks 返回的结果是有一定格式的，其中，output字段存储的是对应的实际插件执行的命令输出结果。不同插件返回的命令结果不同。</p>\n</li>\n<li><p>agent 部署方式</p>\n<p>并未提供部署方案。</p>\n</li>\n<li><p>任务下发方式</p>\n<p>客户端配置任务。无法从通过sensu-server直接下发到客户端。</p>\n</li>\n</ol>\n","site":{"data":{}},"excerpt":"<h2 id=\"1-Sensu-安装部署\"><a href=\"#1-Sensu-安装部署\" class=\"headerlink\" title=\"1.Sensu 安装部署\"></a>1.Sensu 安装部署</h2><h3 id=\"主要特性\"><a href=\"#主要特性\" class=\"headerlink\" title=\"主要特性\"></a>主要特性</h3><p><a href=\"https://sensuapp.org/docs/1.2/overview/what-is-sensu.html\" target=\"_blank\" rel=\"noopener\">首页介绍</a></p>\n<p>Sensu was designed to provide a comprehensive monitoring platform for monitoring infrastructure (servers), services, application health, and business KPIs — and to collect and analyze custom metrics. </p>\n<p>主要包含以下功能与特性：</p>\n<ul>\n<li>检查系统、服务和程序的运行状态。</li>\n<li>基于分布式的设计，能够轻松的动态伸缩规模。</li>\n<li>支持通过插件的形式自定义检查的内容，拥有丰富的插件库。</li>\n<li>收集信息，获取被监控节点上的各项数据指标等。</li>\n<li>可视化的操作界面，提供实时的 GUI 用于显示和操作相关信息。</li>\n<li>内置的集成工具，可用于和其它系统集成，如 PagerDuty、Graphite、Email 等。</li>\n<li>提供丰富的 API 接口，支持通过 API 调用访问事件和客户端信息，触发检测等。</li>\n<li>加密的安全通信，支持各种复杂的网络拓扑。</li>\n</ul>\n<p>包括有类似我们拨测的功能，可对服务可用性进行检查，例如指定页面进行访问。</p>\n<p>结合其他软件：Sensu 从监控节点获取数据，将数据格式化成 Graphite 要求的格式，然后通过调用 Graphite 的接口将数据发给 Graphite。Graphite 将数据存储在时序数据中供 Grafana 绘图使用。最终用户通过在 Grafana 中<a href=\"http://www.liuhaihua.cn/archives/tag/private\" target=\"_blank\" rel=\"noopener\">定制</a>需要显示的数据及显示的方式，获得最终的可视化图表。</p>\n<hr>\n<h3 id=\"架构\"><a href=\"#架构\" class=\"headerlink\" title=\"架构\"></a>架构</h3><p>Sensu 由服务器、客户端、RabbitMQ、Redis 和 API 这五个部分构成。图 1 展示了这些组件之间的关系以及通信的数据流。如图所示，RabbitMQ 用于组件之间的通信，Redis 用于持久化 Sensu 服务器和 Sensu API 的数据。因为客户端都是通过文件进行配置，并且不需要在服务器端配置客户端的信息，所以可以很轻易的增加和减少客户端的数量。由于 Sensu 的服务器和 API 原生支持多节点部署，所以不存在效率的瓶颈问题。从图中可以看到，为了解耦服务器和客户端，通信都是通过 RabbitMQ 进行的。</p>\n<p><img src=\"/img/sensu组成部分.png\" alt=\"组成部分\"></p>\n<p>核心代码由Ruby编写。<a href=\"https://github.com/sensu/sensu\" target=\"_blank\" rel=\"noopener\">Github地址</a></p>\n<p>实际安装中，不一定要使用RabbitMq作为消息中间件。直接使用redis即可。以下安装使用redis。</p>","more":"<h3 id=\"安装部署\"><a href=\"#安装部署\" class=\"headerlink\" title=\"安装部署\"></a>安装部署</h3><p>server/clent 支持平台有</p>\n<p><img src=\"/img/sensu-support-platforms.png\" alt=\"支持平台\"></p>\n<p>Server安装部署：</p>\n<p>安装配置yum源 /etc/yum.repos.d/sensu.repo</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[sensu]</span><br><span class=\"line\">name=sensu</span><br><span class=\"line\">baseurl=https://sensu.global.ssl.fastly.net/yum/$releasever/$basearch/</span><br><span class=\"line\">gpgcheck=0</span><br><span class=\"line\">enabled=1</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">yum install redis -y</span><br><span class=\"line\">yum install sensu -y</span><br></pre></td></tr></table></figure>\n<p>sensu server 配置 /etc/sensu/config.json</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;transport&quot;: &#123;</span><br><span class=\"line\">    &quot;name&quot;: &quot;redis&quot;</span><br><span class=\"line\">  &#125;,</span><br><span class=\"line\">  &quot;api&quot;: &#123;</span><br><span class=\"line\">    &quot;host&quot;: &quot;127.0.0.1&quot;,</span><br><span class=\"line\">    &quot;port&quot;: 4567</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>sensu client/server配置transport.redis</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#123;</span><br><span class=\"line\">  \"redis\":</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">       \"host\": \"127.0.0.1\",</span><br><span class=\"line\">       \"port\": 6379</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>sensu client 配置 /etc/sensu/conf.d/client.json</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;client&quot;: &#123;</span><br><span class=\"line\">    &quot;name&quot;: &quot;rhel-client&quot;,</span><br><span class=\"line\">    &quot;address&quot;: &quot;127.0.0.1&quot;,</span><br><span class=\"line\">    &quot;environment&quot;: &quot;development&quot;,</span><br><span class=\"line\">    &quot;subscriptions&quot;: [</span><br><span class=\"line\">      &quot;dev&quot;,</span><br><span class=\"line\">      &quot;rhel-hosts&quot;</span><br><span class=\"line\">    ],</span><br><span class=\"line\">    &quot;socket&quot;: &#123;</span><br><span class=\"line\">      &quot;bind&quot;: &quot;127.0.0.1&quot;,</span><br><span class=\"line\">      &quot;port&quot;: 3030</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">｝</span><br></pre></td></tr></table></figure>\n<p>sensu dashboard配置 /etc/sensu/dashboard.json</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;sensu&quot;: [</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">      &quot;name&quot;: &quot;sensu&quot;,</span><br><span class=\"line\">      &quot;host&quot;: &quot;127.0.0.1&quot;,</span><br><span class=\"line\">      &quot;port&quot;: 4567</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  ],</span><br><span class=\"line\">  &quot;dashboard&quot;: &#123;</span><br><span class=\"line\">    &quot;host&quot;: &quot;0.0.0.0&quot;,</span><br><span class=\"line\">    &quot;port&quot;: 3000</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>启动进程</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">systemctl enable sensu-&#123;server,api,client&#125; </span><br><span class=\"line\">systemctl start sensu-&#123;server,api,client&#125;</span><br><span class=\"line\">systemctl restart sensu-&#123;server,api,client&#125;</span><br></pre></td></tr></table></figure>\n<p>uchiwa 图形展示界面，需要额外安装。sensu提供了两个版本的展示界面，一个是企业版，一个就是uchiwa 。这里使用uchiwa，<a href=\"https://uchiwa.io/#/download\" target=\"_blank\" rel=\"noopener\">下载链接</a>。 下载完成后，使用yum命令安装</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">yum -y localinstall uchiwa-1.1.1-1.x86_64.rpm</span><br><span class=\"line\">systemctl enable uchiwa</span><br><span class=\"line\">systemctl start uchiwa</span><br></pre></td></tr></table></figure>\n<p>配置好 uchiwa  /etc/sensu/uchiwa.json</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#123;</span><br><span class=\"line\">  \"sensu\": [</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">      \"name\": \"Sensu-Server-1\",</span><br><span class=\"line\">      \"host\": \"127.0.0.1\",</span><br><span class=\"line\">      \"port\": 4567,</span><br><span class=\"line\">      \"timeout\": 10</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  ],</span><br><span class=\"line\">  \"uchiwa\": &#123;</span><br><span class=\"line\">    \"host\": \"0.0.0.0\",</span><br><span class=\"line\">    \"port\": 3000,</span><br><span class=\"line\">    \"refresh\": 10</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p><a href=\"http://192.168.14.165:3000\" target=\"_blank\" rel=\"noopener\">URL</a></p>\n<p><img src=\"/img/uchiwa.png\" alt=\"图例\"></p>\n<p>uchiwa 只是简单的展示功能，并不能对client等进行增删操作，即相当于对Sensu-API 只实现了Get相关的功能</p>\n<p><a href=\"https://sensuapp.org/docs/1.2/api/overview.html\" target=\"_blank\" rel=\"noopener\">HTTP API用例</a></p>\n<h3 id=\"部署过程中遇到的问题\"><a href=\"#部署过程中遇到的问题\" class=\"headerlink\" title=\"部署过程中遇到的问题\"></a>部署过程中遇到的问题</h3><ul>\n<li>uchiwa无法展示client和server，大部分都是json配置文件没有配置清楚导致。可查看日志/var/log/uchiwa，/var/log/sensu/sensu-client.log</li>\n</ul>\n<h2 id=\"2-Sensu-使用\"><a href=\"#2-Sensu-使用\" class=\"headerlink\" title=\"2.Sensu 使用\"></a>2.Sensu 使用</h2><h3 id=\"Sensu-Client\"><a href=\"#Sensu-Client\" class=\"headerlink\" title=\"Sensu Client\"></a>Sensu Client</h3><p>Sensu Client 就是采集客户端agent。</p>\n<p><a href=\"https://blog.csdn.net/houzhe_adore/article/details/48518825\" target=\"_blank\" rel=\"noopener\">介绍博文</a></p>\n<h3 id=\"Proxy-Clients\"><a href=\"#Proxy-Clients\" class=\"headerlink\" title=\"Proxy Clients\"></a>Proxy Clients</h3><h3 id=\"Sensu-Check\"><a href=\"#Sensu-Check\" class=\"headerlink\" title=\"Sensu Check\"></a>Sensu Check</h3><p>Sensu checks allow you to monitor server resources, services, and application health, as well as collect &amp; analyze metrics; they are executed on servers running the Sensu client. Sensu checks allow you to monitor server resources, services, and application health, as well as collect &amp; analyze metrics; they are executed on servers running the Sensu client. </p>\n<p>首先安装check插件</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sensu-install -p process-checks\t</span><br><span class=\"line\">sensu-install -p cpu-checks</span><br></pre></td></tr></table></figure>\n<p>插件是ruby脚本，安装后会在/opt/sensu/embedded/bin/ 路径下下载安装rubu解释器和check插件。process-checks 是进程检测插件，cpu-checks 是cpu检测指标插件。</p>\n<p><a href=\"https://github.com/sensu-plugins/sensu-plugins-process-checks\" target=\"_blank\" rel=\"noopener\">github插件脚本</a></p>\n<p>配置check 服务端路径 /etc/sensu/conf.d/</p>\n<p>检查sensu进程个数 check_cron.json</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#123;</span><br><span class=\"line\">  \"checks\": &#123;</span><br><span class=\"line\">    \"cron\": &#123;</span><br><span class=\"line\">      \"command\": \"check-process.rb -p cron\",</span><br><span class=\"line\">      \"standalone\": true,</span><br><span class=\"line\">      \"subscribers\": [</span><br><span class=\"line\">        \"production\"</span><br><span class=\"line\">      ],</span><br><span class=\"line\">      \"interval\": 60</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>收集cpu指标 metrics_cpu.json</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;checks&quot;: &#123;</span><br><span class=\"line\">    &quot;cpu_metrics&quot;: &#123;</span><br><span class=\"line\">      &quot;type&quot;: &quot;metric&quot;,</span><br><span class=\"line\">      &quot;command&quot;: &quot;metrics-cpu.rb&quot;,</span><br><span class=\"line\">      &quot;subscribers&quot;: [</span><br><span class=\"line\">        &quot;production&quot;</span><br><span class=\"line\">      ],</span><br><span class=\"line\">      &quot;interval&quot;: 10,</span><br><span class=\"line\">      &quot;handler&quot;: &quot;tcp_socket&quot;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>其中 “standalone”: true 表示允许client 单独调度运行，不需要server 调度。”type”: “metric” 表明这是指标类型的采集。</p>\n<p>在/var/log/sensu/sensu-client.log 、sensu-server.log、HTTP API 可以看到执行结果。后续可再对结果进行处理，需借助另外的插件<a href=\"https://github.com/sensu-plugins/sensu-plugins-graphite\" target=\"_blank\" rel=\"noopener\"><strong>sensu-plugins-graphite</strong></a> . </p>\n<p><em>采集一个指标展示，真累。。。</em></p>\n<p>接着通过redis查看cpu指标数据</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">redis-cli</span><br><span class=\"line\">keys * </span><br><span class=\"line\">get result:tick-client:cpu_metrics</span><br><span class=\"line\">\"&#123;\\\"type\\\":\\\"metric\\\",\\\"command\\\":\\\"metrics-cpu.rb\\\",\\\"subscribers\\\":[\\\"production\\\"],\\\"interval\\\":10,\\\"handler\\\":\\\"tcp_socket\\\",\\\"standalone\\\":true,\\\"name\\\":\\\"cpu_metrics\\\",\\\"issued\\\":1524824239,\\\"executed\\\":1524824239,\\\"duration\\\":0.073,\\\"output\\\":\\\"tick.cpu.total.user 8057 1524824240\\\\n...\\\",\\\"status\\\":0&#125;\"</span><br></pre></td></tr></table></figure>\n<p>也可以直接到脚本安装路径，运行ruby脚本，直接看看采集结果</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[root@tick bin]# /opt/sensu/embedded/bin/ruby  check-cpu.rb</span><br><span class=\"line\">CheckCPU TOTAL OK: total=0.25 user=0.0 nice=0.0 system=0.25 idle=99.75 iowait=0.0 irq=0.0 softirq=0.0 steal=0.0 guest=0.0 guest_nice=0.0</span><br></pre></td></tr></table></figure>\n<h3 id=\"Sensu-data-output\"><a href=\"#Sensu-data-output\" class=\"headerlink\" title=\"Sensu data output\"></a>Sensu data output</h3><p>Metric collection checks are used to collect measurements from server resources, services, and applications. Metric collection checks can output metric data in a variety of metric formats:</p>\n<ul>\n<li><a href=\"http://graphite.readthedocs.org/en/latest/feeding-carbon.html#the-plaintext-protocol\" target=\"_blank\" rel=\"noopener\">Graphite plaintext</a></li>\n<li><a href=\"http://nagios.sourceforge.net/docs/3_0/perfdata.html\" target=\"_blank\" rel=\"noopener\">Nagios Performance Data</a></li>\n<li><a href=\"http://opentsdb.net/docs/build/html/user_guide/writing.html\" target=\"_blank\" rel=\"noopener\">OpenTSDB</a></li>\n<li><a href=\"http://metrics20.org/spec/\" target=\"_blank\" rel=\"noopener\">Metrics 2.0 wire format</a></li>\n</ul>\n<p>The output produced by the check <code>command</code> ，貌似不同的插件返回不同的结果集。</p>\n<h3 id=\"Sensu-event-processor\"><a href=\"#Sensu-event-processor\" class=\"headerlink\" title=\"Sensu event processor\"></a>Sensu event processor</h3><p>The Sensu server provides a scalable event processor. Event processing involves conversion of <a href=\"https://sensuapp.org/docs/latest/reference/checks.html#check-results\" target=\"_blank\" rel=\"noopener\">check results</a> into Sensu events, and then applying any defined <a href=\"https://sensuapp.org/docs/latest/reference/filters.html\" target=\"_blank\" rel=\"noopener\">event filters</a>, <a href=\"https://sensuapp.org/docs/latest/reference/mutators.html\" target=\"_blank\" rel=\"noopener\">event data mutators</a>, and <a href=\"https://sensuapp.org/docs/latest/reference/handlers.html\" target=\"_blank\" rel=\"noopener\">event handlers</a>. All event processing happens on a Sensu server system.</p>\n<h3 id=\"Sensu-event-handlers\"><a href=\"#Sensu-event-handlers\" class=\"headerlink\" title=\"Sensu event handlers\"></a>Sensu event handlers</h3><p>Sensu event handlers are for taking action on <a href=\"https://sensuapp.org/docs/latest/reference/events.html\" target=\"_blank\" rel=\"noopener\">events</a> (produced by check results), such as sending an email alert, creating or resolving a PagerDuty incident, or storing metrics in Graphite. There are several types of handlers: <code>pipe</code>, <code>tcp</code>, <code>udp</code>, <code>transport</code>, and <code>set</code>.</p>\n<ul>\n<li><strong>Pipe handlers</strong> execute a command and pass the event data to the created process via <code>STDIN</code>.</li>\n<li><strong>TCP and UDP handlers</strong> send the event data to a remote socket.</li>\n<li><strong>Transport handlers</strong> publish the event data to the Sensu transport (by default, this is RabbitMQ).</li>\n<li><strong>Set handlers</strong> are used to group event handlers, making it easier to manage many event handlers.</li>\n</ul>\n<p>类似于告警。</p>\n<p>这里使用TCP handlers 作为验证，需安装nc 命令。开启两个终端，分别运行以下命令，一个是作为server端，监听6000端口，一个是客户端，往监听端口发送数据。</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">nc -l -k -4 -p 6000      </span><br><span class=\"line\">echo \"testing\" | nc localhost 6000</span><br></pre></td></tr></table></figure>\n<p>新建TCP 配置文件 /etc/sensu/conf.d/tcp_socket.json</p>\n<figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#123;</span><br><span class=\"line\">  <span class=\"string\">\"handlers\"</span>: &#123;</span><br><span class=\"line\">    <span class=\"string\">\"tcp_socket\"</span>: &#123;</span><br><span class=\"line\">      <span class=\"string\">\"type\"</span>: <span class=\"string\">\"tcp\"</span>,</span><br><span class=\"line\">      <span class=\"string\">\"socket\"</span>: &#123;</span><br><span class=\"line\">        <span class=\"string\">\"host\"</span>: <span class=\"string\">\"localhost\"</span>,</span><br><span class=\"line\">        <span class=\"string\">\"port\"</span>: 6000</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>注意，需修改对应的check 配置，把handler 修改为 <em>tcp_socket</em>  </p>\n<p>例如，cron 进程检测的异常情况，即进程不存在<img src=\"/img/check-error-event.png\" alt=\"Check Error Event\"></p>\n<h3 id=\"Sensu-API\"><a href=\"#Sensu-API\" class=\"headerlink\" title=\"Sensu API\"></a>Sensu API</h3><p>常用的API </p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">curl -s http://127.0.0.1:4567/clients | jq .</span><br><span class=\"line\">curl -s http://localhost:4567/events | jq .</span><br><span class=\"line\">curl -s http://localhost:4567/results | jq .</span><br><span class=\"line\">curl -s http://localhost:4567/client | jq .</span><br></pre></td></tr></table></figure>\n<h2 id=\"3-关注事项\"><a href=\"#3-关注事项\" class=\"headerlink\" title=\"3.关注事项\"></a>3.关注事项</h2><ol>\n<li><p>数据收集方式（Pull、Push）   </p>\n<p>默认都是Push，由Client发送采集数据到transport，服务端再去transport 读取数据。</p>\n</li>\n<li><p>数据格式</p>\n<p>check-result-cpu.json</p>\n</li>\n<li><p>数据存储方式</p>\n<p>数据存储在transport 中，可以固化到文件系统</p>\n</li>\n<li><p>数据输出方式</p>\n<p>checks 返回的结果是有一定格式的，其中，output字段存储的是对应的实际插件执行的命令输出结果。不同插件返回的命令结果不同。</p>\n</li>\n<li><p>agent 部署方式</p>\n<p>并未提供部署方案。</p>\n</li>\n<li><p>任务下发方式</p>\n<p>客户端配置任务。无法从通过sensu-server直接下发到客户端。</p>\n</li>\n</ol>"},{"title":"炒股的智慧 --陈江挺","date":"2015-06-12T15:34:28.000Z","_content":"\n# 本书大纲\n>* 第一章就谈些股市的特性及它对人性的挑战。 \n* 第二章讲一些和炒股最直接相关的知识。读者应带着两个疑问来读这一章： 一，什么是影响股价的因素；二， 股票在什么情况下运动正常。 这一章包括三个部分； 基本分析、 技术分析及股票的大市。 当这三部分的分析都给你下面信号的时候，是你在股市胜算最大的时候。 \n* 第三章谈成功的要素。它告诉你炒股成功应做到什么及怎样做， 还告诉你应该具备什么样的心理素质。 知道什么该做并不难， 难在怎么在实践中做好它。 \n* 第四章是何时买卖股票。在这一章你看不到“ 低点买入要谨慎， 高点卖出不要贪” 之类的废话，什么时候才是低？ 高到多高才是高？ 在办公室空想出来的炒股绝招没有什么实用价值。 ***买卖股票的要点在于怎么寻找“临界点”。*** 希望这章能改变你炒股的整个思维方式。 \n* 第五章是华尔街的家训。牛顿说：“我所以能看得更远， 是因为我站在巨人的肩膀上。” 在这一章， 让我们看看炒股这行的先辈们给我们留下了什么经验。 \n* 第六章谈心理建设。人性中根深蒂固的恐惧、 希望、 贪婪影响着我们所做的每一个决定， 使我们常常做不到自己知道应该做的事。 要完全克服人性中的弱点是很困难的， 但我们首先必须知道它是什么及什么是正确的做法。 \n* 第七章分析了什么是大机会及其特点。 读完这一章你就明白了我在谈什么。 这里不只是在谈股票。  \n* 第八章是和炒手谈谈天。在这一章我简述了我的学股历程。 如果人性共通的说法不错的话，你学股走过的道路应和我相似。 希望你在学股的挣扎过程中因为有了路标而能显得平顺一些。  \n* 附录：在今天的社会，我看到很多人为金钱不择手段。 而这本又是教人怎样赚钱的书， 不加点“金 钱的反思”让我觉得这本书不够均衡。 如果因为这一附录能使读者对人生有更进一步的认识，我会觉得努力没有白费。\n\n# 精彩语句\n>* 买卖股票的要点在于怎么寻找“临界点”。 \n* 截短亏损，让利润奔跑！ \n* 关注股票的正常运动、不正常运动和周期运动\n* 有理有据：理就是获胜的概率，据在于你知道怎样找临界点。\n* 不要心存幻想，必须牢牢记住： 股市从来都不会错，它总是走自己要走的路，会错的只有人自己。\n* 正确地感悟股票运动何时正常是最难学的，这也是炒股成功最关键的技巧之一。随着经验的增加，你的悟性越来越好，对股票运动的判断力越来越强，你就能将每次入场的获胜概率从 50%提高到 60%，甚至 70%，慢慢地你就成了炒股专家。要准备\"熬\"。 \n* 你的资金总是在盈利机会最大的时候才留在场内。--何时卖，等重新突破前高，再重新买入，而不是抄底！！！\n* 你如果能够做到仅在临界买点入场，临界卖点出场，入场时牢记止损，并注意分摊风险，你的成功的概率就能提至最高，你也就真正成为炒股专家了。\n<!-- more -->\n\n# 人性的弱点\n好获小利，不吃小亏\n人性讨厌风险\n人好自以为是\n人好跟风  --必须自己建立规则， 并完全由自己为这些规则 所产生的后果负责\n人好因循守旧\n人好报复\n\n# 技术分析\n* 走势图（上升、下降、无势图）\n* 支撑线和阻力线\n* 双肩图和头肩图，倒双肩和倒头肩\n* 均线\n* 综合看图\n\n# 炒股成功的基本要诀\n* 保本，止损，分批建仓\n* 不断盈利，反例:读者们若有机会到赌场看看， 就明白股民们为什么会这样做。 赌客们站在赌台旁， 一注都不肯放过， 生怕下一手就是自己赢钱的机会。 直到输完才会收手。\n* 赚大钱\n>为什么失去赚大钱的机会：\n\t一， 人好小便宜;\n\t二， 不够经验判断股票运动是否正常\n\t另一点要强调的是： 如果你确定股票运动正常， 你的胜算很大， 这时你应该在这只股票上适 当加大下注的比重。 \n* 资金管理，怎样下注\n> 输得起多少\n提高获胜的概率\n抓住正常运动中的股票，参考例子\n注意危险信号，突然大起大落\n\n\n# 正常运动与不正常运动的例子\n>* 如果股票开始一个上升的走势， 比如说准备从 20 元升到 50 元。 你将发现走势开始的前几\n天， 交易量突然增加， 股价开始攀升， 几天以后， 升势停止， 开始下跌。 这是“正常” 的获利回 吐， 下\n跌时的交易量较上升时显著减少。 读者朋友， 这个下调是很正常的， 属正常运动， 千 万不要在这个时候\n将股票卖掉。 如果这只股票具有上冲的潜力， 你会发现在几天内股票又开 始活跃， 交易量又开始增加，\n上次“自然调低” 所失去的“领土” 应在短时间内收复， 股票 开始冲到新的高度。 这次运动将持续一段\n时间， 其间每日通常是收盘价高过开盘价， 偶尔收 盘价低过开盘价， 其差额通常不大； 交易量也不会有\n显著的变化， 通常情况是减少。 或迟或 早， 股票又会开始下跌， 这是新一轮的“ 获利回吐” 。 这次获利\n回吐的股票运动和交易量的特 点应该和第一次很相似。 上述是股票走升势的正常运动。 下面的图显示出\n它们应有的特点。 如果读者觉得还很抽象的话， 我以下用数字来描述一遍， 因为掌握股票正常运动的\n特点对炒 股成功是极其重要的。 在股票 20 元的时候， 交易量增加， 可以是平时的一两倍， 股价从 20 元\n升到 21， 22， 甚至 27 元。 这几天的交易量较前段时间显著增加是其特征。 到 27 元时升势 可能停顿，\n随之开始下调， 股价走势为 27->26->24 元。 这段时间的交易量应较从 20 升到 27 元时的平均交易量为\n少， 即股票从 20->27 元时买盘大过卖盘， 从 27->24 元时的卖盘大过买 盘。 但这如果是正常的升势，\n从 20->27->24 这段时间总的买盘大过卖盘。 在 24 元徘徊几天 后， 股票应开始上升， 交易量又开始增\n加， 这次上冲应很快超过 27 元， 股价走势为 24->25->27->35 元。 当股票冲到 35 元时， 又开始停顿，\n随之下调， 重复第一阶段的运动。 这次下调同样应有交易量减少的特点。\n一只正常运动的股票， 每次上冲的强度通常较上一次更为猛烈。 以我们这个例子， 当股票再 次冲破 35\n元时， 应很容易地直冲到 45 元或 50 元， 其间不会感觉到有很大障碍。 反调是正常 的。 一位专业炒手\n决不能在正常回调的时候被吓出场。 入场后开始有利润了， 每次回调都意 味着纸上利润的减少。 常人的\n第一感觉就是赶快卖掉获利。 这是新手的显著特点。 这也是新 手很难在股市赚到大钱的原因。\n炒手的任务不仅仅在于确认何时股票运动正常， 同样重要的是要能认识股票何时运动不正常。 股票\n这种前进两步、 退后一步的过程可能延续一段时间， 有时可能是很长的一段时间。 这段时间炒手或许在精\n神上会松懈下来， 这是要不得的。 因为往往在你松懈下来的时候， 股票的 运动发生变化。 股票缓缓地周\n期性上升， 突然有一天， 股票狂涨， 从 50 元一下子升到 55 元， 第二天升到 62 元， 这两天交易量突\n然大增， 但在第二天收市的前半小时股票从 62 元跌回 58 元收盘。 第三天开市一样强劲， 股票一下子升\n到 64 元， 但第四天， 股票似乎失去了冲劲， 它 跌回了 61 元（ 如下图）。\n\n\n>* 我是这样理解这一现象的： 股票运动后面的主要力量是大户， 这些大户通常是手握巨资的基金或保险\n公司等。 当这些基金的管理人看好这只股票， 开始吸纳， 你将看到交易量上升， 股 价上升。 通常这些基\n金管理人在吸纳股票时， 并不希望引起大众的注意， 所以这一过程通常 是缓慢的， 它不会上报纸或电视\n的头条。 一旦这些大户吸股完毕， 你通常会听到他们开始公 开地推荐这些股票， 引起大众的注意。 这些\n大户们的信息通常较普通人灵通， 他们也会很快 看到公司有好消息公布， 如开发成功新产品， 盈利较预\n期为好等。 等到这些大户觉得好消息 已全部反映在股价之中， 他们开始准备脱手。 由于他们手中握有的\n股票数量通常很大， 如果 一下子砸进市场， 市场根本承受不了， 他们手中的股票很大部分只能在低价出\n手。 为了解决 这一问题， 他们要找傻瓜用高价来承接这些股票。 找傻瓜的最佳途径就是令股价暴升， 暴\n升 时通常伴随点好消息， 如公司任命新董事长了， 某证券商强力推荐该股票了等等。 报纸电视 整天重复\n这些消息， 引发小股民峰拥入市。 想想看， 小股民因股价暴升而引发贪念入市时买 的股票都是谁卖的？\n一旦大户找足傻瓜， 全身而退， 还去哪里找大买主把股价继续推高？ 你现在明白了股票的正常运动及\n危险信号后面的理由了吗？ 我一直强调， 炒股是艺术， 不是 科学。 当不正常的信号灯亮起， 是否表示这\n只股票就一定要下跌？ 答案是：“否！” 没有人能 够在任何时候百分之百地肯定股票明天会怎么样， 也许\n暴升的理由是公司真的发明了长生不 老药！ 你必须将股票的运动和公司的发展综合起来考虑。 让我重复\n一次： 炒股的最基本信条 是在任何时候， 你手上持有股票的上升潜力必须大过下跌的可能， 否则你就不\n应留在手里。 看到危险信号， 表示你的获胜概率此时已不超出 50%。 每位严肃的炒手都必须注意危\n险信号， 但问题是在内心深处总有些奇怪的力量使他们在该卖 出的时候提不起勇气这么做。 或许是侥幸\n心理在作怪。 在迟疑的这段时间， 他们常看到股票 又跌了好多点， 这时他们会拍着脑袋， 大骂自己傻瓜，\n同时发誓一旦股票一有反弹就走人， “股票跌了这么多， 该会有个小反弹吧？ ” 但反弹来到的时候， 他\n们又忘记了自己的誓言， 因为在他们眼睛里， 这时股票的运动又开始“正常” 起来。 通常， 这样的反弹\n仅是股票在下 跌路上的喘气， 它很快就要继续要走的路。 人性有很多缺陷。 人希望股票会怎么运动， 认\n定 股票会怎么运动， 当股票的运动和预想不符合时， 就会认为股市错了， 自己没有错。 但炒友 们必须牢\n牢记住： 股市从来都不会错， 它总是走自己要走的路， 会错的只有人自己。 你所能做 的只有追随股市。\n见到危险信号， 不要三心二意， 不要存有幻想， 把股票全部脱手。 几天以 后， 也许一切又恢复正常， 你\n一样可以重新入场。 如果能这样做， 你将发现你为自己省下了很多焦虑及学费。\n\n\n# 3-3 成功投资者所具有的共性\n>要有成为投资专家的欲望\n必须具备锲而不舍的精神\n要有“ 与股市斗， 其乐无穷” 的气派\n要甘于做孤独者\n必须具有耐心和自制力\n**必须有一套适合自己的炒股模式**\n必须具有超前的想像力及对未来的判断\n成功的投资者绝不幻想\n要有应用知识的毅力   \n\n一位成功的投资者， 他应十分留意怎样将他的知识应用在炒股中， 他不会为应用这些知识的 枯燥而\n忽略细节。 在日常生活中， 获得知识通常并不困难， 困难在于用毅力应用这些知识。 在炒股问题上， 我\n是坚信“知易行难” 之说的。\n\n\n# 4 何时买卖股票\n学习寻找临界点的过程其实就是学股的过程， 当然其中还包括学习炒股的正确心态。华尔街将炒股的诀巧归纳成两句话： 截短亏损，让利润奔跑！\n关于人性的思考，自我的突破 \n## 4.1 何时买 ！！！ 一定要多读透彻\n>如何选股\n选股之后，如何进场\n如何设置止损点\n## 4.2 何时卖 ！！！ 一定要多读透彻\n何时卖股票的考虑可以分成两部分： 一， 刚进股时怎样选止损点； 二， 有利润后怎样选择合 适的卖点获利。 \n**买今天在上涨的股票：我自己喜欢把止损点定在入市当天的最低点。 比如我今天以 10.75 元买 进股票， 今天的最高价是 11 元， 最低价是 10 元， 我便以 10 元作为止损点。 以我的经验， 如 果我的入场点选的正确，股票开始上升， 它不应跌回我当天入场的最低点。**\n\n>庄家的把戏\n大户们操纵股票其实就是那么几招， 你只要专心， 观察股票的运动和交易量的变 化， 想像你是大户的话会怎么调动公众的心理， 大户的花招其实明显的很。 \n讲白了， 他们想 买进的进修要么静悄悄地， 要么想法引起大众的恐慌性抛售， 前者你会看到交易量增加， 但不明显， 股价慢慢地一步步升高， 后者便是搞一些大家公认的好卖点。\n大户想卖的时候，要么先买进， 造成股价狂升，引发股民的贪念去抢搭轿子， 要么就搞一些公众们共认的好买点。 \n由于他们通常手握巨资， 要做到这些并不困难。便他们的动作必须会从股价的变动及交易量 的变动中露出尾巴，只要你有足够的经验， 你就明白怎样跟着玩。\n\n请再读一遍何时买股票， 何时卖股票这两节， 再体会一下“截短亏损， 让利润奔跑” 这句话， 炒股的诀窍尽在其中。\n\n\n# 8 和炒手们聊天\n## 8.1 学股的四个阶段\n>第四阶段久赌必赢\n一个可行的计划， 不能凭空想像， 它必须有理有据。“理” 就是数学的概率， 如果你每次下注的赢面\n超过 50%， 而且你只下本金的小部分， 不会为几次坏运气就剃光头， 从长期而言你是 胜定了。 道理和开\n赌场一样。“据” 在于你知道怎样找临界点， 在长期的观察和实践过程中， 你知道这些点是出入场的关键\n点， 在这些点操作， 你的赢面超出 50%， 再加上应用“截短亏 损， 让利润奔跑” 的原则， 赢时赢大的，\n亏时亏小的， 你的获胜概率其实远远超出了 50%。 到久赌必赢阶段， 你不应对亏钱和赚钱有任何情绪\n上的波动。 你对止损不再痛苦， 你明白这 是游戏的一部分， 你对赚钱也不再喜悦， 你知道这是必然结果。\n你不再将胜负放在心上， 你 只注重在正确的时间， 做正确的事情。 你知道利润会随之而来。","source":"_posts/wisdom-in-stocks.md","raw":"---\ntitle: 炒股的智慧 --陈江挺\ndate: 2015-06-12 23:34:28\n---\n\n# 本书大纲\n>* 第一章就谈些股市的特性及它对人性的挑战。 \n* 第二章讲一些和炒股最直接相关的知识。读者应带着两个疑问来读这一章： 一，什么是影响股价的因素；二， 股票在什么情况下运动正常。 这一章包括三个部分； 基本分析、 技术分析及股票的大市。 当这三部分的分析都给你下面信号的时候，是你在股市胜算最大的时候。 \n* 第三章谈成功的要素。它告诉你炒股成功应做到什么及怎样做， 还告诉你应该具备什么样的心理素质。 知道什么该做并不难， 难在怎么在实践中做好它。 \n* 第四章是何时买卖股票。在这一章你看不到“ 低点买入要谨慎， 高点卖出不要贪” 之类的废话，什么时候才是低？ 高到多高才是高？ 在办公室空想出来的炒股绝招没有什么实用价值。 ***买卖股票的要点在于怎么寻找“临界点”。*** 希望这章能改变你炒股的整个思维方式。 \n* 第五章是华尔街的家训。牛顿说：“我所以能看得更远， 是因为我站在巨人的肩膀上。” 在这一章， 让我们看看炒股这行的先辈们给我们留下了什么经验。 \n* 第六章谈心理建设。人性中根深蒂固的恐惧、 希望、 贪婪影响着我们所做的每一个决定， 使我们常常做不到自己知道应该做的事。 要完全克服人性中的弱点是很困难的， 但我们首先必须知道它是什么及什么是正确的做法。 \n* 第七章分析了什么是大机会及其特点。 读完这一章你就明白了我在谈什么。 这里不只是在谈股票。  \n* 第八章是和炒手谈谈天。在这一章我简述了我的学股历程。 如果人性共通的说法不错的话，你学股走过的道路应和我相似。 希望你在学股的挣扎过程中因为有了路标而能显得平顺一些。  \n* 附录：在今天的社会，我看到很多人为金钱不择手段。 而这本又是教人怎样赚钱的书， 不加点“金 钱的反思”让我觉得这本书不够均衡。 如果因为这一附录能使读者对人生有更进一步的认识，我会觉得努力没有白费。\n\n# 精彩语句\n>* 买卖股票的要点在于怎么寻找“临界点”。 \n* 截短亏损，让利润奔跑！ \n* 关注股票的正常运动、不正常运动和周期运动\n* 有理有据：理就是获胜的概率，据在于你知道怎样找临界点。\n* 不要心存幻想，必须牢牢记住： 股市从来都不会错，它总是走自己要走的路，会错的只有人自己。\n* 正确地感悟股票运动何时正常是最难学的，这也是炒股成功最关键的技巧之一。随着经验的增加，你的悟性越来越好，对股票运动的判断力越来越强，你就能将每次入场的获胜概率从 50%提高到 60%，甚至 70%，慢慢地你就成了炒股专家。要准备\"熬\"。 \n* 你的资金总是在盈利机会最大的时候才留在场内。--何时卖，等重新突破前高，再重新买入，而不是抄底！！！\n* 你如果能够做到仅在临界买点入场，临界卖点出场，入场时牢记止损，并注意分摊风险，你的成功的概率就能提至最高，你也就真正成为炒股专家了。\n<!-- more -->\n\n# 人性的弱点\n好获小利，不吃小亏\n人性讨厌风险\n人好自以为是\n人好跟风  --必须自己建立规则， 并完全由自己为这些规则 所产生的后果负责\n人好因循守旧\n人好报复\n\n# 技术分析\n* 走势图（上升、下降、无势图）\n* 支撑线和阻力线\n* 双肩图和头肩图，倒双肩和倒头肩\n* 均线\n* 综合看图\n\n# 炒股成功的基本要诀\n* 保本，止损，分批建仓\n* 不断盈利，反例:读者们若有机会到赌场看看， 就明白股民们为什么会这样做。 赌客们站在赌台旁， 一注都不肯放过， 生怕下一手就是自己赢钱的机会。 直到输完才会收手。\n* 赚大钱\n>为什么失去赚大钱的机会：\n\t一， 人好小便宜;\n\t二， 不够经验判断股票运动是否正常\n\t另一点要强调的是： 如果你确定股票运动正常， 你的胜算很大， 这时你应该在这只股票上适 当加大下注的比重。 \n* 资金管理，怎样下注\n> 输得起多少\n提高获胜的概率\n抓住正常运动中的股票，参考例子\n注意危险信号，突然大起大落\n\n\n# 正常运动与不正常运动的例子\n>* 如果股票开始一个上升的走势， 比如说准备从 20 元升到 50 元。 你将发现走势开始的前几\n天， 交易量突然增加， 股价开始攀升， 几天以后， 升势停止， 开始下跌。 这是“正常” 的获利回 吐， 下\n跌时的交易量较上升时显著减少。 读者朋友， 这个下调是很正常的， 属正常运动， 千 万不要在这个时候\n将股票卖掉。 如果这只股票具有上冲的潜力， 你会发现在几天内股票又开 始活跃， 交易量又开始增加，\n上次“自然调低” 所失去的“领土” 应在短时间内收复， 股票 开始冲到新的高度。 这次运动将持续一段\n时间， 其间每日通常是收盘价高过开盘价， 偶尔收 盘价低过开盘价， 其差额通常不大； 交易量也不会有\n显著的变化， 通常情况是减少。 或迟或 早， 股票又会开始下跌， 这是新一轮的“ 获利回吐” 。 这次获利\n回吐的股票运动和交易量的特 点应该和第一次很相似。 上述是股票走升势的正常运动。 下面的图显示出\n它们应有的特点。 如果读者觉得还很抽象的话， 我以下用数字来描述一遍， 因为掌握股票正常运动的\n特点对炒 股成功是极其重要的。 在股票 20 元的时候， 交易量增加， 可以是平时的一两倍， 股价从 20 元\n升到 21， 22， 甚至 27 元。 这几天的交易量较前段时间显著增加是其特征。 到 27 元时升势 可能停顿，\n随之开始下调， 股价走势为 27->26->24 元。 这段时间的交易量应较从 20 升到 27 元时的平均交易量为\n少， 即股票从 20->27 元时买盘大过卖盘， 从 27->24 元时的卖盘大过买 盘。 但这如果是正常的升势，\n从 20->27->24 这段时间总的买盘大过卖盘。 在 24 元徘徊几天 后， 股票应开始上升， 交易量又开始增\n加， 这次上冲应很快超过 27 元， 股价走势为 24->25->27->35 元。 当股票冲到 35 元时， 又开始停顿，\n随之下调， 重复第一阶段的运动。 这次下调同样应有交易量减少的特点。\n一只正常运动的股票， 每次上冲的强度通常较上一次更为猛烈。 以我们这个例子， 当股票再 次冲破 35\n元时， 应很容易地直冲到 45 元或 50 元， 其间不会感觉到有很大障碍。 反调是正常 的。 一位专业炒手\n决不能在正常回调的时候被吓出场。 入场后开始有利润了， 每次回调都意 味着纸上利润的减少。 常人的\n第一感觉就是赶快卖掉获利。 这是新手的显著特点。 这也是新 手很难在股市赚到大钱的原因。\n炒手的任务不仅仅在于确认何时股票运动正常， 同样重要的是要能认识股票何时运动不正常。 股票\n这种前进两步、 退后一步的过程可能延续一段时间， 有时可能是很长的一段时间。 这段时间炒手或许在精\n神上会松懈下来， 这是要不得的。 因为往往在你松懈下来的时候， 股票的 运动发生变化。 股票缓缓地周\n期性上升， 突然有一天， 股票狂涨， 从 50 元一下子升到 55 元， 第二天升到 62 元， 这两天交易量突\n然大增， 但在第二天收市的前半小时股票从 62 元跌回 58 元收盘。 第三天开市一样强劲， 股票一下子升\n到 64 元， 但第四天， 股票似乎失去了冲劲， 它 跌回了 61 元（ 如下图）。\n\n\n>* 我是这样理解这一现象的： 股票运动后面的主要力量是大户， 这些大户通常是手握巨资的基金或保险\n公司等。 当这些基金的管理人看好这只股票， 开始吸纳， 你将看到交易量上升， 股 价上升。 通常这些基\n金管理人在吸纳股票时， 并不希望引起大众的注意， 所以这一过程通常 是缓慢的， 它不会上报纸或电视\n的头条。 一旦这些大户吸股完毕， 你通常会听到他们开始公 开地推荐这些股票， 引起大众的注意。 这些\n大户们的信息通常较普通人灵通， 他们也会很快 看到公司有好消息公布， 如开发成功新产品， 盈利较预\n期为好等。 等到这些大户觉得好消息 已全部反映在股价之中， 他们开始准备脱手。 由于他们手中握有的\n股票数量通常很大， 如果 一下子砸进市场， 市场根本承受不了， 他们手中的股票很大部分只能在低价出\n手。 为了解决 这一问题， 他们要找傻瓜用高价来承接这些股票。 找傻瓜的最佳途径就是令股价暴升， 暴\n升 时通常伴随点好消息， 如公司任命新董事长了， 某证券商强力推荐该股票了等等。 报纸电视 整天重复\n这些消息， 引发小股民峰拥入市。 想想看， 小股民因股价暴升而引发贪念入市时买 的股票都是谁卖的？\n一旦大户找足傻瓜， 全身而退， 还去哪里找大买主把股价继续推高？ 你现在明白了股票的正常运动及\n危险信号后面的理由了吗？ 我一直强调， 炒股是艺术， 不是 科学。 当不正常的信号灯亮起， 是否表示这\n只股票就一定要下跌？ 答案是：“否！” 没有人能 够在任何时候百分之百地肯定股票明天会怎么样， 也许\n暴升的理由是公司真的发明了长生不 老药！ 你必须将股票的运动和公司的发展综合起来考虑。 让我重复\n一次： 炒股的最基本信条 是在任何时候， 你手上持有股票的上升潜力必须大过下跌的可能， 否则你就不\n应留在手里。 看到危险信号， 表示你的获胜概率此时已不超出 50%。 每位严肃的炒手都必须注意危\n险信号， 但问题是在内心深处总有些奇怪的力量使他们在该卖 出的时候提不起勇气这么做。 或许是侥幸\n心理在作怪。 在迟疑的这段时间， 他们常看到股票 又跌了好多点， 这时他们会拍着脑袋， 大骂自己傻瓜，\n同时发誓一旦股票一有反弹就走人， “股票跌了这么多， 该会有个小反弹吧？ ” 但反弹来到的时候， 他\n们又忘记了自己的誓言， 因为在他们眼睛里， 这时股票的运动又开始“正常” 起来。 通常， 这样的反弹\n仅是股票在下 跌路上的喘气， 它很快就要继续要走的路。 人性有很多缺陷。 人希望股票会怎么运动， 认\n定 股票会怎么运动， 当股票的运动和预想不符合时， 就会认为股市错了， 自己没有错。 但炒友 们必须牢\n牢记住： 股市从来都不会错， 它总是走自己要走的路， 会错的只有人自己。 你所能做 的只有追随股市。\n见到危险信号， 不要三心二意， 不要存有幻想， 把股票全部脱手。 几天以 后， 也许一切又恢复正常， 你\n一样可以重新入场。 如果能这样做， 你将发现你为自己省下了很多焦虑及学费。\n\n\n# 3-3 成功投资者所具有的共性\n>要有成为投资专家的欲望\n必须具备锲而不舍的精神\n要有“ 与股市斗， 其乐无穷” 的气派\n要甘于做孤独者\n必须具有耐心和自制力\n**必须有一套适合自己的炒股模式**\n必须具有超前的想像力及对未来的判断\n成功的投资者绝不幻想\n要有应用知识的毅力   \n\n一位成功的投资者， 他应十分留意怎样将他的知识应用在炒股中， 他不会为应用这些知识的 枯燥而\n忽略细节。 在日常生活中， 获得知识通常并不困难， 困难在于用毅力应用这些知识。 在炒股问题上， 我\n是坚信“知易行难” 之说的。\n\n\n# 4 何时买卖股票\n学习寻找临界点的过程其实就是学股的过程， 当然其中还包括学习炒股的正确心态。华尔街将炒股的诀巧归纳成两句话： 截短亏损，让利润奔跑！\n关于人性的思考，自我的突破 \n## 4.1 何时买 ！！！ 一定要多读透彻\n>如何选股\n选股之后，如何进场\n如何设置止损点\n## 4.2 何时卖 ！！！ 一定要多读透彻\n何时卖股票的考虑可以分成两部分： 一， 刚进股时怎样选止损点； 二， 有利润后怎样选择合 适的卖点获利。 \n**买今天在上涨的股票：我自己喜欢把止损点定在入市当天的最低点。 比如我今天以 10.75 元买 进股票， 今天的最高价是 11 元， 最低价是 10 元， 我便以 10 元作为止损点。 以我的经验， 如 果我的入场点选的正确，股票开始上升， 它不应跌回我当天入场的最低点。**\n\n>庄家的把戏\n大户们操纵股票其实就是那么几招， 你只要专心， 观察股票的运动和交易量的变 化， 想像你是大户的话会怎么调动公众的心理， 大户的花招其实明显的很。 \n讲白了， 他们想 买进的进修要么静悄悄地， 要么想法引起大众的恐慌性抛售， 前者你会看到交易量增加， 但不明显， 股价慢慢地一步步升高， 后者便是搞一些大家公认的好卖点。\n大户想卖的时候，要么先买进， 造成股价狂升，引发股民的贪念去抢搭轿子， 要么就搞一些公众们共认的好买点。 \n由于他们通常手握巨资， 要做到这些并不困难。便他们的动作必须会从股价的变动及交易量 的变动中露出尾巴，只要你有足够的经验， 你就明白怎样跟着玩。\n\n请再读一遍何时买股票， 何时卖股票这两节， 再体会一下“截短亏损， 让利润奔跑” 这句话， 炒股的诀窍尽在其中。\n\n\n# 8 和炒手们聊天\n## 8.1 学股的四个阶段\n>第四阶段久赌必赢\n一个可行的计划， 不能凭空想像， 它必须有理有据。“理” 就是数学的概率， 如果你每次下注的赢面\n超过 50%， 而且你只下本金的小部分， 不会为几次坏运气就剃光头， 从长期而言你是 胜定了。 道理和开\n赌场一样。“据” 在于你知道怎样找临界点， 在长期的观察和实践过程中， 你知道这些点是出入场的关键\n点， 在这些点操作， 你的赢面超出 50%， 再加上应用“截短亏 损， 让利润奔跑” 的原则， 赢时赢大的，\n亏时亏小的， 你的获胜概率其实远远超出了 50%。 到久赌必赢阶段， 你不应对亏钱和赚钱有任何情绪\n上的波动。 你对止损不再痛苦， 你明白这 是游戏的一部分， 你对赚钱也不再喜悦， 你知道这是必然结果。\n你不再将胜负放在心上， 你 只注重在正确的时间， 做正确的事情。 你知道利润会随之而来。","slug":"wisdom-in-stocks","published":1,"updated":"2018-04-27T04:11:03.370Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjhajczpf001ce4rwe15yopm9","content":"<h1 id=\"本书大纲\"><a href=\"#本书大纲\" class=\"headerlink\" title=\"本书大纲\"></a>本书大纲</h1><blockquote>\n<ul>\n<li>第一章就谈些股市的特性及它对人性的挑战。 </li>\n<li>第二章讲一些和炒股最直接相关的知识。读者应带着两个疑问来读这一章： 一，什么是影响股价的因素；二， 股票在什么情况下运动正常。 这一章包括三个部分； 基本分析、 技术分析及股票的大市。 当这三部分的分析都给你下面信号的时候，是你在股市胜算最大的时候。 </li>\n<li>第三章谈成功的要素。它告诉你炒股成功应做到什么及怎样做， 还告诉你应该具备什么样的心理素质。 知道什么该做并不难， 难在怎么在实践中做好它。 </li>\n<li>第四章是何时买卖股票。在这一章你看不到“ 低点买入要谨慎， 高点卖出不要贪” 之类的废话，什么时候才是低？ 高到多高才是高？ 在办公室空想出来的炒股绝招没有什么实用价值。 <strong><em>买卖股票的要点在于怎么寻找“临界点”。</em></strong> 希望这章能改变你炒股的整个思维方式。 </li>\n<li>第五章是华尔街的家训。牛顿说：“我所以能看得更远， 是因为我站在巨人的肩膀上。” 在这一章， 让我们看看炒股这行的先辈们给我们留下了什么经验。 </li>\n<li>第六章谈心理建设。人性中根深蒂固的恐惧、 希望、 贪婪影响着我们所做的每一个决定， 使我们常常做不到自己知道应该做的事。 要完全克服人性中的弱点是很困难的， 但我们首先必须知道它是什么及什么是正确的做法。 </li>\n<li>第七章分析了什么是大机会及其特点。 读完这一章你就明白了我在谈什么。 这里不只是在谈股票。  </li>\n<li>第八章是和炒手谈谈天。在这一章我简述了我的学股历程。 如果人性共通的说法不错的话，你学股走过的道路应和我相似。 希望你在学股的挣扎过程中因为有了路标而能显得平顺一些。  </li>\n<li>附录：在今天的社会，我看到很多人为金钱不择手段。 而这本又是教人怎样赚钱的书， 不加点“金 钱的反思”让我觉得这本书不够均衡。 如果因为这一附录能使读者对人生有更进一步的认识，我会觉得努力没有白费。</li>\n</ul>\n</blockquote>\n<h1 id=\"精彩语句\"><a href=\"#精彩语句\" class=\"headerlink\" title=\"精彩语句\"></a>精彩语句</h1><blockquote>\n<ul>\n<li>买卖股票的要点在于怎么寻找“临界点”。 </li>\n<li>截短亏损，让利润奔跑！ </li>\n<li>关注股票的正常运动、不正常运动和周期运动</li>\n<li>有理有据：理就是获胜的概率，据在于你知道怎样找临界点。</li>\n<li>不要心存幻想，必须牢牢记住： 股市从来都不会错，它总是走自己要走的路，会错的只有人自己。</li>\n<li>正确地感悟股票运动何时正常是最难学的，这也是炒股成功最关键的技巧之一。随着经验的增加，你的悟性越来越好，对股票运动的判断力越来越强，你就能将每次入场的获胜概率从 50%提高到 60%，甚至 70%，慢慢地你就成了炒股专家。要准备”熬”。 </li>\n<li>你的资金总是在盈利机会最大的时候才留在场内。–何时卖，等重新突破前高，再重新买入，而不是抄底！！！</li>\n<li>你如果能够做到仅在临界买点入场，临界卖点出场，入场时牢记止损，并注意分摊风险，你的成功的概率就能提至最高，你也就真正成为炒股专家了。<a id=\"more\"></a>\n</li>\n</ul>\n</blockquote>\n<h1 id=\"人性的弱点\"><a href=\"#人性的弱点\" class=\"headerlink\" title=\"人性的弱点\"></a>人性的弱点</h1><p>好获小利，不吃小亏<br>人性讨厌风险<br>人好自以为是<br>人好跟风  –必须自己建立规则， 并完全由自己为这些规则 所产生的后果负责<br>人好因循守旧<br>人好报复</p>\n<h1 id=\"技术分析\"><a href=\"#技术分析\" class=\"headerlink\" title=\"技术分析\"></a>技术分析</h1><ul>\n<li>走势图（上升、下降、无势图）</li>\n<li>支撑线和阻力线</li>\n<li>双肩图和头肩图，倒双肩和倒头肩</li>\n<li>均线</li>\n<li>综合看图</li>\n</ul>\n<h1 id=\"炒股成功的基本要诀\"><a href=\"#炒股成功的基本要诀\" class=\"headerlink\" title=\"炒股成功的基本要诀\"></a>炒股成功的基本要诀</h1><ul>\n<li>保本，止损，分批建仓</li>\n<li>不断盈利，反例:读者们若有机会到赌场看看， 就明白股民们为什么会这样做。 赌客们站在赌台旁， 一注都不肯放过， 生怕下一手就是自己赢钱的机会。 直到输完才会收手。</li>\n<li>赚大钱<blockquote>\n<p>为什么失去赚大钱的机会：<br>  一， 人好小便宜;<br>  二， 不够经验判断股票运动是否正常<br>  另一点要强调的是： 如果你确定股票运动正常， 你的胜算很大， 这时你应该在这只股票上适 当加大下注的比重。 </p>\n</blockquote>\n</li>\n<li>资金管理，怎样下注<blockquote>\n<p>输得起多少<br>提高获胜的概率<br>抓住正常运动中的股票，参考例子<br>注意危险信号，突然大起大落</p>\n</blockquote>\n</li>\n</ul>\n<h1 id=\"正常运动与不正常运动的例子\"><a href=\"#正常运动与不正常运动的例子\" class=\"headerlink\" title=\"正常运动与不正常运动的例子\"></a>正常运动与不正常运动的例子</h1><blockquote>\n<ul>\n<li>如果股票开始一个上升的走势， 比如说准备从 20 元升到 50 元。 你将发现走势开始的前几<br>天， 交易量突然增加， 股价开始攀升， 几天以后， 升势停止， 开始下跌。 这是“正常” 的获利回 吐， 下<br>跌时的交易量较上升时显著减少。 读者朋友， 这个下调是很正常的， 属正常运动， 千 万不要在这个时候<br>将股票卖掉。 如果这只股票具有上冲的潜力， 你会发现在几天内股票又开 始活跃， 交易量又开始增加，<br>上次“自然调低” 所失去的“领土” 应在短时间内收复， 股票 开始冲到新的高度。 这次运动将持续一段<br>时间， 其间每日通常是收盘价高过开盘价， 偶尔收 盘价低过开盘价， 其差额通常不大； 交易量也不会有<br>显著的变化， 通常情况是减少。 或迟或 早， 股票又会开始下跌， 这是新一轮的“ 获利回吐” 。 这次获利<br>回吐的股票运动和交易量的特 点应该和第一次很相似。 上述是股票走升势的正常运动。 下面的图显示出<br>它们应有的特点。 如果读者觉得还很抽象的话， 我以下用数字来描述一遍， 因为掌握股票正常运动的<br>特点对炒 股成功是极其重要的。 在股票 20 元的时候， 交易量增加， 可以是平时的一两倍， 股价从 20 元<br>升到 21， 22， 甚至 27 元。 这几天的交易量较前段时间显著增加是其特征。 到 27 元时升势 可能停顿，<br>随之开始下调， 股价走势为 27-&gt;26-&gt;24 元。 这段时间的交易量应较从 20 升到 27 元时的平均交易量为<br>少， 即股票从 20-&gt;27 元时买盘大过卖盘， 从 27-&gt;24 元时的卖盘大过买 盘。 但这如果是正常的升势，<br>从 20-&gt;27-&gt;24 这段时间总的买盘大过卖盘。 在 24 元徘徊几天 后， 股票应开始上升， 交易量又开始增<br>加， 这次上冲应很快超过 27 元， 股价走势为 24-&gt;25-&gt;27-&gt;35 元。 当股票冲到 35 元时， 又开始停顿，<br>随之下调， 重复第一阶段的运动。 这次下调同样应有交易量减少的特点。<br>一只正常运动的股票， 每次上冲的强度通常较上一次更为猛烈。 以我们这个例子， 当股票再 次冲破 35<br>元时， 应很容易地直冲到 45 元或 50 元， 其间不会感觉到有很大障碍。 反调是正常 的。 一位专业炒手<br>决不能在正常回调的时候被吓出场。 入场后开始有利润了， 每次回调都意 味着纸上利润的减少。 常人的<br>第一感觉就是赶快卖掉获利。 这是新手的显著特点。 这也是新 手很难在股市赚到大钱的原因。<br>炒手的任务不仅仅在于确认何时股票运动正常， 同样重要的是要能认识股票何时运动不正常。 股票<br>这种前进两步、 退后一步的过程可能延续一段时间， 有时可能是很长的一段时间。 这段时间炒手或许在精<br>神上会松懈下来， 这是要不得的。 因为往往在你松懈下来的时候， 股票的 运动发生变化。 股票缓缓地周<br>期性上升， 突然有一天， 股票狂涨， 从 50 元一下子升到 55 元， 第二天升到 62 元， 这两天交易量突<br>然大增， 但在第二天收市的前半小时股票从 62 元跌回 58 元收盘。 第三天开市一样强劲， 股票一下子升<br>到 64 元， 但第四天， 股票似乎失去了冲劲， 它 跌回了 61 元（ 如下图）。</li>\n</ul>\n<ul>\n<li>我是这样理解这一现象的： 股票运动后面的主要力量是大户， 这些大户通常是手握巨资的基金或保险<br>公司等。 当这些基金的管理人看好这只股票， 开始吸纳， 你将看到交易量上升， 股 价上升。 通常这些基<br>金管理人在吸纳股票时， 并不希望引起大众的注意， 所以这一过程通常 是缓慢的， 它不会上报纸或电视<br>的头条。 一旦这些大户吸股完毕， 你通常会听到他们开始公 开地推荐这些股票， 引起大众的注意。 这些<br>大户们的信息通常较普通人灵通， 他们也会很快 看到公司有好消息公布， 如开发成功新产品， 盈利较预<br>期为好等。 等到这些大户觉得好消息 已全部反映在股价之中， 他们开始准备脱手。 由于他们手中握有的<br>股票数量通常很大， 如果 一下子砸进市场， 市场根本承受不了， 他们手中的股票很大部分只能在低价出<br>手。 为了解决 这一问题， 他们要找傻瓜用高价来承接这些股票。 找傻瓜的最佳途径就是令股价暴升， 暴<br>升 时通常伴随点好消息， 如公司任命新董事长了， 某证券商强力推荐该股票了等等。 报纸电视 整天重复<br>这些消息， 引发小股民峰拥入市。 想想看， 小股民因股价暴升而引发贪念入市时买 的股票都是谁卖的？<br>一旦大户找足傻瓜， 全身而退， 还去哪里找大买主把股价继续推高？ 你现在明白了股票的正常运动及<br>危险信号后面的理由了吗？ 我一直强调， 炒股是艺术， 不是 科学。 当不正常的信号灯亮起， 是否表示这<br>只股票就一定要下跌？ 答案是：“否！” 没有人能 够在任何时候百分之百地肯定股票明天会怎么样， 也许<br>暴升的理由是公司真的发明了长生不 老药！ 你必须将股票的运动和公司的发展综合起来考虑。 让我重复<br>一次： 炒股的最基本信条 是在任何时候， 你手上持有股票的上升潜力必须大过下跌的可能， 否则你就不<br>应留在手里。 看到危险信号， 表示你的获胜概率此时已不超出 50%。 每位严肃的炒手都必须注意危<br>险信号， 但问题是在内心深处总有些奇怪的力量使他们在该卖 出的时候提不起勇气这么做。 或许是侥幸<br>心理在作怪。 在迟疑的这段时间， 他们常看到股票 又跌了好多点， 这时他们会拍着脑袋， 大骂自己傻瓜，<br>同时发誓一旦股票一有反弹就走人， “股票跌了这么多， 该会有个小反弹吧？ ” 但反弹来到的时候， 他<br>们又忘记了自己的誓言， 因为在他们眼睛里， 这时股票的运动又开始“正常” 起来。 通常， 这样的反弹<br>仅是股票在下 跌路上的喘气， 它很快就要继续要走的路。 人性有很多缺陷。 人希望股票会怎么运动， 认<br>定 股票会怎么运动， 当股票的运动和预想不符合时， 就会认为股市错了， 自己没有错。 但炒友 们必须牢<br>牢记住： 股市从来都不会错， 它总是走自己要走的路， 会错的只有人自己。 你所能做 的只有追随股市。<br>见到危险信号， 不要三心二意， 不要存有幻想， 把股票全部脱手。 几天以 后， 也许一切又恢复正常， 你<br>一样可以重新入场。 如果能这样做， 你将发现你为自己省下了很多焦虑及学费。</li>\n</ul>\n</blockquote>\n<h1 id=\"3-3-成功投资者所具有的共性\"><a href=\"#3-3-成功投资者所具有的共性\" class=\"headerlink\" title=\"3-3 成功投资者所具有的共性\"></a>3-3 成功投资者所具有的共性</h1><blockquote>\n<p>要有成为投资专家的欲望<br>必须具备锲而不舍的精神<br>要有“ 与股市斗， 其乐无穷” 的气派<br>要甘于做孤独者<br>必须具有耐心和自制力<br><strong>必须有一套适合自己的炒股模式</strong><br>必须具有超前的想像力及对未来的判断<br>成功的投资者绝不幻想<br>要有应用知识的毅力   </p>\n</blockquote>\n<p>一位成功的投资者， 他应十分留意怎样将他的知识应用在炒股中， 他不会为应用这些知识的 枯燥而<br>忽略细节。 在日常生活中， 获得知识通常并不困难， 困难在于用毅力应用这些知识。 在炒股问题上， 我<br>是坚信“知易行难” 之说的。</p>\n<h1 id=\"4-何时买卖股票\"><a href=\"#4-何时买卖股票\" class=\"headerlink\" title=\"4 何时买卖股票\"></a>4 何时买卖股票</h1><p>学习寻找临界点的过程其实就是学股的过程， 当然其中还包括学习炒股的正确心态。华尔街将炒股的诀巧归纳成两句话： 截短亏损，让利润奔跑！<br>关于人性的思考，自我的突破 </p>\n<h2 id=\"4-1-何时买-！！！-一定要多读透彻\"><a href=\"#4-1-何时买-！！！-一定要多读透彻\" class=\"headerlink\" title=\"4.1 何时买 ！！！ 一定要多读透彻\"></a>4.1 何时买 ！！！ 一定要多读透彻</h2><blockquote>\n<p>如何选股<br>选股之后，如何进场<br>如何设置止损点</p>\n<h2 id=\"4-2-何时卖-！！！-一定要多读透彻\"><a href=\"#4-2-何时卖-！！！-一定要多读透彻\" class=\"headerlink\" title=\"4.2 何时卖 ！！！ 一定要多读透彻\"></a>4.2 何时卖 ！！！ 一定要多读透彻</h2><p>何时卖股票的考虑可以分成两部分： 一， 刚进股时怎样选止损点； 二， 有利润后怎样选择合 适的卖点获利。<br><strong>买今天在上涨的股票：我自己喜欢把止损点定在入市当天的最低点。 比如我今天以 10.75 元买 进股票， 今天的最高价是 11 元， 最低价是 10 元， 我便以 10 元作为止损点。 以我的经验， 如 果我的入场点选的正确，股票开始上升， 它不应跌回我当天入场的最低点。</strong></p>\n<p>庄家的把戏<br>大户们操纵股票其实就是那么几招， 你只要专心， 观察股票的运动和交易量的变 化， 想像你是大户的话会怎么调动公众的心理， 大户的花招其实明显的很。<br>讲白了， 他们想 买进的进修要么静悄悄地， 要么想法引起大众的恐慌性抛售， 前者你会看到交易量增加， 但不明显， 股价慢慢地一步步升高， 后者便是搞一些大家公认的好卖点。<br>大户想卖的时候，要么先买进， 造成股价狂升，引发股民的贪念去抢搭轿子， 要么就搞一些公众们共认的好买点。<br>由于他们通常手握巨资， 要做到这些并不困难。便他们的动作必须会从股价的变动及交易量 的变动中露出尾巴，只要你有足够的经验， 你就明白怎样跟着玩。</p>\n</blockquote>\n<p>请再读一遍何时买股票， 何时卖股票这两节， 再体会一下“截短亏损， 让利润奔跑” 这句话， 炒股的诀窍尽在其中。</p>\n<h1 id=\"8-和炒手们聊天\"><a href=\"#8-和炒手们聊天\" class=\"headerlink\" title=\"8 和炒手们聊天\"></a>8 和炒手们聊天</h1><h2 id=\"8-1-学股的四个阶段\"><a href=\"#8-1-学股的四个阶段\" class=\"headerlink\" title=\"8.1 学股的四个阶段\"></a>8.1 学股的四个阶段</h2><blockquote>\n<p>第四阶段久赌必赢<br>一个可行的计划， 不能凭空想像， 它必须有理有据。“理” 就是数学的概率， 如果你每次下注的赢面<br>超过 50%， 而且你只下本金的小部分， 不会为几次坏运气就剃光头， 从长期而言你是 胜定了。 道理和开<br>赌场一样。“据” 在于你知道怎样找临界点， 在长期的观察和实践过程中， 你知道这些点是出入场的关键<br>点， 在这些点操作， 你的赢面超出 50%， 再加上应用“截短亏 损， 让利润奔跑” 的原则， 赢时赢大的，<br>亏时亏小的， 你的获胜概率其实远远超出了 50%。 到久赌必赢阶段， 你不应对亏钱和赚钱有任何情绪<br>上的波动。 你对止损不再痛苦， 你明白这 是游戏的一部分， 你对赚钱也不再喜悦， 你知道这是必然结果。<br>你不再将胜负放在心上， 你 只注重在正确的时间， 做正确的事情。 你知道利润会随之而来。</p>\n</blockquote>\n","site":{"data":{}},"excerpt":"<h1 id=\"本书大纲\"><a href=\"#本书大纲\" class=\"headerlink\" title=\"本书大纲\"></a>本书大纲</h1><blockquote>\n<ul>\n<li>第一章就谈些股市的特性及它对人性的挑战。 </li>\n<li>第二章讲一些和炒股最直接相关的知识。读者应带着两个疑问来读这一章： 一，什么是影响股价的因素；二， 股票在什么情况下运动正常。 这一章包括三个部分； 基本分析、 技术分析及股票的大市。 当这三部分的分析都给你下面信号的时候，是你在股市胜算最大的时候。 </li>\n<li>第三章谈成功的要素。它告诉你炒股成功应做到什么及怎样做， 还告诉你应该具备什么样的心理素质。 知道什么该做并不难， 难在怎么在实践中做好它。 </li>\n<li>第四章是何时买卖股票。在这一章你看不到“ 低点买入要谨慎， 高点卖出不要贪” 之类的废话，什么时候才是低？ 高到多高才是高？ 在办公室空想出来的炒股绝招没有什么实用价值。 <strong><em>买卖股票的要点在于怎么寻找“临界点”。</em></strong> 希望这章能改变你炒股的整个思维方式。 </li>\n<li>第五章是华尔街的家训。牛顿说：“我所以能看得更远， 是因为我站在巨人的肩膀上。” 在这一章， 让我们看看炒股这行的先辈们给我们留下了什么经验。 </li>\n<li>第六章谈心理建设。人性中根深蒂固的恐惧、 希望、 贪婪影响着我们所做的每一个决定， 使我们常常做不到自己知道应该做的事。 要完全克服人性中的弱点是很困难的， 但我们首先必须知道它是什么及什么是正确的做法。 </li>\n<li>第七章分析了什么是大机会及其特点。 读完这一章你就明白了我在谈什么。 这里不只是在谈股票。  </li>\n<li>第八章是和炒手谈谈天。在这一章我简述了我的学股历程。 如果人性共通的说法不错的话，你学股走过的道路应和我相似。 希望你在学股的挣扎过程中因为有了路标而能显得平顺一些。  </li>\n<li>附录：在今天的社会，我看到很多人为金钱不择手段。 而这本又是教人怎样赚钱的书， 不加点“金 钱的反思”让我觉得这本书不够均衡。 如果因为这一附录能使读者对人生有更进一步的认识，我会觉得努力没有白费。</li>\n</ul>\n</blockquote>\n<h1 id=\"精彩语句\"><a href=\"#精彩语句\" class=\"headerlink\" title=\"精彩语句\"></a>精彩语句</h1><blockquote>\n<ul>\n<li>买卖股票的要点在于怎么寻找“临界点”。 </li>\n<li>截短亏损，让利润奔跑！ </li>\n<li>关注股票的正常运动、不正常运动和周期运动</li>\n<li>有理有据：理就是获胜的概率，据在于你知道怎样找临界点。</li>\n<li>不要心存幻想，必须牢牢记住： 股市从来都不会错，它总是走自己要走的路，会错的只有人自己。</li>\n<li>正确地感悟股票运动何时正常是最难学的，这也是炒股成功最关键的技巧之一。随着经验的增加，你的悟性越来越好，对股票运动的判断力越来越强，你就能将每次入场的获胜概率从 50%提高到 60%，甚至 70%，慢慢地你就成了炒股专家。要准备”熬”。 </li>\n<li>你的资金总是在盈利机会最大的时候才留在场内。–何时卖，等重新突破前高，再重新买入，而不是抄底！！！</li>\n<li>你如果能够做到仅在临界买点入场，临界卖点出场，入场时牢记止损，并注意分摊风险，你的成功的概率就能提至最高，你也就真正成为炒股专家了。","more":"</li>\n</ul>\n</blockquote>\n<h1 id=\"人性的弱点\"><a href=\"#人性的弱点\" class=\"headerlink\" title=\"人性的弱点\"></a>人性的弱点</h1><p>好获小利，不吃小亏<br>人性讨厌风险<br>人好自以为是<br>人好跟风  –必须自己建立规则， 并完全由自己为这些规则 所产生的后果负责<br>人好因循守旧<br>人好报复</p>\n<h1 id=\"技术分析\"><a href=\"#技术分析\" class=\"headerlink\" title=\"技术分析\"></a>技术分析</h1><ul>\n<li>走势图（上升、下降、无势图）</li>\n<li>支撑线和阻力线</li>\n<li>双肩图和头肩图，倒双肩和倒头肩</li>\n<li>均线</li>\n<li>综合看图</li>\n</ul>\n<h1 id=\"炒股成功的基本要诀\"><a href=\"#炒股成功的基本要诀\" class=\"headerlink\" title=\"炒股成功的基本要诀\"></a>炒股成功的基本要诀</h1><ul>\n<li>保本，止损，分批建仓</li>\n<li>不断盈利，反例:读者们若有机会到赌场看看， 就明白股民们为什么会这样做。 赌客们站在赌台旁， 一注都不肯放过， 生怕下一手就是自己赢钱的机会。 直到输完才会收手。</li>\n<li>赚大钱<blockquote>\n<p>为什么失去赚大钱的机会：<br>  一， 人好小便宜;<br>  二， 不够经验判断股票运动是否正常<br>  另一点要强调的是： 如果你确定股票运动正常， 你的胜算很大， 这时你应该在这只股票上适 当加大下注的比重。 </p>\n</blockquote>\n</li>\n<li>资金管理，怎样下注<blockquote>\n<p>输得起多少<br>提高获胜的概率<br>抓住正常运动中的股票，参考例子<br>注意危险信号，突然大起大落</p>\n</blockquote>\n</li>\n</ul>\n<h1 id=\"正常运动与不正常运动的例子\"><a href=\"#正常运动与不正常运动的例子\" class=\"headerlink\" title=\"正常运动与不正常运动的例子\"></a>正常运动与不正常运动的例子</h1><blockquote>\n<ul>\n<li>如果股票开始一个上升的走势， 比如说准备从 20 元升到 50 元。 你将发现走势开始的前几<br>天， 交易量突然增加， 股价开始攀升， 几天以后， 升势停止， 开始下跌。 这是“正常” 的获利回 吐， 下<br>跌时的交易量较上升时显著减少。 读者朋友， 这个下调是很正常的， 属正常运动， 千 万不要在这个时候<br>将股票卖掉。 如果这只股票具有上冲的潜力， 你会发现在几天内股票又开 始活跃， 交易量又开始增加，<br>上次“自然调低” 所失去的“领土” 应在短时间内收复， 股票 开始冲到新的高度。 这次运动将持续一段<br>时间， 其间每日通常是收盘价高过开盘价， 偶尔收 盘价低过开盘价， 其差额通常不大； 交易量也不会有<br>显著的变化， 通常情况是减少。 或迟或 早， 股票又会开始下跌， 这是新一轮的“ 获利回吐” 。 这次获利<br>回吐的股票运动和交易量的特 点应该和第一次很相似。 上述是股票走升势的正常运动。 下面的图显示出<br>它们应有的特点。 如果读者觉得还很抽象的话， 我以下用数字来描述一遍， 因为掌握股票正常运动的<br>特点对炒 股成功是极其重要的。 在股票 20 元的时候， 交易量增加， 可以是平时的一两倍， 股价从 20 元<br>升到 21， 22， 甚至 27 元。 这几天的交易量较前段时间显著增加是其特征。 到 27 元时升势 可能停顿，<br>随之开始下调， 股价走势为 27-&gt;26-&gt;24 元。 这段时间的交易量应较从 20 升到 27 元时的平均交易量为<br>少， 即股票从 20-&gt;27 元时买盘大过卖盘， 从 27-&gt;24 元时的卖盘大过买 盘。 但这如果是正常的升势，<br>从 20-&gt;27-&gt;24 这段时间总的买盘大过卖盘。 在 24 元徘徊几天 后， 股票应开始上升， 交易量又开始增<br>加， 这次上冲应很快超过 27 元， 股价走势为 24-&gt;25-&gt;27-&gt;35 元。 当股票冲到 35 元时， 又开始停顿，<br>随之下调， 重复第一阶段的运动。 这次下调同样应有交易量减少的特点。<br>一只正常运动的股票， 每次上冲的强度通常较上一次更为猛烈。 以我们这个例子， 当股票再 次冲破 35<br>元时， 应很容易地直冲到 45 元或 50 元， 其间不会感觉到有很大障碍。 反调是正常 的。 一位专业炒手<br>决不能在正常回调的时候被吓出场。 入场后开始有利润了， 每次回调都意 味着纸上利润的减少。 常人的<br>第一感觉就是赶快卖掉获利。 这是新手的显著特点。 这也是新 手很难在股市赚到大钱的原因。<br>炒手的任务不仅仅在于确认何时股票运动正常， 同样重要的是要能认识股票何时运动不正常。 股票<br>这种前进两步、 退后一步的过程可能延续一段时间， 有时可能是很长的一段时间。 这段时间炒手或许在精<br>神上会松懈下来， 这是要不得的。 因为往往在你松懈下来的时候， 股票的 运动发生变化。 股票缓缓地周<br>期性上升， 突然有一天， 股票狂涨， 从 50 元一下子升到 55 元， 第二天升到 62 元， 这两天交易量突<br>然大增， 但在第二天收市的前半小时股票从 62 元跌回 58 元收盘。 第三天开市一样强劲， 股票一下子升<br>到 64 元， 但第四天， 股票似乎失去了冲劲， 它 跌回了 61 元（ 如下图）。</li>\n</ul>\n<ul>\n<li>我是这样理解这一现象的： 股票运动后面的主要力量是大户， 这些大户通常是手握巨资的基金或保险<br>公司等。 当这些基金的管理人看好这只股票， 开始吸纳， 你将看到交易量上升， 股 价上升。 通常这些基<br>金管理人在吸纳股票时， 并不希望引起大众的注意， 所以这一过程通常 是缓慢的， 它不会上报纸或电视<br>的头条。 一旦这些大户吸股完毕， 你通常会听到他们开始公 开地推荐这些股票， 引起大众的注意。 这些<br>大户们的信息通常较普通人灵通， 他们也会很快 看到公司有好消息公布， 如开发成功新产品， 盈利较预<br>期为好等。 等到这些大户觉得好消息 已全部反映在股价之中， 他们开始准备脱手。 由于他们手中握有的<br>股票数量通常很大， 如果 一下子砸进市场， 市场根本承受不了， 他们手中的股票很大部分只能在低价出<br>手。 为了解决 这一问题， 他们要找傻瓜用高价来承接这些股票。 找傻瓜的最佳途径就是令股价暴升， 暴<br>升 时通常伴随点好消息， 如公司任命新董事长了， 某证券商强力推荐该股票了等等。 报纸电视 整天重复<br>这些消息， 引发小股民峰拥入市。 想想看， 小股民因股价暴升而引发贪念入市时买 的股票都是谁卖的？<br>一旦大户找足傻瓜， 全身而退， 还去哪里找大买主把股价继续推高？ 你现在明白了股票的正常运动及<br>危险信号后面的理由了吗？ 我一直强调， 炒股是艺术， 不是 科学。 当不正常的信号灯亮起， 是否表示这<br>只股票就一定要下跌？ 答案是：“否！” 没有人能 够在任何时候百分之百地肯定股票明天会怎么样， 也许<br>暴升的理由是公司真的发明了长生不 老药！ 你必须将股票的运动和公司的发展综合起来考虑。 让我重复<br>一次： 炒股的最基本信条 是在任何时候， 你手上持有股票的上升潜力必须大过下跌的可能， 否则你就不<br>应留在手里。 看到危险信号， 表示你的获胜概率此时已不超出 50%。 每位严肃的炒手都必须注意危<br>险信号， 但问题是在内心深处总有些奇怪的力量使他们在该卖 出的时候提不起勇气这么做。 或许是侥幸<br>心理在作怪。 在迟疑的这段时间， 他们常看到股票 又跌了好多点， 这时他们会拍着脑袋， 大骂自己傻瓜，<br>同时发誓一旦股票一有反弹就走人， “股票跌了这么多， 该会有个小反弹吧？ ” 但反弹来到的时候， 他<br>们又忘记了自己的誓言， 因为在他们眼睛里， 这时股票的运动又开始“正常” 起来。 通常， 这样的反弹<br>仅是股票在下 跌路上的喘气， 它很快就要继续要走的路。 人性有很多缺陷。 人希望股票会怎么运动， 认<br>定 股票会怎么运动， 当股票的运动和预想不符合时， 就会认为股市错了， 自己没有错。 但炒友 们必须牢<br>牢记住： 股市从来都不会错， 它总是走自己要走的路， 会错的只有人自己。 你所能做 的只有追随股市。<br>见到危险信号， 不要三心二意， 不要存有幻想， 把股票全部脱手。 几天以 后， 也许一切又恢复正常， 你<br>一样可以重新入场。 如果能这样做， 你将发现你为自己省下了很多焦虑及学费。</li>\n</ul>\n</blockquote>\n<h1 id=\"3-3-成功投资者所具有的共性\"><a href=\"#3-3-成功投资者所具有的共性\" class=\"headerlink\" title=\"3-3 成功投资者所具有的共性\"></a>3-3 成功投资者所具有的共性</h1><blockquote>\n<p>要有成为投资专家的欲望<br>必须具备锲而不舍的精神<br>要有“ 与股市斗， 其乐无穷” 的气派<br>要甘于做孤独者<br>必须具有耐心和自制力<br><strong>必须有一套适合自己的炒股模式</strong><br>必须具有超前的想像力及对未来的判断<br>成功的投资者绝不幻想<br>要有应用知识的毅力   </p>\n</blockquote>\n<p>一位成功的投资者， 他应十分留意怎样将他的知识应用在炒股中， 他不会为应用这些知识的 枯燥而<br>忽略细节。 在日常生活中， 获得知识通常并不困难， 困难在于用毅力应用这些知识。 在炒股问题上， 我<br>是坚信“知易行难” 之说的。</p>\n<h1 id=\"4-何时买卖股票\"><a href=\"#4-何时买卖股票\" class=\"headerlink\" title=\"4 何时买卖股票\"></a>4 何时买卖股票</h1><p>学习寻找临界点的过程其实就是学股的过程， 当然其中还包括学习炒股的正确心态。华尔街将炒股的诀巧归纳成两句话： 截短亏损，让利润奔跑！<br>关于人性的思考，自我的突破 </p>\n<h2 id=\"4-1-何时买-！！！-一定要多读透彻\"><a href=\"#4-1-何时买-！！！-一定要多读透彻\" class=\"headerlink\" title=\"4.1 何时买 ！！！ 一定要多读透彻\"></a>4.1 何时买 ！！！ 一定要多读透彻</h2><blockquote>\n<p>如何选股<br>选股之后，如何进场<br>如何设置止损点</p>\n<h2 id=\"4-2-何时卖-！！！-一定要多读透彻\"><a href=\"#4-2-何时卖-！！！-一定要多读透彻\" class=\"headerlink\" title=\"4.2 何时卖 ！！！ 一定要多读透彻\"></a>4.2 何时卖 ！！！ 一定要多读透彻</h2><p>何时卖股票的考虑可以分成两部分： 一， 刚进股时怎样选止损点； 二， 有利润后怎样选择合 适的卖点获利。<br><strong>买今天在上涨的股票：我自己喜欢把止损点定在入市当天的最低点。 比如我今天以 10.75 元买 进股票， 今天的最高价是 11 元， 最低价是 10 元， 我便以 10 元作为止损点。 以我的经验， 如 果我的入场点选的正确，股票开始上升， 它不应跌回我当天入场的最低点。</strong></p>\n<p>庄家的把戏<br>大户们操纵股票其实就是那么几招， 你只要专心， 观察股票的运动和交易量的变 化， 想像你是大户的话会怎么调动公众的心理， 大户的花招其实明显的很。<br>讲白了， 他们想 买进的进修要么静悄悄地， 要么想法引起大众的恐慌性抛售， 前者你会看到交易量增加， 但不明显， 股价慢慢地一步步升高， 后者便是搞一些大家公认的好卖点。<br>大户想卖的时候，要么先买进， 造成股价狂升，引发股民的贪念去抢搭轿子， 要么就搞一些公众们共认的好买点。<br>由于他们通常手握巨资， 要做到这些并不困难。便他们的动作必须会从股价的变动及交易量 的变动中露出尾巴，只要你有足够的经验， 你就明白怎样跟着玩。</p>\n</blockquote>\n<p>请再读一遍何时买股票， 何时卖股票这两节， 再体会一下“截短亏损， 让利润奔跑” 这句话， 炒股的诀窍尽在其中。</p>\n<h1 id=\"8-和炒手们聊天\"><a href=\"#8-和炒手们聊天\" class=\"headerlink\" title=\"8 和炒手们聊天\"></a>8 和炒手们聊天</h1><h2 id=\"8-1-学股的四个阶段\"><a href=\"#8-1-学股的四个阶段\" class=\"headerlink\" title=\"8.1 学股的四个阶段\"></a>8.1 学股的四个阶段</h2><blockquote>\n<p>第四阶段久赌必赢<br>一个可行的计划， 不能凭空想像， 它必须有理有据。“理” 就是数学的概率， 如果你每次下注的赢面<br>超过 50%， 而且你只下本金的小部分， 不会为几次坏运气就剃光头， 从长期而言你是 胜定了。 道理和开<br>赌场一样。“据” 在于你知道怎样找临界点， 在长期的观察和实践过程中， 你知道这些点是出入场的关键<br>点， 在这些点操作， 你的赢面超出 50%， 再加上应用“截短亏 损， 让利润奔跑” 的原则， 赢时赢大的，<br>亏时亏小的， 你的获胜概率其实远远超出了 50%。 到久赌必赢阶段， 你不应对亏钱和赚钱有任何情绪<br>上的波动。 你对止损不再痛苦， 你明白这 是游戏的一部分， 你对赚钱也不再喜悦， 你知道这是必然结果。<br>你不再将胜负放在心上， 你 只注重在正确的时间， 做正确的事情。 你知道利润会随之而来。</p>\n</blockquote>"},{"title":"Zookeeper 监控","date":"2017-08-20T12:07:50.000Z","_content":"ZooKeeper是一个分布式的，开放源码的分布式应用程序协调服务，是Google的Chubby一个开源的实现，是Hadoop和Hbase的重要组件。它是一个为分布式应用提供一致性服务的软件，提供的功能包括：配置维护、域名服务、分布式同步、组服务等。\n# Zookeeper 命令行监控方法  \n[zookeeper monitor](http://zookeeper.apache.org/doc/r3.4.6/zookeeperAdmin.html#sc_zkCommands \"zookeeper 4字节命令监控\")\n\n常见四字命令：\n* `echo stat|nc 127.0.0.1 2181 `来查看哪个节点被选择作为follower或者leader\n* `echo ruok|nc 127.0.0.1 2181 `测试是否启动了该Server，若回复imok表示已经启动。\n* `echo cons | nc 127.0.0.1 2181 `列出所有连接到服务器的客户端的完全的连接/会话的详情。\n* `echo wchs | nc 127.0.0.1 2181 `列出服务器 watch 的详细信息。\n\n<!-- more -->\n\n# Zookeeper Jmx监控方法  \n需先启用JMX端口。修改zookeeper的启动脚本vim zkServer.sh。找到启动参数ZOOMAIN，修改为下面值。其中local.only=false，设为false才能在远程建立连接。\n可使用Jconsole 通过IP、jmx端口连接到Zookeeper。","source":"_posts/zookeeper-monitor.md","raw":"---\ntitle: Zookeeper 监控\ndate: 2017-08-20 20:07:50\ntags: \n - Zookeeper\n - Monitor \ncategory: 技术 \n---\nZooKeeper是一个分布式的，开放源码的分布式应用程序协调服务，是Google的Chubby一个开源的实现，是Hadoop和Hbase的重要组件。它是一个为分布式应用提供一致性服务的软件，提供的功能包括：配置维护、域名服务、分布式同步、组服务等。\n# Zookeeper 命令行监控方法  \n[zookeeper monitor](http://zookeeper.apache.org/doc/r3.4.6/zookeeperAdmin.html#sc_zkCommands \"zookeeper 4字节命令监控\")\n\n常见四字命令：\n* `echo stat|nc 127.0.0.1 2181 `来查看哪个节点被选择作为follower或者leader\n* `echo ruok|nc 127.0.0.1 2181 `测试是否启动了该Server，若回复imok表示已经启动。\n* `echo cons | nc 127.0.0.1 2181 `列出所有连接到服务器的客户端的完全的连接/会话的详情。\n* `echo wchs | nc 127.0.0.1 2181 `列出服务器 watch 的详细信息。\n\n<!-- more -->\n\n# Zookeeper Jmx监控方法  \n需先启用JMX端口。修改zookeeper的启动脚本vim zkServer.sh。找到启动参数ZOOMAIN，修改为下面值。其中local.only=false，设为false才能在远程建立连接。\n可使用Jconsole 通过IP、jmx端口连接到Zookeeper。","slug":"zookeeper-monitor","published":1,"updated":"2018-04-27T04:11:03.373Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjhajczpp001ge4rw72egfcwg","content":"<p>ZooKeeper是一个分布式的，开放源码的分布式应用程序协调服务，是Google的Chubby一个开源的实现，是Hadoop和Hbase的重要组件。它是一个为分布式应用提供一致性服务的软件，提供的功能包括：配置维护、域名服务、分布式同步、组服务等。</p>\n<h1 id=\"Zookeeper-命令行监控方法\"><a href=\"#Zookeeper-命令行监控方法\" class=\"headerlink\" title=\"Zookeeper 命令行监控方法\"></a>Zookeeper 命令行监控方法</h1><p><a href=\"http://zookeeper.apache.org/doc/r3.4.6/zookeeperAdmin.html#sc_zkCommands\" title=\"zookeeper 4字节命令监控\" target=\"_blank\" rel=\"external\">zookeeper monitor</a></p>\n<p>常见四字命令：</p>\n<ul>\n<li><code>echo stat|nc 127.0.0.1 2181</code>来查看哪个节点被选择作为follower或者leader</li>\n<li><code>echo ruok|nc 127.0.0.1 2181</code>测试是否启动了该Server，若回复imok表示已经启动。</li>\n<li><code>echo cons | nc 127.0.0.1 2181</code>列出所有连接到服务器的客户端的完全的连接/会话的详情。</li>\n<li><code>echo wchs | nc 127.0.0.1 2181</code>列出服务器 watch 的详细信息。</li>\n</ul>\n<a id=\"more\"></a>\n<h1 id=\"Zookeeper-Jmx监控方法\"><a href=\"#Zookeeper-Jmx监控方法\" class=\"headerlink\" title=\"Zookeeper Jmx监控方法\"></a>Zookeeper Jmx监控方法</h1><p>需先启用JMX端口。修改zookeeper的启动脚本vim zkServer.sh。找到启动参数ZOOMAIN，修改为下面值。其中local.only=false，设为false才能在远程建立连接。<br>可使用Jconsole 通过IP、jmx端口连接到Zookeeper。</p>\n","site":{"data":{}},"excerpt":"<p>ZooKeeper是一个分布式的，开放源码的分布式应用程序协调服务，是Google的Chubby一个开源的实现，是Hadoop和Hbase的重要组件。它是一个为分布式应用提供一致性服务的软件，提供的功能包括：配置维护、域名服务、分布式同步、组服务等。</p>\n<h1 id=\"Zookeeper-命令行监控方法\"><a href=\"#Zookeeper-命令行监控方法\" class=\"headerlink\" title=\"Zookeeper 命令行监控方法\"></a>Zookeeper 命令行监控方法</h1><p><a href=\"http://zookeeper.apache.org/doc/r3.4.6/zookeeperAdmin.html#sc_zkCommands\" title=\"zookeeper 4字节命令监控\" target=\"_blank\" rel=\"external\">zookeeper monitor</a></p>\n<p>常见四字命令：</p>\n<ul>\n<li><code>echo stat|nc 127.0.0.1 2181</code>来查看哪个节点被选择作为follower或者leader</li>\n<li><code>echo ruok|nc 127.0.0.1 2181</code>测试是否启动了该Server，若回复imok表示已经启动。</li>\n<li><code>echo cons | nc 127.0.0.1 2181</code>列出所有连接到服务器的客户端的完全的连接/会话的详情。</li>\n<li><code>echo wchs | nc 127.0.0.1 2181</code>列出服务器 watch 的详细信息。</li>\n</ul>","more":"<h1 id=\"Zookeeper-Jmx监控方法\"><a href=\"#Zookeeper-Jmx监控方法\" class=\"headerlink\" title=\"Zookeeper Jmx监控方法\"></a>Zookeeper Jmx监控方法</h1><p>需先启用JMX端口。修改zookeeper的启动脚本vim zkServer.sh。找到启动参数ZOOMAIN，修改为下面值。其中local.only=false，设为false才能在远程建立连接。<br>可使用Jconsole 通过IP、jmx端口连接到Zookeeper。</p>"},{"title":"Docker Swarm管理节点高可用分析","date":"2018-10-19T08:53:53.000Z","_content":"\n# 1.简介\n\n## 1.1 swarm是什么\n\n`Swarm` 是使用 [`SwarmKit`](https://github.com/docker/swarmkit/) 构建的 Docker 引擎内置（原生）的集群管理和编排工具。其主要作用是把若干台Docker主机抽象为一个整体，并且通过一个入口统一管理这些Docker主机上的各种Docker资源。Swarm和Kubernetes比较类似，但是更加轻量，具有的功能也较kubernetes更少一些。\n\nDocker v1.12 是一个非常重要的版本，Docker 重新实现了集群的编排方式。在此之前，提供集群功能的 Docker Swarm 是一个单独的软件，而且依赖外部数据库（比如 Consul、etcd 或 Zookeeper）。从 v1.12 开始，Docker Swarm 的功能已经完全与 Docker Engine 集成，要管理集群，只需要启动 Swarm Mode。安装好 Docker，Swarm 就已经在那里了，服务发现也在那里了（不需要安装 Consul 等外部数据库）。\n\nDocker的Swarm(集群)模式，集成很多工具和特性，比如：跨主机上快速部署服务，服务的快速扩展，集群的管理整合到docker引擎，这意味着可以不可以不使用第三方管理工具。分散设计，声明式的服务模型，可扩展，状态协调处理，多主机网络，分布式的服务发现，负载均衡，滚动更新，安全（通信的加密）等等。 \n\n![](\\img\\docker-swarm-logo.png)\n\n官方网站：https://docs.docker.com/engine/swarm/\n\n<!-- more -->\n\n## 1.2 swarm基本概念\n\n### 1.2.1 节点 \n\n运行 Docker 的主机可以主动初始化一个 `Swarm` 集群或者加入一个已存在的 `Swarm` 集群，这样这个运行 Docker 的主机就成为一个 `Swarm` 集群的节点 (`node`) 。\n\n节点分为管理 (`manager`) 节点和工作 (`worker`) 节点。\n\n管理节点用于 `Swarm` 集群的管理，`docker swarm` 命令基本只能在管理节点执行（节点退出集群命令 `docker swarm leave` 可以在工作节点执行）。一个 `Swarm` 集群可以有多个管理节点，但只有一个管理节点可以成为 `leader`，`leader` 通过 `raft` 协议实现。\n\n工作节点是任务执行节点，管理节点将服务 (`service`) 下发至工作节点执行。管理节点默认也作为工作节点。也可以通过配置让服务只运行在管理节点。\n\nDocker 官网的这张图片形象的展示了集群中管理节点与工作节点的关系。\n\n![](\\img\\swarm-node.png)\n\n### 1.2.2 服务和任务\n\n任务 （`Task`）是 `Swarm` 中的最小的调度单位，目前来说就是一个单一的容器。\n\n服务 （`Services`） 是指一组任务的集合，服务定义了任务的属性。服务有两种模式：\n\n- `replicated services` 按照一定规则在各个工作节点上运行指定个数的任务。\n- `global services` 每个工作节点上运行一个任务\n\n两种模式通过 `docker service create` 的 `--mode` 参数指定。\n\n### 1.2.3 负载均衡\n\n`manager`群集管理器使用 **ingress load balancing** 来对外公开群集提供的服务。`manager`可以自动为**PublishedPort** 分配服务，也可以手动配置。如果未指定端口，则swarm管理器会为服务分配30000-32767范围内的端口。\n\n外部组件（例如云负载平衡器）可以访问群集中任何节点的PublishedPort上的服务，无论该节点当前是否正在运行该服务的任务。群集中的所有节点都将入口连接到正在运行的任务实例。\n\nSwarm模式有一个内部DNS组件，可以自动为swarm中的每个服务分配一个DNS条目。群集管理器使用**内部负载平衡**来根据服务的DNS名称在群集内的服务之间分发请求。\n\n\n\n## 1.3 swarm特性\n\n- **集群管理与Docker Engine集成:**使用Docker Engine CLI来创建一个你能部署应用服务到Docker Engine的swarm。你不需要其他编排软件来创建或管理swarm。\n\n- **分散式设计：**Docker Engine不是在部署时处理节点角色之间的差异，而是在运行时扮演自己角色。你可以使用Docker Engine部署两种类型的节点，管理器和worker。这意味着你可以从单个磁盘映像构建整个swarm。\n\n- **声明性服务模型：** Docker Engine使用声明性方法来定义应用程序堆栈中各种服务的所需状态。例如，你可以描述由消息队列服务和数据库后端的Web前端服务组成的应用程序。\n\n- **伸缩性：**对于每个服务，你可以声明要运行的任务数。当你向上或向下缩放时，swarm管理器通过添加或删除任务来自动适应，以保持所需状态。\n\n- **期望的状态协调：**swarm管理器节点持续监控群集状态，并调整你描述的期望状态与实际状态之间的任何差异。 例如，如果设置运行一个10个副本容器的服务，这时worker机器托管其中的两个副本崩溃，管理器则将创建两个新副本以替换已崩溃的副本。 swarm管理器将新副本分配给正在运行和可用的worker。\n\n- **多主机网络：**你可以为服务指定覆盖网络([overlay](https://www.centos.bz/tag/overlay/) network)。 当swarm管理器初始化或更新应用程序时，它会自动为容器在覆盖网络(overlay network)上分配地址。\n\n- **服务发现：**Swarm管理器节点为swarm中的每个服务分配唯一的DNS名称，并负载平衡运行中的容器。 你可以通过嵌入在swarm中的DNS服务器查询在swarm中运行中的每个容器。\n\n- **负载平衡：**你可以将服务的端口暴露给外部的负载均衡器。 在内部，swarm允许你指定如何在节点之间分发服务容器。\n\n- **安全通信：**swarm中的每个节点强制执行TLS相互验证和加密，以保护其自身与所有其他节点之间的通信。 你可以选择使用自签名根证书或来自自定义根CA的证书。\n\n- **滚动更新：**在上线新功能期间，你可以增量地应用服务更新到节点。 swarm管理器允许你控制将服务部署到不同节点集之间的延迟。 如果出现任何问题，你可以将任务回滚到服务的先前版本。\n\n\n# 2.架构分析\n\n## 2.1 基本架构\n\nDocker Swarm提供了基本的集群能力，能够使多个Docker Engine组合成一个group，提供多容器服务。Swarm使用标准的Docker API，启动容器可以直接使用docker run命令。Swarm更核心的则是关注如何选择一个主机并在其上启动容器，最终运行服务。 Docker Swarm基本架构，如下图所示： \n\n![](\\img\\docker-swarm-architecture.png)\n\n\n\n如上图所示，Swarm Node表示加入Swarm集群中的一个Docker Engine实例，基于该Docker Engine可以创建并管理多个Docker容器。其中，最开始创建Swarm集群的时候，Swarm Manager便是集群中的第一个Swarm Node。在所有的Node中，又根据其职能划分为Manager Node和Worker Node。\n\n### 2.1 Manager Node\n\nManger 节点，顾名思义，是进行 Swarm 集群的管理工作的，它的管理工作集中在如下部分，\n\n- 维护一个集群的状态；\n- 对 Services 进行调度；\n- 为 Swarm 提供外部可调用的 API 接口；\n\nManager 节点需要时刻维护和保存当前 Swarm 集群中各个节点的一致性状态，这里主要是指各个 Tasks 的执行的状态和其它节点的状态；因为 Swarm 集群是一个典型的分布式集群，在保证一致性上，Manager 节点采用 [Raft](https://raft.github.io/raft.pdf) 协议来保证分布式场景下的数据一致性；\n\n通常为了保证 Manager 节点的高可用，Docker 建议采用奇数个 Manager 节点，这样的话，你可以在 Manager 失败的时候不用关机维护，我们给出如下的建议：\n\n- 3 个 Manager 节点最多可以同时容忍 1 个 Manager 节点失效的情况下保证高可用；\n- 5 个 Manager 节点最多可以同时容忍 2 个 Manager 节点失效的情况下保证高可用；\n- N 个 Manager 节点最多可以同时容忍 (N−1)/2个 Manager 节点失效的情况下保证高可用；\n- Docker 建议最多最多的情况下，使用 7 个 Manager 节点就够了，否则反而会降低集群的性能了。\n\n### 2.2 Worker Node\n\nWorker Node接收由Manager Node调度并指派的Task，启动一个Docker容器来运行指定的服务，并且Worker Node需要向Manager Node汇报被指派的Task的执行状态。\n\n### 2.3  更换角色\n\n通过 docker node promote 命令将一个 Worker 节点提升为 Manager 节点。通常情况下，该命令使用在维护的过程中，需要将 Manager 节点占时下线进行维护操作；同样可以使用 docker node demote 将某个 manager 节点降级为 worker 节点。 \n\n\n\n## 2.2 设计架构\n\n![](\\img\\swarm-architecture-1.jpg)\n\n从前文可知， Swarm 集群的管理工作是由manager节点实现。如上图所示，manager节点实现的功能主要包括：node discovery，scheduler,cluster管理等。同时，为了保证Manager 节点的高可用，Manager 节点需要时刻维护和保存当前 Swarm 集群中各个节点的一致性状态。在保证一致性上，Manager 节点采用 [Raft](https://raft.github.io/raft.pdf) 协议来保证分布式场景下的数据一致性；\n\nDocker Swarm内置了Raft一致性算法，可以保证分布式系统的数据保持一致性同步。Etcd, Consul等高可用键值存储系统也是采用了这种算法。这个算法的作用简单点说就是随时保证集群中有一个Leader，由Leader接收数据更新，再同步到其他各个Follower节点。在Swarm中的作用表现为当一个Leader 节点 down掉时，系统会立即选取出另一个Leader节点，由于这个节点同步了之前节点的所有数据，所以可以无缝地管理集群。\n\nRaft的详细解释可以参考[《The Secret Lives of Data--Raft: Understandable Distributed Consensus》](http://thesecretlivesofdata.com/raft/)。\n\n### 2.2.1 跨主机容器通信\n\nDocker Swarm 内置的跨主机容器通信方案是overlay网络，这是一个基于vxlan协议的网络实现。VxLAN 可将二层数据封装到 UDP 进行传输，VxLAN 提供与 VLAN 相同的以太网二层服务，但是拥有更强的扩展性和灵活性。 overlay 通过虚拟出一个子网，让处于不同主机的容器能透明地使用这个子网。所以跨主机的容器通信就变成了在同一个子网下的容器通信，看上去就像是同一主机下的bridge网络通信。\n\n为支持容器跨主机通信，Docker 提供了 overlay driver，使用户可以创建基于 VxLAN 的 overlay 网络。其实，docker 会创建一个 bridge 网络 “docker_gwbridge”，为所有连接到 overlay 网络的容器提供访问外网的能力。``docker network inspect docker_gwbridge`  查看网络信息。\n\n![](\\img\\overlay-gwbridge.jpg)\n\n下面我们讨论下overlay 网络的具体实现：\n\ndocker 会为每个 overlay 网络创建一个独立的 network namespace，其中会有一个 linux bridge br0，endpoint 还是由 veth pair 实现，一端连接到容器中（即 eth0），另一端连接到 namespace 的 br0 上。\n\nbr0 除了连接所有的 endpoint，还会连接一个 vxlan 设备，用于与其他 host 建立 vxlan tunnel。容器之间的数据就是通过这个 tunnel 通信的。逻辑网络拓扑结构如图所示：\n\n![](\\img/overlay.jpg)\n\n### 2.2.2 服务发现\n\ndocker Swarm mode下会为每个节点的docker engine内置一个DNS server，各个节点间的DNS server通过control plane的gossip协议互相交互信息。此处DNS server用于容器间的服务发现。swarm mode会为每个 --net=自定义网络的service分配一个DNS entry。目前必须是自定义网络，比如overaly。而bridge和routing mesh的service，是不会分配DNS的。\n\n那么，下面就来详细介绍服务发现的原理。\n\n每个Docker容器都有一个DNS解析器，它将DNS查询转发到docker engine，该引擎充当DNS服务器。docker 引擎收到请求后就会在发出请求的容器所在的所有网络中，检查域名对应的是不是一个容器或者是服务，如果是，docker引擎就会从存储的key-value建值对中查找这个容器名、任务名、或者服务名对应的IP地址，并把这个IP地址或者是服务的虚拟IP地址返回给发起请求的域名解析器。  \n\n由上可知，docker的服务发现的作用范围是网络级别，也就意味着只有在同一个网络上的容器或任务才能利用内嵌的DNS服务来相互发现，不在同一个网络里面的服务是不能解析名称的，另外，为了安全和性能只有当一个节点上有容器或任务在某个网络里面时，这个节点才会存储那个网络里面的DNS记录。\n\n如果目的容器或服务和源容器不在同一个网络里面，Docker引擎会把这个DNS查询转发到配置的默认DNS服务  。\n\n![](\\img\\service-dns.png)\n\n在上面的例子中，总共有两个服务myservice和client，其中myservice有两个容器，这两个服务在同一个网里面。在client里针对docker.com和myservice各执行了一个curl操作，下面时执行的流程： \n\n- 为了client解析docker.com和myservice，DNS查询进行初始化\n- 容器内建的解析器在127.0.0.11:53拦截到这个DNS查询请求，并把请求转发到docker引擎的DNS服务\n- myservice被解析成服务对应的虚拟IP（10.0.0.3），在接下来的内部负载均衡阶段再被解析成一个具体任务的IP地址。如果是容器名称这一步直接解析成容器对应的IP地址（10.0.0.4或者10.0.0.5）。\n- docker.com在mynet网络上不能被解析成服务，所以这个请求被转发到配置好的默认DNS服务器（8.8.8.8）上。\n\n### 2.2.3 负载均衡\n\n负载均衡分为两种：Swarm集群内的service之间的相互访问需要做负载均衡，称为内部负载均衡（Internal LB）；从Swarm集群外部访问服务的公开端口，也需要做负载均衡，称外部部负载均衡(Exteral LB or Ingress LB)。\n\n- Internal LB\n\n内部负载均衡就是我们在上一段提到的服务发现，集群内部通过DNS访问service时，Swarm默认通过VIP（virtual IP）、iptables、IPVS转发到某个容器。 \n\n![](\\img\\intelnal-lb.jpg)\n\n当在docker swarm集群模式下创建一个服务时，会自动在服务所属的网络上给服务额外的分配一个虚拟IP，当解析服务名字时就会返回这个虚拟IP。对虚拟IP的请求会通过overlay网络自动的负载到这个服务所有的健康任务上。这个方式也避免了客户端的负载均衡，因为只有单独的一个IP会返回到客户端，docker会处理虚拟IP到具体任务的路由，并把请求平均的分配给所有的健康任务。  \n\n![](C:\\Users\\jalon\\OneDrive\\01笔记\\Docker\\internal-lb-2.png)\n\n```\n# 创建overlay网络：mynet \n$ docker network create -d overlay mynet  \na59umzkdj2r0ua7x8jxd84dhr \n# 利用mynet网络创建myservice服务，并复制两份  \n$ docker service create --network mynet --name myservice --replicas 2 busybox ping localhost  \n78t5r8cr0f0h6k2c3k7ih4l6f5\n# 通过下面的命令查看myservice对应的虚拟IP \n$ docker service inspect myservice  \n...\n\"VirtualIPs\": [ \n\t{  \n     \"NetworkID\": \"a59umzkdj2r0ua7x8jxd84dhr\",  \n     \t\t\t\"Addr\": \"10.0.0.3/24\"  \n      },  \n]  \t\n```\n\n注：swarm中服务还有另外一种负载均衡技术可选DNS round robin (DNS RR) （在创建服务时通过--endpoint-mode配置项指定），在DNSRR模式下，docker不再为服务创建VIP，docker DNS服务直接利用轮询的策略把服务名称直接解析成一个容器的IP地址。 \n\n- Exteral LB（Ingress LB 或者 Swarm Mode Routing Mesh)\n\n看名字就知道，这个负载均衡方式和前面提到的Ingress网络有关。Swarm网络要提供对外访问的服务就需要打开公开端口，并映射到宿主机。Ingress LB就是外部通过公开端口访问集群时做的负载均衡。\n\n当创建或更新一个服务时，你可以利用--publish选项把一个服务暴露到外部，在docker swarm模式下发布一个端口意味着在集群中的所有节点都会监听这个端口，这时当访问一个监听了端口但是并没有对应服务运行在其上的节点会发生什么呢？    接下来就该我们的路由网（routing mesh）出场了，路由网时docker1.12引入的一个新特性，它结合了IPVS和iptables创建了一个强大的集群范围的L4层负载均衡，它使所有节点接收服务暴露端口的请求成为可能。当任意节点接收到针对某个服务暴露的TCP/UDP端口的请求时，这个节点会利用预先定义过的Ingress overlay网络，把请求转发给服务对应的虚拟IP。ingress网络和其他的overlay网络一样，只是它的目的是为了转换来自客户端到集群的请求，它也是利用我们前一小节介绍过的基于VIP的负载均衡技术。 \n\n启动服务后，你可以为应用程序创建外部DNS记录，并将其映射到任何或所有Docker swarm节点。你无需担心你的容器具体运行在那个节点上，因为有了路由网这个特性后，你的集群看起来就像是单独的一个节点一样。  \n\n```\n#在集群中创建一个复制两份的服务，并暴露在8000端口  \n$ docker service create --name app --replicas 2 --network appnet --publish 8000:80 nginx  \n```\n\n![](\\img\\external-routing-mesh.png)\n\n上面这个图表明了路由网是怎么工作的： \n\n- 服务（app）拥有两份复制，并把端口映射到外部端口的8000\n- 路由网在集群中的所有节点上都暴露出8000\n- 外部对服务app的请求可以是任意节点，在本例子中外部的负载均衡器将请求转发到了没有app服务的主机上\n- docker swarm的IPVS利用ingress overlay网路将请求重新转发到运行着app服务的节点的容器中\n\n注：以上服务发现和负载均衡参考文档 https://success.docker.com/article/ucp-service-discovery\n\n\n\n## 2.3 Services 架构\n\n在微服务部署的过程中，通常将某一个微服务封装为 Service 在 Swarm 中部署执行，通常你需要通过指定容器 image 以及需要在容器中执行的 Commands 来创建你的 Service，除此之外，通常，还需要配置如下选项，\n\n- 指定可以在 Swarm 之外可以被访问的服务端口号 port，\n- 指定加入某个 Overlay 网络以便 Service 与 Service 之间可以建立连接并通讯，\n- 指定该 Service 所要使用的 CPU 和 内存的大小，\n- 指定一个滚动更新的策略 (Rolling Update Policy)\n- 指定多少个 Task 的副本 (replicas) 在 Swarm 集群中同时存在\n\n### 2.3.1 服务和任务\n\n![](\\img\\services-diagram.png)\n\nServices，Tasks 和 Containers 之间的关系可以用上面这张图来描述。来看这张图的逻辑，表示用户想要通过 Manager 节点部署一个有 3 个副本的 Nginx 的 Service，Manager 节点接收到用户的 Service definition 后，便开始对该 Service 进行调度，将会在当前可用的Worker（或者Manager ）节点中启动相应的 Tasks 以及相关的副本；所以可以看到，Service 实际上是 Task 的定义，而 Task 则是执行在节点上的程序。\n\nTask 是什么呢？其实就是一个 Container，只是，在 Swarm 中，每个 Task 都有自己的名字和编号，如图，比如 nginx.1、nginx.2 和 nginx.3，这些 Container 各自运行在各自的 node 上，当然，一个 node 上可以运行多个 Container；\n\n### 2.3.2 任务调度\n\nDocker swarm mode 模式的底层技术实际上就是指的是调度器( scheduler )和编排器( orchestrator )；下面这张图展示了 Swarm mode 如何从一个 Service 的创建请求并且成功将该 Service 分发到两个 worker 节点上执行的过程。\n\n![](\\img\\swarm-service-lifecycle.png)\n\n\n\n- 首先，看上半部分Swarm manager \n  1. 用户通过 Docker Engine Client 使用命令 docker service create 提交 Service definition，\n  2. 根据 Service definition 创建相应的 Task，\n  3. 为 Task 分配 IP 地址，\n     注意，这是分配运行在 Swarm 集群中 Container 的 IP 地址，该 IP 地址最佳的分配地点是在这里，因为 Manager 节点上保存得有最新最全的 Tasks 的状态信息，为了保证不与其他的 Task 分配到相同的 IP，所以在这里就将 IP 地址给初始化好；\n  4. 将 Task 分发到 Node 上，可以是 Manager 节点也可以使 Worker 节点，\n  5. 对 Worker 节点进行相应的初始化使得它可以执行 Task\n- 接着，看下半部分Swarm  Work \n  该部分就相对简单许多\n  1. 首先连接 manager 的分配器( scheduler)检查该 task\n  2. 验证通过以后，便开始通过 Worker 节点上的执行器( executor )执行；\n\n注意，上述 task 的执行过程是一种单向机制，比如它会按顺序的依次经历 assigned, prepared 和 running 等执行状态，不过在某些特殊情况下，在执行过程中，某个 task 失败了( fails )，编排器( orchestrator )直接将该 task 以及它的 container 给删除掉，然后在其它节点上另外创建并执行该 task；\n\n\n\n### 2.3.3 调度策略\n\nSwarm在scheduler节点（leader节点）运行容器的时候，会根据指定的策略来计算最适合运行容器的节点，目前支持的策略有：spread, binpack, random。\n\n　　1）Random 顾名思义，就是随机选择一个Node来运行容器，一般用作调试用，spread和binpack策略会根据各个节点的可用的CPU, RAM以及正在运 行的容器的数量来计算应该运行容器的节点。\n\n　　2）Spread 在同等条件下，Spread策略会选择运行容器最少的那台节点来运行新的容器，binpack策略会选择运行容器最集中的那台机器来运行新的节点。 使用Spread策略会使得容器会均衡的分布在集群中的各个节点上运行，一旦一个节点挂掉了只会损失少部分的容器。\n\n　　3）Binpack Binpack策略最大化的避免容器碎片化，就是说binpack策略尽可能的把还未使用的节点留给需要更大空间的容器运行，尽可能的把容器运行在 一个节点上面。(The binpack strategy causes Swarm to optimize for the container which is most packed.)\n\n### 2.3.4 服务副本与全局服务 \n\nDocker Swarm支持服务的扩容缩容，Swarm通过`--mode`选项设置服务类型，提供了两种模式：一种是replicated，我们可以指定服务Task的个数（也就是需要创建几个冗余副本），这也是Swarm默认使用的服务类型；另一种是global，这样会在Swarm集群的每个Node上都创建一个服务。如下图所示（出自Docker官网），是一个包含replicated和global模式的Swarm集群： \n\n![](\\img\\docker-swarm-replicated-vs-global.png)\n\n\n\n上图中，黄色表示的replicated模式下的Service Replicas，灰色表示global模式下Service的分布。 \n\n在Swarm mode下使用Docker，可以实现部署运行服务、服务扩容缩容、删除服务、滚动更新等功能，下面我们详细说明。\n\n# 3.集群搭建\n\n## 3.1 环境配置\n\n虚拟机信息\n\n| IP            | hostname | 配置    | 角色    |\n| ------------- | -------- | ------- | ------- |\n| 172.21.111.56 | swarm01  | 4CPU/6G | Manager |\n| 172.21.111.57 | swarm02  | 4CPU/6G | Manager |\n| 172.21.111.58 | swarm03  | 4CPU/6G | Manager |\n\n为验证后续的swarm manager 高可用，将这三台节点都配置成为Manager节点。\n\n\n\n## 3.2 前置准备\n\n### 3.2.1 修改主机名\n\n```\n hostnamectl --static --transient set-hostname swarm01\n```\n\n修改/etc/hosts 。三台主机上均设置一致。\n\n```\n[root@swarm01 ~]# cat /etc/hosts\n127.0.0.1   swarm-manager localhost localhost.localdomain localhost4 localhost4.localdomain4\n::1         localhost localhost.localdomain localhost6 localhost6.localdomain6\n172.21.111.56 swarm01\n172.21.111.57 swarm02\n172.21.111.58 swarm03\n```\n\n\n\n### 3.2.2 配置互信\n\n```bash\nssh-keygen -t rsa\nssh-copy-id -i .ssh/id_rsa.pub 172.21.111.57\n```\n\n### 3.2.3 关闭防火墙和SELinux\n\n```\nsystemctl disable firewalld\nsystemctl stop firewalld\n```\n\n修改 /etc/selinux/config ，修改配置项SELINUX=enforcing为disabled。\n\n## 3.2 安装Docker服务 \n\n如前文所示，从 v1.12 开始，Docker Swarm 的功能已经完全与 Docker Engine 集成，要管理集群，只需要启动 Swarm Mode。安装好 Docker，Swarm 就已经在那里了，服务发现也在那里了（不需要安装 Consul 等外部数据库）。配置好yum 源之后，即可安装docker-ce。\n\n```\\\nyum install -y docker-ce\nsystemctl enable docker\nsystemctl start docker\n```\n\n\n\n## 3.3 集群初始化\n\n### 3.2.1 初始化主节点\n\n使用命令`docker swarm init` 即可初始化当前节点为主节点。如果主机上有多张网卡的时候，需指定网卡` docker swarm init  --advertise-addr  [IP]   --listen-addr  [IP:PORT]`   \n\n节点自动解锁： docker swarm init --autolock=false\n\n```\n[root@swarm01 ~]# docker swarm init --autolock=false  --advertise-addr 172.21.111.56 --listen-addr 172.21.111.56:2377   \nSwarm initialized: current node (epoic1y0vv830vnwbc6nnacjc) is now a manager.\n\nTo add a worker to this swarm, run the following command:\n\n    docker swarm join --token SWMTKN-1-5t9jv2o6z3ep9mk5c2ae41bpqojla1g8cfjo3qlsuj2sxi934l-af2x3lh8vs6dnpepy66kdw5mv 172.21.111.56:2377\n\nTo add a manager to this swarm, run 'docker swarm join-token manager' and follow the instructions.\n```\n\n此例，在swarm01中创建了主节点。从输出中，看到添加集群有两种方式，一种是以manager 的方式添加，一种则是以worker节点方式添加。默认输出只给出了添加worker的命令，manager 命令则需要通过`docker swarm join-token manager`  获取。\n\n```\n[root@swarm01 ~]# docker swarm join-token manager\nTo add a manager to this swarm, run the following command:\n\n    docker swarm join --token SWMTKN-1-5t9jv2o6z3ep9mk5c2ae41bpqojla1g8cfjo3qlsuj2sxi934l-eszh52z00ktao9j7lovfvjgqy 172.21.111.56:2377\n```\n\n### 3.2.2 添加manager 节点\n\n分别登陆swarm02、swarm03，执行上一节得到的命令。\n\n```\n[root@swarm03 ~]#docker swarm join --token SWMTKN-1-5t9jv2o6z3ep9mk5c2ae41bpqojla1g8cfjo3qlsuj2sxi934l-eszh52z00ktao9j7lovfvjgqy 172.21.111.56:2377\nThis node joined a swarm as a manager.\n```\n\n注：不要在节点上配置http代理，否则会添加节点失败。\n\n### 3.2.3 离开集群\n\n如果想退出集群，可以在该节点上直接退出。\n\n```\n[root@swarm03 ~]# docker swarm leave\n[root@swarm03 ~]# docker swarm leave --force \n```\n\n### 3.2.4 查看集群节点信息\n\n登陆任意一台Manager节点。都可以查看当前集群的节点信息。\n\n```\n[root@swarm02 ~]# docker node ls\nID                            HOSTNAME            STATUS              AVAILABILITY        MANAGER STATUS      ENGINE VERSION\nepoic1y0vv830vnwbc6nnacjc     swarm01             Ready               Active              Leader              18.03.1-ce\nnsqqv181e0r7mu77azixtqhzl *   swarm02             Ready               Active              Reachable           18.03.1-ce\nmkkxaeqergz8xw21bbkd47q1c     swarm03             Ready               Active              Reachable           18.03.1-ce\n\n```\n\n上面信息中，AVAILABILITY表示Swarm Scheduler是否可以向集群中的某个Node指派Task，对应有如下三种状态：\n\n- Active：集群中该Node可以被指派Task\n- Pause：集群中该Node不可以被指派新的Task，但是其他已经存在的Task保持运行\n- Drain：集群中该Node不可以被指派新的Task，Swarm Scheduler停掉已经存在的Task，并将它们调度到可用的Node上\n\n另外，可以看到Manager Status状态也有分类，分别是Leader、Reacheable、Unreachable。\n\n- Leader：对应Raft算法的Leader节点\n- Reacheable：对应Raft算法的Flower节点，网络可达、可用的Flower节点\n- Unreachable：对应Raft算法的Flower节点，网络不可达、不可用的Flower节点\n\n查看单个节点详细信息\n\n```\n[root@swarm02 ~]# docker node inspect swarm01\n```\n\n关于Node的其他操作，比如状态变更、添加标签、提权和降权等，详见附录2\n\n\n\n# 4.高可用实践\n\n## 4.1 应用服务管理\n\ndocker swarm 还可以使用 stack 这一模型，用来部署管理和关联不同的服务。stack 功能比service 还要强大，因为它具备多服务编排部署的能力。**stack file** 是一种 yaml 格式的文件，类似于 docker-compose.yml 文件，它定义了一个或多个服务，并定义了服务的环境变量、部署标签、容器数量以及相关的环境特定配置等。 \n\n以下实验仅仅创建利用stack 创建单一服务。\n\n### 4.1.1 创建服务\n\n服务配置文件docker-swarm.yml ，内容如下\n\n```\nversion: '3'\nservices:\n  web:\n    image: \"httpd:v1\"\n    deploy:\n      replicas: 3\n      resources:\n        limits:\n          cpus: \"0.5\"\n          memory: 256M\n      restart_policy:\n        condition: on-failure\n    ports:\n     - \"7080:80\"\n    volumes:\n     - /var/www/html:/usr/local/apache2/htdocs\n```\n\nimage: \"httpd\" ：httpd是docker hub 下载的镜像。提供简单的httpd服务，此处用来做服务都高可用验证。\n\nreplicas: 3 表示该服务副本有3个，可对服务副本进行扩缩容，后续会讲到。\n\n/var/www/html:/usr/local/apache2/htdocs ：表示httpd 首页的内容。各个swarm节点上展示的内容分别是其主机名信息。\n\n在docker-swarm.yml 同级目录下，执行命令\n\n```\n[root@swarm01 swarm]# docker stack deploy -c docker-swarm.yml myservice\nCreating network myservice_default\nCreating service myservice_web\n```\n\n### 4.1.2 查看服务\n\n查看服务创建是否成功。\n\n```\n[root@swarm01 swarm]# docker stack ls\nNAME                SERVICES\nmyservice           1\n[root@swarm01 swarm]# docker stack services myservice\nID                  NAME                MODE                REPLICAS            IMAGE               PORTS\nkom8x49f7qv2        myservice_web       replicated          3/3                 httpd:latest        *:7080->80/tcp\n[root@swarm01 swarm]# docker stack ps myservice\nID                  NAME                IMAGE               NODE                DESIRED STATE       CURRENT STATE            ERROR               PORTS\nsybn1dur1wmw        myservice_web.1     httpd:latest        swarm03             Running             Running 45 seconds ago\nmlcb5r9vzv6z        myservice_web.2     httpd:latest        swarm03             Running             Running 45 seconds ago\nfw21lcu3zp83        myservice_web.3     httpd:latest        swarm01             Running             Running 45 seconds ago\n```\n\n可以看到 swarm01 上面运行了1个容器，swarm03上面运行了2个，而swarm02上面一个容器都没有，这是咋回事呢？哦，原来我们在创建集群过程中，对swarm02 做了状态变更的操作。` docker node update  --availability drain  swarm02`. 查看集群状态。\n\n```\n[root@swarm01 swarm]# docker node ls\nID                            HOSTNAME            STATUS              AVAILABILITY        MANAGER STATUS      ENGINE VERSION\nepoic1y0vv830vnwbc6nnacjc *   swarm01             Ready               Active              Leader              18.03.1-ce\nnsqqv181e0r7mu77azixtqhzl     swarm02             Ready               Drain               Reachable           18.03.1-ce\nmkkxaeqergz8xw21bbkd47q1c     swarm03             Ready               Active              Reachable           18.03.1-ce\n```\n\n此时打开浏览器，输入HTTP URL。任意swarm集群节点都可以访问httpd服务。\n\n![](C:\\Users\\jalon\\OneDrive\\01笔记\\Docker\\swarm-web.png)\n\n需要说明的是，由于3个节点的/var/www/html 路径未做共享存储。因此swarm会随机显示一个swarm节点的网页内容。\n\n### 4.1.3 服务扩缩容\n\n修改docker-swarm.yml 配置字段 `replicas: 5` ，将容器个数提升到5个。由于实验环境资源有限，因此这里将swarm02状态变更为可以状态，用以分配容器。\n\n` docker node update  --availability active swarm02`.\n\n重新执行部署命令`docker stack deploy -c docker-swarm.yml myservice`\n\n也可以通过命令直接对服务进行操作，但是不推荐。 `docker service scale 服务ID=服务个数`\n\n```\n[root@swarm01 swarm]# docker stack ps myservice\nID                  NAME                IMAGE               NODE                DESIRED STATE       CURRENT STATE                    ERROR               PORTS\nunwepav5z9nz        myservice_web.1     httpd:v1            swarm02             Running             Running less than a second ago\n3j5o1tktcg1l        myservice_web.2     httpd:v1            swarm01             Running             Preparing 3 seconds ago\nwl1unt6lynft        myservice_web.3     httpd:v1            swarm03             Running             Running less than a second ago\n[root@swarm01 swarm]# vi docker-swarm.yml\n[root@swarm01 swarm]# docker stack deploy -c docker-swarm.yml myservice\nUpdating service myservice_web (id: 85quzwi5k0btkn5wlht4jbco9)\nimage httpd:v1 could not be accessed on a registry to record\nits digest. Each node will access httpd:v1 independently,\npossibly leading to different nodes running different\nversions of the image.\n\n[root@swarm01 swarm]# docker stack ps myservice\nID                  NAME                IMAGE               NODE                DESIRED STATE       CURRENT STATE                    ERROR               PORTS\nunwepav5z9nz        myservice_web.1     httpd:v1            swarm02             Running             Running 24 seconds ago\n3j5o1tktcg1l        myservice_web.2     httpd:v1            swarm01             Running             Running 31 seconds ago\nwl1unt6lynft        myservice_web.3     httpd:v1            swarm03             Running             Running 24 seconds ago\nujedjxbuffxp        myservice_web.4     httpd:v1            swarm01             Running             Running less than a second ago\nml9qvjh2add7        myservice_web.5     httpd:v1            swarm02             Running             Running less than a second ago\n```\n\n同样，对于服务缩容，也仅需要修改`replicas:` 配置即可。这里不再演示。\n\n### 4.1.4 删除服务\n\n例如，删除myredis应用服务，执行docker service rm myredis，则应用服务myredis的全部副本都会被删除。 \n\n本例使用的是stack ，删除stack ，即可删除其下的服务。没错，docker语法非常类似,`docker stack rm myservice`。\n\n需要注意的是，对swarm上面服务的操作，都必须在manager节点上运行。\n\n### 4.1.5 服务升降级\n\n服务升降级，对应到docker中，则利用了容器镜像的更新，升级很好理解，直接把tag 标签版本往上提。那么降级，其实也可以理解为另外一种“升级“，只是镜像内容是上一个版本的而已。对于镜像标签的命令应该遵循一套严格的规范，这里不再赘述。\n\n先介绍利用service 命令行工具的用法。\n\n服务的滚动更新，这里我参考官网文档的例子说明。在Manager Node上执行如下命令：\n\n```\ndocker service create --replicas 3  --name redis --update-delay 10s redis:3.0.6\n```\n\n上面通过指定 --update-delay 选项，表示需要进行更新的服务，每次成功部署一个，延迟10秒钟，然后再更新下一个服务。如果某个服务更新失败，则Swarm的调度器就会暂停本次服务的部署更新。\n\n另外，也可以更新已经部署的服务所在容器中使用的Image的版本，例如执行如下命令：\n\n将Redis服务对应的Image版本有3.0.6更新为3.0.7，同样，如果更新失败，则暂停本次更新。\n\n那么，stack file 如何定义滚动更新呢？\n\n```\n      update_config:\n        parallelism: 2\n        delay: 10s\n```\n\n修改docker-swarm.yml 如下\n\n```\nversion: '3'\nservices:\n  web:\n    image: \"httpd:v2\"\n    deploy:\n      replicas: 3\n      update_config:\n        parallelism: 2\n        delay: 30s\n      resources:\n        limits:\n          cpus: \"0.5\"\n          memory: 256M\n      restart_policy:\n        condition: on-failure\n    ports:\n     - \"7080:80\"\n    volumes:\n     - /var/www/html:/usr/local/apache2/htdocs\n```\n\nparallelism 表示同时升级的个数，delay 则表示间隔多长时间升级。\n\n同时将httpd 由v1 升级到v2，这里我们使用同一个镜像，只是tag 修改了一下。查看服务状态\n\n```\n[root@swarm01 swarm]# docker stack ps myservice\nID                  NAME                  IMAGE               NODE                DESIRED STATE       CURRENT STATE                 ERROR               PORTS\nqk8pvvc9nmv0        myservice_web.1       httpd:v2            swarm01             Running             Running 56 seconds ago\nunwepav5z9nz         \\_ myservice_web.1   httpd:v1            swarm02             Shutdown            Shutdown 48 seconds ago\nuyj0pm1df9hl        myservice_web.2       httpd:v2            swarm02             Running             Running 46 seconds ago\n3j5o1tktcg1l         \\_ myservice_web.2   httpd:v1            swarm01             Shutdown            Shutdown about a minute ago\nqdjov5zf5alb        myservice_web.3       httpd:v2            swarm03             Running             Running 10 seconds ago\nwl1unt6lynft         \\_ myservice_web.3   httpd:v1            swarm03             Shutdown            Shutdown 12 seconds ago\n```\n\n可以看到同时只有2个容器在升级，同时第3个容器也是在间隔了30s 之后，才开始升级。\n\n --filter 过滤条件`docker stack ps  --filter  \"desired-state=running\"  myservice`\n\n## 4.2 应用服务高可用\n\n书接上文。在前面创建的myservice的基础上，进行容器应用的高可用验证。\n\n### 4.2.1 模拟容器故障\n\n查看当前服务状态\n\n```\n[root@swarm01 swarm]# docker stack ps  --filter  \"desired-state=running\"  myservice\nID                  NAME                IMAGE               NODE                DESIRED STATE       CURRENT STATE            ERROR               PORTS\nqk8pvvc9nmv0        myservice_web.1     httpd:v2            swarm01             Running             Running 16 minutes ago\nuyj0pm1df9hl        myservice_web.2     httpd:v2            swarm02             Running             Running 16 minutes ago\nqdjov5zf5alb        myservice_web.3     httpd:v2            swarm03             Running             Running 15 minutes ago\n```\n\nswarm01-03 分别运行3个容器。现在登陆swam03节点，将其容器进程Kill 掉。模拟容器意外故障。\n\n将stack ps 得到的swarm03节点的ID `qdjov5zf5alb` 代入命令inspect。\n\n```\n[root@swarm01 swarm]# docker inspect qdjov5zf5alb\n[\n    {\n        \"ID\": \"qdjov5zf5albdm1o7vhuwoxjt\",\n...\n        \"Spec\": {\n            \"ContainerSpec\": {\n                \"Image\": \"httpd:v2\",\n                \"Labels\": {\n                    \"com.docker.stack.namespace\": \"myservice\"\n                },\n                \"Privileges\": {\n                    \"CredentialSpec\": null,\n                    \"SELinuxContext\": null\n                },\n                \"Mounts\": [\n                    {\n                        \"Type\": \"bind\",\n                        \"Source\": \"/var/www/html\",\n                        \"Target\": \"/usr/local/apache2/htdocs\"\n                    }\n                ],\n                \"Isolation\": \"default\"\n            },\n            \"Resources\": {\n                \"Limits\": {\n                    \"NanoCPUs\": 500000000,\n                    \"MemoryBytes\": 268435456\n                }\n            },\n....\n        \"ServiceID\": \"85quzwi5k0btkn5wlht4jbco9\",\n        \"Slot\": 3,\n        \"NodeID\": \"mkkxaeqergz8xw21bbkd47q1c\",\n        \"Status\": {\n            \"Timestamp\": \"2018-08-07T09:02:25.674376822Z\",\n            \"State\": \"running\",\n            \"Message\": \"started\",\n            \"ContainerStatus\": {\n                \"ContainerID\": \"fa7d228e0c1981bfc78bca0b81363d7ca44cac99844c2996afe528ae3ae9a129\",\n                \"PID\": 8852,\n                \"ExitCode\": 0\n            },\n            \"PortStatus\": {}\n        },\n```\n\n得到得到其对于的容器fa7d228e0c1981bfc78bca0b81363d7ca44cac99844c2996afe528ae3ae9a129 （swarm03节点上运行）\n\n登陆swarm03节点，查看容器fa7d22 对应的进程。得到其进程号，并将其杀死。\n\n```\n[root@swarm03 /]# ps -ef|grep fa7d228e0c19\nroot      8832  1129  0 05:02 ?        00:00:00 docker-containerd-shim -namespace moby -workdir /var/lib/docker/containerd/daemon/io.containerd.runtime.v1.linux/moby/fa7d228e0c1981bfc78bca0b81363d7ca44cac99844c2996afe528ae3ae9a129 -address /var/run/docker/containerd/docker-containerd.sock -containerd-binary /usr/bin/docker-containerd -runtime-root /var/run/docker/runtime-runc\nroot      9989  1090  0 05:32 pts/0    00:00:00 grep --color=auto fa7d228e0c19\n[root@swarm03 /]# kill -9 8832\n```\n\n### 4.2.2 故障查看\n\n立即查看服务状态。\n\n```\n[root@swarm03 /]# docker stack ps myservice\nID                  NAME                  IMAGE               NODE                DESIRED STATE       CURRENT STATE             ERROR                         PORTS\nqk8pvvc9nmv0        myservice_web.1       httpd:v2            swarm01             Running             Running 32 minutes ago\nunwepav5z9nz         \\_ myservice_web.1   httpd:v1            swarm02             Shutdown            Shutdown 32 minutes ago\nuyj0pm1df9hl        myservice_web.2       httpd:v2            swarm02             Running             Running 32 minutes ago\n3j5o1tktcg1l         \\_ myservice_web.2   httpd:v1            swarm01             Shutdown            Shutdown 32 minutes ago\nhx741f0kye5a        myservice_web.3       httpd:v2            swarm03             Running             Running 29 seconds ago\nqdjov5zf5alb         \\_ myservice_web.3   httpd:v2            swarm03             Shutdown            Failed 36 seconds ago     \"task: non-zero exit (137)\"\nwl1unt6lynft         \\_ myservice_web.3   httpd:v1            swarm03             Shutdown            Shutdown 31 minutes ago\n```\n\n可以看到，swarm03上面的容器myservice_web.3 立即被重启。故障即刻恢复。\n\n### 4.2.3 结论\n\n通过以上实验我们得出，在docker swarm 集群服务中，任意容器出现故障意外死亡，都会被重启。满足应用服务高可用的需求。但是，可能会有同学有疑问，你这个只是kill掉进程，模拟的是进程级别的故障，如果是主机宕机呢，swarm是否也能满足呢？答案是肯定的！\n\n在下一个章节，就让我们继续模拟主机级别的故障，验证服务的高可用。同时，由于我们环境配置的均是Manager节点，我们也可以同时验证Swarm Manager 的高可用。让我们拭目以待吧！\n\n## 4.3 Swarm Manager 高可用\n\n查看当前swarm集群节点信息\n\n```\n[root@swarm01 swarm]# docker node ls\nID                            HOSTNAME            STATUS              AVAILABILITY        MANAGER STATUS      ENGINE VERSION\nepoic1y0vv830vnwbc6nnacjc *   swarm01             Ready               Active              Leader              18.03.1-ce\nnsqqv181e0r7mu77azixtqhzl     swarm02             Ready               Active              Reachable           18.03.1-ce\nmkkxaeqergz8xw21bbkd47q1c     swarm03             Ready               Active              Reachable           18.03.1-ce\n\n```\n\n以上可知，swarm01 为swarm 集群管理节点。倘若swarm01 突然宕机关机会怎样呢，整个swarm 集群会失效，陷入瘫痪吗？其上运行的容器服务是否会全部挂掉，无法继续提供服务?\n\n### 4.3.1 模拟主机宕机\n\n将虚拟机swarm01 直接关机。\n\n### 4.3.2 故障查看\n\n 登陆swarm03 ，查看swarm集群状态。\n\n```\n[root@swarm03 /]# docker node ls\nID                            HOSTNAME            STATUS              AVAILABILITY        MANAGER STATUS      ENGINE VERSION\nepoic1y0vv830vnwbc6nnacjc     swarm01             Unknown             Active              Unreachable         18.03.1-ce\nnsqqv181e0r7mu77azixtqhzl     swarm02             Ready               Active              Reachable           18.03.1-ce\nmkkxaeqergz8xw21bbkd47q1c *   swarm03             Ready               Active              Leader              18.03.1-ce\n\n```\n\n在经历短暂的间隙后，可看到swarm集群认定swarm01 现在处于Unknown 状态，MANAGER STATUS 处于Unreachable状态。同时，swarm集群基于raft 算法，重新推举出新的Leader swarm03。\n\n再过一段时间，swarm01 状态变成Down\n\n```\n[root@swarm03 ~]# docker node ls\nID                            HOSTNAME            STATUS              AVAILABILITY        MANAGER STATUS      ENGINE VERSION\nepoic1y0vv830vnwbc6nnacjc     swarm01             Down                Active              Unreachable         18.03.1-ce\nnsqqv181e0r7mu77azixtqhzl     swarm02             Ready               Active              Reachable           18.03.1-ce\nmkkxaeqergz8xw21bbkd47q1c *   swarm03             Ready               Active              Leader              18.03.1-ce\n\n```\n\n查看服务状态\n\n```\n[root@swarm03 /]# docker stack ps myservice\nID                  NAME                  IMAGE               NODE                DESIRED STATE       CURRENT STATE                ERROR                         PORTS\nzfbvcz5iu8g7        myservice_web.1       httpd:v2            swarm03             Running             Running 2 minutes ago\nqk8pvvc9nmv0         \\_ myservice_web.1   httpd:v2            swarm01             Shutdown            Running about an hour ago\nunwepav5z9nz         \\_ myservice_web.1   httpd:v1            swarm02             Shutdown            Shutdown about an hour ago\nuyj0pm1df9hl        myservice_web.2       httpd:v2            swarm02             Running             Running 3 minutes ago\n3j5o1tktcg1l         \\_ myservice_web.2   httpd:v1            swarm01             Shutdown            Shutdown about an hour ago\nhx741f0kye5a        myservice_web.3       httpd:v2            swarm03             Running             Running 3 minutes ago\nqdjov5zf5alb         \\_ myservice_web.3   httpd:v2            swarm03             Shutdown            Failed 21 minutes ago        \"task: non-zero exit (137)\"\nwl1unt6lynft         \\_ myservice_web.3   httpd:v1            swarm03             Shutdown            Shutdown about an hour ago\n```\n\n可以看到容器服务个数依然维持在3个，原先在swarm01上面运行的容器，被分配到swarm02和swarm03上面。现在对服务进行扩容操作，以验证此时swarm集群是否对服务还有管理能力。\n\n```\n[root@swarm03 swarm]# docker stack deploy -c docker-swarm.yml myservice\nUpdating service myservice_web (id: 85quzwi5k0btkn5wlht4jbco9)\nimage httpd:v2 could not be accessed on a registry to record\nits digest. Each node will access httpd:v2 independently,\npossibly leading to different nodes running different\nversions of the image.\n\n[root@swarm03 swarm]# docker stack ps  myservice\nID                  NAME                  IMAGE               NODE                DESIRED STATE       CURRENT STATE                ERROR                         PORTS\nzfbvcz5iu8g7        myservice_web.1       httpd:v2            swarm03             Running             Running 6 minutes ago\nqk8pvvc9nmv0         \\_ myservice_web.1   httpd:v2            swarm01             Shutdown            Running about an hour ago\nunwepav5z9nz         \\_ myservice_web.1   httpd:v1            swarm02             Shutdown            Shutdown about an hour ago\nuyj0pm1df9hl        myservice_web.2       httpd:v2            swarm02             Running             Running 6 minutes ago\n3j5o1tktcg1l         \\_ myservice_web.2   httpd:v1            swarm01             Shutdown            Shutdown about an hour ago\nhx741f0kye5a        myservice_web.3       httpd:v2            swarm03             Running             Running 6 minutes ago\nqdjov5zf5alb         \\_ myservice_web.3   httpd:v2            swarm03             Shutdown            Failed 25 minutes ago        \"task: non-zero exit (137)\"\nwl1unt6lynft         \\_ myservice_web.3   httpd:v1            swarm03             Shutdown            Shutdown about an hour ago\n6nifzsbysug5        myservice_web.4       httpd:v2            swarm02             Running             Running 20 seconds ago\nuwgnl2usv59a        myservice_web.5       httpd:v2            swarm02             Running             Running 20 seconds ago\n```\n\n可以看到，此时swarm集群依然具有管理能力（服务扩容能力）。也从侧面验证swarm manager的高可用能力。\n\n此时重启swarm01 ，看看 swarm01节点是否会自动加入集群。\n\n登陆swarm01 ，查看集群状态\n\n```\n[root@swarm01 ~]# docker node ls\nError response from daemon: Swarm is encrypted and needs to be unlocked before it can be used. Please use \"docker swarm unlock\" to unlock it.\n```\n\n可以看到，此时swarm01 处于被锁的状态。需要解锁。\n\n```\n[root@swarm01 ~]# docker swarm unlock\nPlease enter unlock key:\nError response from daemon: invalid key string\n```\n\n那么  unlock key 从哪里取值呢？答案就是，需要在可用的集群管理节点，比如swarm03上面执行命令\n\n```\n[root@swarm03 ~]# docker swarm unlock-key\nTo unlock a swarm manager after it restarts, run the `docker swarm unlock`\ncommand and provide the following key:\n\n    SWMKEY-1-796L7+nU55V/qGCMPgBvKS/5PyotvNaKPS4SJ0AUe+g\n\nPlease remember to store this key in a password manager, since without it you\nwill not be able to restart the manager.\n```\n\n通过该 unlock key ： SWMKEY-1-796L7+nU55V/qGCMPgBvKS/5PyotvNaKPS4SJ0AUe+g ，重新加入集群。\n\n此时在swarm01上面查看集群状态，即可看到已经重新加入集群。\n\n```bash\n[root@swarm01 ~]# docker node ls\nID                            HOSTNAME            STATUS              AVAILABILITY        MANAGER STATUS      ENGINE VERSION\nepoic1y0vv830vnwbc6nnacjc *   swarm01             Ready               Active              Reachable           18.03.1-ce\nnsqqv181e0r7mu77azixtqhzl     swarm02             Ready               Active              Reachable           18.03.1-ce\nmkkxaeqergz8xw21bbkd47q1c     swarm03             Ready               Active              Leader              18.03.1-ce\n```\n\n但是如果再把swarm03关掉，即同时宕掉2个节点，则此时3个管理节点，只剩下一个节点，不满足[Raft](https://raft.github.io/raft.pdf) 协议的节点个数要求（>N/2），集群功能就会失效，不能再管理集群，但是集群上的服务还能够继续运行。\n\n继续我们的脚步，将swarm03,swarm01 都关机。\n\n登陆swarm02，查看集群状态\n\n ```\n[root@swarm02 ~]# docker node ls\nError response from daemon: rpc error: code = Unknown desc = The swarm does not have a leader. It's possible that too few managers are online. Make sure more than half of the managers are online.\n[root@swarm02 ~]# docker stack ps myservice\nError response from daemon: rpc error: code = Unknown desc = The swarm does not have a leader. It's possible that too few managers are online. Make sure more than half of the managers are online.\n ```\n\n可以看到，此时集群功能已经失效，不能通过swarm 命令查看集群和管理集群了。那么运行其上的任务呢？\n\n```\n[root@swarm02 ~]# docker container ls\nCONTAINER ID        IMAGE                COMMAND              CREATED             STATUS              PORTS                    NAMES\n2cc1b76be2cf        httpd:v2             \"httpd-foreground\"   2 hours ago         Up 2 hours          80/tcp                   myservice_web.5.uwgnl2usv59aygb5e9a8cig2y\n96034bff8902        httpd:v2             \"httpd-foreground\"   2 hours ago         Up 2 hours          80/tcp                   myservice_web.4.6nifzsbysug5medburznrrr7h\n82beb10e4cac        httpd:v2             \"httpd-foreground\"   3 hours ago         Up 3 hours          80/tcp                   myservice_web.2.uyj0pm1df9hlx9vdvck36ilbl\nea9f879cdf12        composetest_web:v1   \"python app.py\"      3 weeks ago         Up 5 hours          0.0.0.0:5000->5000/tcp   elegant_borg\n\n```\n\n还好还好，通过 `docker container ls`  查看swarm02节点上的容器，可以看到仍然有3个容器在运行，这些都是之前已经分配好的。显然，swarm集群即使失效了，也不会影响其上运行的容器服务正常运行，只是失去了管理和调度的能力了。\n\n此时，如果再次打开swarm01 和 swarm03的电源，他们会重新组建集群吗？从上文中我们知道，离线的manager节点要重新加入集群需要解锁，而钥匙需要通过原集群运行`docker swarm unlock-key` 获取。那么显然，原集群已经失效了。那此时会怎样呢？\n\n不幸的是，确实swarm01 和 swarm03 都无法自动加入集群，确实需要通过`docker swarm unlock`进行解锁。但是幸运的是，**unlock key 是不变的**，跟刚才swarm01宕机时重新加入的key 是一样的。分别在swarm01 和swarm03  执行解锁操作。集群最终恢复正常。因此需要在创建集群后，第一时间保存unlock key 。\n\n```\n[root@swarm03 ~]# docker node ls\nID                            HOSTNAME            STATUS              AVAILABILITY        MANAGER STATUS      ENGINE VERSION\nepoic1y0vv830vnwbc6nnacjc     swarm01             Ready               Active              Reachable           18.03.1-ce\nnsqqv181e0r7mu77azixtqhzl     swarm02             Ready               Active              Leader              18.03.1-ce\nmkkxaeqergz8xw21bbkd47q1c *   swarm03             Ready               Active              Reachable           18.03.1-ce\n```\n\n注：在创建集群的时候，可设置自动解锁，即主机重启时，不用输入密钥，即可重新加入集群\n\n```\ndocker swarm init --autolock=false\ndocker swarm update --autolock=false\n```\n\n\n\n\n\n### 4.3.3 结论\n\n从以上实验中也可以看出，只要管理节点正常，服务的高可用就能达到，但是管理节点的个数必须保持在总的管理节点个数的一半以上，即3个只允许宕机一台，5台只允许宕机2台。而总的管理节点的个数一般不超过7个，如果太多的话，集群内管理节点通信消耗会比较大。由于偶数性价比不高（因为4台也只能宕机掉1台跟3台时是一样的），所以管理节点的个数一般都是奇数。\n\n管理节点的个数以及允许宕机的个数如下：\n\n| mannager node | 允许宕机个数 | 服务运行状态 |\n| :-----------: | :----------: | :----------: |\n|       3       |      1       |     正常     |\n|       5       |      2       |     正常     |\n|       7       |      3       |     正常     |\n\n\n\n## 4.4 网络故障转移\n\n### 4.4.1 模拟网络分区\n\n环境说明：仍然是3台虚拟机组成的swarm管理集群 \n\nhd_hd-1服务：运行4个副本，分别swarm01/swarm02 各一个容器，swarm03运行2个容器\n\n将swarm02网卡拔掉，模拟网络故障分区。\n\n备注使用 --autolock=false 初始化集群：docker swarm init --autolock=false  --advertise-addr 172.21.111.56 --listen-addr 172.21.111.56:2377  \n\n###  4.4.2 观察swarm node信息\n\n查看集群信息和服务状态\n\n```\n[root@swarm01 httpd]# docker node ls\nID                            HOSTNAME            STATUS              AVAILABILITY        MANAGER STATUS      ENGINE VERSION\nwk8jaypu23vr63082arpsl4ut *   swarm01             Ready               Active              Leader              18.03.1-ce\nq9lcw91vus73wyly174iy72h3     swarm02             Down                Active              Unreachable         18.03.1-ce\n1xelzfyr5kbn7kk70le45anei     swarm03             Ready               Active              Reachable           18.06.1-ce\n[root@swarm01 httpd]# docker service ps hd_hd-1\nID                  NAME                IMAGE               NODE                DESIRED STATE       CURRENT STATE           ERROR               PORTS\nt604bqtcj588        hd_hd-1.1           httpd:latest        swarm01             Running             Running 2 minutes ago\nxfajbfz51nd9         \\_ hd_hd-1.1       httpd:latest        swarm02             Shutdown            Running 8 minutes ago\n8clt10jl1xyf        hd_hd-1.2           httpd:latest        swarm01             Running             Running 6 minutes ago\npy5cavsog5mo        hd_hd-1.3           httpd:latest        swarm03             Running             Running 7 minutes ago\noigopryy1kro        hd_hd-1.4           httpd:latest        swarm03             Running             Running 7 minutes ago\n```\n\n我们可以发现，集群认为swarm02现在已经失联（不论是宕机还是网络端开）。现在swarm02上面的容器被迁移到swarm01上面（调度策略应该是swarm01上面目前运行的容器个数比swarm03少）。\n\n通过虚拟机控制台登陆swarm02，验证集群功能和容器是否在运行\n\n![1537414250767](C:\\Users\\jalon\\OneDrive\\01笔记\\Docker\\swarm-network.png)\n\n我们可以发现，swarm02由于当前是一个节点，根据raft算法，处于swarm集群功能不可用状态。docker ps查看容器，发现容器仍然在正常运行。得出结论，swarm集群功能失效，但是不影响原先已经运行的docker容器。\n\n综合swarm01/swarm03上面的容器个数和swarm02上面的容器个数，现在已经有5个容器了。而我们预设的容器个数是4个。那么当swarm02重新加入集群，会发生什么事情呢？\n\n### 4.4.3 网络故障恢复\n\n把swarm02网卡重新接上。swarm02 会自动加入集群。查看集群信息和服务信息。\n\n```\n[root@swarm01 httpd]# docker node ls\nID                            HOSTNAME            STATUS              AVAILABILITY        MANAGER STATUS      ENGINE VERSION\nlkivi02vx5z939z0x7lacuarl *   swarm01             Ready               Active              Leader              18.03.1-ce\npdp0awtf34o4x60x812h07a6e     swarm02             Ready               Active              Reachable           18.03.1-ce\nqavrh9e6pjdq2aoxpucl6nohm     swarm03             Ready               Active              Reachable           18.06.1-ce\n[root@swarm01 httpd]# docker service ps hd_hd-1\nID                  NAME                IMAGE               NODE                DESIRED STATE       CURRENT STATE            ERROR               PORTS\npepxi7bmzi8r        hd_hd-1.1           httpd:latest        swarm01             Running             Running 2 minutes ago\nyf42dr6fqvl5         \\_ hd_hd-1.1       httpd:latest        swarm02             Shutdown            Shutdown 2 minutes ago\nj6iq7tpsnwb0        hd_hd-1.2           httpd:latest        swarm03             Running             Running 2 minutes ago\nk23a3ezbocit        hd_hd-1.3           httpd:latest        swarm03             Running             Running 2 minutes ago\nbqra1k9m0ojc        hd_hd-1.4           httpd:latest        swarm01             Running             Running 2 minutes ago\n```\n\n我们发现swarm02已经正常加入集群。登陆swarm02，查看swarm02上面原先的容器也已经停掉了。符合我们的预期。\n\n![1537419010126](C:\\Users\\jalon\\OneDrive\\01笔记\\Docker\\swarm-network-2.png)\n\n\n\n# 5.附录\n\n## 5.1参考文章：\n\ndocker swarm介绍：\n\nhttp://shiyanjun.cn/archives/1625.html\n\nhttps://jiayi.space/post/docker-swarmrong-qi-ji-qun-guan-li-gong-ju\n\nhttp://www.shangyang.me/2018/02/01/docker-swarm-03-architect/\n\nhttp://blog.51cto.com/cloudman/1952873\n\nhttps://www.cnblogs.com/bigberg/p/8761047.html\n\n服务发现：https://success.docker.com/article/ucp-service-discovery\n\nraft协议：http://thesecretlivesofdata.com/raft/\n\n## 5.2 Swarm Node 节点操作\n\n### 5.2.1 状态变更 \n\n前面我们已经提到过，Node的AVAILABILITY有三种状态：Active、Pause、Drain，对某个Node进行变更，可以将其AVAILABILITY值通过Docker CLI修改为对应的状态即可，下面是常见的变更操作：\n\n- 设置Manager Node只具有管理功能\n- 对服务进行停机维护，可以修改AVAILABILITY为Drain状态\n- 暂停一个Node，然后该Node就不再接收新的Task\n- 恢复一个不可用或者暂停的Node\n\n例如，将Manager Node的AVAILABILITY值修改为Drain状态，使其只具备管理功能，执行如下命令：\n\n```\n[root@swarm02 ~]# docker node update  --availability drain  swarm02\nswarm02\n[root@swarm02 ~]# docker node ls\nID                            HOSTNAME            STATUS              AVAILABILITY        MANAGER STATUS      ENGINE VERSION\nepoic1y0vv830vnwbc6nnacjc     swarm01             Ready               Active              Leader              18.03.1-ce\nnsqqv181e0r7mu77azixtqhzl *   swarm02             Ready               Drain               Reachable           18.03.1-ce\nmkkxaeqergz8xw21bbkd47q1c     swarm03             Ready               Active              Reachable           18.03.1-ce\n```\n\n这样，Manager Node不能被指派Task，也就是不能部署实际的Docker容器来运行服务，而只是作为管理Node的角色。 \n\n### 5.2.2 添加标签\n\n每个Node的主机配置情况可能不同，比如有的适合运行CPU密集型应用，有的适合运行IO密集型应用，Swarm支持给每个Node添加标签元数据，这样可以根据Node的标签，来选择性地调度某个服务部署到期望的一组Node上。 给SWarm集群中的某个Worker Node添加标签，执行如下命令格式如下： \n\n```\n[root@swarm02 ~]# docker node update  --label-add  app=dev swarm02\nswarm02\n```\n\n### 5.2.3 提权/降权\n\n改变Node的角色，Worker Node可以变为Manager Node，这样实际Worker Node有工作Node变成了管理Node，对应操作分别是： \n\n```\ndocker node promote swarm02\ndocker node demote swarm02\n```\n\n\n\n## 5.3 网络管理\n\n### 5.3.1 添加overlay 网络\n\noverlay网络是swarm中默认的跨主机网络模型。在Swarm集群中可以使用Overlay网络来连接到一个或多个服务。具体添加Overlay网络，首先，我们需要创建在Manager Node上创建一个Overlay网络，执行如下命令： \n\n`docker network create --driver overlay my-network`\n\n创建完Overlay网络my-network以后，Swarm集群中所有的Manager Node都可以访问该网络。然后，我们在创建服务的时候，只需要指定使用的网络为已存在的Overlay网络即可，如下命令所示： \n\n`docker service create --replicas 3 --network my-network --name myweb  nginx`\n\n这样，如果Swarm集群中其他Node上的Docker容器也使用my-network这个网络，那么处于该Overlay网络中的所有容器之间，通过网络可以连通。  \n\n\n\n\n\n## 5.4 API\n\n```\n$ curl --unix-socket /var/run/docker.sock -H \"Content-Type: application/json\" \\\n  -d '{\"Image\": \"alpine\", \"Cmd\": [\"echo\", \"hello world\"]}' \\\n  -X POST http:/v1.24/containers/create\n{\"Id\":\"1c6594faf5\",\"Warnings\":null}\n\n$ curl --unix-socket /var/run/docker.sock -X POST http:/v1.24/containers/1c6594faf5/start\n\n$ curl --unix-socket /var/run/docker.sock -X POST http:/v1.24/containers/1c6594faf5/wait\n{\"StatusCode\":0}\n\n$ curl --unix-socket /var/run/docker.sock \"http:/v1.24/containers/1c6594faf5/logs?stdout=1\"\nhello world\n```\n\nhttps://docs.docker.com/develop/sdk/#sdk-and-api-quickstart\n\nhttps://www.programmableweb.com/api/docker-swarm\n\n\n\n## 5.5 Docker 三剑客\n\n- docker-compose 负责组织应用，应用由哪些服务组成，每个服务由多少个容器，均在该配置文件里面配置。\n- docker-machine 负责提供虚拟机，该虚拟机具备docker daemon服务能力。创建的虚拟机，可以用来组成docker swarm集群。docker-machine 并不是必须的功能。完全可以手动安装docker 服务。\n- docker swarm 负责组织docker集群，组织跨主机的docker集群管理能力。\n\n","source":"_posts/docker-swarm-manager-ha.md","raw":"---\ntitle: Docker Swarm管理节点高可用分析\ndate: 2018-10-19 16:53:53\ntags: \n - Docker Swarm\n - HA\ncategory: 技术 \n---\n\n# 1.简介\n\n## 1.1 swarm是什么\n\n`Swarm` 是使用 [`SwarmKit`](https://github.com/docker/swarmkit/) 构建的 Docker 引擎内置（原生）的集群管理和编排工具。其主要作用是把若干台Docker主机抽象为一个整体，并且通过一个入口统一管理这些Docker主机上的各种Docker资源。Swarm和Kubernetes比较类似，但是更加轻量，具有的功能也较kubernetes更少一些。\n\nDocker v1.12 是一个非常重要的版本，Docker 重新实现了集群的编排方式。在此之前，提供集群功能的 Docker Swarm 是一个单独的软件，而且依赖外部数据库（比如 Consul、etcd 或 Zookeeper）。从 v1.12 开始，Docker Swarm 的功能已经完全与 Docker Engine 集成，要管理集群，只需要启动 Swarm Mode。安装好 Docker，Swarm 就已经在那里了，服务发现也在那里了（不需要安装 Consul 等外部数据库）。\n\nDocker的Swarm(集群)模式，集成很多工具和特性，比如：跨主机上快速部署服务，服务的快速扩展，集群的管理整合到docker引擎，这意味着可以不可以不使用第三方管理工具。分散设计，声明式的服务模型，可扩展，状态协调处理，多主机网络，分布式的服务发现，负载均衡，滚动更新，安全（通信的加密）等等。 \n\n![](\\img\\docker-swarm-logo.png)\n\n官方网站：https://docs.docker.com/engine/swarm/\n\n<!-- more -->\n\n## 1.2 swarm基本概念\n\n### 1.2.1 节点 \n\n运行 Docker 的主机可以主动初始化一个 `Swarm` 集群或者加入一个已存在的 `Swarm` 集群，这样这个运行 Docker 的主机就成为一个 `Swarm` 集群的节点 (`node`) 。\n\n节点分为管理 (`manager`) 节点和工作 (`worker`) 节点。\n\n管理节点用于 `Swarm` 集群的管理，`docker swarm` 命令基本只能在管理节点执行（节点退出集群命令 `docker swarm leave` 可以在工作节点执行）。一个 `Swarm` 集群可以有多个管理节点，但只有一个管理节点可以成为 `leader`，`leader` 通过 `raft` 协议实现。\n\n工作节点是任务执行节点，管理节点将服务 (`service`) 下发至工作节点执行。管理节点默认也作为工作节点。也可以通过配置让服务只运行在管理节点。\n\nDocker 官网的这张图片形象的展示了集群中管理节点与工作节点的关系。\n\n![](\\img\\swarm-node.png)\n\n### 1.2.2 服务和任务\n\n任务 （`Task`）是 `Swarm` 中的最小的调度单位，目前来说就是一个单一的容器。\n\n服务 （`Services`） 是指一组任务的集合，服务定义了任务的属性。服务有两种模式：\n\n- `replicated services` 按照一定规则在各个工作节点上运行指定个数的任务。\n- `global services` 每个工作节点上运行一个任务\n\n两种模式通过 `docker service create` 的 `--mode` 参数指定。\n\n### 1.2.3 负载均衡\n\n`manager`群集管理器使用 **ingress load balancing** 来对外公开群集提供的服务。`manager`可以自动为**PublishedPort** 分配服务，也可以手动配置。如果未指定端口，则swarm管理器会为服务分配30000-32767范围内的端口。\n\n外部组件（例如云负载平衡器）可以访问群集中任何节点的PublishedPort上的服务，无论该节点当前是否正在运行该服务的任务。群集中的所有节点都将入口连接到正在运行的任务实例。\n\nSwarm模式有一个内部DNS组件，可以自动为swarm中的每个服务分配一个DNS条目。群集管理器使用**内部负载平衡**来根据服务的DNS名称在群集内的服务之间分发请求。\n\n\n\n## 1.3 swarm特性\n\n- **集群管理与Docker Engine集成:**使用Docker Engine CLI来创建一个你能部署应用服务到Docker Engine的swarm。你不需要其他编排软件来创建或管理swarm。\n\n- **分散式设计：**Docker Engine不是在部署时处理节点角色之间的差异，而是在运行时扮演自己角色。你可以使用Docker Engine部署两种类型的节点，管理器和worker。这意味着你可以从单个磁盘映像构建整个swarm。\n\n- **声明性服务模型：** Docker Engine使用声明性方法来定义应用程序堆栈中各种服务的所需状态。例如，你可以描述由消息队列服务和数据库后端的Web前端服务组成的应用程序。\n\n- **伸缩性：**对于每个服务，你可以声明要运行的任务数。当你向上或向下缩放时，swarm管理器通过添加或删除任务来自动适应，以保持所需状态。\n\n- **期望的状态协调：**swarm管理器节点持续监控群集状态，并调整你描述的期望状态与实际状态之间的任何差异。 例如，如果设置运行一个10个副本容器的服务，这时worker机器托管其中的两个副本崩溃，管理器则将创建两个新副本以替换已崩溃的副本。 swarm管理器将新副本分配给正在运行和可用的worker。\n\n- **多主机网络：**你可以为服务指定覆盖网络([overlay](https://www.centos.bz/tag/overlay/) network)。 当swarm管理器初始化或更新应用程序时，它会自动为容器在覆盖网络(overlay network)上分配地址。\n\n- **服务发现：**Swarm管理器节点为swarm中的每个服务分配唯一的DNS名称，并负载平衡运行中的容器。 你可以通过嵌入在swarm中的DNS服务器查询在swarm中运行中的每个容器。\n\n- **负载平衡：**你可以将服务的端口暴露给外部的负载均衡器。 在内部，swarm允许你指定如何在节点之间分发服务容器。\n\n- **安全通信：**swarm中的每个节点强制执行TLS相互验证和加密，以保护其自身与所有其他节点之间的通信。 你可以选择使用自签名根证书或来自自定义根CA的证书。\n\n- **滚动更新：**在上线新功能期间，你可以增量地应用服务更新到节点。 swarm管理器允许你控制将服务部署到不同节点集之间的延迟。 如果出现任何问题，你可以将任务回滚到服务的先前版本。\n\n\n# 2.架构分析\n\n## 2.1 基本架构\n\nDocker Swarm提供了基本的集群能力，能够使多个Docker Engine组合成一个group，提供多容器服务。Swarm使用标准的Docker API，启动容器可以直接使用docker run命令。Swarm更核心的则是关注如何选择一个主机并在其上启动容器，最终运行服务。 Docker Swarm基本架构，如下图所示： \n\n![](\\img\\docker-swarm-architecture.png)\n\n\n\n如上图所示，Swarm Node表示加入Swarm集群中的一个Docker Engine实例，基于该Docker Engine可以创建并管理多个Docker容器。其中，最开始创建Swarm集群的时候，Swarm Manager便是集群中的第一个Swarm Node。在所有的Node中，又根据其职能划分为Manager Node和Worker Node。\n\n### 2.1 Manager Node\n\nManger 节点，顾名思义，是进行 Swarm 集群的管理工作的，它的管理工作集中在如下部分，\n\n- 维护一个集群的状态；\n- 对 Services 进行调度；\n- 为 Swarm 提供外部可调用的 API 接口；\n\nManager 节点需要时刻维护和保存当前 Swarm 集群中各个节点的一致性状态，这里主要是指各个 Tasks 的执行的状态和其它节点的状态；因为 Swarm 集群是一个典型的分布式集群，在保证一致性上，Manager 节点采用 [Raft](https://raft.github.io/raft.pdf) 协议来保证分布式场景下的数据一致性；\n\n通常为了保证 Manager 节点的高可用，Docker 建议采用奇数个 Manager 节点，这样的话，你可以在 Manager 失败的时候不用关机维护，我们给出如下的建议：\n\n- 3 个 Manager 节点最多可以同时容忍 1 个 Manager 节点失效的情况下保证高可用；\n- 5 个 Manager 节点最多可以同时容忍 2 个 Manager 节点失效的情况下保证高可用；\n- N 个 Manager 节点最多可以同时容忍 (N−1)/2个 Manager 节点失效的情况下保证高可用；\n- Docker 建议最多最多的情况下，使用 7 个 Manager 节点就够了，否则反而会降低集群的性能了。\n\n### 2.2 Worker Node\n\nWorker Node接收由Manager Node调度并指派的Task，启动一个Docker容器来运行指定的服务，并且Worker Node需要向Manager Node汇报被指派的Task的执行状态。\n\n### 2.3  更换角色\n\n通过 docker node promote 命令将一个 Worker 节点提升为 Manager 节点。通常情况下，该命令使用在维护的过程中，需要将 Manager 节点占时下线进行维护操作；同样可以使用 docker node demote 将某个 manager 节点降级为 worker 节点。 \n\n\n\n## 2.2 设计架构\n\n![](\\img\\swarm-architecture-1.jpg)\n\n从前文可知， Swarm 集群的管理工作是由manager节点实现。如上图所示，manager节点实现的功能主要包括：node discovery，scheduler,cluster管理等。同时，为了保证Manager 节点的高可用，Manager 节点需要时刻维护和保存当前 Swarm 集群中各个节点的一致性状态。在保证一致性上，Manager 节点采用 [Raft](https://raft.github.io/raft.pdf) 协议来保证分布式场景下的数据一致性；\n\nDocker Swarm内置了Raft一致性算法，可以保证分布式系统的数据保持一致性同步。Etcd, Consul等高可用键值存储系统也是采用了这种算法。这个算法的作用简单点说就是随时保证集群中有一个Leader，由Leader接收数据更新，再同步到其他各个Follower节点。在Swarm中的作用表现为当一个Leader 节点 down掉时，系统会立即选取出另一个Leader节点，由于这个节点同步了之前节点的所有数据，所以可以无缝地管理集群。\n\nRaft的详细解释可以参考[《The Secret Lives of Data--Raft: Understandable Distributed Consensus》](http://thesecretlivesofdata.com/raft/)。\n\n### 2.2.1 跨主机容器通信\n\nDocker Swarm 内置的跨主机容器通信方案是overlay网络，这是一个基于vxlan协议的网络实现。VxLAN 可将二层数据封装到 UDP 进行传输，VxLAN 提供与 VLAN 相同的以太网二层服务，但是拥有更强的扩展性和灵活性。 overlay 通过虚拟出一个子网，让处于不同主机的容器能透明地使用这个子网。所以跨主机的容器通信就变成了在同一个子网下的容器通信，看上去就像是同一主机下的bridge网络通信。\n\n为支持容器跨主机通信，Docker 提供了 overlay driver，使用户可以创建基于 VxLAN 的 overlay 网络。其实，docker 会创建一个 bridge 网络 “docker_gwbridge”，为所有连接到 overlay 网络的容器提供访问外网的能力。``docker network inspect docker_gwbridge`  查看网络信息。\n\n![](\\img\\overlay-gwbridge.jpg)\n\n下面我们讨论下overlay 网络的具体实现：\n\ndocker 会为每个 overlay 网络创建一个独立的 network namespace，其中会有一个 linux bridge br0，endpoint 还是由 veth pair 实现，一端连接到容器中（即 eth0），另一端连接到 namespace 的 br0 上。\n\nbr0 除了连接所有的 endpoint，还会连接一个 vxlan 设备，用于与其他 host 建立 vxlan tunnel。容器之间的数据就是通过这个 tunnel 通信的。逻辑网络拓扑结构如图所示：\n\n![](\\img/overlay.jpg)\n\n### 2.2.2 服务发现\n\ndocker Swarm mode下会为每个节点的docker engine内置一个DNS server，各个节点间的DNS server通过control plane的gossip协议互相交互信息。此处DNS server用于容器间的服务发现。swarm mode会为每个 --net=自定义网络的service分配一个DNS entry。目前必须是自定义网络，比如overaly。而bridge和routing mesh的service，是不会分配DNS的。\n\n那么，下面就来详细介绍服务发现的原理。\n\n每个Docker容器都有一个DNS解析器，它将DNS查询转发到docker engine，该引擎充当DNS服务器。docker 引擎收到请求后就会在发出请求的容器所在的所有网络中，检查域名对应的是不是一个容器或者是服务，如果是，docker引擎就会从存储的key-value建值对中查找这个容器名、任务名、或者服务名对应的IP地址，并把这个IP地址或者是服务的虚拟IP地址返回给发起请求的域名解析器。  \n\n由上可知，docker的服务发现的作用范围是网络级别，也就意味着只有在同一个网络上的容器或任务才能利用内嵌的DNS服务来相互发现，不在同一个网络里面的服务是不能解析名称的，另外，为了安全和性能只有当一个节点上有容器或任务在某个网络里面时，这个节点才会存储那个网络里面的DNS记录。\n\n如果目的容器或服务和源容器不在同一个网络里面，Docker引擎会把这个DNS查询转发到配置的默认DNS服务  。\n\n![](\\img\\service-dns.png)\n\n在上面的例子中，总共有两个服务myservice和client，其中myservice有两个容器，这两个服务在同一个网里面。在client里针对docker.com和myservice各执行了一个curl操作，下面时执行的流程： \n\n- 为了client解析docker.com和myservice，DNS查询进行初始化\n- 容器内建的解析器在127.0.0.11:53拦截到这个DNS查询请求，并把请求转发到docker引擎的DNS服务\n- myservice被解析成服务对应的虚拟IP（10.0.0.3），在接下来的内部负载均衡阶段再被解析成一个具体任务的IP地址。如果是容器名称这一步直接解析成容器对应的IP地址（10.0.0.4或者10.0.0.5）。\n- docker.com在mynet网络上不能被解析成服务，所以这个请求被转发到配置好的默认DNS服务器（8.8.8.8）上。\n\n### 2.2.3 负载均衡\n\n负载均衡分为两种：Swarm集群内的service之间的相互访问需要做负载均衡，称为内部负载均衡（Internal LB）；从Swarm集群外部访问服务的公开端口，也需要做负载均衡，称外部部负载均衡(Exteral LB or Ingress LB)。\n\n- Internal LB\n\n内部负载均衡就是我们在上一段提到的服务发现，集群内部通过DNS访问service时，Swarm默认通过VIP（virtual IP）、iptables、IPVS转发到某个容器。 \n\n![](\\img\\intelnal-lb.jpg)\n\n当在docker swarm集群模式下创建一个服务时，会自动在服务所属的网络上给服务额外的分配一个虚拟IP，当解析服务名字时就会返回这个虚拟IP。对虚拟IP的请求会通过overlay网络自动的负载到这个服务所有的健康任务上。这个方式也避免了客户端的负载均衡，因为只有单独的一个IP会返回到客户端，docker会处理虚拟IP到具体任务的路由，并把请求平均的分配给所有的健康任务。  \n\n![](C:\\Users\\jalon\\OneDrive\\01笔记\\Docker\\internal-lb-2.png)\n\n```\n# 创建overlay网络：mynet \n$ docker network create -d overlay mynet  \na59umzkdj2r0ua7x8jxd84dhr \n# 利用mynet网络创建myservice服务，并复制两份  \n$ docker service create --network mynet --name myservice --replicas 2 busybox ping localhost  \n78t5r8cr0f0h6k2c3k7ih4l6f5\n# 通过下面的命令查看myservice对应的虚拟IP \n$ docker service inspect myservice  \n...\n\"VirtualIPs\": [ \n\t{  \n     \"NetworkID\": \"a59umzkdj2r0ua7x8jxd84dhr\",  \n     \t\t\t\"Addr\": \"10.0.0.3/24\"  \n      },  \n]  \t\n```\n\n注：swarm中服务还有另外一种负载均衡技术可选DNS round robin (DNS RR) （在创建服务时通过--endpoint-mode配置项指定），在DNSRR模式下，docker不再为服务创建VIP，docker DNS服务直接利用轮询的策略把服务名称直接解析成一个容器的IP地址。 \n\n- Exteral LB（Ingress LB 或者 Swarm Mode Routing Mesh)\n\n看名字就知道，这个负载均衡方式和前面提到的Ingress网络有关。Swarm网络要提供对外访问的服务就需要打开公开端口，并映射到宿主机。Ingress LB就是外部通过公开端口访问集群时做的负载均衡。\n\n当创建或更新一个服务时，你可以利用--publish选项把一个服务暴露到外部，在docker swarm模式下发布一个端口意味着在集群中的所有节点都会监听这个端口，这时当访问一个监听了端口但是并没有对应服务运行在其上的节点会发生什么呢？    接下来就该我们的路由网（routing mesh）出场了，路由网时docker1.12引入的一个新特性，它结合了IPVS和iptables创建了一个强大的集群范围的L4层负载均衡，它使所有节点接收服务暴露端口的请求成为可能。当任意节点接收到针对某个服务暴露的TCP/UDP端口的请求时，这个节点会利用预先定义过的Ingress overlay网络，把请求转发给服务对应的虚拟IP。ingress网络和其他的overlay网络一样，只是它的目的是为了转换来自客户端到集群的请求，它也是利用我们前一小节介绍过的基于VIP的负载均衡技术。 \n\n启动服务后，你可以为应用程序创建外部DNS记录，并将其映射到任何或所有Docker swarm节点。你无需担心你的容器具体运行在那个节点上，因为有了路由网这个特性后，你的集群看起来就像是单独的一个节点一样。  \n\n```\n#在集群中创建一个复制两份的服务，并暴露在8000端口  \n$ docker service create --name app --replicas 2 --network appnet --publish 8000:80 nginx  \n```\n\n![](\\img\\external-routing-mesh.png)\n\n上面这个图表明了路由网是怎么工作的： \n\n- 服务（app）拥有两份复制，并把端口映射到外部端口的8000\n- 路由网在集群中的所有节点上都暴露出8000\n- 外部对服务app的请求可以是任意节点，在本例子中外部的负载均衡器将请求转发到了没有app服务的主机上\n- docker swarm的IPVS利用ingress overlay网路将请求重新转发到运行着app服务的节点的容器中\n\n注：以上服务发现和负载均衡参考文档 https://success.docker.com/article/ucp-service-discovery\n\n\n\n## 2.3 Services 架构\n\n在微服务部署的过程中，通常将某一个微服务封装为 Service 在 Swarm 中部署执行，通常你需要通过指定容器 image 以及需要在容器中执行的 Commands 来创建你的 Service，除此之外，通常，还需要配置如下选项，\n\n- 指定可以在 Swarm 之外可以被访问的服务端口号 port，\n- 指定加入某个 Overlay 网络以便 Service 与 Service 之间可以建立连接并通讯，\n- 指定该 Service 所要使用的 CPU 和 内存的大小，\n- 指定一个滚动更新的策略 (Rolling Update Policy)\n- 指定多少个 Task 的副本 (replicas) 在 Swarm 集群中同时存在\n\n### 2.3.1 服务和任务\n\n![](\\img\\services-diagram.png)\n\nServices，Tasks 和 Containers 之间的关系可以用上面这张图来描述。来看这张图的逻辑，表示用户想要通过 Manager 节点部署一个有 3 个副本的 Nginx 的 Service，Manager 节点接收到用户的 Service definition 后，便开始对该 Service 进行调度，将会在当前可用的Worker（或者Manager ）节点中启动相应的 Tasks 以及相关的副本；所以可以看到，Service 实际上是 Task 的定义，而 Task 则是执行在节点上的程序。\n\nTask 是什么呢？其实就是一个 Container，只是，在 Swarm 中，每个 Task 都有自己的名字和编号，如图，比如 nginx.1、nginx.2 和 nginx.3，这些 Container 各自运行在各自的 node 上，当然，一个 node 上可以运行多个 Container；\n\n### 2.3.2 任务调度\n\nDocker swarm mode 模式的底层技术实际上就是指的是调度器( scheduler )和编排器( orchestrator )；下面这张图展示了 Swarm mode 如何从一个 Service 的创建请求并且成功将该 Service 分发到两个 worker 节点上执行的过程。\n\n![](\\img\\swarm-service-lifecycle.png)\n\n\n\n- 首先，看上半部分Swarm manager \n  1. 用户通过 Docker Engine Client 使用命令 docker service create 提交 Service definition，\n  2. 根据 Service definition 创建相应的 Task，\n  3. 为 Task 分配 IP 地址，\n     注意，这是分配运行在 Swarm 集群中 Container 的 IP 地址，该 IP 地址最佳的分配地点是在这里，因为 Manager 节点上保存得有最新最全的 Tasks 的状态信息，为了保证不与其他的 Task 分配到相同的 IP，所以在这里就将 IP 地址给初始化好；\n  4. 将 Task 分发到 Node 上，可以是 Manager 节点也可以使 Worker 节点，\n  5. 对 Worker 节点进行相应的初始化使得它可以执行 Task\n- 接着，看下半部分Swarm  Work \n  该部分就相对简单许多\n  1. 首先连接 manager 的分配器( scheduler)检查该 task\n  2. 验证通过以后，便开始通过 Worker 节点上的执行器( executor )执行；\n\n注意，上述 task 的执行过程是一种单向机制，比如它会按顺序的依次经历 assigned, prepared 和 running 等执行状态，不过在某些特殊情况下，在执行过程中，某个 task 失败了( fails )，编排器( orchestrator )直接将该 task 以及它的 container 给删除掉，然后在其它节点上另外创建并执行该 task；\n\n\n\n### 2.3.3 调度策略\n\nSwarm在scheduler节点（leader节点）运行容器的时候，会根据指定的策略来计算最适合运行容器的节点，目前支持的策略有：spread, binpack, random。\n\n　　1）Random 顾名思义，就是随机选择一个Node来运行容器，一般用作调试用，spread和binpack策略会根据各个节点的可用的CPU, RAM以及正在运 行的容器的数量来计算应该运行容器的节点。\n\n　　2）Spread 在同等条件下，Spread策略会选择运行容器最少的那台节点来运行新的容器，binpack策略会选择运行容器最集中的那台机器来运行新的节点。 使用Spread策略会使得容器会均衡的分布在集群中的各个节点上运行，一旦一个节点挂掉了只会损失少部分的容器。\n\n　　3）Binpack Binpack策略最大化的避免容器碎片化，就是说binpack策略尽可能的把还未使用的节点留给需要更大空间的容器运行，尽可能的把容器运行在 一个节点上面。(The binpack strategy causes Swarm to optimize for the container which is most packed.)\n\n### 2.3.4 服务副本与全局服务 \n\nDocker Swarm支持服务的扩容缩容，Swarm通过`--mode`选项设置服务类型，提供了两种模式：一种是replicated，我们可以指定服务Task的个数（也就是需要创建几个冗余副本），这也是Swarm默认使用的服务类型；另一种是global，这样会在Swarm集群的每个Node上都创建一个服务。如下图所示（出自Docker官网），是一个包含replicated和global模式的Swarm集群： \n\n![](\\img\\docker-swarm-replicated-vs-global.png)\n\n\n\n上图中，黄色表示的replicated模式下的Service Replicas，灰色表示global模式下Service的分布。 \n\n在Swarm mode下使用Docker，可以实现部署运行服务、服务扩容缩容、删除服务、滚动更新等功能，下面我们详细说明。\n\n# 3.集群搭建\n\n## 3.1 环境配置\n\n虚拟机信息\n\n| IP            | hostname | 配置    | 角色    |\n| ------------- | -------- | ------- | ------- |\n| 172.21.111.56 | swarm01  | 4CPU/6G | Manager |\n| 172.21.111.57 | swarm02  | 4CPU/6G | Manager |\n| 172.21.111.58 | swarm03  | 4CPU/6G | Manager |\n\n为验证后续的swarm manager 高可用，将这三台节点都配置成为Manager节点。\n\n\n\n## 3.2 前置准备\n\n### 3.2.1 修改主机名\n\n```\n hostnamectl --static --transient set-hostname swarm01\n```\n\n修改/etc/hosts 。三台主机上均设置一致。\n\n```\n[root@swarm01 ~]# cat /etc/hosts\n127.0.0.1   swarm-manager localhost localhost.localdomain localhost4 localhost4.localdomain4\n::1         localhost localhost.localdomain localhost6 localhost6.localdomain6\n172.21.111.56 swarm01\n172.21.111.57 swarm02\n172.21.111.58 swarm03\n```\n\n\n\n### 3.2.2 配置互信\n\n```bash\nssh-keygen -t rsa\nssh-copy-id -i .ssh/id_rsa.pub 172.21.111.57\n```\n\n### 3.2.3 关闭防火墙和SELinux\n\n```\nsystemctl disable firewalld\nsystemctl stop firewalld\n```\n\n修改 /etc/selinux/config ，修改配置项SELINUX=enforcing为disabled。\n\n## 3.2 安装Docker服务 \n\n如前文所示，从 v1.12 开始，Docker Swarm 的功能已经完全与 Docker Engine 集成，要管理集群，只需要启动 Swarm Mode。安装好 Docker，Swarm 就已经在那里了，服务发现也在那里了（不需要安装 Consul 等外部数据库）。配置好yum 源之后，即可安装docker-ce。\n\n```\\\nyum install -y docker-ce\nsystemctl enable docker\nsystemctl start docker\n```\n\n\n\n## 3.3 集群初始化\n\n### 3.2.1 初始化主节点\n\n使用命令`docker swarm init` 即可初始化当前节点为主节点。如果主机上有多张网卡的时候，需指定网卡` docker swarm init  --advertise-addr  [IP]   --listen-addr  [IP:PORT]`   \n\n节点自动解锁： docker swarm init --autolock=false\n\n```\n[root@swarm01 ~]# docker swarm init --autolock=false  --advertise-addr 172.21.111.56 --listen-addr 172.21.111.56:2377   \nSwarm initialized: current node (epoic1y0vv830vnwbc6nnacjc) is now a manager.\n\nTo add a worker to this swarm, run the following command:\n\n    docker swarm join --token SWMTKN-1-5t9jv2o6z3ep9mk5c2ae41bpqojla1g8cfjo3qlsuj2sxi934l-af2x3lh8vs6dnpepy66kdw5mv 172.21.111.56:2377\n\nTo add a manager to this swarm, run 'docker swarm join-token manager' and follow the instructions.\n```\n\n此例，在swarm01中创建了主节点。从输出中，看到添加集群有两种方式，一种是以manager 的方式添加，一种则是以worker节点方式添加。默认输出只给出了添加worker的命令，manager 命令则需要通过`docker swarm join-token manager`  获取。\n\n```\n[root@swarm01 ~]# docker swarm join-token manager\nTo add a manager to this swarm, run the following command:\n\n    docker swarm join --token SWMTKN-1-5t9jv2o6z3ep9mk5c2ae41bpqojla1g8cfjo3qlsuj2sxi934l-eszh52z00ktao9j7lovfvjgqy 172.21.111.56:2377\n```\n\n### 3.2.2 添加manager 节点\n\n分别登陆swarm02、swarm03，执行上一节得到的命令。\n\n```\n[root@swarm03 ~]#docker swarm join --token SWMTKN-1-5t9jv2o6z3ep9mk5c2ae41bpqojla1g8cfjo3qlsuj2sxi934l-eszh52z00ktao9j7lovfvjgqy 172.21.111.56:2377\nThis node joined a swarm as a manager.\n```\n\n注：不要在节点上配置http代理，否则会添加节点失败。\n\n### 3.2.3 离开集群\n\n如果想退出集群，可以在该节点上直接退出。\n\n```\n[root@swarm03 ~]# docker swarm leave\n[root@swarm03 ~]# docker swarm leave --force \n```\n\n### 3.2.4 查看集群节点信息\n\n登陆任意一台Manager节点。都可以查看当前集群的节点信息。\n\n```\n[root@swarm02 ~]# docker node ls\nID                            HOSTNAME            STATUS              AVAILABILITY        MANAGER STATUS      ENGINE VERSION\nepoic1y0vv830vnwbc6nnacjc     swarm01             Ready               Active              Leader              18.03.1-ce\nnsqqv181e0r7mu77azixtqhzl *   swarm02             Ready               Active              Reachable           18.03.1-ce\nmkkxaeqergz8xw21bbkd47q1c     swarm03             Ready               Active              Reachable           18.03.1-ce\n\n```\n\n上面信息中，AVAILABILITY表示Swarm Scheduler是否可以向集群中的某个Node指派Task，对应有如下三种状态：\n\n- Active：集群中该Node可以被指派Task\n- Pause：集群中该Node不可以被指派新的Task，但是其他已经存在的Task保持运行\n- Drain：集群中该Node不可以被指派新的Task，Swarm Scheduler停掉已经存在的Task，并将它们调度到可用的Node上\n\n另外，可以看到Manager Status状态也有分类，分别是Leader、Reacheable、Unreachable。\n\n- Leader：对应Raft算法的Leader节点\n- Reacheable：对应Raft算法的Flower节点，网络可达、可用的Flower节点\n- Unreachable：对应Raft算法的Flower节点，网络不可达、不可用的Flower节点\n\n查看单个节点详细信息\n\n```\n[root@swarm02 ~]# docker node inspect swarm01\n```\n\n关于Node的其他操作，比如状态变更、添加标签、提权和降权等，详见附录2\n\n\n\n# 4.高可用实践\n\n## 4.1 应用服务管理\n\ndocker swarm 还可以使用 stack 这一模型，用来部署管理和关联不同的服务。stack 功能比service 还要强大，因为它具备多服务编排部署的能力。**stack file** 是一种 yaml 格式的文件，类似于 docker-compose.yml 文件，它定义了一个或多个服务，并定义了服务的环境变量、部署标签、容器数量以及相关的环境特定配置等。 \n\n以下实验仅仅创建利用stack 创建单一服务。\n\n### 4.1.1 创建服务\n\n服务配置文件docker-swarm.yml ，内容如下\n\n```\nversion: '3'\nservices:\n  web:\n    image: \"httpd:v1\"\n    deploy:\n      replicas: 3\n      resources:\n        limits:\n          cpus: \"0.5\"\n          memory: 256M\n      restart_policy:\n        condition: on-failure\n    ports:\n     - \"7080:80\"\n    volumes:\n     - /var/www/html:/usr/local/apache2/htdocs\n```\n\nimage: \"httpd\" ：httpd是docker hub 下载的镜像。提供简单的httpd服务，此处用来做服务都高可用验证。\n\nreplicas: 3 表示该服务副本有3个，可对服务副本进行扩缩容，后续会讲到。\n\n/var/www/html:/usr/local/apache2/htdocs ：表示httpd 首页的内容。各个swarm节点上展示的内容分别是其主机名信息。\n\n在docker-swarm.yml 同级目录下，执行命令\n\n```\n[root@swarm01 swarm]# docker stack deploy -c docker-swarm.yml myservice\nCreating network myservice_default\nCreating service myservice_web\n```\n\n### 4.1.2 查看服务\n\n查看服务创建是否成功。\n\n```\n[root@swarm01 swarm]# docker stack ls\nNAME                SERVICES\nmyservice           1\n[root@swarm01 swarm]# docker stack services myservice\nID                  NAME                MODE                REPLICAS            IMAGE               PORTS\nkom8x49f7qv2        myservice_web       replicated          3/3                 httpd:latest        *:7080->80/tcp\n[root@swarm01 swarm]# docker stack ps myservice\nID                  NAME                IMAGE               NODE                DESIRED STATE       CURRENT STATE            ERROR               PORTS\nsybn1dur1wmw        myservice_web.1     httpd:latest        swarm03             Running             Running 45 seconds ago\nmlcb5r9vzv6z        myservice_web.2     httpd:latest        swarm03             Running             Running 45 seconds ago\nfw21lcu3zp83        myservice_web.3     httpd:latest        swarm01             Running             Running 45 seconds ago\n```\n\n可以看到 swarm01 上面运行了1个容器，swarm03上面运行了2个，而swarm02上面一个容器都没有，这是咋回事呢？哦，原来我们在创建集群过程中，对swarm02 做了状态变更的操作。` docker node update  --availability drain  swarm02`. 查看集群状态。\n\n```\n[root@swarm01 swarm]# docker node ls\nID                            HOSTNAME            STATUS              AVAILABILITY        MANAGER STATUS      ENGINE VERSION\nepoic1y0vv830vnwbc6nnacjc *   swarm01             Ready               Active              Leader              18.03.1-ce\nnsqqv181e0r7mu77azixtqhzl     swarm02             Ready               Drain               Reachable           18.03.1-ce\nmkkxaeqergz8xw21bbkd47q1c     swarm03             Ready               Active              Reachable           18.03.1-ce\n```\n\n此时打开浏览器，输入HTTP URL。任意swarm集群节点都可以访问httpd服务。\n\n![](C:\\Users\\jalon\\OneDrive\\01笔记\\Docker\\swarm-web.png)\n\n需要说明的是，由于3个节点的/var/www/html 路径未做共享存储。因此swarm会随机显示一个swarm节点的网页内容。\n\n### 4.1.3 服务扩缩容\n\n修改docker-swarm.yml 配置字段 `replicas: 5` ，将容器个数提升到5个。由于实验环境资源有限，因此这里将swarm02状态变更为可以状态，用以分配容器。\n\n` docker node update  --availability active swarm02`.\n\n重新执行部署命令`docker stack deploy -c docker-swarm.yml myservice`\n\n也可以通过命令直接对服务进行操作，但是不推荐。 `docker service scale 服务ID=服务个数`\n\n```\n[root@swarm01 swarm]# docker stack ps myservice\nID                  NAME                IMAGE               NODE                DESIRED STATE       CURRENT STATE                    ERROR               PORTS\nunwepav5z9nz        myservice_web.1     httpd:v1            swarm02             Running             Running less than a second ago\n3j5o1tktcg1l        myservice_web.2     httpd:v1            swarm01             Running             Preparing 3 seconds ago\nwl1unt6lynft        myservice_web.3     httpd:v1            swarm03             Running             Running less than a second ago\n[root@swarm01 swarm]# vi docker-swarm.yml\n[root@swarm01 swarm]# docker stack deploy -c docker-swarm.yml myservice\nUpdating service myservice_web (id: 85quzwi5k0btkn5wlht4jbco9)\nimage httpd:v1 could not be accessed on a registry to record\nits digest. Each node will access httpd:v1 independently,\npossibly leading to different nodes running different\nversions of the image.\n\n[root@swarm01 swarm]# docker stack ps myservice\nID                  NAME                IMAGE               NODE                DESIRED STATE       CURRENT STATE                    ERROR               PORTS\nunwepav5z9nz        myservice_web.1     httpd:v1            swarm02             Running             Running 24 seconds ago\n3j5o1tktcg1l        myservice_web.2     httpd:v1            swarm01             Running             Running 31 seconds ago\nwl1unt6lynft        myservice_web.3     httpd:v1            swarm03             Running             Running 24 seconds ago\nujedjxbuffxp        myservice_web.4     httpd:v1            swarm01             Running             Running less than a second ago\nml9qvjh2add7        myservice_web.5     httpd:v1            swarm02             Running             Running less than a second ago\n```\n\n同样，对于服务缩容，也仅需要修改`replicas:` 配置即可。这里不再演示。\n\n### 4.1.4 删除服务\n\n例如，删除myredis应用服务，执行docker service rm myredis，则应用服务myredis的全部副本都会被删除。 \n\n本例使用的是stack ，删除stack ，即可删除其下的服务。没错，docker语法非常类似,`docker stack rm myservice`。\n\n需要注意的是，对swarm上面服务的操作，都必须在manager节点上运行。\n\n### 4.1.5 服务升降级\n\n服务升降级，对应到docker中，则利用了容器镜像的更新，升级很好理解，直接把tag 标签版本往上提。那么降级，其实也可以理解为另外一种“升级“，只是镜像内容是上一个版本的而已。对于镜像标签的命令应该遵循一套严格的规范，这里不再赘述。\n\n先介绍利用service 命令行工具的用法。\n\n服务的滚动更新，这里我参考官网文档的例子说明。在Manager Node上执行如下命令：\n\n```\ndocker service create --replicas 3  --name redis --update-delay 10s redis:3.0.6\n```\n\n上面通过指定 --update-delay 选项，表示需要进行更新的服务，每次成功部署一个，延迟10秒钟，然后再更新下一个服务。如果某个服务更新失败，则Swarm的调度器就会暂停本次服务的部署更新。\n\n另外，也可以更新已经部署的服务所在容器中使用的Image的版本，例如执行如下命令：\n\n将Redis服务对应的Image版本有3.0.6更新为3.0.7，同样，如果更新失败，则暂停本次更新。\n\n那么，stack file 如何定义滚动更新呢？\n\n```\n      update_config:\n        parallelism: 2\n        delay: 10s\n```\n\n修改docker-swarm.yml 如下\n\n```\nversion: '3'\nservices:\n  web:\n    image: \"httpd:v2\"\n    deploy:\n      replicas: 3\n      update_config:\n        parallelism: 2\n        delay: 30s\n      resources:\n        limits:\n          cpus: \"0.5\"\n          memory: 256M\n      restart_policy:\n        condition: on-failure\n    ports:\n     - \"7080:80\"\n    volumes:\n     - /var/www/html:/usr/local/apache2/htdocs\n```\n\nparallelism 表示同时升级的个数，delay 则表示间隔多长时间升级。\n\n同时将httpd 由v1 升级到v2，这里我们使用同一个镜像，只是tag 修改了一下。查看服务状态\n\n```\n[root@swarm01 swarm]# docker stack ps myservice\nID                  NAME                  IMAGE               NODE                DESIRED STATE       CURRENT STATE                 ERROR               PORTS\nqk8pvvc9nmv0        myservice_web.1       httpd:v2            swarm01             Running             Running 56 seconds ago\nunwepav5z9nz         \\_ myservice_web.1   httpd:v1            swarm02             Shutdown            Shutdown 48 seconds ago\nuyj0pm1df9hl        myservice_web.2       httpd:v2            swarm02             Running             Running 46 seconds ago\n3j5o1tktcg1l         \\_ myservice_web.2   httpd:v1            swarm01             Shutdown            Shutdown about a minute ago\nqdjov5zf5alb        myservice_web.3       httpd:v2            swarm03             Running             Running 10 seconds ago\nwl1unt6lynft         \\_ myservice_web.3   httpd:v1            swarm03             Shutdown            Shutdown 12 seconds ago\n```\n\n可以看到同时只有2个容器在升级，同时第3个容器也是在间隔了30s 之后，才开始升级。\n\n --filter 过滤条件`docker stack ps  --filter  \"desired-state=running\"  myservice`\n\n## 4.2 应用服务高可用\n\n书接上文。在前面创建的myservice的基础上，进行容器应用的高可用验证。\n\n### 4.2.1 模拟容器故障\n\n查看当前服务状态\n\n```\n[root@swarm01 swarm]# docker stack ps  --filter  \"desired-state=running\"  myservice\nID                  NAME                IMAGE               NODE                DESIRED STATE       CURRENT STATE            ERROR               PORTS\nqk8pvvc9nmv0        myservice_web.1     httpd:v2            swarm01             Running             Running 16 minutes ago\nuyj0pm1df9hl        myservice_web.2     httpd:v2            swarm02             Running             Running 16 minutes ago\nqdjov5zf5alb        myservice_web.3     httpd:v2            swarm03             Running             Running 15 minutes ago\n```\n\nswarm01-03 分别运行3个容器。现在登陆swam03节点，将其容器进程Kill 掉。模拟容器意外故障。\n\n将stack ps 得到的swarm03节点的ID `qdjov5zf5alb` 代入命令inspect。\n\n```\n[root@swarm01 swarm]# docker inspect qdjov5zf5alb\n[\n    {\n        \"ID\": \"qdjov5zf5albdm1o7vhuwoxjt\",\n...\n        \"Spec\": {\n            \"ContainerSpec\": {\n                \"Image\": \"httpd:v2\",\n                \"Labels\": {\n                    \"com.docker.stack.namespace\": \"myservice\"\n                },\n                \"Privileges\": {\n                    \"CredentialSpec\": null,\n                    \"SELinuxContext\": null\n                },\n                \"Mounts\": [\n                    {\n                        \"Type\": \"bind\",\n                        \"Source\": \"/var/www/html\",\n                        \"Target\": \"/usr/local/apache2/htdocs\"\n                    }\n                ],\n                \"Isolation\": \"default\"\n            },\n            \"Resources\": {\n                \"Limits\": {\n                    \"NanoCPUs\": 500000000,\n                    \"MemoryBytes\": 268435456\n                }\n            },\n....\n        \"ServiceID\": \"85quzwi5k0btkn5wlht4jbco9\",\n        \"Slot\": 3,\n        \"NodeID\": \"mkkxaeqergz8xw21bbkd47q1c\",\n        \"Status\": {\n            \"Timestamp\": \"2018-08-07T09:02:25.674376822Z\",\n            \"State\": \"running\",\n            \"Message\": \"started\",\n            \"ContainerStatus\": {\n                \"ContainerID\": \"fa7d228e0c1981bfc78bca0b81363d7ca44cac99844c2996afe528ae3ae9a129\",\n                \"PID\": 8852,\n                \"ExitCode\": 0\n            },\n            \"PortStatus\": {}\n        },\n```\n\n得到得到其对于的容器fa7d228e0c1981bfc78bca0b81363d7ca44cac99844c2996afe528ae3ae9a129 （swarm03节点上运行）\n\n登陆swarm03节点，查看容器fa7d22 对应的进程。得到其进程号，并将其杀死。\n\n```\n[root@swarm03 /]# ps -ef|grep fa7d228e0c19\nroot      8832  1129  0 05:02 ?        00:00:00 docker-containerd-shim -namespace moby -workdir /var/lib/docker/containerd/daemon/io.containerd.runtime.v1.linux/moby/fa7d228e0c1981bfc78bca0b81363d7ca44cac99844c2996afe528ae3ae9a129 -address /var/run/docker/containerd/docker-containerd.sock -containerd-binary /usr/bin/docker-containerd -runtime-root /var/run/docker/runtime-runc\nroot      9989  1090  0 05:32 pts/0    00:00:00 grep --color=auto fa7d228e0c19\n[root@swarm03 /]# kill -9 8832\n```\n\n### 4.2.2 故障查看\n\n立即查看服务状态。\n\n```\n[root@swarm03 /]# docker stack ps myservice\nID                  NAME                  IMAGE               NODE                DESIRED STATE       CURRENT STATE             ERROR                         PORTS\nqk8pvvc9nmv0        myservice_web.1       httpd:v2            swarm01             Running             Running 32 minutes ago\nunwepav5z9nz         \\_ myservice_web.1   httpd:v1            swarm02             Shutdown            Shutdown 32 minutes ago\nuyj0pm1df9hl        myservice_web.2       httpd:v2            swarm02             Running             Running 32 minutes ago\n3j5o1tktcg1l         \\_ myservice_web.2   httpd:v1            swarm01             Shutdown            Shutdown 32 minutes ago\nhx741f0kye5a        myservice_web.3       httpd:v2            swarm03             Running             Running 29 seconds ago\nqdjov5zf5alb         \\_ myservice_web.3   httpd:v2            swarm03             Shutdown            Failed 36 seconds ago     \"task: non-zero exit (137)\"\nwl1unt6lynft         \\_ myservice_web.3   httpd:v1            swarm03             Shutdown            Shutdown 31 minutes ago\n```\n\n可以看到，swarm03上面的容器myservice_web.3 立即被重启。故障即刻恢复。\n\n### 4.2.3 结论\n\n通过以上实验我们得出，在docker swarm 集群服务中，任意容器出现故障意外死亡，都会被重启。满足应用服务高可用的需求。但是，可能会有同学有疑问，你这个只是kill掉进程，模拟的是进程级别的故障，如果是主机宕机呢，swarm是否也能满足呢？答案是肯定的！\n\n在下一个章节，就让我们继续模拟主机级别的故障，验证服务的高可用。同时，由于我们环境配置的均是Manager节点，我们也可以同时验证Swarm Manager 的高可用。让我们拭目以待吧！\n\n## 4.3 Swarm Manager 高可用\n\n查看当前swarm集群节点信息\n\n```\n[root@swarm01 swarm]# docker node ls\nID                            HOSTNAME            STATUS              AVAILABILITY        MANAGER STATUS      ENGINE VERSION\nepoic1y0vv830vnwbc6nnacjc *   swarm01             Ready               Active              Leader              18.03.1-ce\nnsqqv181e0r7mu77azixtqhzl     swarm02             Ready               Active              Reachable           18.03.1-ce\nmkkxaeqergz8xw21bbkd47q1c     swarm03             Ready               Active              Reachable           18.03.1-ce\n\n```\n\n以上可知，swarm01 为swarm 集群管理节点。倘若swarm01 突然宕机关机会怎样呢，整个swarm 集群会失效，陷入瘫痪吗？其上运行的容器服务是否会全部挂掉，无法继续提供服务?\n\n### 4.3.1 模拟主机宕机\n\n将虚拟机swarm01 直接关机。\n\n### 4.3.2 故障查看\n\n 登陆swarm03 ，查看swarm集群状态。\n\n```\n[root@swarm03 /]# docker node ls\nID                            HOSTNAME            STATUS              AVAILABILITY        MANAGER STATUS      ENGINE VERSION\nepoic1y0vv830vnwbc6nnacjc     swarm01             Unknown             Active              Unreachable         18.03.1-ce\nnsqqv181e0r7mu77azixtqhzl     swarm02             Ready               Active              Reachable           18.03.1-ce\nmkkxaeqergz8xw21bbkd47q1c *   swarm03             Ready               Active              Leader              18.03.1-ce\n\n```\n\n在经历短暂的间隙后，可看到swarm集群认定swarm01 现在处于Unknown 状态，MANAGER STATUS 处于Unreachable状态。同时，swarm集群基于raft 算法，重新推举出新的Leader swarm03。\n\n再过一段时间，swarm01 状态变成Down\n\n```\n[root@swarm03 ~]# docker node ls\nID                            HOSTNAME            STATUS              AVAILABILITY        MANAGER STATUS      ENGINE VERSION\nepoic1y0vv830vnwbc6nnacjc     swarm01             Down                Active              Unreachable         18.03.1-ce\nnsqqv181e0r7mu77azixtqhzl     swarm02             Ready               Active              Reachable           18.03.1-ce\nmkkxaeqergz8xw21bbkd47q1c *   swarm03             Ready               Active              Leader              18.03.1-ce\n\n```\n\n查看服务状态\n\n```\n[root@swarm03 /]# docker stack ps myservice\nID                  NAME                  IMAGE               NODE                DESIRED STATE       CURRENT STATE                ERROR                         PORTS\nzfbvcz5iu8g7        myservice_web.1       httpd:v2            swarm03             Running             Running 2 minutes ago\nqk8pvvc9nmv0         \\_ myservice_web.1   httpd:v2            swarm01             Shutdown            Running about an hour ago\nunwepav5z9nz         \\_ myservice_web.1   httpd:v1            swarm02             Shutdown            Shutdown about an hour ago\nuyj0pm1df9hl        myservice_web.2       httpd:v2            swarm02             Running             Running 3 minutes ago\n3j5o1tktcg1l         \\_ myservice_web.2   httpd:v1            swarm01             Shutdown            Shutdown about an hour ago\nhx741f0kye5a        myservice_web.3       httpd:v2            swarm03             Running             Running 3 minutes ago\nqdjov5zf5alb         \\_ myservice_web.3   httpd:v2            swarm03             Shutdown            Failed 21 minutes ago        \"task: non-zero exit (137)\"\nwl1unt6lynft         \\_ myservice_web.3   httpd:v1            swarm03             Shutdown            Shutdown about an hour ago\n```\n\n可以看到容器服务个数依然维持在3个，原先在swarm01上面运行的容器，被分配到swarm02和swarm03上面。现在对服务进行扩容操作，以验证此时swarm集群是否对服务还有管理能力。\n\n```\n[root@swarm03 swarm]# docker stack deploy -c docker-swarm.yml myservice\nUpdating service myservice_web (id: 85quzwi5k0btkn5wlht4jbco9)\nimage httpd:v2 could not be accessed on a registry to record\nits digest. Each node will access httpd:v2 independently,\npossibly leading to different nodes running different\nversions of the image.\n\n[root@swarm03 swarm]# docker stack ps  myservice\nID                  NAME                  IMAGE               NODE                DESIRED STATE       CURRENT STATE                ERROR                         PORTS\nzfbvcz5iu8g7        myservice_web.1       httpd:v2            swarm03             Running             Running 6 minutes ago\nqk8pvvc9nmv0         \\_ myservice_web.1   httpd:v2            swarm01             Shutdown            Running about an hour ago\nunwepav5z9nz         \\_ myservice_web.1   httpd:v1            swarm02             Shutdown            Shutdown about an hour ago\nuyj0pm1df9hl        myservice_web.2       httpd:v2            swarm02             Running             Running 6 minutes ago\n3j5o1tktcg1l         \\_ myservice_web.2   httpd:v1            swarm01             Shutdown            Shutdown about an hour ago\nhx741f0kye5a        myservice_web.3       httpd:v2            swarm03             Running             Running 6 minutes ago\nqdjov5zf5alb         \\_ myservice_web.3   httpd:v2            swarm03             Shutdown            Failed 25 minutes ago        \"task: non-zero exit (137)\"\nwl1unt6lynft         \\_ myservice_web.3   httpd:v1            swarm03             Shutdown            Shutdown about an hour ago\n6nifzsbysug5        myservice_web.4       httpd:v2            swarm02             Running             Running 20 seconds ago\nuwgnl2usv59a        myservice_web.5       httpd:v2            swarm02             Running             Running 20 seconds ago\n```\n\n可以看到，此时swarm集群依然具有管理能力（服务扩容能力）。也从侧面验证swarm manager的高可用能力。\n\n此时重启swarm01 ，看看 swarm01节点是否会自动加入集群。\n\n登陆swarm01 ，查看集群状态\n\n```\n[root@swarm01 ~]# docker node ls\nError response from daemon: Swarm is encrypted and needs to be unlocked before it can be used. Please use \"docker swarm unlock\" to unlock it.\n```\n\n可以看到，此时swarm01 处于被锁的状态。需要解锁。\n\n```\n[root@swarm01 ~]# docker swarm unlock\nPlease enter unlock key:\nError response from daemon: invalid key string\n```\n\n那么  unlock key 从哪里取值呢？答案就是，需要在可用的集群管理节点，比如swarm03上面执行命令\n\n```\n[root@swarm03 ~]# docker swarm unlock-key\nTo unlock a swarm manager after it restarts, run the `docker swarm unlock`\ncommand and provide the following key:\n\n    SWMKEY-1-796L7+nU55V/qGCMPgBvKS/5PyotvNaKPS4SJ0AUe+g\n\nPlease remember to store this key in a password manager, since without it you\nwill not be able to restart the manager.\n```\n\n通过该 unlock key ： SWMKEY-1-796L7+nU55V/qGCMPgBvKS/5PyotvNaKPS4SJ0AUe+g ，重新加入集群。\n\n此时在swarm01上面查看集群状态，即可看到已经重新加入集群。\n\n```bash\n[root@swarm01 ~]# docker node ls\nID                            HOSTNAME            STATUS              AVAILABILITY        MANAGER STATUS      ENGINE VERSION\nepoic1y0vv830vnwbc6nnacjc *   swarm01             Ready               Active              Reachable           18.03.1-ce\nnsqqv181e0r7mu77azixtqhzl     swarm02             Ready               Active              Reachable           18.03.1-ce\nmkkxaeqergz8xw21bbkd47q1c     swarm03             Ready               Active              Leader              18.03.1-ce\n```\n\n但是如果再把swarm03关掉，即同时宕掉2个节点，则此时3个管理节点，只剩下一个节点，不满足[Raft](https://raft.github.io/raft.pdf) 协议的节点个数要求（>N/2），集群功能就会失效，不能再管理集群，但是集群上的服务还能够继续运行。\n\n继续我们的脚步，将swarm03,swarm01 都关机。\n\n登陆swarm02，查看集群状态\n\n ```\n[root@swarm02 ~]# docker node ls\nError response from daemon: rpc error: code = Unknown desc = The swarm does not have a leader. It's possible that too few managers are online. Make sure more than half of the managers are online.\n[root@swarm02 ~]# docker stack ps myservice\nError response from daemon: rpc error: code = Unknown desc = The swarm does not have a leader. It's possible that too few managers are online. Make sure more than half of the managers are online.\n ```\n\n可以看到，此时集群功能已经失效，不能通过swarm 命令查看集群和管理集群了。那么运行其上的任务呢？\n\n```\n[root@swarm02 ~]# docker container ls\nCONTAINER ID        IMAGE                COMMAND              CREATED             STATUS              PORTS                    NAMES\n2cc1b76be2cf        httpd:v2             \"httpd-foreground\"   2 hours ago         Up 2 hours          80/tcp                   myservice_web.5.uwgnl2usv59aygb5e9a8cig2y\n96034bff8902        httpd:v2             \"httpd-foreground\"   2 hours ago         Up 2 hours          80/tcp                   myservice_web.4.6nifzsbysug5medburznrrr7h\n82beb10e4cac        httpd:v2             \"httpd-foreground\"   3 hours ago         Up 3 hours          80/tcp                   myservice_web.2.uyj0pm1df9hlx9vdvck36ilbl\nea9f879cdf12        composetest_web:v1   \"python app.py\"      3 weeks ago         Up 5 hours          0.0.0.0:5000->5000/tcp   elegant_borg\n\n```\n\n还好还好，通过 `docker container ls`  查看swarm02节点上的容器，可以看到仍然有3个容器在运行，这些都是之前已经分配好的。显然，swarm集群即使失效了，也不会影响其上运行的容器服务正常运行，只是失去了管理和调度的能力了。\n\n此时，如果再次打开swarm01 和 swarm03的电源，他们会重新组建集群吗？从上文中我们知道，离线的manager节点要重新加入集群需要解锁，而钥匙需要通过原集群运行`docker swarm unlock-key` 获取。那么显然，原集群已经失效了。那此时会怎样呢？\n\n不幸的是，确实swarm01 和 swarm03 都无法自动加入集群，确实需要通过`docker swarm unlock`进行解锁。但是幸运的是，**unlock key 是不变的**，跟刚才swarm01宕机时重新加入的key 是一样的。分别在swarm01 和swarm03  执行解锁操作。集群最终恢复正常。因此需要在创建集群后，第一时间保存unlock key 。\n\n```\n[root@swarm03 ~]# docker node ls\nID                            HOSTNAME            STATUS              AVAILABILITY        MANAGER STATUS      ENGINE VERSION\nepoic1y0vv830vnwbc6nnacjc     swarm01             Ready               Active              Reachable           18.03.1-ce\nnsqqv181e0r7mu77azixtqhzl     swarm02             Ready               Active              Leader              18.03.1-ce\nmkkxaeqergz8xw21bbkd47q1c *   swarm03             Ready               Active              Reachable           18.03.1-ce\n```\n\n注：在创建集群的时候，可设置自动解锁，即主机重启时，不用输入密钥，即可重新加入集群\n\n```\ndocker swarm init --autolock=false\ndocker swarm update --autolock=false\n```\n\n\n\n\n\n### 4.3.3 结论\n\n从以上实验中也可以看出，只要管理节点正常，服务的高可用就能达到，但是管理节点的个数必须保持在总的管理节点个数的一半以上，即3个只允许宕机一台，5台只允许宕机2台。而总的管理节点的个数一般不超过7个，如果太多的话，集群内管理节点通信消耗会比较大。由于偶数性价比不高（因为4台也只能宕机掉1台跟3台时是一样的），所以管理节点的个数一般都是奇数。\n\n管理节点的个数以及允许宕机的个数如下：\n\n| mannager node | 允许宕机个数 | 服务运行状态 |\n| :-----------: | :----------: | :----------: |\n|       3       |      1       |     正常     |\n|       5       |      2       |     正常     |\n|       7       |      3       |     正常     |\n\n\n\n## 4.4 网络故障转移\n\n### 4.4.1 模拟网络分区\n\n环境说明：仍然是3台虚拟机组成的swarm管理集群 \n\nhd_hd-1服务：运行4个副本，分别swarm01/swarm02 各一个容器，swarm03运行2个容器\n\n将swarm02网卡拔掉，模拟网络故障分区。\n\n备注使用 --autolock=false 初始化集群：docker swarm init --autolock=false  --advertise-addr 172.21.111.56 --listen-addr 172.21.111.56:2377  \n\n###  4.4.2 观察swarm node信息\n\n查看集群信息和服务状态\n\n```\n[root@swarm01 httpd]# docker node ls\nID                            HOSTNAME            STATUS              AVAILABILITY        MANAGER STATUS      ENGINE VERSION\nwk8jaypu23vr63082arpsl4ut *   swarm01             Ready               Active              Leader              18.03.1-ce\nq9lcw91vus73wyly174iy72h3     swarm02             Down                Active              Unreachable         18.03.1-ce\n1xelzfyr5kbn7kk70le45anei     swarm03             Ready               Active              Reachable           18.06.1-ce\n[root@swarm01 httpd]# docker service ps hd_hd-1\nID                  NAME                IMAGE               NODE                DESIRED STATE       CURRENT STATE           ERROR               PORTS\nt604bqtcj588        hd_hd-1.1           httpd:latest        swarm01             Running             Running 2 minutes ago\nxfajbfz51nd9         \\_ hd_hd-1.1       httpd:latest        swarm02             Shutdown            Running 8 minutes ago\n8clt10jl1xyf        hd_hd-1.2           httpd:latest        swarm01             Running             Running 6 minutes ago\npy5cavsog5mo        hd_hd-1.3           httpd:latest        swarm03             Running             Running 7 minutes ago\noigopryy1kro        hd_hd-1.4           httpd:latest        swarm03             Running             Running 7 minutes ago\n```\n\n我们可以发现，集群认为swarm02现在已经失联（不论是宕机还是网络端开）。现在swarm02上面的容器被迁移到swarm01上面（调度策略应该是swarm01上面目前运行的容器个数比swarm03少）。\n\n通过虚拟机控制台登陆swarm02，验证集群功能和容器是否在运行\n\n![1537414250767](C:\\Users\\jalon\\OneDrive\\01笔记\\Docker\\swarm-network.png)\n\n我们可以发现，swarm02由于当前是一个节点，根据raft算法，处于swarm集群功能不可用状态。docker ps查看容器，发现容器仍然在正常运行。得出结论，swarm集群功能失效，但是不影响原先已经运行的docker容器。\n\n综合swarm01/swarm03上面的容器个数和swarm02上面的容器个数，现在已经有5个容器了。而我们预设的容器个数是4个。那么当swarm02重新加入集群，会发生什么事情呢？\n\n### 4.4.3 网络故障恢复\n\n把swarm02网卡重新接上。swarm02 会自动加入集群。查看集群信息和服务信息。\n\n```\n[root@swarm01 httpd]# docker node ls\nID                            HOSTNAME            STATUS              AVAILABILITY        MANAGER STATUS      ENGINE VERSION\nlkivi02vx5z939z0x7lacuarl *   swarm01             Ready               Active              Leader              18.03.1-ce\npdp0awtf34o4x60x812h07a6e     swarm02             Ready               Active              Reachable           18.03.1-ce\nqavrh9e6pjdq2aoxpucl6nohm     swarm03             Ready               Active              Reachable           18.06.1-ce\n[root@swarm01 httpd]# docker service ps hd_hd-1\nID                  NAME                IMAGE               NODE                DESIRED STATE       CURRENT STATE            ERROR               PORTS\npepxi7bmzi8r        hd_hd-1.1           httpd:latest        swarm01             Running             Running 2 minutes ago\nyf42dr6fqvl5         \\_ hd_hd-1.1       httpd:latest        swarm02             Shutdown            Shutdown 2 minutes ago\nj6iq7tpsnwb0        hd_hd-1.2           httpd:latest        swarm03             Running             Running 2 minutes ago\nk23a3ezbocit        hd_hd-1.3           httpd:latest        swarm03             Running             Running 2 minutes ago\nbqra1k9m0ojc        hd_hd-1.4           httpd:latest        swarm01             Running             Running 2 minutes ago\n```\n\n我们发现swarm02已经正常加入集群。登陆swarm02，查看swarm02上面原先的容器也已经停掉了。符合我们的预期。\n\n![1537419010126](C:\\Users\\jalon\\OneDrive\\01笔记\\Docker\\swarm-network-2.png)\n\n\n\n# 5.附录\n\n## 5.1参考文章：\n\ndocker swarm介绍：\n\nhttp://shiyanjun.cn/archives/1625.html\n\nhttps://jiayi.space/post/docker-swarmrong-qi-ji-qun-guan-li-gong-ju\n\nhttp://www.shangyang.me/2018/02/01/docker-swarm-03-architect/\n\nhttp://blog.51cto.com/cloudman/1952873\n\nhttps://www.cnblogs.com/bigberg/p/8761047.html\n\n服务发现：https://success.docker.com/article/ucp-service-discovery\n\nraft协议：http://thesecretlivesofdata.com/raft/\n\n## 5.2 Swarm Node 节点操作\n\n### 5.2.1 状态变更 \n\n前面我们已经提到过，Node的AVAILABILITY有三种状态：Active、Pause、Drain，对某个Node进行变更，可以将其AVAILABILITY值通过Docker CLI修改为对应的状态即可，下面是常见的变更操作：\n\n- 设置Manager Node只具有管理功能\n- 对服务进行停机维护，可以修改AVAILABILITY为Drain状态\n- 暂停一个Node，然后该Node就不再接收新的Task\n- 恢复一个不可用或者暂停的Node\n\n例如，将Manager Node的AVAILABILITY值修改为Drain状态，使其只具备管理功能，执行如下命令：\n\n```\n[root@swarm02 ~]# docker node update  --availability drain  swarm02\nswarm02\n[root@swarm02 ~]# docker node ls\nID                            HOSTNAME            STATUS              AVAILABILITY        MANAGER STATUS      ENGINE VERSION\nepoic1y0vv830vnwbc6nnacjc     swarm01             Ready               Active              Leader              18.03.1-ce\nnsqqv181e0r7mu77azixtqhzl *   swarm02             Ready               Drain               Reachable           18.03.1-ce\nmkkxaeqergz8xw21bbkd47q1c     swarm03             Ready               Active              Reachable           18.03.1-ce\n```\n\n这样，Manager Node不能被指派Task，也就是不能部署实际的Docker容器来运行服务，而只是作为管理Node的角色。 \n\n### 5.2.2 添加标签\n\n每个Node的主机配置情况可能不同，比如有的适合运行CPU密集型应用，有的适合运行IO密集型应用，Swarm支持给每个Node添加标签元数据，这样可以根据Node的标签，来选择性地调度某个服务部署到期望的一组Node上。 给SWarm集群中的某个Worker Node添加标签，执行如下命令格式如下： \n\n```\n[root@swarm02 ~]# docker node update  --label-add  app=dev swarm02\nswarm02\n```\n\n### 5.2.3 提权/降权\n\n改变Node的角色，Worker Node可以变为Manager Node，这样实际Worker Node有工作Node变成了管理Node，对应操作分别是： \n\n```\ndocker node promote swarm02\ndocker node demote swarm02\n```\n\n\n\n## 5.3 网络管理\n\n### 5.3.1 添加overlay 网络\n\noverlay网络是swarm中默认的跨主机网络模型。在Swarm集群中可以使用Overlay网络来连接到一个或多个服务。具体添加Overlay网络，首先，我们需要创建在Manager Node上创建一个Overlay网络，执行如下命令： \n\n`docker network create --driver overlay my-network`\n\n创建完Overlay网络my-network以后，Swarm集群中所有的Manager Node都可以访问该网络。然后，我们在创建服务的时候，只需要指定使用的网络为已存在的Overlay网络即可，如下命令所示： \n\n`docker service create --replicas 3 --network my-network --name myweb  nginx`\n\n这样，如果Swarm集群中其他Node上的Docker容器也使用my-network这个网络，那么处于该Overlay网络中的所有容器之间，通过网络可以连通。  \n\n\n\n\n\n## 5.4 API\n\n```\n$ curl --unix-socket /var/run/docker.sock -H \"Content-Type: application/json\" \\\n  -d '{\"Image\": \"alpine\", \"Cmd\": [\"echo\", \"hello world\"]}' \\\n  -X POST http:/v1.24/containers/create\n{\"Id\":\"1c6594faf5\",\"Warnings\":null}\n\n$ curl --unix-socket /var/run/docker.sock -X POST http:/v1.24/containers/1c6594faf5/start\n\n$ curl --unix-socket /var/run/docker.sock -X POST http:/v1.24/containers/1c6594faf5/wait\n{\"StatusCode\":0}\n\n$ curl --unix-socket /var/run/docker.sock \"http:/v1.24/containers/1c6594faf5/logs?stdout=1\"\nhello world\n```\n\nhttps://docs.docker.com/develop/sdk/#sdk-and-api-quickstart\n\nhttps://www.programmableweb.com/api/docker-swarm\n\n\n\n## 5.5 Docker 三剑客\n\n- docker-compose 负责组织应用，应用由哪些服务组成，每个服务由多少个容器，均在该配置文件里面配置。\n- docker-machine 负责提供虚拟机，该虚拟机具备docker daemon服务能力。创建的虚拟机，可以用来组成docker swarm集群。docker-machine 并不是必须的功能。完全可以手动安装docker 服务。\n- docker swarm 负责组织docker集群，组织跨主机的docker集群管理能力。\n\n","slug":"docker-swarm-manager-ha","published":1,"updated":"2018-10-20T08:34:17.341Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjnh6x610000064b2ft51a525","content":"<h1 id=\"1-简介\"><a href=\"#1-简介\" class=\"headerlink\" title=\"1.简介\"></a>1.简介</h1><h2 id=\"1-1-swarm是什么\"><a href=\"#1-1-swarm是什么\" class=\"headerlink\" title=\"1.1 swarm是什么\"></a>1.1 swarm是什么</h2><p><code>Swarm</code> 是使用 <a href=\"https://github.com/docker/swarmkit/\" target=\"_blank\" rel=\"noopener\"><code>SwarmKit</code></a> 构建的 Docker 引擎内置（原生）的集群管理和编排工具。其主要作用是把若干台Docker主机抽象为一个整体，并且通过一个入口统一管理这些Docker主机上的各种Docker资源。Swarm和Kubernetes比较类似，但是更加轻量，具有的功能也较kubernetes更少一些。</p>\n<p>Docker v1.12 是一个非常重要的版本，Docker 重新实现了集群的编排方式。在此之前，提供集群功能的 Docker Swarm 是一个单独的软件，而且依赖外部数据库（比如 Consul、etcd 或 Zookeeper）。从 v1.12 开始，Docker Swarm 的功能已经完全与 Docker Engine 集成，要管理集群，只需要启动 Swarm Mode。安装好 Docker，Swarm 就已经在那里了，服务发现也在那里了（不需要安装 Consul 等外部数据库）。</p>\n<p>Docker的Swarm(集群)模式，集成很多工具和特性，比如：跨主机上快速部署服务，服务的快速扩展，集群的管理整合到docker引擎，这意味着可以不可以不使用第三方管理工具。分散设计，声明式的服务模型，可扩展，状态协调处理，多主机网络，分布式的服务发现，负载均衡，滚动更新，安全（通信的加密）等等。 </p>\n<p><img src=\"\\img\\docker-swarm-logo.png\" alt=\"\"></p>\n<p>官方网站：<a href=\"https://docs.docker.com/engine/swarm/\" target=\"_blank\" rel=\"noopener\">https://docs.docker.com/engine/swarm/</a></p>\n<a id=\"more\"></a>\n<h2 id=\"1-2-swarm基本概念\"><a href=\"#1-2-swarm基本概念\" class=\"headerlink\" title=\"1.2 swarm基本概念\"></a>1.2 swarm基本概念</h2><h3 id=\"1-2-1-节点\"><a href=\"#1-2-1-节点\" class=\"headerlink\" title=\"1.2.1 节点\"></a>1.2.1 节点</h3><p>运行 Docker 的主机可以主动初始化一个 <code>Swarm</code> 集群或者加入一个已存在的 <code>Swarm</code> 集群，这样这个运行 Docker 的主机就成为一个 <code>Swarm</code> 集群的节点 (<code>node</code>) 。</p>\n<p>节点分为管理 (<code>manager</code>) 节点和工作 (<code>worker</code>) 节点。</p>\n<p>管理节点用于 <code>Swarm</code> 集群的管理，<code>docker swarm</code> 命令基本只能在管理节点执行（节点退出集群命令 <code>docker swarm leave</code> 可以在工作节点执行）。一个 <code>Swarm</code> 集群可以有多个管理节点，但只有一个管理节点可以成为 <code>leader</code>，<code>leader</code> 通过 <code>raft</code> 协议实现。</p>\n<p>工作节点是任务执行节点，管理节点将服务 (<code>service</code>) 下发至工作节点执行。管理节点默认也作为工作节点。也可以通过配置让服务只运行在管理节点。</p>\n<p>Docker 官网的这张图片形象的展示了集群中管理节点与工作节点的关系。</p>\n<p><img src=\"\\img\\swarm-node.png\" alt=\"\"></p>\n<h3 id=\"1-2-2-服务和任务\"><a href=\"#1-2-2-服务和任务\" class=\"headerlink\" title=\"1.2.2 服务和任务\"></a>1.2.2 服务和任务</h3><p>任务 （<code>Task</code>）是 <code>Swarm</code> 中的最小的调度单位，目前来说就是一个单一的容器。</p>\n<p>服务 （<code>Services</code>） 是指一组任务的集合，服务定义了任务的属性。服务有两种模式：</p>\n<ul>\n<li><code>replicated services</code> 按照一定规则在各个工作节点上运行指定个数的任务。</li>\n<li><code>global services</code> 每个工作节点上运行一个任务</li>\n</ul>\n<p>两种模式通过 <code>docker service create</code> 的 <code>--mode</code> 参数指定。</p>\n<h3 id=\"1-2-3-负载均衡\"><a href=\"#1-2-3-负载均衡\" class=\"headerlink\" title=\"1.2.3 负载均衡\"></a>1.2.3 负载均衡</h3><p><code>manager</code>群集管理器使用 <strong>ingress load balancing</strong> 来对外公开群集提供的服务。<code>manager</code>可以自动为<strong>PublishedPort</strong> 分配服务，也可以手动配置。如果未指定端口，则swarm管理器会为服务分配30000-32767范围内的端口。</p>\n<p>外部组件（例如云负载平衡器）可以访问群集中任何节点的PublishedPort上的服务，无论该节点当前是否正在运行该服务的任务。群集中的所有节点都将入口连接到正在运行的任务实例。</p>\n<p>Swarm模式有一个内部DNS组件，可以自动为swarm中的每个服务分配一个DNS条目。群集管理器使用<strong>内部负载平衡</strong>来根据服务的DNS名称在群集内的服务之间分发请求。</p>\n<h2 id=\"1-3-swarm特性\"><a href=\"#1-3-swarm特性\" class=\"headerlink\" title=\"1.3 swarm特性\"></a>1.3 swarm特性</h2><ul>\n<li><p><strong>集群管理与Docker Engine集成:</strong>使用Docker Engine CLI来创建一个你能部署应用服务到Docker Engine的swarm。你不需要其他编排软件来创建或管理swarm。</p>\n</li>\n<li><p><strong>分散式设计：</strong>Docker Engine不是在部署时处理节点角色之间的差异，而是在运行时扮演自己角色。你可以使用Docker Engine部署两种类型的节点，管理器和worker。这意味着你可以从单个磁盘映像构建整个swarm。</p>\n</li>\n<li><p><strong>声明性服务模型：</strong> Docker Engine使用声明性方法来定义应用程序堆栈中各种服务的所需状态。例如，你可以描述由消息队列服务和数据库后端的Web前端服务组成的应用程序。</p>\n</li>\n<li><p><strong>伸缩性：</strong>对于每个服务，你可以声明要运行的任务数。当你向上或向下缩放时，swarm管理器通过添加或删除任务来自动适应，以保持所需状态。</p>\n</li>\n<li><p><strong>期望的状态协调：</strong>swarm管理器节点持续监控群集状态，并调整你描述的期望状态与实际状态之间的任何差异。 例如，如果设置运行一个10个副本容器的服务，这时worker机器托管其中的两个副本崩溃，管理器则将创建两个新副本以替换已崩溃的副本。 swarm管理器将新副本分配给正在运行和可用的worker。</p>\n</li>\n<li><p><strong>多主机网络：</strong>你可以为服务指定覆盖网络(<a href=\"https://www.centos.bz/tag/overlay/\" target=\"_blank\" rel=\"noopener\">overlay</a> network)。 当swarm管理器初始化或更新应用程序时，它会自动为容器在覆盖网络(overlay network)上分配地址。</p>\n</li>\n<li><p><strong>服务发现：</strong>Swarm管理器节点为swarm中的每个服务分配唯一的DNS名称，并负载平衡运行中的容器。 你可以通过嵌入在swarm中的DNS服务器查询在swarm中运行中的每个容器。</p>\n</li>\n<li><p><strong>负载平衡：</strong>你可以将服务的端口暴露给外部的负载均衡器。 在内部，swarm允许你指定如何在节点之间分发服务容器。</p>\n</li>\n<li><p><strong>安全通信：</strong>swarm中的每个节点强制执行TLS相互验证和加密，以保护其自身与所有其他节点之间的通信。 你可以选择使用自签名根证书或来自自定义根CA的证书。</p>\n</li>\n<li><p><strong>滚动更新：</strong>在上线新功能期间，你可以增量地应用服务更新到节点。 swarm管理器允许你控制将服务部署到不同节点集之间的延迟。 如果出现任何问题，你可以将任务回滚到服务的先前版本。</p>\n</li>\n</ul>\n<h1 id=\"2-架构分析\"><a href=\"#2-架构分析\" class=\"headerlink\" title=\"2.架构分析\"></a>2.架构分析</h1><h2 id=\"2-1-基本架构\"><a href=\"#2-1-基本架构\" class=\"headerlink\" title=\"2.1 基本架构\"></a>2.1 基本架构</h2><p>Docker Swarm提供了基本的集群能力，能够使多个Docker Engine组合成一个group，提供多容器服务。Swarm使用标准的Docker API，启动容器可以直接使用docker run命令。Swarm更核心的则是关注如何选择一个主机并在其上启动容器，最终运行服务。 Docker Swarm基本架构，如下图所示： </p>\n<p><img src=\"\\img\\docker-swarm-architecture.png\" alt=\"\"></p>\n<p>如上图所示，Swarm Node表示加入Swarm集群中的一个Docker Engine实例，基于该Docker Engine可以创建并管理多个Docker容器。其中，最开始创建Swarm集群的时候，Swarm Manager便是集群中的第一个Swarm Node。在所有的Node中，又根据其职能划分为Manager Node和Worker Node。</p>\n<h3 id=\"2-1-Manager-Node\"><a href=\"#2-1-Manager-Node\" class=\"headerlink\" title=\"2.1 Manager Node\"></a>2.1 Manager Node</h3><p>Manger 节点，顾名思义，是进行 Swarm 集群的管理工作的，它的管理工作集中在如下部分，</p>\n<ul>\n<li>维护一个集群的状态；</li>\n<li>对 Services 进行调度；</li>\n<li>为 Swarm 提供外部可调用的 API 接口；</li>\n</ul>\n<p>Manager 节点需要时刻维护和保存当前 Swarm 集群中各个节点的一致性状态，这里主要是指各个 Tasks 的执行的状态和其它节点的状态；因为 Swarm 集群是一个典型的分布式集群，在保证一致性上，Manager 节点采用 <a href=\"https://raft.github.io/raft.pdf\" target=\"_blank\" rel=\"noopener\">Raft</a> 协议来保证分布式场景下的数据一致性；</p>\n<p>通常为了保证 Manager 节点的高可用，Docker 建议采用奇数个 Manager 节点，这样的话，你可以在 Manager 失败的时候不用关机维护，我们给出如下的建议：</p>\n<ul>\n<li>3 个 Manager 节点最多可以同时容忍 1 个 Manager 节点失效的情况下保证高可用；</li>\n<li>5 个 Manager 节点最多可以同时容忍 2 个 Manager 节点失效的情况下保证高可用；</li>\n<li>N 个 Manager 节点最多可以同时容忍 (N−1)/2个 Manager 节点失效的情况下保证高可用；</li>\n<li>Docker 建议最多最多的情况下，使用 7 个 Manager 节点就够了，否则反而会降低集群的性能了。</li>\n</ul>\n<h3 id=\"2-2-Worker-Node\"><a href=\"#2-2-Worker-Node\" class=\"headerlink\" title=\"2.2 Worker Node\"></a>2.2 Worker Node</h3><p>Worker Node接收由Manager Node调度并指派的Task，启动一个Docker容器来运行指定的服务，并且Worker Node需要向Manager Node汇报被指派的Task的执行状态。</p>\n<h3 id=\"2-3-更换角色\"><a href=\"#2-3-更换角色\" class=\"headerlink\" title=\"2.3  更换角色\"></a>2.3  更换角色</h3><p>通过 docker node promote 命令将一个 Worker 节点提升为 Manager 节点。通常情况下，该命令使用在维护的过程中，需要将 Manager 节点占时下线进行维护操作；同样可以使用 docker node demote 将某个 manager 节点降级为 worker 节点。 </p>\n<h2 id=\"2-2-设计架构\"><a href=\"#2-2-设计架构\" class=\"headerlink\" title=\"2.2 设计架构\"></a>2.2 设计架构</h2><p><img src=\"\\img\\swarm-architecture-1.jpg\" alt=\"\"></p>\n<p>从前文可知， Swarm 集群的管理工作是由manager节点实现。如上图所示，manager节点实现的功能主要包括：node discovery，scheduler,cluster管理等。同时，为了保证Manager 节点的高可用，Manager 节点需要时刻维护和保存当前 Swarm 集群中各个节点的一致性状态。在保证一致性上，Manager 节点采用 <a href=\"https://raft.github.io/raft.pdf\" target=\"_blank\" rel=\"noopener\">Raft</a> 协议来保证分布式场景下的数据一致性；</p>\n<p>Docker Swarm内置了Raft一致性算法，可以保证分布式系统的数据保持一致性同步。Etcd, Consul等高可用键值存储系统也是采用了这种算法。这个算法的作用简单点说就是随时保证集群中有一个Leader，由Leader接收数据更新，再同步到其他各个Follower节点。在Swarm中的作用表现为当一个Leader 节点 down掉时，系统会立即选取出另一个Leader节点，由于这个节点同步了之前节点的所有数据，所以可以无缝地管理集群。</p>\n<p>Raft的详细解释可以参考<a href=\"http://thesecretlivesofdata.com/raft/\" target=\"_blank\" rel=\"noopener\">《The Secret Lives of Data–Raft: Understandable Distributed Consensus》</a>。</p>\n<h3 id=\"2-2-1-跨主机容器通信\"><a href=\"#2-2-1-跨主机容器通信\" class=\"headerlink\" title=\"2.2.1 跨主机容器通信\"></a>2.2.1 跨主机容器通信</h3><p>Docker Swarm 内置的跨主机容器通信方案是overlay网络，这是一个基于vxlan协议的网络实现。VxLAN 可将二层数据封装到 UDP 进行传输，VxLAN 提供与 VLAN 相同的以太网二层服务，但是拥有更强的扩展性和灵活性。 overlay 通过虚拟出一个子网，让处于不同主机的容器能透明地使用这个子网。所以跨主机的容器通信就变成了在同一个子网下的容器通信，看上去就像是同一主机下的bridge网络通信。</p>\n<p>为支持容器跨主机通信，Docker 提供了 overlay driver，使用户可以创建基于 VxLAN 的 overlay 网络。其实，docker 会创建一个 bridge 网络 “docker_gwbridge”，为所有连接到 overlay 网络的容器提供访问外网的能力。<code>`docker network inspect docker_gwbridge</code>  查看网络信息。</p>\n<p><img src=\"\\img\\overlay-gwbridge.jpg\" alt=\"\"></p>\n<p>下面我们讨论下overlay 网络的具体实现：</p>\n<p>docker 会为每个 overlay 网络创建一个独立的 network namespace，其中会有一个 linux bridge br0，endpoint 还是由 veth pair 实现，一端连接到容器中（即 eth0），另一端连接到 namespace 的 br0 上。</p>\n<p>br0 除了连接所有的 endpoint，还会连接一个 vxlan 设备，用于与其他 host 建立 vxlan tunnel。容器之间的数据就是通过这个 tunnel 通信的。逻辑网络拓扑结构如图所示：</p>\n<p><img src=\"\\img/overlay.jpg\" alt=\"\"></p>\n<h3 id=\"2-2-2-服务发现\"><a href=\"#2-2-2-服务发现\" class=\"headerlink\" title=\"2.2.2 服务发现\"></a>2.2.2 服务发现</h3><p>docker Swarm mode下会为每个节点的docker engine内置一个DNS server，各个节点间的DNS server通过control plane的gossip协议互相交互信息。此处DNS server用于容器间的服务发现。swarm mode会为每个 –net=自定义网络的service分配一个DNS entry。目前必须是自定义网络，比如overaly。而bridge和routing mesh的service，是不会分配DNS的。</p>\n<p>那么，下面就来详细介绍服务发现的原理。</p>\n<p>每个Docker容器都有一个DNS解析器，它将DNS查询转发到docker engine，该引擎充当DNS服务器。docker 引擎收到请求后就会在发出请求的容器所在的所有网络中，检查域名对应的是不是一个容器或者是服务，如果是，docker引擎就会从存储的key-value建值对中查找这个容器名、任务名、或者服务名对应的IP地址，并把这个IP地址或者是服务的虚拟IP地址返回给发起请求的域名解析器。  </p>\n<p>由上可知，docker的服务发现的作用范围是网络级别，也就意味着只有在同一个网络上的容器或任务才能利用内嵌的DNS服务来相互发现，不在同一个网络里面的服务是不能解析名称的，另外，为了安全和性能只有当一个节点上有容器或任务在某个网络里面时，这个节点才会存储那个网络里面的DNS记录。</p>\n<p>如果目的容器或服务和源容器不在同一个网络里面，Docker引擎会把这个DNS查询转发到配置的默认DNS服务  。</p>\n<p><img src=\"\\img\\service-dns.png\" alt=\"\"></p>\n<p>在上面的例子中，总共有两个服务myservice和client，其中myservice有两个容器，这两个服务在同一个网里面。在client里针对docker.com和myservice各执行了一个curl操作，下面时执行的流程： </p>\n<ul>\n<li>为了client解析docker.com和myservice，DNS查询进行初始化</li>\n<li>容器内建的解析器在127.0.0.11:53拦截到这个DNS查询请求，并把请求转发到docker引擎的DNS服务</li>\n<li>myservice被解析成服务对应的虚拟IP（10.0.0.3），在接下来的内部负载均衡阶段再被解析成一个具体任务的IP地址。如果是容器名称这一步直接解析成容器对应的IP地址（10.0.0.4或者10.0.0.5）。</li>\n<li>docker.com在mynet网络上不能被解析成服务，所以这个请求被转发到配置好的默认DNS服务器（8.8.8.8）上。</li>\n</ul>\n<h3 id=\"2-2-3-负载均衡\"><a href=\"#2-2-3-负载均衡\" class=\"headerlink\" title=\"2.2.3 负载均衡\"></a>2.2.3 负载均衡</h3><p>负载均衡分为两种：Swarm集群内的service之间的相互访问需要做负载均衡，称为内部负载均衡（Internal LB）；从Swarm集群外部访问服务的公开端口，也需要做负载均衡，称外部部负载均衡(Exteral LB or Ingress LB)。</p>\n<ul>\n<li>Internal LB</li>\n</ul>\n<p>内部负载均衡就是我们在上一段提到的服务发现，集群内部通过DNS访问service时，Swarm默认通过VIP（virtual IP）、iptables、IPVS转发到某个容器。 </p>\n<p><img src=\"\\img\\intelnal-lb.jpg\" alt=\"\"></p>\n<p>当在docker swarm集群模式下创建一个服务时，会自动在服务所属的网络上给服务额外的分配一个虚拟IP，当解析服务名字时就会返回这个虚拟IP。对虚拟IP的请求会通过overlay网络自动的负载到这个服务所有的健康任务上。这个方式也避免了客户端的负载均衡，因为只有单独的一个IP会返回到客户端，docker会处理虚拟IP到具体任务的路由，并把请求平均的分配给所有的健康任务。  </p>\n<p><img src=\"C:\\Users\\jalon\\OneDrive\\01笔记\\Docker\\internal-lb-2.png\" alt=\"\"></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># 创建overlay网络：mynet </span><br><span class=\"line\">$ docker network create -d overlay mynet  </span><br><span class=\"line\">a59umzkdj2r0ua7x8jxd84dhr </span><br><span class=\"line\"># 利用mynet网络创建myservice服务，并复制两份  </span><br><span class=\"line\">$ docker service create --network mynet --name myservice --replicas 2 busybox ping localhost  </span><br><span class=\"line\">78t5r8cr0f0h6k2c3k7ih4l6f5</span><br><span class=\"line\"># 通过下面的命令查看myservice对应的虚拟IP </span><br><span class=\"line\">$ docker service inspect myservice  </span><br><span class=\"line\">...</span><br><span class=\"line\">&quot;VirtualIPs&quot;: [ </span><br><span class=\"line\">\t&#123;  </span><br><span class=\"line\">     &quot;NetworkID&quot;: &quot;a59umzkdj2r0ua7x8jxd84dhr&quot;,  </span><br><span class=\"line\">     \t\t\t&quot;Addr&quot;: &quot;10.0.0.3/24&quot;  </span><br><span class=\"line\">      &#125;,  </span><br><span class=\"line\">]</span><br></pre></td></tr></table></figure>\n<p>注：swarm中服务还有另外一种负载均衡技术可选DNS round robin (DNS RR) （在创建服务时通过–endpoint-mode配置项指定），在DNSRR模式下，docker不再为服务创建VIP，docker DNS服务直接利用轮询的策略把服务名称直接解析成一个容器的IP地址。 </p>\n<ul>\n<li>Exteral LB（Ingress LB 或者 Swarm Mode Routing Mesh)</li>\n</ul>\n<p>看名字就知道，这个负载均衡方式和前面提到的Ingress网络有关。Swarm网络要提供对外访问的服务就需要打开公开端口，并映射到宿主机。Ingress LB就是外部通过公开端口访问集群时做的负载均衡。</p>\n<p>当创建或更新一个服务时，你可以利用–publish选项把一个服务暴露到外部，在docker swarm模式下发布一个端口意味着在集群中的所有节点都会监听这个端口，这时当访问一个监听了端口但是并没有对应服务运行在其上的节点会发生什么呢？    接下来就该我们的路由网（routing mesh）出场了，路由网时docker1.12引入的一个新特性，它结合了IPVS和iptables创建了一个强大的集群范围的L4层负载均衡，它使所有节点接收服务暴露端口的请求成为可能。当任意节点接收到针对某个服务暴露的TCP/UDP端口的请求时，这个节点会利用预先定义过的Ingress overlay网络，把请求转发给服务对应的虚拟IP。ingress网络和其他的overlay网络一样，只是它的目的是为了转换来自客户端到集群的请求，它也是利用我们前一小节介绍过的基于VIP的负载均衡技术。 </p>\n<p>启动服务后，你可以为应用程序创建外部DNS记录，并将其映射到任何或所有Docker swarm节点。你无需担心你的容器具体运行在那个节点上，因为有了路由网这个特性后，你的集群看起来就像是单独的一个节点一样。  </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#在集群中创建一个复制两份的服务，并暴露在8000端口  </span><br><span class=\"line\">$ docker service create --name app --replicas 2 --network appnet --publish 8000:80 nginx</span><br></pre></td></tr></table></figure>\n<p><img src=\"\\img\\external-routing-mesh.png\" alt=\"\"></p>\n<p>上面这个图表明了路由网是怎么工作的： </p>\n<ul>\n<li>服务（app）拥有两份复制，并把端口映射到外部端口的8000</li>\n<li>路由网在集群中的所有节点上都暴露出8000</li>\n<li>外部对服务app的请求可以是任意节点，在本例子中外部的负载均衡器将请求转发到了没有app服务的主机上</li>\n<li>docker swarm的IPVS利用ingress overlay网路将请求重新转发到运行着app服务的节点的容器中</li>\n</ul>\n<p>注：以上服务发现和负载均衡参考文档 <a href=\"https://success.docker.com/article/ucp-service-discovery\" target=\"_blank\" rel=\"noopener\">https://success.docker.com/article/ucp-service-discovery</a></p>\n<h2 id=\"2-3-Services-架构\"><a href=\"#2-3-Services-架构\" class=\"headerlink\" title=\"2.3 Services 架构\"></a>2.3 Services 架构</h2><p>在微服务部署的过程中，通常将某一个微服务封装为 Service 在 Swarm 中部署执行，通常你需要通过指定容器 image 以及需要在容器中执行的 Commands 来创建你的 Service，除此之外，通常，还需要配置如下选项，</p>\n<ul>\n<li>指定可以在 Swarm 之外可以被访问的服务端口号 port，</li>\n<li>指定加入某个 Overlay 网络以便 Service 与 Service 之间可以建立连接并通讯，</li>\n<li>指定该 Service 所要使用的 CPU 和 内存的大小，</li>\n<li>指定一个滚动更新的策略 (Rolling Update Policy)</li>\n<li>指定多少个 Task 的副本 (replicas) 在 Swarm 集群中同时存在</li>\n</ul>\n<h3 id=\"2-3-1-服务和任务\"><a href=\"#2-3-1-服务和任务\" class=\"headerlink\" title=\"2.3.1 服务和任务\"></a>2.3.1 服务和任务</h3><p><img src=\"\\img\\services-diagram.png\" alt=\"\"></p>\n<p>Services，Tasks 和 Containers 之间的关系可以用上面这张图来描述。来看这张图的逻辑，表示用户想要通过 Manager 节点部署一个有 3 个副本的 Nginx 的 Service，Manager 节点接收到用户的 Service definition 后，便开始对该 Service 进行调度，将会在当前可用的Worker（或者Manager ）节点中启动相应的 Tasks 以及相关的副本；所以可以看到，Service 实际上是 Task 的定义，而 Task 则是执行在节点上的程序。</p>\n<p>Task 是什么呢？其实就是一个 Container，只是，在 Swarm 中，每个 Task 都有自己的名字和编号，如图，比如 nginx.1、nginx.2 和 nginx.3，这些 Container 各自运行在各自的 node 上，当然，一个 node 上可以运行多个 Container；</p>\n<h3 id=\"2-3-2-任务调度\"><a href=\"#2-3-2-任务调度\" class=\"headerlink\" title=\"2.3.2 任务调度\"></a>2.3.2 任务调度</h3><p>Docker swarm mode 模式的底层技术实际上就是指的是调度器( scheduler )和编排器( orchestrator )；下面这张图展示了 Swarm mode 如何从一个 Service 的创建请求并且成功将该 Service 分发到两个 worker 节点上执行的过程。</p>\n<p><img src=\"\\img\\swarm-service-lifecycle.png\" alt=\"\"></p>\n<ul>\n<li>首先，看上半部分Swarm manager <ol>\n<li>用户通过 Docker Engine Client 使用命令 docker service create 提交 Service definition，</li>\n<li>根据 Service definition 创建相应的 Task，</li>\n<li>为 Task 分配 IP 地址，<br>注意，这是分配运行在 Swarm 集群中 Container 的 IP 地址，该 IP 地址最佳的分配地点是在这里，因为 Manager 节点上保存得有最新最全的 Tasks 的状态信息，为了保证不与其他的 Task 分配到相同的 IP，所以在这里就将 IP 地址给初始化好；</li>\n<li>将 Task 分发到 Node 上，可以是 Manager 节点也可以使 Worker 节点，</li>\n<li>对 Worker 节点进行相应的初始化使得它可以执行 Task</li>\n</ol>\n</li>\n<li>接着，看下半部分Swarm  Work<br>该部分就相对简单许多<ol>\n<li>首先连接 manager 的分配器( scheduler)检查该 task</li>\n<li>验证通过以后，便开始通过 Worker 节点上的执行器( executor )执行；</li>\n</ol>\n</li>\n</ul>\n<p>注意，上述 task 的执行过程是一种单向机制，比如它会按顺序的依次经历 assigned, prepared 和 running 等执行状态，不过在某些特殊情况下，在执行过程中，某个 task 失败了( fails )，编排器( orchestrator )直接将该 task 以及它的 container 给删除掉，然后在其它节点上另外创建并执行该 task；</p>\n<h3 id=\"2-3-3-调度策略\"><a href=\"#2-3-3-调度策略\" class=\"headerlink\" title=\"2.3.3 调度策略\"></a>2.3.3 调度策略</h3><p>Swarm在scheduler节点（leader节点）运行容器的时候，会根据指定的策略来计算最适合运行容器的节点，目前支持的策略有：spread, binpack, random。</p>\n<p>　　1）Random 顾名思义，就是随机选择一个Node来运行容器，一般用作调试用，spread和binpack策略会根据各个节点的可用的CPU, RAM以及正在运 行的容器的数量来计算应该运行容器的节点。</p>\n<p>　　2）Spread 在同等条件下，Spread策略会选择运行容器最少的那台节点来运行新的容器，binpack策略会选择运行容器最集中的那台机器来运行新的节点。 使用Spread策略会使得容器会均衡的分布在集群中的各个节点上运行，一旦一个节点挂掉了只会损失少部分的容器。</p>\n<p>　　3）Binpack Binpack策略最大化的避免容器碎片化，就是说binpack策略尽可能的把还未使用的节点留给需要更大空间的容器运行，尽可能的把容器运行在 一个节点上面。(The binpack strategy causes Swarm to optimize for the container which is most packed.)</p>\n<h3 id=\"2-3-4-服务副本与全局服务\"><a href=\"#2-3-4-服务副本与全局服务\" class=\"headerlink\" title=\"2.3.4 服务副本与全局服务\"></a>2.3.4 服务副本与全局服务</h3><p>Docker Swarm支持服务的扩容缩容，Swarm通过<code>--mode</code>选项设置服务类型，提供了两种模式：一种是replicated，我们可以指定服务Task的个数（也就是需要创建几个冗余副本），这也是Swarm默认使用的服务类型；另一种是global，这样会在Swarm集群的每个Node上都创建一个服务。如下图所示（出自Docker官网），是一个包含replicated和global模式的Swarm集群： </p>\n<p><img src=\"\\img\\docker-swarm-replicated-vs-global.png\" alt=\"\"></p>\n<p>上图中，黄色表示的replicated模式下的Service Replicas，灰色表示global模式下Service的分布。 </p>\n<p>在Swarm mode下使用Docker，可以实现部署运行服务、服务扩容缩容、删除服务、滚动更新等功能，下面我们详细说明。</p>\n<h1 id=\"3-集群搭建\"><a href=\"#3-集群搭建\" class=\"headerlink\" title=\"3.集群搭建\"></a>3.集群搭建</h1><h2 id=\"3-1-环境配置\"><a href=\"#3-1-环境配置\" class=\"headerlink\" title=\"3.1 环境配置\"></a>3.1 环境配置</h2><p>虚拟机信息</p>\n<table>\n<thead>\n<tr>\n<th>IP</th>\n<th>hostname</th>\n<th>配置</th>\n<th>角色</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>172.21.111.56</td>\n<td>swarm01</td>\n<td>4CPU/6G</td>\n<td>Manager</td>\n</tr>\n<tr>\n<td>172.21.111.57</td>\n<td>swarm02</td>\n<td>4CPU/6G</td>\n<td>Manager</td>\n</tr>\n<tr>\n<td>172.21.111.58</td>\n<td>swarm03</td>\n<td>4CPU/6G</td>\n<td>Manager</td>\n</tr>\n</tbody>\n</table>\n<p>为验证后续的swarm manager 高可用，将这三台节点都配置成为Manager节点。</p>\n<h2 id=\"3-2-前置准备\"><a href=\"#3-2-前置准备\" class=\"headerlink\" title=\"3.2 前置准备\"></a>3.2 前置准备</h2><h3 id=\"3-2-1-修改主机名\"><a href=\"#3-2-1-修改主机名\" class=\"headerlink\" title=\"3.2.1 修改主机名\"></a>3.2.1 修改主机名</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hostnamectl --static --transient set-hostname swarm01</span><br></pre></td></tr></table></figure>\n<p>修改/etc/hosts 。三台主机上均设置一致。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[root@swarm01 ~]# cat /etc/hosts</span><br><span class=\"line\">127.0.0.1   swarm-manager localhost localhost.localdomain localhost4 localhost4.localdomain4</span><br><span class=\"line\">::1         localhost localhost.localdomain localhost6 localhost6.localdomain6</span><br><span class=\"line\">172.21.111.56 swarm01</span><br><span class=\"line\">172.21.111.57 swarm02</span><br><span class=\"line\">172.21.111.58 swarm03</span><br></pre></td></tr></table></figure>\n<h3 id=\"3-2-2-配置互信\"><a href=\"#3-2-2-配置互信\" class=\"headerlink\" title=\"3.2.2 配置互信\"></a>3.2.2 配置互信</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ssh-keygen -t rsa</span><br><span class=\"line\">ssh-copy-id -i .ssh/id_rsa.pub 172.21.111.57</span><br></pre></td></tr></table></figure>\n<h3 id=\"3-2-3-关闭防火墙和SELinux\"><a href=\"#3-2-3-关闭防火墙和SELinux\" class=\"headerlink\" title=\"3.2.3 关闭防火墙和SELinux\"></a>3.2.3 关闭防火墙和SELinux</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">systemctl disable firewalld</span><br><span class=\"line\">systemctl stop firewalld</span><br></pre></td></tr></table></figure>\n<p>修改 /etc/selinux/config ，修改配置项SELINUX=enforcing为disabled。</p>\n<h2 id=\"3-2-安装Docker服务\"><a href=\"#3-2-安装Docker服务\" class=\"headerlink\" title=\"3.2 安装Docker服务\"></a>3.2 安装Docker服务</h2><p>如前文所示，从 v1.12 开始，Docker Swarm 的功能已经完全与 Docker Engine 集成，要管理集群，只需要启动 Swarm Mode。安装好 Docker，Swarm 就已经在那里了，服务发现也在那里了（不需要安装 Consul 等外部数据库）。配置好yum 源之后，即可安装docker-ce。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">yum install -y docker-ce</span><br><span class=\"line\">systemctl enable docker</span><br><span class=\"line\">systemctl start docker</span><br></pre></td></tr></table></figure>\n<h2 id=\"3-3-集群初始化\"><a href=\"#3-3-集群初始化\" class=\"headerlink\" title=\"3.3 集群初始化\"></a>3.3 集群初始化</h2><h3 id=\"3-2-1-初始化主节点\"><a href=\"#3-2-1-初始化主节点\" class=\"headerlink\" title=\"3.2.1 初始化主节点\"></a>3.2.1 初始化主节点</h3><p>使用命令<code>docker swarm init</code> 即可初始化当前节点为主节点。如果主机上有多张网卡的时候，需指定网卡<code>docker swarm init  --advertise-addr  [IP]   --listen-addr  [IP:PORT]</code>   </p>\n<p>节点自动解锁： docker swarm init –autolock=false</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[root@swarm01 ~]# docker swarm init --autolock=false  --advertise-addr 172.21.111.56 --listen-addr 172.21.111.56:2377   </span><br><span class=\"line\">Swarm initialized: current node (epoic1y0vv830vnwbc6nnacjc) is now a manager.</span><br><span class=\"line\"></span><br><span class=\"line\">To add a worker to this swarm, run the following command:</span><br><span class=\"line\"></span><br><span class=\"line\">    docker swarm join --token SWMTKN-1-5t9jv2o6z3ep9mk5c2ae41bpqojla1g8cfjo3qlsuj2sxi934l-af2x3lh8vs6dnpepy66kdw5mv 172.21.111.56:2377</span><br><span class=\"line\"></span><br><span class=\"line\">To add a manager to this swarm, run &apos;docker swarm join-token manager&apos; and follow the instructions.</span><br></pre></td></tr></table></figure>\n<p>此例，在swarm01中创建了主节点。从输出中，看到添加集群有两种方式，一种是以manager 的方式添加，一种则是以worker节点方式添加。默认输出只给出了添加worker的命令，manager 命令则需要通过<code>docker swarm join-token manager</code>  获取。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[root@swarm01 ~]# docker swarm join-token manager</span><br><span class=\"line\">To add a manager to this swarm, run the following command:</span><br><span class=\"line\"></span><br><span class=\"line\">    docker swarm join --token SWMTKN-1-5t9jv2o6z3ep9mk5c2ae41bpqojla1g8cfjo3qlsuj2sxi934l-eszh52z00ktao9j7lovfvjgqy 172.21.111.56:2377</span><br></pre></td></tr></table></figure>\n<h3 id=\"3-2-2-添加manager-节点\"><a href=\"#3-2-2-添加manager-节点\" class=\"headerlink\" title=\"3.2.2 添加manager 节点\"></a>3.2.2 添加manager 节点</h3><p>分别登陆swarm02、swarm03，执行上一节得到的命令。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[root@swarm03 ~]#docker swarm join --token SWMTKN-1-5t9jv2o6z3ep9mk5c2ae41bpqojla1g8cfjo3qlsuj2sxi934l-eszh52z00ktao9j7lovfvjgqy 172.21.111.56:2377</span><br><span class=\"line\">This node joined a swarm as a manager.</span><br></pre></td></tr></table></figure>\n<p>注：不要在节点上配置http代理，否则会添加节点失败。</p>\n<h3 id=\"3-2-3-离开集群\"><a href=\"#3-2-3-离开集群\" class=\"headerlink\" title=\"3.2.3 离开集群\"></a>3.2.3 离开集群</h3><p>如果想退出集群，可以在该节点上直接退出。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[root@swarm03 ~]# docker swarm leave</span><br><span class=\"line\">[root@swarm03 ~]# docker swarm leave --force</span><br></pre></td></tr></table></figure>\n<h3 id=\"3-2-4-查看集群节点信息\"><a href=\"#3-2-4-查看集群节点信息\" class=\"headerlink\" title=\"3.2.4 查看集群节点信息\"></a>3.2.4 查看集群节点信息</h3><p>登陆任意一台Manager节点。都可以查看当前集群的节点信息。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[root@swarm02 ~]# docker node ls</span><br><span class=\"line\">ID                            HOSTNAME            STATUS              AVAILABILITY        MANAGER STATUS      ENGINE VERSION</span><br><span class=\"line\">epoic1y0vv830vnwbc6nnacjc     swarm01             Ready               Active              Leader              18.03.1-ce</span><br><span class=\"line\">nsqqv181e0r7mu77azixtqhzl *   swarm02             Ready               Active              Reachable           18.03.1-ce</span><br><span class=\"line\">mkkxaeqergz8xw21bbkd47q1c     swarm03             Ready               Active              Reachable           18.03.1-ce</span><br></pre></td></tr></table></figure>\n<p>上面信息中，AVAILABILITY表示Swarm Scheduler是否可以向集群中的某个Node指派Task，对应有如下三种状态：</p>\n<ul>\n<li>Active：集群中该Node可以被指派Task</li>\n<li>Pause：集群中该Node不可以被指派新的Task，但是其他已经存在的Task保持运行</li>\n<li>Drain：集群中该Node不可以被指派新的Task，Swarm Scheduler停掉已经存在的Task，并将它们调度到可用的Node上</li>\n</ul>\n<p>另外，可以看到Manager Status状态也有分类，分别是Leader、Reacheable、Unreachable。</p>\n<ul>\n<li>Leader：对应Raft算法的Leader节点</li>\n<li>Reacheable：对应Raft算法的Flower节点，网络可达、可用的Flower节点</li>\n<li>Unreachable：对应Raft算法的Flower节点，网络不可达、不可用的Flower节点</li>\n</ul>\n<p>查看单个节点详细信息</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[root@swarm02 ~]# docker node inspect swarm01</span><br></pre></td></tr></table></figure>\n<p>关于Node的其他操作，比如状态变更、添加标签、提权和降权等，详见附录2</p>\n<h1 id=\"4-高可用实践\"><a href=\"#4-高可用实践\" class=\"headerlink\" title=\"4.高可用实践\"></a>4.高可用实践</h1><h2 id=\"4-1-应用服务管理\"><a href=\"#4-1-应用服务管理\" class=\"headerlink\" title=\"4.1 应用服务管理\"></a>4.1 应用服务管理</h2><p>docker swarm 还可以使用 stack 这一模型，用来部署管理和关联不同的服务。stack 功能比service 还要强大，因为它具备多服务编排部署的能力。<strong>stack file</strong> 是一种 yaml 格式的文件，类似于 docker-compose.yml 文件，它定义了一个或多个服务，并定义了服务的环境变量、部署标签、容器数量以及相关的环境特定配置等。 </p>\n<p>以下实验仅仅创建利用stack 创建单一服务。</p>\n<h3 id=\"4-1-1-创建服务\"><a href=\"#4-1-1-创建服务\" class=\"headerlink\" title=\"4.1.1 创建服务\"></a>4.1.1 创建服务</h3><p>服务配置文件docker-swarm.yml ，内容如下</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">version: &apos;3&apos;</span><br><span class=\"line\">services:</span><br><span class=\"line\">  web:</span><br><span class=\"line\">    image: &quot;httpd:v1&quot;</span><br><span class=\"line\">    deploy:</span><br><span class=\"line\">      replicas: 3</span><br><span class=\"line\">      resources:</span><br><span class=\"line\">        limits:</span><br><span class=\"line\">          cpus: &quot;0.5&quot;</span><br><span class=\"line\">          memory: 256M</span><br><span class=\"line\">      restart_policy:</span><br><span class=\"line\">        condition: on-failure</span><br><span class=\"line\">    ports:</span><br><span class=\"line\">     - &quot;7080:80&quot;</span><br><span class=\"line\">    volumes:</span><br><span class=\"line\">     - /var/www/html:/usr/local/apache2/htdocs</span><br></pre></td></tr></table></figure>\n<p>image: “httpd” ：httpd是docker hub 下载的镜像。提供简单的httpd服务，此处用来做服务都高可用验证。</p>\n<p>replicas: 3 表示该服务副本有3个，可对服务副本进行扩缩容，后续会讲到。</p>\n<p>/var/www/html:/usr/local/apache2/htdocs ：表示httpd 首页的内容。各个swarm节点上展示的内容分别是其主机名信息。</p>\n<p>在docker-swarm.yml 同级目录下，执行命令</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[root@swarm01 swarm]# docker stack deploy -c docker-swarm.yml myservice</span><br><span class=\"line\">Creating network myservice_default</span><br><span class=\"line\">Creating service myservice_web</span><br></pre></td></tr></table></figure>\n<h3 id=\"4-1-2-查看服务\"><a href=\"#4-1-2-查看服务\" class=\"headerlink\" title=\"4.1.2 查看服务\"></a>4.1.2 查看服务</h3><p>查看服务创建是否成功。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[root@swarm01 swarm]# docker stack ls</span><br><span class=\"line\">NAME                SERVICES</span><br><span class=\"line\">myservice           1</span><br><span class=\"line\">[root@swarm01 swarm]# docker stack services myservice</span><br><span class=\"line\">ID                  NAME                MODE                REPLICAS            IMAGE               PORTS</span><br><span class=\"line\">kom8x49f7qv2        myservice_web       replicated          3/3                 httpd:latest        *:7080-&gt;80/tcp</span><br><span class=\"line\">[root@swarm01 swarm]# docker stack ps myservice</span><br><span class=\"line\">ID                  NAME                IMAGE               NODE                DESIRED STATE       CURRENT STATE            ERROR               PORTS</span><br><span class=\"line\">sybn1dur1wmw        myservice_web.1     httpd:latest        swarm03             Running             Running 45 seconds ago</span><br><span class=\"line\">mlcb5r9vzv6z        myservice_web.2     httpd:latest        swarm03             Running             Running 45 seconds ago</span><br><span class=\"line\">fw21lcu3zp83        myservice_web.3     httpd:latest        swarm01             Running             Running 45 seconds ago</span><br></pre></td></tr></table></figure>\n<p>可以看到 swarm01 上面运行了1个容器，swarm03上面运行了2个，而swarm02上面一个容器都没有，这是咋回事呢？哦，原来我们在创建集群过程中，对swarm02 做了状态变更的操作。<code>docker node update  --availability drain  swarm02</code>. 查看集群状态。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[root@swarm01 swarm]# docker node ls</span><br><span class=\"line\">ID                            HOSTNAME            STATUS              AVAILABILITY        MANAGER STATUS      ENGINE VERSION</span><br><span class=\"line\">epoic1y0vv830vnwbc6nnacjc *   swarm01             Ready               Active              Leader              18.03.1-ce</span><br><span class=\"line\">nsqqv181e0r7mu77azixtqhzl     swarm02             Ready               Drain               Reachable           18.03.1-ce</span><br><span class=\"line\">mkkxaeqergz8xw21bbkd47q1c     swarm03             Ready               Active              Reachable           18.03.1-ce</span><br></pre></td></tr></table></figure>\n<p>此时打开浏览器，输入HTTP URL。任意swarm集群节点都可以访问httpd服务。</p>\n<p><img src=\"C:\\Users\\jalon\\OneDrive\\01笔记\\Docker\\swarm-web.png\" alt=\"\"></p>\n<p>需要说明的是，由于3个节点的/var/www/html 路径未做共享存储。因此swarm会随机显示一个swarm节点的网页内容。</p>\n<h3 id=\"4-1-3-服务扩缩容\"><a href=\"#4-1-3-服务扩缩容\" class=\"headerlink\" title=\"4.1.3 服务扩缩容\"></a>4.1.3 服务扩缩容</h3><p>修改docker-swarm.yml 配置字段 <code>replicas: 5</code> ，将容器个数提升到5个。由于实验环境资源有限，因此这里将swarm02状态变更为可以状态，用以分配容器。</p>\n<p><code>docker node update  --availability active swarm02</code>.</p>\n<p>重新执行部署命令<code>docker stack deploy -c docker-swarm.yml myservice</code></p>\n<p>也可以通过命令直接对服务进行操作，但是不推荐。 <code>docker service scale 服务ID=服务个数</code></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[root@swarm01 swarm]# docker stack ps myservice</span><br><span class=\"line\">ID                  NAME                IMAGE               NODE                DESIRED STATE       CURRENT STATE                    ERROR               PORTS</span><br><span class=\"line\">unwepav5z9nz        myservice_web.1     httpd:v1            swarm02             Running             Running less than a second ago</span><br><span class=\"line\">3j5o1tktcg1l        myservice_web.2     httpd:v1            swarm01             Running             Preparing 3 seconds ago</span><br><span class=\"line\">wl1unt6lynft        myservice_web.3     httpd:v1            swarm03             Running             Running less than a second ago</span><br><span class=\"line\">[root@swarm01 swarm]# vi docker-swarm.yml</span><br><span class=\"line\">[root@swarm01 swarm]# docker stack deploy -c docker-swarm.yml myservice</span><br><span class=\"line\">Updating service myservice_web (id: 85quzwi5k0btkn5wlht4jbco9)</span><br><span class=\"line\">image httpd:v1 could not be accessed on a registry to record</span><br><span class=\"line\">its digest. Each node will access httpd:v1 independently,</span><br><span class=\"line\">possibly leading to different nodes running different</span><br><span class=\"line\">versions of the image.</span><br><span class=\"line\"></span><br><span class=\"line\">[root@swarm01 swarm]# docker stack ps myservice</span><br><span class=\"line\">ID                  NAME                IMAGE               NODE                DESIRED STATE       CURRENT STATE                    ERROR               PORTS</span><br><span class=\"line\">unwepav5z9nz        myservice_web.1     httpd:v1            swarm02             Running             Running 24 seconds ago</span><br><span class=\"line\">3j5o1tktcg1l        myservice_web.2     httpd:v1            swarm01             Running             Running 31 seconds ago</span><br><span class=\"line\">wl1unt6lynft        myservice_web.3     httpd:v1            swarm03             Running             Running 24 seconds ago</span><br><span class=\"line\">ujedjxbuffxp        myservice_web.4     httpd:v1            swarm01             Running             Running less than a second ago</span><br><span class=\"line\">ml9qvjh2add7        myservice_web.5     httpd:v1            swarm02             Running             Running less than a second ago</span><br></pre></td></tr></table></figure>\n<p>同样，对于服务缩容，也仅需要修改<code>replicas:</code> 配置即可。这里不再演示。</p>\n<h3 id=\"4-1-4-删除服务\"><a href=\"#4-1-4-删除服务\" class=\"headerlink\" title=\"4.1.4 删除服务\"></a>4.1.4 删除服务</h3><p>例如，删除myredis应用服务，执行docker service rm myredis，则应用服务myredis的全部副本都会被删除。 </p>\n<p>本例使用的是stack ，删除stack ，即可删除其下的服务。没错，docker语法非常类似,<code>docker stack rm myservice</code>。</p>\n<p>需要注意的是，对swarm上面服务的操作，都必须在manager节点上运行。</p>\n<h3 id=\"4-1-5-服务升降级\"><a href=\"#4-1-5-服务升降级\" class=\"headerlink\" title=\"4.1.5 服务升降级\"></a>4.1.5 服务升降级</h3><p>服务升降级，对应到docker中，则利用了容器镜像的更新，升级很好理解，直接把tag 标签版本往上提。那么降级，其实也可以理解为另外一种“升级“，只是镜像内容是上一个版本的而已。对于镜像标签的命令应该遵循一套严格的规范，这里不再赘述。</p>\n<p>先介绍利用service 命令行工具的用法。</p>\n<p>服务的滚动更新，这里我参考官网文档的例子说明。在Manager Node上执行如下命令：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">docker service create --replicas 3  --name redis --update-delay 10s redis:3.0.6</span><br></pre></td></tr></table></figure>\n<p>上面通过指定 –update-delay 选项，表示需要进行更新的服务，每次成功部署一个，延迟10秒钟，然后再更新下一个服务。如果某个服务更新失败，则Swarm的调度器就会暂停本次服务的部署更新。</p>\n<p>另外，也可以更新已经部署的服务所在容器中使用的Image的版本，例如执行如下命令：</p>\n<p>将Redis服务对应的Image版本有3.0.6更新为3.0.7，同样，如果更新失败，则暂停本次更新。</p>\n<p>那么，stack file 如何定义滚动更新呢？</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">update_config:</span><br><span class=\"line\">  parallelism: 2</span><br><span class=\"line\">  delay: 10s</span><br></pre></td></tr></table></figure>\n<p>修改docker-swarm.yml 如下</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">version: &apos;3&apos;</span><br><span class=\"line\">services:</span><br><span class=\"line\">  web:</span><br><span class=\"line\">    image: &quot;httpd:v2&quot;</span><br><span class=\"line\">    deploy:</span><br><span class=\"line\">      replicas: 3</span><br><span class=\"line\">      update_config:</span><br><span class=\"line\">        parallelism: 2</span><br><span class=\"line\">        delay: 30s</span><br><span class=\"line\">      resources:</span><br><span class=\"line\">        limits:</span><br><span class=\"line\">          cpus: &quot;0.5&quot;</span><br><span class=\"line\">          memory: 256M</span><br><span class=\"line\">      restart_policy:</span><br><span class=\"line\">        condition: on-failure</span><br><span class=\"line\">    ports:</span><br><span class=\"line\">     - &quot;7080:80&quot;</span><br><span class=\"line\">    volumes:</span><br><span class=\"line\">     - /var/www/html:/usr/local/apache2/htdocs</span><br></pre></td></tr></table></figure>\n<p>parallelism 表示同时升级的个数，delay 则表示间隔多长时间升级。</p>\n<p>同时将httpd 由v1 升级到v2，这里我们使用同一个镜像，只是tag 修改了一下。查看服务状态</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[root@swarm01 swarm]# docker stack ps myservice</span><br><span class=\"line\">ID                  NAME                  IMAGE               NODE                DESIRED STATE       CURRENT STATE                 ERROR               PORTS</span><br><span class=\"line\">qk8pvvc9nmv0        myservice_web.1       httpd:v2            swarm01             Running             Running 56 seconds ago</span><br><span class=\"line\">unwepav5z9nz         \\_ myservice_web.1   httpd:v1            swarm02             Shutdown            Shutdown 48 seconds ago</span><br><span class=\"line\">uyj0pm1df9hl        myservice_web.2       httpd:v2            swarm02             Running             Running 46 seconds ago</span><br><span class=\"line\">3j5o1tktcg1l         \\_ myservice_web.2   httpd:v1            swarm01             Shutdown            Shutdown about a minute ago</span><br><span class=\"line\">qdjov5zf5alb        myservice_web.3       httpd:v2            swarm03             Running             Running 10 seconds ago</span><br><span class=\"line\">wl1unt6lynft         \\_ myservice_web.3   httpd:v1            swarm03             Shutdown            Shutdown 12 seconds ago</span><br></pre></td></tr></table></figure>\n<p>可以看到同时只有2个容器在升级，同时第3个容器也是在间隔了30s 之后，才开始升级。</p>\n<p> –filter 过滤条件<code>docker stack ps  --filter  &quot;desired-state=running&quot;  myservice</code></p>\n<h2 id=\"4-2-应用服务高可用\"><a href=\"#4-2-应用服务高可用\" class=\"headerlink\" title=\"4.2 应用服务高可用\"></a>4.2 应用服务高可用</h2><p>书接上文。在前面创建的myservice的基础上，进行容器应用的高可用验证。</p>\n<h3 id=\"4-2-1-模拟容器故障\"><a href=\"#4-2-1-模拟容器故障\" class=\"headerlink\" title=\"4.2.1 模拟容器故障\"></a>4.2.1 模拟容器故障</h3><p>查看当前服务状态</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[root@swarm01 swarm]# docker stack ps  --filter  &quot;desired-state=running&quot;  myservice</span><br><span class=\"line\">ID                  NAME                IMAGE               NODE                DESIRED STATE       CURRENT STATE            ERROR               PORTS</span><br><span class=\"line\">qk8pvvc9nmv0        myservice_web.1     httpd:v2            swarm01             Running             Running 16 minutes ago</span><br><span class=\"line\">uyj0pm1df9hl        myservice_web.2     httpd:v2            swarm02             Running             Running 16 minutes ago</span><br><span class=\"line\">qdjov5zf5alb        myservice_web.3     httpd:v2            swarm03             Running             Running 15 minutes ago</span><br></pre></td></tr></table></figure>\n<p>swarm01-03 分别运行3个容器。现在登陆swam03节点，将其容器进程Kill 掉。模拟容器意外故障。</p>\n<p>将stack ps 得到的swarm03节点的ID <code>qdjov5zf5alb</code> 代入命令inspect。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[root@swarm01 swarm]# docker inspect qdjov5zf5alb</span><br><span class=\"line\">[</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        &quot;ID&quot;: &quot;qdjov5zf5albdm1o7vhuwoxjt&quot;,</span><br><span class=\"line\">...</span><br><span class=\"line\">        &quot;Spec&quot;: &#123;</span><br><span class=\"line\">            &quot;ContainerSpec&quot;: &#123;</span><br><span class=\"line\">                &quot;Image&quot;: &quot;httpd:v2&quot;,</span><br><span class=\"line\">                &quot;Labels&quot;: &#123;</span><br><span class=\"line\">                    &quot;com.docker.stack.namespace&quot;: &quot;myservice&quot;</span><br><span class=\"line\">                &#125;,</span><br><span class=\"line\">                &quot;Privileges&quot;: &#123;</span><br><span class=\"line\">                    &quot;CredentialSpec&quot;: null,</span><br><span class=\"line\">                    &quot;SELinuxContext&quot;: null</span><br><span class=\"line\">                &#125;,</span><br><span class=\"line\">                &quot;Mounts&quot;: [</span><br><span class=\"line\">                    &#123;</span><br><span class=\"line\">                        &quot;Type&quot;: &quot;bind&quot;,</span><br><span class=\"line\">                        &quot;Source&quot;: &quot;/var/www/html&quot;,</span><br><span class=\"line\">                        &quot;Target&quot;: &quot;/usr/local/apache2/htdocs&quot;</span><br><span class=\"line\">                    &#125;</span><br><span class=\"line\">                ],</span><br><span class=\"line\">                &quot;Isolation&quot;: &quot;default&quot;</span><br><span class=\"line\">            &#125;,</span><br><span class=\"line\">            &quot;Resources&quot;: &#123;</span><br><span class=\"line\">                &quot;Limits&quot;: &#123;</span><br><span class=\"line\">                    &quot;NanoCPUs&quot;: 500000000,</span><br><span class=\"line\">                    &quot;MemoryBytes&quot;: 268435456</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">            &#125;,</span><br><span class=\"line\">....</span><br><span class=\"line\">        &quot;ServiceID&quot;: &quot;85quzwi5k0btkn5wlht4jbco9&quot;,</span><br><span class=\"line\">        &quot;Slot&quot;: 3,</span><br><span class=\"line\">        &quot;NodeID&quot;: &quot;mkkxaeqergz8xw21bbkd47q1c&quot;,</span><br><span class=\"line\">        &quot;Status&quot;: &#123;</span><br><span class=\"line\">            &quot;Timestamp&quot;: &quot;2018-08-07T09:02:25.674376822Z&quot;,</span><br><span class=\"line\">            &quot;State&quot;: &quot;running&quot;,</span><br><span class=\"line\">            &quot;Message&quot;: &quot;started&quot;,</span><br><span class=\"line\">            &quot;ContainerStatus&quot;: &#123;</span><br><span class=\"line\">                &quot;ContainerID&quot;: &quot;fa7d228e0c1981bfc78bca0b81363d7ca44cac99844c2996afe528ae3ae9a129&quot;,</span><br><span class=\"line\">                &quot;PID&quot;: 8852,</span><br><span class=\"line\">                &quot;ExitCode&quot;: 0</span><br><span class=\"line\">            &#125;,</span><br><span class=\"line\">            &quot;PortStatus&quot;: &#123;&#125;</span><br><span class=\"line\">        &#125;,</span><br></pre></td></tr></table></figure>\n<p>得到得到其对于的容器fa7d228e0c1981bfc78bca0b81363d7ca44cac99844c2996afe528ae3ae9a129 （swarm03节点上运行）</p>\n<p>登陆swarm03节点，查看容器fa7d22 对应的进程。得到其进程号，并将其杀死。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[root@swarm03 /]# ps -ef|grep fa7d228e0c19</span><br><span class=\"line\">root      8832  1129  0 05:02 ?        00:00:00 docker-containerd-shim -namespace moby -workdir /var/lib/docker/containerd/daemon/io.containerd.runtime.v1.linux/moby/fa7d228e0c1981bfc78bca0b81363d7ca44cac99844c2996afe528ae3ae9a129 -address /var/run/docker/containerd/docker-containerd.sock -containerd-binary /usr/bin/docker-containerd -runtime-root /var/run/docker/runtime-runc</span><br><span class=\"line\">root      9989  1090  0 05:32 pts/0    00:00:00 grep --color=auto fa7d228e0c19</span><br><span class=\"line\">[root@swarm03 /]# kill -9 8832</span><br></pre></td></tr></table></figure>\n<h3 id=\"4-2-2-故障查看\"><a href=\"#4-2-2-故障查看\" class=\"headerlink\" title=\"4.2.2 故障查看\"></a>4.2.2 故障查看</h3><p>立即查看服务状态。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[root@swarm03 /]# docker stack ps myservice</span><br><span class=\"line\">ID                  NAME                  IMAGE               NODE                DESIRED STATE       CURRENT STATE             ERROR                         PORTS</span><br><span class=\"line\">qk8pvvc9nmv0        myservice_web.1       httpd:v2            swarm01             Running             Running 32 minutes ago</span><br><span class=\"line\">unwepav5z9nz         \\_ myservice_web.1   httpd:v1            swarm02             Shutdown            Shutdown 32 minutes ago</span><br><span class=\"line\">uyj0pm1df9hl        myservice_web.2       httpd:v2            swarm02             Running             Running 32 minutes ago</span><br><span class=\"line\">3j5o1tktcg1l         \\_ myservice_web.2   httpd:v1            swarm01             Shutdown            Shutdown 32 minutes ago</span><br><span class=\"line\">hx741f0kye5a        myservice_web.3       httpd:v2            swarm03             Running             Running 29 seconds ago</span><br><span class=\"line\">qdjov5zf5alb         \\_ myservice_web.3   httpd:v2            swarm03             Shutdown            Failed 36 seconds ago     &quot;task: non-zero exit (137)&quot;</span><br><span class=\"line\">wl1unt6lynft         \\_ myservice_web.3   httpd:v1            swarm03             Shutdown            Shutdown 31 minutes ago</span><br></pre></td></tr></table></figure>\n<p>可以看到，swarm03上面的容器myservice_web.3 立即被重启。故障即刻恢复。</p>\n<h3 id=\"4-2-3-结论\"><a href=\"#4-2-3-结论\" class=\"headerlink\" title=\"4.2.3 结论\"></a>4.2.3 结论</h3><p>通过以上实验我们得出，在docker swarm 集群服务中，任意容器出现故障意外死亡，都会被重启。满足应用服务高可用的需求。但是，可能会有同学有疑问，你这个只是kill掉进程，模拟的是进程级别的故障，如果是主机宕机呢，swarm是否也能满足呢？答案是肯定的！</p>\n<p>在下一个章节，就让我们继续模拟主机级别的故障，验证服务的高可用。同时，由于我们环境配置的均是Manager节点，我们也可以同时验证Swarm Manager 的高可用。让我们拭目以待吧！</p>\n<h2 id=\"4-3-Swarm-Manager-高可用\"><a href=\"#4-3-Swarm-Manager-高可用\" class=\"headerlink\" title=\"4.3 Swarm Manager 高可用\"></a>4.3 Swarm Manager 高可用</h2><p>查看当前swarm集群节点信息</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[root@swarm01 swarm]# docker node ls</span><br><span class=\"line\">ID                            HOSTNAME            STATUS              AVAILABILITY        MANAGER STATUS      ENGINE VERSION</span><br><span class=\"line\">epoic1y0vv830vnwbc6nnacjc *   swarm01             Ready               Active              Leader              18.03.1-ce</span><br><span class=\"line\">nsqqv181e0r7mu77azixtqhzl     swarm02             Ready               Active              Reachable           18.03.1-ce</span><br><span class=\"line\">mkkxaeqergz8xw21bbkd47q1c     swarm03             Ready               Active              Reachable           18.03.1-ce</span><br></pre></td></tr></table></figure>\n<p>以上可知，swarm01 为swarm 集群管理节点。倘若swarm01 突然宕机关机会怎样呢，整个swarm 集群会失效，陷入瘫痪吗？其上运行的容器服务是否会全部挂掉，无法继续提供服务?</p>\n<h3 id=\"4-3-1-模拟主机宕机\"><a href=\"#4-3-1-模拟主机宕机\" class=\"headerlink\" title=\"4.3.1 模拟主机宕机\"></a>4.3.1 模拟主机宕机</h3><p>将虚拟机swarm01 直接关机。</p>\n<h3 id=\"4-3-2-故障查看\"><a href=\"#4-3-2-故障查看\" class=\"headerlink\" title=\"4.3.2 故障查看\"></a>4.3.2 故障查看</h3><p> 登陆swarm03 ，查看swarm集群状态。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[root@swarm03 /]# docker node ls</span><br><span class=\"line\">ID                            HOSTNAME            STATUS              AVAILABILITY        MANAGER STATUS      ENGINE VERSION</span><br><span class=\"line\">epoic1y0vv830vnwbc6nnacjc     swarm01             Unknown             Active              Unreachable         18.03.1-ce</span><br><span class=\"line\">nsqqv181e0r7mu77azixtqhzl     swarm02             Ready               Active              Reachable           18.03.1-ce</span><br><span class=\"line\">mkkxaeqergz8xw21bbkd47q1c *   swarm03             Ready               Active              Leader              18.03.1-ce</span><br></pre></td></tr></table></figure>\n<p>在经历短暂的间隙后，可看到swarm集群认定swarm01 现在处于Unknown 状态，MANAGER STATUS 处于Unreachable状态。同时，swarm集群基于raft 算法，重新推举出新的Leader swarm03。</p>\n<p>再过一段时间，swarm01 状态变成Down</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[root@swarm03 ~]# docker node ls</span><br><span class=\"line\">ID                            HOSTNAME            STATUS              AVAILABILITY        MANAGER STATUS      ENGINE VERSION</span><br><span class=\"line\">epoic1y0vv830vnwbc6nnacjc     swarm01             Down                Active              Unreachable         18.03.1-ce</span><br><span class=\"line\">nsqqv181e0r7mu77azixtqhzl     swarm02             Ready               Active              Reachable           18.03.1-ce</span><br><span class=\"line\">mkkxaeqergz8xw21bbkd47q1c *   swarm03             Ready               Active              Leader              18.03.1-ce</span><br></pre></td></tr></table></figure>\n<p>查看服务状态</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[root@swarm03 /]# docker stack ps myservice</span><br><span class=\"line\">ID                  NAME                  IMAGE               NODE                DESIRED STATE       CURRENT STATE                ERROR                         PORTS</span><br><span class=\"line\">zfbvcz5iu8g7        myservice_web.1       httpd:v2            swarm03             Running             Running 2 minutes ago</span><br><span class=\"line\">qk8pvvc9nmv0         \\_ myservice_web.1   httpd:v2            swarm01             Shutdown            Running about an hour ago</span><br><span class=\"line\">unwepav5z9nz         \\_ myservice_web.1   httpd:v1            swarm02             Shutdown            Shutdown about an hour ago</span><br><span class=\"line\">uyj0pm1df9hl        myservice_web.2       httpd:v2            swarm02             Running             Running 3 minutes ago</span><br><span class=\"line\">3j5o1tktcg1l         \\_ myservice_web.2   httpd:v1            swarm01             Shutdown            Shutdown about an hour ago</span><br><span class=\"line\">hx741f0kye5a        myservice_web.3       httpd:v2            swarm03             Running             Running 3 minutes ago</span><br><span class=\"line\">qdjov5zf5alb         \\_ myservice_web.3   httpd:v2            swarm03             Shutdown            Failed 21 minutes ago        &quot;task: non-zero exit (137)&quot;</span><br><span class=\"line\">wl1unt6lynft         \\_ myservice_web.3   httpd:v1            swarm03             Shutdown            Shutdown about an hour ago</span><br></pre></td></tr></table></figure>\n<p>可以看到容器服务个数依然维持在3个，原先在swarm01上面运行的容器，被分配到swarm02和swarm03上面。现在对服务进行扩容操作，以验证此时swarm集群是否对服务还有管理能力。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[root@swarm03 swarm]# docker stack deploy -c docker-swarm.yml myservice</span><br><span class=\"line\">Updating service myservice_web (id: 85quzwi5k0btkn5wlht4jbco9)</span><br><span class=\"line\">image httpd:v2 could not be accessed on a registry to record</span><br><span class=\"line\">its digest. Each node will access httpd:v2 independently,</span><br><span class=\"line\">possibly leading to different nodes running different</span><br><span class=\"line\">versions of the image.</span><br><span class=\"line\"></span><br><span class=\"line\">[root@swarm03 swarm]# docker stack ps  myservice</span><br><span class=\"line\">ID                  NAME                  IMAGE               NODE                DESIRED STATE       CURRENT STATE                ERROR                         PORTS</span><br><span class=\"line\">zfbvcz5iu8g7        myservice_web.1       httpd:v2            swarm03             Running             Running 6 minutes ago</span><br><span class=\"line\">qk8pvvc9nmv0         \\_ myservice_web.1   httpd:v2            swarm01             Shutdown            Running about an hour ago</span><br><span class=\"line\">unwepav5z9nz         \\_ myservice_web.1   httpd:v1            swarm02             Shutdown            Shutdown about an hour ago</span><br><span class=\"line\">uyj0pm1df9hl        myservice_web.2       httpd:v2            swarm02             Running             Running 6 minutes ago</span><br><span class=\"line\">3j5o1tktcg1l         \\_ myservice_web.2   httpd:v1            swarm01             Shutdown            Shutdown about an hour ago</span><br><span class=\"line\">hx741f0kye5a        myservice_web.3       httpd:v2            swarm03             Running             Running 6 minutes ago</span><br><span class=\"line\">qdjov5zf5alb         \\_ myservice_web.3   httpd:v2            swarm03             Shutdown            Failed 25 minutes ago        &quot;task: non-zero exit (137)&quot;</span><br><span class=\"line\">wl1unt6lynft         \\_ myservice_web.3   httpd:v1            swarm03             Shutdown            Shutdown about an hour ago</span><br><span class=\"line\">6nifzsbysug5        myservice_web.4       httpd:v2            swarm02             Running             Running 20 seconds ago</span><br><span class=\"line\">uwgnl2usv59a        myservice_web.5       httpd:v2            swarm02             Running             Running 20 seconds ago</span><br></pre></td></tr></table></figure>\n<p>可以看到，此时swarm集群依然具有管理能力（服务扩容能力）。也从侧面验证swarm manager的高可用能力。</p>\n<p>此时重启swarm01 ，看看 swarm01节点是否会自动加入集群。</p>\n<p>登陆swarm01 ，查看集群状态</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[root@swarm01 ~]# docker node ls</span><br><span class=\"line\">Error response from daemon: Swarm is encrypted and needs to be unlocked before it can be used. Please use &quot;docker swarm unlock&quot; to unlock it.</span><br></pre></td></tr></table></figure>\n<p>可以看到，此时swarm01 处于被锁的状态。需要解锁。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[root@swarm01 ~]# docker swarm unlock</span><br><span class=\"line\">Please enter unlock key:</span><br><span class=\"line\">Error response from daemon: invalid key string</span><br></pre></td></tr></table></figure>\n<p>那么  unlock key 从哪里取值呢？答案就是，需要在可用的集群管理节点，比如swarm03上面执行命令</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[root@swarm03 ~]# docker swarm unlock-key</span><br><span class=\"line\">To unlock a swarm manager after it restarts, run the `docker swarm unlock`</span><br><span class=\"line\">command and provide the following key:</span><br><span class=\"line\"></span><br><span class=\"line\">    SWMKEY-1-796L7+nU55V/qGCMPgBvKS/5PyotvNaKPS4SJ0AUe+g</span><br><span class=\"line\"></span><br><span class=\"line\">Please remember to store this key in a password manager, since without it you</span><br><span class=\"line\">will not be able to restart the manager.</span><br></pre></td></tr></table></figure>\n<p>通过该 unlock key ： SWMKEY-1-796L7+nU55V/qGCMPgBvKS/5PyotvNaKPS4SJ0AUe+g ，重新加入集群。</p>\n<p>此时在swarm01上面查看集群状态，即可看到已经重新加入集群。</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[root@swarm01 ~]<span class=\"comment\"># docker node ls</span></span><br><span class=\"line\">ID                            HOSTNAME            STATUS              AVAILABILITY        MANAGER STATUS      ENGINE VERSION</span><br><span class=\"line\">epoic1y0vv830vnwbc6nnacjc *   swarm01             Ready               Active              Reachable           18.03.1-ce</span><br><span class=\"line\">nsqqv181e0r7mu77azixtqhzl     swarm02             Ready               Active              Reachable           18.03.1-ce</span><br><span class=\"line\">mkkxaeqergz8xw21bbkd47q1c     swarm03             Ready               Active              Leader              18.03.1-ce</span><br></pre></td></tr></table></figure>\n<p>但是如果再把swarm03关掉，即同时宕掉2个节点，则此时3个管理节点，只剩下一个节点，不满足<a href=\"https://raft.github.io/raft.pdf\" target=\"_blank\" rel=\"noopener\">Raft</a> 协议的节点个数要求（&gt;N/2），集群功能就会失效，不能再管理集群，但是集群上的服务还能够继续运行。</p>\n<p>继续我们的脚步，将swarm03,swarm01 都关机。</p>\n<p>登陆swarm02，查看集群状态</p>\n <figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[root@swarm02 ~]# docker node ls</span><br><span class=\"line\">Error response from daemon: rpc error: code = Unknown desc = The swarm does not have a leader. It&apos;s possible that too few managers are online. Make sure more than half of the managers are online.</span><br><span class=\"line\">[root@swarm02 ~]# docker stack ps myservice</span><br><span class=\"line\">Error response from daemon: rpc error: code = Unknown desc = The swarm does not have a leader. It&apos;s possible that too few managers are online. Make sure more than half of the managers are online.</span><br></pre></td></tr></table></figure>\n<p>可以看到，此时集群功能已经失效，不能通过swarm 命令查看集群和管理集群了。那么运行其上的任务呢？</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[root@swarm02 ~]# docker container ls</span><br><span class=\"line\">CONTAINER ID        IMAGE                COMMAND              CREATED             STATUS              PORTS                    NAMES</span><br><span class=\"line\">2cc1b76be2cf        httpd:v2             &quot;httpd-foreground&quot;   2 hours ago         Up 2 hours          80/tcp                   myservice_web.5.uwgnl2usv59aygb5e9a8cig2y</span><br><span class=\"line\">96034bff8902        httpd:v2             &quot;httpd-foreground&quot;   2 hours ago         Up 2 hours          80/tcp                   myservice_web.4.6nifzsbysug5medburznrrr7h</span><br><span class=\"line\">82beb10e4cac        httpd:v2             &quot;httpd-foreground&quot;   3 hours ago         Up 3 hours          80/tcp                   myservice_web.2.uyj0pm1df9hlx9vdvck36ilbl</span><br><span class=\"line\">ea9f879cdf12        composetest_web:v1   &quot;python app.py&quot;      3 weeks ago         Up 5 hours          0.0.0.0:5000-&gt;5000/tcp   elegant_borg</span><br></pre></td></tr></table></figure>\n<p>还好还好，通过 <code>docker container ls</code>  查看swarm02节点上的容器，可以看到仍然有3个容器在运行，这些都是之前已经分配好的。显然，swarm集群即使失效了，也不会影响其上运行的容器服务正常运行，只是失去了管理和调度的能力了。</p>\n<p>此时，如果再次打开swarm01 和 swarm03的电源，他们会重新组建集群吗？从上文中我们知道，离线的manager节点要重新加入集群需要解锁，而钥匙需要通过原集群运行<code>docker swarm unlock-key</code> 获取。那么显然，原集群已经失效了。那此时会怎样呢？</p>\n<p>不幸的是，确实swarm01 和 swarm03 都无法自动加入集群，确实需要通过<code>docker swarm unlock</code>进行解锁。但是幸运的是，<strong>unlock key 是不变的</strong>，跟刚才swarm01宕机时重新加入的key 是一样的。分别在swarm01 和swarm03  执行解锁操作。集群最终恢复正常。因此需要在创建集群后，第一时间保存unlock key 。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[root@swarm03 ~]# docker node ls</span><br><span class=\"line\">ID                            HOSTNAME            STATUS              AVAILABILITY        MANAGER STATUS      ENGINE VERSION</span><br><span class=\"line\">epoic1y0vv830vnwbc6nnacjc     swarm01             Ready               Active              Reachable           18.03.1-ce</span><br><span class=\"line\">nsqqv181e0r7mu77azixtqhzl     swarm02             Ready               Active              Leader              18.03.1-ce</span><br><span class=\"line\">mkkxaeqergz8xw21bbkd47q1c *   swarm03             Ready               Active              Reachable           18.03.1-ce</span><br></pre></td></tr></table></figure>\n<p>注：在创建集群的时候，可设置自动解锁，即主机重启时，不用输入密钥，即可重新加入集群</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">docker swarm init --autolock=false</span><br><span class=\"line\">docker swarm update --autolock=false</span><br></pre></td></tr></table></figure>\n<h3 id=\"4-3-3-结论\"><a href=\"#4-3-3-结论\" class=\"headerlink\" title=\"4.3.3 结论\"></a>4.3.3 结论</h3><p>从以上实验中也可以看出，只要管理节点正常，服务的高可用就能达到，但是管理节点的个数必须保持在总的管理节点个数的一半以上，即3个只允许宕机一台，5台只允许宕机2台。而总的管理节点的个数一般不超过7个，如果太多的话，集群内管理节点通信消耗会比较大。由于偶数性价比不高（因为4台也只能宕机掉1台跟3台时是一样的），所以管理节点的个数一般都是奇数。</p>\n<p>管理节点的个数以及允许宕机的个数如下：</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">mannager node</th>\n<th style=\"text-align:center\">允许宕机个数</th>\n<th style=\"text-align:center\">服务运行状态</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">3</td>\n<td style=\"text-align:center\">1</td>\n<td style=\"text-align:center\">正常</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">5</td>\n<td style=\"text-align:center\">2</td>\n<td style=\"text-align:center\">正常</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">7</td>\n<td style=\"text-align:center\">3</td>\n<td style=\"text-align:center\">正常</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"4-4-网络故障转移\"><a href=\"#4-4-网络故障转移\" class=\"headerlink\" title=\"4.4 网络故障转移\"></a>4.4 网络故障转移</h2><h3 id=\"4-4-1-模拟网络分区\"><a href=\"#4-4-1-模拟网络分区\" class=\"headerlink\" title=\"4.4.1 模拟网络分区\"></a>4.4.1 模拟网络分区</h3><p>环境说明：仍然是3台虚拟机组成的swarm管理集群 </p>\n<p>hd_hd-1服务：运行4个副本，分别swarm01/swarm02 各一个容器，swarm03运行2个容器</p>\n<p>将swarm02网卡拔掉，模拟网络故障分区。</p>\n<p>备注使用 –autolock=false 初始化集群：docker swarm init –autolock=false  –advertise-addr 172.21.111.56 –listen-addr 172.21.111.56:2377  </p>\n<h3 id=\"4-4-2-观察swarm-node信息\"><a href=\"#4-4-2-观察swarm-node信息\" class=\"headerlink\" title=\"4.4.2 观察swarm node信息\"></a>4.4.2 观察swarm node信息</h3><p>查看集群信息和服务状态</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[root@swarm01 httpd]# docker node ls</span><br><span class=\"line\">ID                            HOSTNAME            STATUS              AVAILABILITY        MANAGER STATUS      ENGINE VERSION</span><br><span class=\"line\">wk8jaypu23vr63082arpsl4ut *   swarm01             Ready               Active              Leader              18.03.1-ce</span><br><span class=\"line\">q9lcw91vus73wyly174iy72h3     swarm02             Down                Active              Unreachable         18.03.1-ce</span><br><span class=\"line\">1xelzfyr5kbn7kk70le45anei     swarm03             Ready               Active              Reachable           18.06.1-ce</span><br><span class=\"line\">[root@swarm01 httpd]# docker service ps hd_hd-1</span><br><span class=\"line\">ID                  NAME                IMAGE               NODE                DESIRED STATE       CURRENT STATE           ERROR               PORTS</span><br><span class=\"line\">t604bqtcj588        hd_hd-1.1           httpd:latest        swarm01             Running             Running 2 minutes ago</span><br><span class=\"line\">xfajbfz51nd9         \\_ hd_hd-1.1       httpd:latest        swarm02             Shutdown            Running 8 minutes ago</span><br><span class=\"line\">8clt10jl1xyf        hd_hd-1.2           httpd:latest        swarm01             Running             Running 6 minutes ago</span><br><span class=\"line\">py5cavsog5mo        hd_hd-1.3           httpd:latest        swarm03             Running             Running 7 minutes ago</span><br><span class=\"line\">oigopryy1kro        hd_hd-1.4           httpd:latest        swarm03             Running             Running 7 minutes ago</span><br></pre></td></tr></table></figure>\n<p>我们可以发现，集群认为swarm02现在已经失联（不论是宕机还是网络端开）。现在swarm02上面的容器被迁移到swarm01上面（调度策略应该是swarm01上面目前运行的容器个数比swarm03少）。</p>\n<p>通过虚拟机控制台登陆swarm02，验证集群功能和容器是否在运行</p>\n<p><img src=\"C:\\Users\\jalon\\OneDrive\\01笔记\\Docker\\swarm-network.png\" alt=\"1537414250767\"></p>\n<p>我们可以发现，swarm02由于当前是一个节点，根据raft算法，处于swarm集群功能不可用状态。docker ps查看容器，发现容器仍然在正常运行。得出结论，swarm集群功能失效，但是不影响原先已经运行的docker容器。</p>\n<p>综合swarm01/swarm03上面的容器个数和swarm02上面的容器个数，现在已经有5个容器了。而我们预设的容器个数是4个。那么当swarm02重新加入集群，会发生什么事情呢？</p>\n<h3 id=\"4-4-3-网络故障恢复\"><a href=\"#4-4-3-网络故障恢复\" class=\"headerlink\" title=\"4.4.3 网络故障恢复\"></a>4.4.3 网络故障恢复</h3><p>把swarm02网卡重新接上。swarm02 会自动加入集群。查看集群信息和服务信息。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[root@swarm01 httpd]# docker node ls</span><br><span class=\"line\">ID                            HOSTNAME            STATUS              AVAILABILITY        MANAGER STATUS      ENGINE VERSION</span><br><span class=\"line\">lkivi02vx5z939z0x7lacuarl *   swarm01             Ready               Active              Leader              18.03.1-ce</span><br><span class=\"line\">pdp0awtf34o4x60x812h07a6e     swarm02             Ready               Active              Reachable           18.03.1-ce</span><br><span class=\"line\">qavrh9e6pjdq2aoxpucl6nohm     swarm03             Ready               Active              Reachable           18.06.1-ce</span><br><span class=\"line\">[root@swarm01 httpd]# docker service ps hd_hd-1</span><br><span class=\"line\">ID                  NAME                IMAGE               NODE                DESIRED STATE       CURRENT STATE            ERROR               PORTS</span><br><span class=\"line\">pepxi7bmzi8r        hd_hd-1.1           httpd:latest        swarm01             Running             Running 2 minutes ago</span><br><span class=\"line\">yf42dr6fqvl5         \\_ hd_hd-1.1       httpd:latest        swarm02             Shutdown            Shutdown 2 minutes ago</span><br><span class=\"line\">j6iq7tpsnwb0        hd_hd-1.2           httpd:latest        swarm03             Running             Running 2 minutes ago</span><br><span class=\"line\">k23a3ezbocit        hd_hd-1.3           httpd:latest        swarm03             Running             Running 2 minutes ago</span><br><span class=\"line\">bqra1k9m0ojc        hd_hd-1.4           httpd:latest        swarm01             Running             Running 2 minutes ago</span><br></pre></td></tr></table></figure>\n<p>我们发现swarm02已经正常加入集群。登陆swarm02，查看swarm02上面原先的容器也已经停掉了。符合我们的预期。</p>\n<p><img src=\"C:\\Users\\jalon\\OneDrive\\01笔记\\Docker\\swarm-network-2.png\" alt=\"1537419010126\"></p>\n<h1 id=\"5-附录\"><a href=\"#5-附录\" class=\"headerlink\" title=\"5.附录\"></a>5.附录</h1><h2 id=\"5-1参考文章：\"><a href=\"#5-1参考文章：\" class=\"headerlink\" title=\"5.1参考文章：\"></a>5.1参考文章：</h2><p>docker swarm介绍：</p>\n<p><a href=\"http://shiyanjun.cn/archives/1625.html\" target=\"_blank\" rel=\"noopener\">http://shiyanjun.cn/archives/1625.html</a></p>\n<p><a href=\"https://jiayi.space/post/docker-swarmrong-qi-ji-qun-guan-li-gong-ju\" target=\"_blank\" rel=\"noopener\">https://jiayi.space/post/docker-swarmrong-qi-ji-qun-guan-li-gong-ju</a></p>\n<p><a href=\"http://www.shangyang.me/2018/02/01/docker-swarm-03-architect/\" target=\"_blank\" rel=\"noopener\">http://www.shangyang.me/2018/02/01/docker-swarm-03-architect/</a></p>\n<p><a href=\"http://blog.51cto.com/cloudman/1952873\" target=\"_blank\" rel=\"noopener\">http://blog.51cto.com/cloudman/1952873</a></p>\n<p><a href=\"https://www.cnblogs.com/bigberg/p/8761047.html\" target=\"_blank\" rel=\"noopener\">https://www.cnblogs.com/bigberg/p/8761047.html</a></p>\n<p>服务发现：<a href=\"https://success.docker.com/article/ucp-service-discovery\" target=\"_blank\" rel=\"noopener\">https://success.docker.com/article/ucp-service-discovery</a></p>\n<p>raft协议：<a href=\"http://thesecretlivesofdata.com/raft/\" target=\"_blank\" rel=\"noopener\">http://thesecretlivesofdata.com/raft/</a></p>\n<h2 id=\"5-2-Swarm-Node-节点操作\"><a href=\"#5-2-Swarm-Node-节点操作\" class=\"headerlink\" title=\"5.2 Swarm Node 节点操作\"></a>5.2 Swarm Node 节点操作</h2><h3 id=\"5-2-1-状态变更\"><a href=\"#5-2-1-状态变更\" class=\"headerlink\" title=\"5.2.1 状态变更\"></a>5.2.1 状态变更</h3><p>前面我们已经提到过，Node的AVAILABILITY有三种状态：Active、Pause、Drain，对某个Node进行变更，可以将其AVAILABILITY值通过Docker CLI修改为对应的状态即可，下面是常见的变更操作：</p>\n<ul>\n<li>设置Manager Node只具有管理功能</li>\n<li>对服务进行停机维护，可以修改AVAILABILITY为Drain状态</li>\n<li>暂停一个Node，然后该Node就不再接收新的Task</li>\n<li>恢复一个不可用或者暂停的Node</li>\n</ul>\n<p>例如，将Manager Node的AVAILABILITY值修改为Drain状态，使其只具备管理功能，执行如下命令：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[root@swarm02 ~]# docker node update  --availability drain  swarm02</span><br><span class=\"line\">swarm02</span><br><span class=\"line\">[root@swarm02 ~]# docker node ls</span><br><span class=\"line\">ID                            HOSTNAME            STATUS              AVAILABILITY        MANAGER STATUS      ENGINE VERSION</span><br><span class=\"line\">epoic1y0vv830vnwbc6nnacjc     swarm01             Ready               Active              Leader              18.03.1-ce</span><br><span class=\"line\">nsqqv181e0r7mu77azixtqhzl *   swarm02             Ready               Drain               Reachable           18.03.1-ce</span><br><span class=\"line\">mkkxaeqergz8xw21bbkd47q1c     swarm03             Ready               Active              Reachable           18.03.1-ce</span><br></pre></td></tr></table></figure>\n<p>这样，Manager Node不能被指派Task，也就是不能部署实际的Docker容器来运行服务，而只是作为管理Node的角色。 </p>\n<h3 id=\"5-2-2-添加标签\"><a href=\"#5-2-2-添加标签\" class=\"headerlink\" title=\"5.2.2 添加标签\"></a>5.2.2 添加标签</h3><p>每个Node的主机配置情况可能不同，比如有的适合运行CPU密集型应用，有的适合运行IO密集型应用，Swarm支持给每个Node添加标签元数据，这样可以根据Node的标签，来选择性地调度某个服务部署到期望的一组Node上。 给SWarm集群中的某个Worker Node添加标签，执行如下命令格式如下： </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[root@swarm02 ~]# docker node update  --label-add  app=dev swarm02</span><br><span class=\"line\">swarm02</span><br></pre></td></tr></table></figure>\n<h3 id=\"5-2-3-提权-降权\"><a href=\"#5-2-3-提权-降权\" class=\"headerlink\" title=\"5.2.3 提权/降权\"></a>5.2.3 提权/降权</h3><p>改变Node的角色，Worker Node可以变为Manager Node，这样实际Worker Node有工作Node变成了管理Node，对应操作分别是： </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">docker node promote swarm02</span><br><span class=\"line\">docker node demote swarm02</span><br></pre></td></tr></table></figure>\n<h2 id=\"5-3-网络管理\"><a href=\"#5-3-网络管理\" class=\"headerlink\" title=\"5.3 网络管理\"></a>5.3 网络管理</h2><h3 id=\"5-3-1-添加overlay-网络\"><a href=\"#5-3-1-添加overlay-网络\" class=\"headerlink\" title=\"5.3.1 添加overlay 网络\"></a>5.3.1 添加overlay 网络</h3><p>overlay网络是swarm中默认的跨主机网络模型。在Swarm集群中可以使用Overlay网络来连接到一个或多个服务。具体添加Overlay网络，首先，我们需要创建在Manager Node上创建一个Overlay网络，执行如下命令： </p>\n<p><code>docker network create --driver overlay my-network</code></p>\n<p>创建完Overlay网络my-network以后，Swarm集群中所有的Manager Node都可以访问该网络。然后，我们在创建服务的时候，只需要指定使用的网络为已存在的Overlay网络即可，如下命令所示： </p>\n<p><code>docker service create --replicas 3 --network my-network --name myweb  nginx</code></p>\n<p>这样，如果Swarm集群中其他Node上的Docker容器也使用my-network这个网络，那么处于该Overlay网络中的所有容器之间，通过网络可以连通。  </p>\n<h2 id=\"5-4-API\"><a href=\"#5-4-API\" class=\"headerlink\" title=\"5.4 API\"></a>5.4 API</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ curl --unix-socket /var/run/docker.sock -H &quot;Content-Type: application/json&quot; \\</span><br><span class=\"line\">  -d &apos;&#123;&quot;Image&quot;: &quot;alpine&quot;, &quot;Cmd&quot;: [&quot;echo&quot;, &quot;hello world&quot;]&#125;&apos; \\</span><br><span class=\"line\">  -X POST http:/v1.24/containers/create</span><br><span class=\"line\">&#123;&quot;Id&quot;:&quot;1c6594faf5&quot;,&quot;Warnings&quot;:null&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">$ curl --unix-socket /var/run/docker.sock -X POST http:/v1.24/containers/1c6594faf5/start</span><br><span class=\"line\"></span><br><span class=\"line\">$ curl --unix-socket /var/run/docker.sock -X POST http:/v1.24/containers/1c6594faf5/wait</span><br><span class=\"line\">&#123;&quot;StatusCode&quot;:0&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">$ curl --unix-socket /var/run/docker.sock &quot;http:/v1.24/containers/1c6594faf5/logs?stdout=1&quot;</span><br><span class=\"line\">hello world</span><br></pre></td></tr></table></figure>\n<p><a href=\"https://docs.docker.com/develop/sdk/#sdk-and-api-quickstart\" target=\"_blank\" rel=\"noopener\">https://docs.docker.com/develop/sdk/#sdk-and-api-quickstart</a></p>\n<p><a href=\"https://www.programmableweb.com/api/docker-swarm\" target=\"_blank\" rel=\"noopener\">https://www.programmableweb.com/api/docker-swarm</a></p>\n<h2 id=\"5-5-Docker-三剑客\"><a href=\"#5-5-Docker-三剑客\" class=\"headerlink\" title=\"5.5 Docker 三剑客\"></a>5.5 Docker 三剑客</h2><ul>\n<li>docker-compose 负责组织应用，应用由哪些服务组成，每个服务由多少个容器，均在该配置文件里面配置。</li>\n<li>docker-machine 负责提供虚拟机，该虚拟机具备docker daemon服务能力。创建的虚拟机，可以用来组成docker swarm集群。docker-machine 并不是必须的功能。完全可以手动安装docker 服务。</li>\n<li>docker swarm 负责组织docker集群，组织跨主机的docker集群管理能力。</li>\n</ul>\n","site":{"data":{}},"excerpt":"<h1 id=\"1-简介\"><a href=\"#1-简介\" class=\"headerlink\" title=\"1.简介\"></a>1.简介</h1><h2 id=\"1-1-swarm是什么\"><a href=\"#1-1-swarm是什么\" class=\"headerlink\" title=\"1.1 swarm是什么\"></a>1.1 swarm是什么</h2><p><code>Swarm</code> 是使用 <a href=\"https://github.com/docker/swarmkit/\" target=\"_blank\" rel=\"noopener\"><code>SwarmKit</code></a> 构建的 Docker 引擎内置（原生）的集群管理和编排工具。其主要作用是把若干台Docker主机抽象为一个整体，并且通过一个入口统一管理这些Docker主机上的各种Docker资源。Swarm和Kubernetes比较类似，但是更加轻量，具有的功能也较kubernetes更少一些。</p>\n<p>Docker v1.12 是一个非常重要的版本，Docker 重新实现了集群的编排方式。在此之前，提供集群功能的 Docker Swarm 是一个单独的软件，而且依赖外部数据库（比如 Consul、etcd 或 Zookeeper）。从 v1.12 开始，Docker Swarm 的功能已经完全与 Docker Engine 集成，要管理集群，只需要启动 Swarm Mode。安装好 Docker，Swarm 就已经在那里了，服务发现也在那里了（不需要安装 Consul 等外部数据库）。</p>\n<p>Docker的Swarm(集群)模式，集成很多工具和特性，比如：跨主机上快速部署服务，服务的快速扩展，集群的管理整合到docker引擎，这意味着可以不可以不使用第三方管理工具。分散设计，声明式的服务模型，可扩展，状态协调处理，多主机网络，分布式的服务发现，负载均衡，滚动更新，安全（通信的加密）等等。 </p>\n<p><img src=\"\\img\\docker-swarm-logo.png\" alt=\"\"></p>\n<p>官方网站：<a href=\"https://docs.docker.com/engine/swarm/\" target=\"_blank\" rel=\"noopener\">https://docs.docker.com/engine/swarm/</a></p>","more":"<h2 id=\"1-2-swarm基本概念\"><a href=\"#1-2-swarm基本概念\" class=\"headerlink\" title=\"1.2 swarm基本概念\"></a>1.2 swarm基本概念</h2><h3 id=\"1-2-1-节点\"><a href=\"#1-2-1-节点\" class=\"headerlink\" title=\"1.2.1 节点\"></a>1.2.1 节点</h3><p>运行 Docker 的主机可以主动初始化一个 <code>Swarm</code> 集群或者加入一个已存在的 <code>Swarm</code> 集群，这样这个运行 Docker 的主机就成为一个 <code>Swarm</code> 集群的节点 (<code>node</code>) 。</p>\n<p>节点分为管理 (<code>manager</code>) 节点和工作 (<code>worker</code>) 节点。</p>\n<p>管理节点用于 <code>Swarm</code> 集群的管理，<code>docker swarm</code> 命令基本只能在管理节点执行（节点退出集群命令 <code>docker swarm leave</code> 可以在工作节点执行）。一个 <code>Swarm</code> 集群可以有多个管理节点，但只有一个管理节点可以成为 <code>leader</code>，<code>leader</code> 通过 <code>raft</code> 协议实现。</p>\n<p>工作节点是任务执行节点，管理节点将服务 (<code>service</code>) 下发至工作节点执行。管理节点默认也作为工作节点。也可以通过配置让服务只运行在管理节点。</p>\n<p>Docker 官网的这张图片形象的展示了集群中管理节点与工作节点的关系。</p>\n<p><img src=\"\\img\\swarm-node.png\" alt=\"\"></p>\n<h3 id=\"1-2-2-服务和任务\"><a href=\"#1-2-2-服务和任务\" class=\"headerlink\" title=\"1.2.2 服务和任务\"></a>1.2.2 服务和任务</h3><p>任务 （<code>Task</code>）是 <code>Swarm</code> 中的最小的调度单位，目前来说就是一个单一的容器。</p>\n<p>服务 （<code>Services</code>） 是指一组任务的集合，服务定义了任务的属性。服务有两种模式：</p>\n<ul>\n<li><code>replicated services</code> 按照一定规则在各个工作节点上运行指定个数的任务。</li>\n<li><code>global services</code> 每个工作节点上运行一个任务</li>\n</ul>\n<p>两种模式通过 <code>docker service create</code> 的 <code>--mode</code> 参数指定。</p>\n<h3 id=\"1-2-3-负载均衡\"><a href=\"#1-2-3-负载均衡\" class=\"headerlink\" title=\"1.2.3 负载均衡\"></a>1.2.3 负载均衡</h3><p><code>manager</code>群集管理器使用 <strong>ingress load balancing</strong> 来对外公开群集提供的服务。<code>manager</code>可以自动为<strong>PublishedPort</strong> 分配服务，也可以手动配置。如果未指定端口，则swarm管理器会为服务分配30000-32767范围内的端口。</p>\n<p>外部组件（例如云负载平衡器）可以访问群集中任何节点的PublishedPort上的服务，无论该节点当前是否正在运行该服务的任务。群集中的所有节点都将入口连接到正在运行的任务实例。</p>\n<p>Swarm模式有一个内部DNS组件，可以自动为swarm中的每个服务分配一个DNS条目。群集管理器使用<strong>内部负载平衡</strong>来根据服务的DNS名称在群集内的服务之间分发请求。</p>\n<h2 id=\"1-3-swarm特性\"><a href=\"#1-3-swarm特性\" class=\"headerlink\" title=\"1.3 swarm特性\"></a>1.3 swarm特性</h2><ul>\n<li><p><strong>集群管理与Docker Engine集成:</strong>使用Docker Engine CLI来创建一个你能部署应用服务到Docker Engine的swarm。你不需要其他编排软件来创建或管理swarm。</p>\n</li>\n<li><p><strong>分散式设计：</strong>Docker Engine不是在部署时处理节点角色之间的差异，而是在运行时扮演自己角色。你可以使用Docker Engine部署两种类型的节点，管理器和worker。这意味着你可以从单个磁盘映像构建整个swarm。</p>\n</li>\n<li><p><strong>声明性服务模型：</strong> Docker Engine使用声明性方法来定义应用程序堆栈中各种服务的所需状态。例如，你可以描述由消息队列服务和数据库后端的Web前端服务组成的应用程序。</p>\n</li>\n<li><p><strong>伸缩性：</strong>对于每个服务，你可以声明要运行的任务数。当你向上或向下缩放时，swarm管理器通过添加或删除任务来自动适应，以保持所需状态。</p>\n</li>\n<li><p><strong>期望的状态协调：</strong>swarm管理器节点持续监控群集状态，并调整你描述的期望状态与实际状态之间的任何差异。 例如，如果设置运行一个10个副本容器的服务，这时worker机器托管其中的两个副本崩溃，管理器则将创建两个新副本以替换已崩溃的副本。 swarm管理器将新副本分配给正在运行和可用的worker。</p>\n</li>\n<li><p><strong>多主机网络：</strong>你可以为服务指定覆盖网络(<a href=\"https://www.centos.bz/tag/overlay/\" target=\"_blank\" rel=\"noopener\">overlay</a> network)。 当swarm管理器初始化或更新应用程序时，它会自动为容器在覆盖网络(overlay network)上分配地址。</p>\n</li>\n<li><p><strong>服务发现：</strong>Swarm管理器节点为swarm中的每个服务分配唯一的DNS名称，并负载平衡运行中的容器。 你可以通过嵌入在swarm中的DNS服务器查询在swarm中运行中的每个容器。</p>\n</li>\n<li><p><strong>负载平衡：</strong>你可以将服务的端口暴露给外部的负载均衡器。 在内部，swarm允许你指定如何在节点之间分发服务容器。</p>\n</li>\n<li><p><strong>安全通信：</strong>swarm中的每个节点强制执行TLS相互验证和加密，以保护其自身与所有其他节点之间的通信。 你可以选择使用自签名根证书或来自自定义根CA的证书。</p>\n</li>\n<li><p><strong>滚动更新：</strong>在上线新功能期间，你可以增量地应用服务更新到节点。 swarm管理器允许你控制将服务部署到不同节点集之间的延迟。 如果出现任何问题，你可以将任务回滚到服务的先前版本。</p>\n</li>\n</ul>\n<h1 id=\"2-架构分析\"><a href=\"#2-架构分析\" class=\"headerlink\" title=\"2.架构分析\"></a>2.架构分析</h1><h2 id=\"2-1-基本架构\"><a href=\"#2-1-基本架构\" class=\"headerlink\" title=\"2.1 基本架构\"></a>2.1 基本架构</h2><p>Docker Swarm提供了基本的集群能力，能够使多个Docker Engine组合成一个group，提供多容器服务。Swarm使用标准的Docker API，启动容器可以直接使用docker run命令。Swarm更核心的则是关注如何选择一个主机并在其上启动容器，最终运行服务。 Docker Swarm基本架构，如下图所示： </p>\n<p><img src=\"\\img\\docker-swarm-architecture.png\" alt=\"\"></p>\n<p>如上图所示，Swarm Node表示加入Swarm集群中的一个Docker Engine实例，基于该Docker Engine可以创建并管理多个Docker容器。其中，最开始创建Swarm集群的时候，Swarm Manager便是集群中的第一个Swarm Node。在所有的Node中，又根据其职能划分为Manager Node和Worker Node。</p>\n<h3 id=\"2-1-Manager-Node\"><a href=\"#2-1-Manager-Node\" class=\"headerlink\" title=\"2.1 Manager Node\"></a>2.1 Manager Node</h3><p>Manger 节点，顾名思义，是进行 Swarm 集群的管理工作的，它的管理工作集中在如下部分，</p>\n<ul>\n<li>维护一个集群的状态；</li>\n<li>对 Services 进行调度；</li>\n<li>为 Swarm 提供外部可调用的 API 接口；</li>\n</ul>\n<p>Manager 节点需要时刻维护和保存当前 Swarm 集群中各个节点的一致性状态，这里主要是指各个 Tasks 的执行的状态和其它节点的状态；因为 Swarm 集群是一个典型的分布式集群，在保证一致性上，Manager 节点采用 <a href=\"https://raft.github.io/raft.pdf\" target=\"_blank\" rel=\"noopener\">Raft</a> 协议来保证分布式场景下的数据一致性；</p>\n<p>通常为了保证 Manager 节点的高可用，Docker 建议采用奇数个 Manager 节点，这样的话，你可以在 Manager 失败的时候不用关机维护，我们给出如下的建议：</p>\n<ul>\n<li>3 个 Manager 节点最多可以同时容忍 1 个 Manager 节点失效的情况下保证高可用；</li>\n<li>5 个 Manager 节点最多可以同时容忍 2 个 Manager 节点失效的情况下保证高可用；</li>\n<li>N 个 Manager 节点最多可以同时容忍 (N−1)/2个 Manager 节点失效的情况下保证高可用；</li>\n<li>Docker 建议最多最多的情况下，使用 7 个 Manager 节点就够了，否则反而会降低集群的性能了。</li>\n</ul>\n<h3 id=\"2-2-Worker-Node\"><a href=\"#2-2-Worker-Node\" class=\"headerlink\" title=\"2.2 Worker Node\"></a>2.2 Worker Node</h3><p>Worker Node接收由Manager Node调度并指派的Task，启动一个Docker容器来运行指定的服务，并且Worker Node需要向Manager Node汇报被指派的Task的执行状态。</p>\n<h3 id=\"2-3-更换角色\"><a href=\"#2-3-更换角色\" class=\"headerlink\" title=\"2.3  更换角色\"></a>2.3  更换角色</h3><p>通过 docker node promote 命令将一个 Worker 节点提升为 Manager 节点。通常情况下，该命令使用在维护的过程中，需要将 Manager 节点占时下线进行维护操作；同样可以使用 docker node demote 将某个 manager 节点降级为 worker 节点。 </p>\n<h2 id=\"2-2-设计架构\"><a href=\"#2-2-设计架构\" class=\"headerlink\" title=\"2.2 设计架构\"></a>2.2 设计架构</h2><p><img src=\"\\img\\swarm-architecture-1.jpg\" alt=\"\"></p>\n<p>从前文可知， Swarm 集群的管理工作是由manager节点实现。如上图所示，manager节点实现的功能主要包括：node discovery，scheduler,cluster管理等。同时，为了保证Manager 节点的高可用，Manager 节点需要时刻维护和保存当前 Swarm 集群中各个节点的一致性状态。在保证一致性上，Manager 节点采用 <a href=\"https://raft.github.io/raft.pdf\" target=\"_blank\" rel=\"noopener\">Raft</a> 协议来保证分布式场景下的数据一致性；</p>\n<p>Docker Swarm内置了Raft一致性算法，可以保证分布式系统的数据保持一致性同步。Etcd, Consul等高可用键值存储系统也是采用了这种算法。这个算法的作用简单点说就是随时保证集群中有一个Leader，由Leader接收数据更新，再同步到其他各个Follower节点。在Swarm中的作用表现为当一个Leader 节点 down掉时，系统会立即选取出另一个Leader节点，由于这个节点同步了之前节点的所有数据，所以可以无缝地管理集群。</p>\n<p>Raft的详细解释可以参考<a href=\"http://thesecretlivesofdata.com/raft/\" target=\"_blank\" rel=\"noopener\">《The Secret Lives of Data–Raft: Understandable Distributed Consensus》</a>。</p>\n<h3 id=\"2-2-1-跨主机容器通信\"><a href=\"#2-2-1-跨主机容器通信\" class=\"headerlink\" title=\"2.2.1 跨主机容器通信\"></a>2.2.1 跨主机容器通信</h3><p>Docker Swarm 内置的跨主机容器通信方案是overlay网络，这是一个基于vxlan协议的网络实现。VxLAN 可将二层数据封装到 UDP 进行传输，VxLAN 提供与 VLAN 相同的以太网二层服务，但是拥有更强的扩展性和灵活性。 overlay 通过虚拟出一个子网，让处于不同主机的容器能透明地使用这个子网。所以跨主机的容器通信就变成了在同一个子网下的容器通信，看上去就像是同一主机下的bridge网络通信。</p>\n<p>为支持容器跨主机通信，Docker 提供了 overlay driver，使用户可以创建基于 VxLAN 的 overlay 网络。其实，docker 会创建一个 bridge 网络 “docker_gwbridge”，为所有连接到 overlay 网络的容器提供访问外网的能力。<code>`docker network inspect docker_gwbridge</code>  查看网络信息。</p>\n<p><img src=\"\\img\\overlay-gwbridge.jpg\" alt=\"\"></p>\n<p>下面我们讨论下overlay 网络的具体实现：</p>\n<p>docker 会为每个 overlay 网络创建一个独立的 network namespace，其中会有一个 linux bridge br0，endpoint 还是由 veth pair 实现，一端连接到容器中（即 eth0），另一端连接到 namespace 的 br0 上。</p>\n<p>br0 除了连接所有的 endpoint，还会连接一个 vxlan 设备，用于与其他 host 建立 vxlan tunnel。容器之间的数据就是通过这个 tunnel 通信的。逻辑网络拓扑结构如图所示：</p>\n<p><img src=\"\\img/overlay.jpg\" alt=\"\"></p>\n<h3 id=\"2-2-2-服务发现\"><a href=\"#2-2-2-服务发现\" class=\"headerlink\" title=\"2.2.2 服务发现\"></a>2.2.2 服务发现</h3><p>docker Swarm mode下会为每个节点的docker engine内置一个DNS server，各个节点间的DNS server通过control plane的gossip协议互相交互信息。此处DNS server用于容器间的服务发现。swarm mode会为每个 –net=自定义网络的service分配一个DNS entry。目前必须是自定义网络，比如overaly。而bridge和routing mesh的service，是不会分配DNS的。</p>\n<p>那么，下面就来详细介绍服务发现的原理。</p>\n<p>每个Docker容器都有一个DNS解析器，它将DNS查询转发到docker engine，该引擎充当DNS服务器。docker 引擎收到请求后就会在发出请求的容器所在的所有网络中，检查域名对应的是不是一个容器或者是服务，如果是，docker引擎就会从存储的key-value建值对中查找这个容器名、任务名、或者服务名对应的IP地址，并把这个IP地址或者是服务的虚拟IP地址返回给发起请求的域名解析器。  </p>\n<p>由上可知，docker的服务发现的作用范围是网络级别，也就意味着只有在同一个网络上的容器或任务才能利用内嵌的DNS服务来相互发现，不在同一个网络里面的服务是不能解析名称的，另外，为了安全和性能只有当一个节点上有容器或任务在某个网络里面时，这个节点才会存储那个网络里面的DNS记录。</p>\n<p>如果目的容器或服务和源容器不在同一个网络里面，Docker引擎会把这个DNS查询转发到配置的默认DNS服务  。</p>\n<p><img src=\"\\img\\service-dns.png\" alt=\"\"></p>\n<p>在上面的例子中，总共有两个服务myservice和client，其中myservice有两个容器，这两个服务在同一个网里面。在client里针对docker.com和myservice各执行了一个curl操作，下面时执行的流程： </p>\n<ul>\n<li>为了client解析docker.com和myservice，DNS查询进行初始化</li>\n<li>容器内建的解析器在127.0.0.11:53拦截到这个DNS查询请求，并把请求转发到docker引擎的DNS服务</li>\n<li>myservice被解析成服务对应的虚拟IP（10.0.0.3），在接下来的内部负载均衡阶段再被解析成一个具体任务的IP地址。如果是容器名称这一步直接解析成容器对应的IP地址（10.0.0.4或者10.0.0.5）。</li>\n<li>docker.com在mynet网络上不能被解析成服务，所以这个请求被转发到配置好的默认DNS服务器（8.8.8.8）上。</li>\n</ul>\n<h3 id=\"2-2-3-负载均衡\"><a href=\"#2-2-3-负载均衡\" class=\"headerlink\" title=\"2.2.3 负载均衡\"></a>2.2.3 负载均衡</h3><p>负载均衡分为两种：Swarm集群内的service之间的相互访问需要做负载均衡，称为内部负载均衡（Internal LB）；从Swarm集群外部访问服务的公开端口，也需要做负载均衡，称外部部负载均衡(Exteral LB or Ingress LB)。</p>\n<ul>\n<li>Internal LB</li>\n</ul>\n<p>内部负载均衡就是我们在上一段提到的服务发现，集群内部通过DNS访问service时，Swarm默认通过VIP（virtual IP）、iptables、IPVS转发到某个容器。 </p>\n<p><img src=\"\\img\\intelnal-lb.jpg\" alt=\"\"></p>\n<p>当在docker swarm集群模式下创建一个服务时，会自动在服务所属的网络上给服务额外的分配一个虚拟IP，当解析服务名字时就会返回这个虚拟IP。对虚拟IP的请求会通过overlay网络自动的负载到这个服务所有的健康任务上。这个方式也避免了客户端的负载均衡，因为只有单独的一个IP会返回到客户端，docker会处理虚拟IP到具体任务的路由，并把请求平均的分配给所有的健康任务。  </p>\n<p><img src=\"C:\\Users\\jalon\\OneDrive\\01笔记\\Docker\\internal-lb-2.png\" alt=\"\"></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># 创建overlay网络：mynet </span><br><span class=\"line\">$ docker network create -d overlay mynet  </span><br><span class=\"line\">a59umzkdj2r0ua7x8jxd84dhr </span><br><span class=\"line\"># 利用mynet网络创建myservice服务，并复制两份  </span><br><span class=\"line\">$ docker service create --network mynet --name myservice --replicas 2 busybox ping localhost  </span><br><span class=\"line\">78t5r8cr0f0h6k2c3k7ih4l6f5</span><br><span class=\"line\"># 通过下面的命令查看myservice对应的虚拟IP </span><br><span class=\"line\">$ docker service inspect myservice  </span><br><span class=\"line\">...</span><br><span class=\"line\">&quot;VirtualIPs&quot;: [ </span><br><span class=\"line\">\t&#123;  </span><br><span class=\"line\">     &quot;NetworkID&quot;: &quot;a59umzkdj2r0ua7x8jxd84dhr&quot;,  </span><br><span class=\"line\">     \t\t\t&quot;Addr&quot;: &quot;10.0.0.3/24&quot;  </span><br><span class=\"line\">      &#125;,  </span><br><span class=\"line\">]</span><br></pre></td></tr></table></figure>\n<p>注：swarm中服务还有另外一种负载均衡技术可选DNS round robin (DNS RR) （在创建服务时通过–endpoint-mode配置项指定），在DNSRR模式下，docker不再为服务创建VIP，docker DNS服务直接利用轮询的策略把服务名称直接解析成一个容器的IP地址。 </p>\n<ul>\n<li>Exteral LB（Ingress LB 或者 Swarm Mode Routing Mesh)</li>\n</ul>\n<p>看名字就知道，这个负载均衡方式和前面提到的Ingress网络有关。Swarm网络要提供对外访问的服务就需要打开公开端口，并映射到宿主机。Ingress LB就是外部通过公开端口访问集群时做的负载均衡。</p>\n<p>当创建或更新一个服务时，你可以利用–publish选项把一个服务暴露到外部，在docker swarm模式下发布一个端口意味着在集群中的所有节点都会监听这个端口，这时当访问一个监听了端口但是并没有对应服务运行在其上的节点会发生什么呢？    接下来就该我们的路由网（routing mesh）出场了，路由网时docker1.12引入的一个新特性，它结合了IPVS和iptables创建了一个强大的集群范围的L4层负载均衡，它使所有节点接收服务暴露端口的请求成为可能。当任意节点接收到针对某个服务暴露的TCP/UDP端口的请求时，这个节点会利用预先定义过的Ingress overlay网络，把请求转发给服务对应的虚拟IP。ingress网络和其他的overlay网络一样，只是它的目的是为了转换来自客户端到集群的请求，它也是利用我们前一小节介绍过的基于VIP的负载均衡技术。 </p>\n<p>启动服务后，你可以为应用程序创建外部DNS记录，并将其映射到任何或所有Docker swarm节点。你无需担心你的容器具体运行在那个节点上，因为有了路由网这个特性后，你的集群看起来就像是单独的一个节点一样。  </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#在集群中创建一个复制两份的服务，并暴露在8000端口  </span><br><span class=\"line\">$ docker service create --name app --replicas 2 --network appnet --publish 8000:80 nginx</span><br></pre></td></tr></table></figure>\n<p><img src=\"\\img\\external-routing-mesh.png\" alt=\"\"></p>\n<p>上面这个图表明了路由网是怎么工作的： </p>\n<ul>\n<li>服务（app）拥有两份复制，并把端口映射到外部端口的8000</li>\n<li>路由网在集群中的所有节点上都暴露出8000</li>\n<li>外部对服务app的请求可以是任意节点，在本例子中外部的负载均衡器将请求转发到了没有app服务的主机上</li>\n<li>docker swarm的IPVS利用ingress overlay网路将请求重新转发到运行着app服务的节点的容器中</li>\n</ul>\n<p>注：以上服务发现和负载均衡参考文档 <a href=\"https://success.docker.com/article/ucp-service-discovery\" target=\"_blank\" rel=\"noopener\">https://success.docker.com/article/ucp-service-discovery</a></p>\n<h2 id=\"2-3-Services-架构\"><a href=\"#2-3-Services-架构\" class=\"headerlink\" title=\"2.3 Services 架构\"></a>2.3 Services 架构</h2><p>在微服务部署的过程中，通常将某一个微服务封装为 Service 在 Swarm 中部署执行，通常你需要通过指定容器 image 以及需要在容器中执行的 Commands 来创建你的 Service，除此之外，通常，还需要配置如下选项，</p>\n<ul>\n<li>指定可以在 Swarm 之外可以被访问的服务端口号 port，</li>\n<li>指定加入某个 Overlay 网络以便 Service 与 Service 之间可以建立连接并通讯，</li>\n<li>指定该 Service 所要使用的 CPU 和 内存的大小，</li>\n<li>指定一个滚动更新的策略 (Rolling Update Policy)</li>\n<li>指定多少个 Task 的副本 (replicas) 在 Swarm 集群中同时存在</li>\n</ul>\n<h3 id=\"2-3-1-服务和任务\"><a href=\"#2-3-1-服务和任务\" class=\"headerlink\" title=\"2.3.1 服务和任务\"></a>2.3.1 服务和任务</h3><p><img src=\"\\img\\services-diagram.png\" alt=\"\"></p>\n<p>Services，Tasks 和 Containers 之间的关系可以用上面这张图来描述。来看这张图的逻辑，表示用户想要通过 Manager 节点部署一个有 3 个副本的 Nginx 的 Service，Manager 节点接收到用户的 Service definition 后，便开始对该 Service 进行调度，将会在当前可用的Worker（或者Manager ）节点中启动相应的 Tasks 以及相关的副本；所以可以看到，Service 实际上是 Task 的定义，而 Task 则是执行在节点上的程序。</p>\n<p>Task 是什么呢？其实就是一个 Container，只是，在 Swarm 中，每个 Task 都有自己的名字和编号，如图，比如 nginx.1、nginx.2 和 nginx.3，这些 Container 各自运行在各自的 node 上，当然，一个 node 上可以运行多个 Container；</p>\n<h3 id=\"2-3-2-任务调度\"><a href=\"#2-3-2-任务调度\" class=\"headerlink\" title=\"2.3.2 任务调度\"></a>2.3.2 任务调度</h3><p>Docker swarm mode 模式的底层技术实际上就是指的是调度器( scheduler )和编排器( orchestrator )；下面这张图展示了 Swarm mode 如何从一个 Service 的创建请求并且成功将该 Service 分发到两个 worker 节点上执行的过程。</p>\n<p><img src=\"\\img\\swarm-service-lifecycle.png\" alt=\"\"></p>\n<ul>\n<li>首先，看上半部分Swarm manager <ol>\n<li>用户通过 Docker Engine Client 使用命令 docker service create 提交 Service definition，</li>\n<li>根据 Service definition 创建相应的 Task，</li>\n<li>为 Task 分配 IP 地址，<br>注意，这是分配运行在 Swarm 集群中 Container 的 IP 地址，该 IP 地址最佳的分配地点是在这里，因为 Manager 节点上保存得有最新最全的 Tasks 的状态信息，为了保证不与其他的 Task 分配到相同的 IP，所以在这里就将 IP 地址给初始化好；</li>\n<li>将 Task 分发到 Node 上，可以是 Manager 节点也可以使 Worker 节点，</li>\n<li>对 Worker 节点进行相应的初始化使得它可以执行 Task</li>\n</ol>\n</li>\n<li>接着，看下半部分Swarm  Work<br>该部分就相对简单许多<ol>\n<li>首先连接 manager 的分配器( scheduler)检查该 task</li>\n<li>验证通过以后，便开始通过 Worker 节点上的执行器( executor )执行；</li>\n</ol>\n</li>\n</ul>\n<p>注意，上述 task 的执行过程是一种单向机制，比如它会按顺序的依次经历 assigned, prepared 和 running 等执行状态，不过在某些特殊情况下，在执行过程中，某个 task 失败了( fails )，编排器( orchestrator )直接将该 task 以及它的 container 给删除掉，然后在其它节点上另外创建并执行该 task；</p>\n<h3 id=\"2-3-3-调度策略\"><a href=\"#2-3-3-调度策略\" class=\"headerlink\" title=\"2.3.3 调度策略\"></a>2.3.3 调度策略</h3><p>Swarm在scheduler节点（leader节点）运行容器的时候，会根据指定的策略来计算最适合运行容器的节点，目前支持的策略有：spread, binpack, random。</p>\n<p>　　1）Random 顾名思义，就是随机选择一个Node来运行容器，一般用作调试用，spread和binpack策略会根据各个节点的可用的CPU, RAM以及正在运 行的容器的数量来计算应该运行容器的节点。</p>\n<p>　　2）Spread 在同等条件下，Spread策略会选择运行容器最少的那台节点来运行新的容器，binpack策略会选择运行容器最集中的那台机器来运行新的节点。 使用Spread策略会使得容器会均衡的分布在集群中的各个节点上运行，一旦一个节点挂掉了只会损失少部分的容器。</p>\n<p>　　3）Binpack Binpack策略最大化的避免容器碎片化，就是说binpack策略尽可能的把还未使用的节点留给需要更大空间的容器运行，尽可能的把容器运行在 一个节点上面。(The binpack strategy causes Swarm to optimize for the container which is most packed.)</p>\n<h3 id=\"2-3-4-服务副本与全局服务\"><a href=\"#2-3-4-服务副本与全局服务\" class=\"headerlink\" title=\"2.3.4 服务副本与全局服务\"></a>2.3.4 服务副本与全局服务</h3><p>Docker Swarm支持服务的扩容缩容，Swarm通过<code>--mode</code>选项设置服务类型，提供了两种模式：一种是replicated，我们可以指定服务Task的个数（也就是需要创建几个冗余副本），这也是Swarm默认使用的服务类型；另一种是global，这样会在Swarm集群的每个Node上都创建一个服务。如下图所示（出自Docker官网），是一个包含replicated和global模式的Swarm集群： </p>\n<p><img src=\"\\img\\docker-swarm-replicated-vs-global.png\" alt=\"\"></p>\n<p>上图中，黄色表示的replicated模式下的Service Replicas，灰色表示global模式下Service的分布。 </p>\n<p>在Swarm mode下使用Docker，可以实现部署运行服务、服务扩容缩容、删除服务、滚动更新等功能，下面我们详细说明。</p>\n<h1 id=\"3-集群搭建\"><a href=\"#3-集群搭建\" class=\"headerlink\" title=\"3.集群搭建\"></a>3.集群搭建</h1><h2 id=\"3-1-环境配置\"><a href=\"#3-1-环境配置\" class=\"headerlink\" title=\"3.1 环境配置\"></a>3.1 环境配置</h2><p>虚拟机信息</p>\n<table>\n<thead>\n<tr>\n<th>IP</th>\n<th>hostname</th>\n<th>配置</th>\n<th>角色</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>172.21.111.56</td>\n<td>swarm01</td>\n<td>4CPU/6G</td>\n<td>Manager</td>\n</tr>\n<tr>\n<td>172.21.111.57</td>\n<td>swarm02</td>\n<td>4CPU/6G</td>\n<td>Manager</td>\n</tr>\n<tr>\n<td>172.21.111.58</td>\n<td>swarm03</td>\n<td>4CPU/6G</td>\n<td>Manager</td>\n</tr>\n</tbody>\n</table>\n<p>为验证后续的swarm manager 高可用，将这三台节点都配置成为Manager节点。</p>\n<h2 id=\"3-2-前置准备\"><a href=\"#3-2-前置准备\" class=\"headerlink\" title=\"3.2 前置准备\"></a>3.2 前置准备</h2><h3 id=\"3-2-1-修改主机名\"><a href=\"#3-2-1-修改主机名\" class=\"headerlink\" title=\"3.2.1 修改主机名\"></a>3.2.1 修改主机名</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hostnamectl --static --transient set-hostname swarm01</span><br></pre></td></tr></table></figure>\n<p>修改/etc/hosts 。三台主机上均设置一致。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[root@swarm01 ~]# cat /etc/hosts</span><br><span class=\"line\">127.0.0.1   swarm-manager localhost localhost.localdomain localhost4 localhost4.localdomain4</span><br><span class=\"line\">::1         localhost localhost.localdomain localhost6 localhost6.localdomain6</span><br><span class=\"line\">172.21.111.56 swarm01</span><br><span class=\"line\">172.21.111.57 swarm02</span><br><span class=\"line\">172.21.111.58 swarm03</span><br></pre></td></tr></table></figure>\n<h3 id=\"3-2-2-配置互信\"><a href=\"#3-2-2-配置互信\" class=\"headerlink\" title=\"3.2.2 配置互信\"></a>3.2.2 配置互信</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ssh-keygen -t rsa</span><br><span class=\"line\">ssh-copy-id -i .ssh/id_rsa.pub 172.21.111.57</span><br></pre></td></tr></table></figure>\n<h3 id=\"3-2-3-关闭防火墙和SELinux\"><a href=\"#3-2-3-关闭防火墙和SELinux\" class=\"headerlink\" title=\"3.2.3 关闭防火墙和SELinux\"></a>3.2.3 关闭防火墙和SELinux</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">systemctl disable firewalld</span><br><span class=\"line\">systemctl stop firewalld</span><br></pre></td></tr></table></figure>\n<p>修改 /etc/selinux/config ，修改配置项SELINUX=enforcing为disabled。</p>\n<h2 id=\"3-2-安装Docker服务\"><a href=\"#3-2-安装Docker服务\" class=\"headerlink\" title=\"3.2 安装Docker服务\"></a>3.2 安装Docker服务</h2><p>如前文所示，从 v1.12 开始，Docker Swarm 的功能已经完全与 Docker Engine 集成，要管理集群，只需要启动 Swarm Mode。安装好 Docker，Swarm 就已经在那里了，服务发现也在那里了（不需要安装 Consul 等外部数据库）。配置好yum 源之后，即可安装docker-ce。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">yum install -y docker-ce</span><br><span class=\"line\">systemctl enable docker</span><br><span class=\"line\">systemctl start docker</span><br></pre></td></tr></table></figure>\n<h2 id=\"3-3-集群初始化\"><a href=\"#3-3-集群初始化\" class=\"headerlink\" title=\"3.3 集群初始化\"></a>3.3 集群初始化</h2><h3 id=\"3-2-1-初始化主节点\"><a href=\"#3-2-1-初始化主节点\" class=\"headerlink\" title=\"3.2.1 初始化主节点\"></a>3.2.1 初始化主节点</h3><p>使用命令<code>docker swarm init</code> 即可初始化当前节点为主节点。如果主机上有多张网卡的时候，需指定网卡<code>docker swarm init  --advertise-addr  [IP]   --listen-addr  [IP:PORT]</code>   </p>\n<p>节点自动解锁： docker swarm init –autolock=false</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[root@swarm01 ~]# docker swarm init --autolock=false  --advertise-addr 172.21.111.56 --listen-addr 172.21.111.56:2377   </span><br><span class=\"line\">Swarm initialized: current node (epoic1y0vv830vnwbc6nnacjc) is now a manager.</span><br><span class=\"line\"></span><br><span class=\"line\">To add a worker to this swarm, run the following command:</span><br><span class=\"line\"></span><br><span class=\"line\">    docker swarm join --token SWMTKN-1-5t9jv2o6z3ep9mk5c2ae41bpqojla1g8cfjo3qlsuj2sxi934l-af2x3lh8vs6dnpepy66kdw5mv 172.21.111.56:2377</span><br><span class=\"line\"></span><br><span class=\"line\">To add a manager to this swarm, run &apos;docker swarm join-token manager&apos; and follow the instructions.</span><br></pre></td></tr></table></figure>\n<p>此例，在swarm01中创建了主节点。从输出中，看到添加集群有两种方式，一种是以manager 的方式添加，一种则是以worker节点方式添加。默认输出只给出了添加worker的命令，manager 命令则需要通过<code>docker swarm join-token manager</code>  获取。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[root@swarm01 ~]# docker swarm join-token manager</span><br><span class=\"line\">To add a manager to this swarm, run the following command:</span><br><span class=\"line\"></span><br><span class=\"line\">    docker swarm join --token SWMTKN-1-5t9jv2o6z3ep9mk5c2ae41bpqojla1g8cfjo3qlsuj2sxi934l-eszh52z00ktao9j7lovfvjgqy 172.21.111.56:2377</span><br></pre></td></tr></table></figure>\n<h3 id=\"3-2-2-添加manager-节点\"><a href=\"#3-2-2-添加manager-节点\" class=\"headerlink\" title=\"3.2.2 添加manager 节点\"></a>3.2.2 添加manager 节点</h3><p>分别登陆swarm02、swarm03，执行上一节得到的命令。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[root@swarm03 ~]#docker swarm join --token SWMTKN-1-5t9jv2o6z3ep9mk5c2ae41bpqojla1g8cfjo3qlsuj2sxi934l-eszh52z00ktao9j7lovfvjgqy 172.21.111.56:2377</span><br><span class=\"line\">This node joined a swarm as a manager.</span><br></pre></td></tr></table></figure>\n<p>注：不要在节点上配置http代理，否则会添加节点失败。</p>\n<h3 id=\"3-2-3-离开集群\"><a href=\"#3-2-3-离开集群\" class=\"headerlink\" title=\"3.2.3 离开集群\"></a>3.2.3 离开集群</h3><p>如果想退出集群，可以在该节点上直接退出。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[root@swarm03 ~]# docker swarm leave</span><br><span class=\"line\">[root@swarm03 ~]# docker swarm leave --force</span><br></pre></td></tr></table></figure>\n<h3 id=\"3-2-4-查看集群节点信息\"><a href=\"#3-2-4-查看集群节点信息\" class=\"headerlink\" title=\"3.2.4 查看集群节点信息\"></a>3.2.4 查看集群节点信息</h3><p>登陆任意一台Manager节点。都可以查看当前集群的节点信息。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[root@swarm02 ~]# docker node ls</span><br><span class=\"line\">ID                            HOSTNAME            STATUS              AVAILABILITY        MANAGER STATUS      ENGINE VERSION</span><br><span class=\"line\">epoic1y0vv830vnwbc6nnacjc     swarm01             Ready               Active              Leader              18.03.1-ce</span><br><span class=\"line\">nsqqv181e0r7mu77azixtqhzl *   swarm02             Ready               Active              Reachable           18.03.1-ce</span><br><span class=\"line\">mkkxaeqergz8xw21bbkd47q1c     swarm03             Ready               Active              Reachable           18.03.1-ce</span><br></pre></td></tr></table></figure>\n<p>上面信息中，AVAILABILITY表示Swarm Scheduler是否可以向集群中的某个Node指派Task，对应有如下三种状态：</p>\n<ul>\n<li>Active：集群中该Node可以被指派Task</li>\n<li>Pause：集群中该Node不可以被指派新的Task，但是其他已经存在的Task保持运行</li>\n<li>Drain：集群中该Node不可以被指派新的Task，Swarm Scheduler停掉已经存在的Task，并将它们调度到可用的Node上</li>\n</ul>\n<p>另外，可以看到Manager Status状态也有分类，分别是Leader、Reacheable、Unreachable。</p>\n<ul>\n<li>Leader：对应Raft算法的Leader节点</li>\n<li>Reacheable：对应Raft算法的Flower节点，网络可达、可用的Flower节点</li>\n<li>Unreachable：对应Raft算法的Flower节点，网络不可达、不可用的Flower节点</li>\n</ul>\n<p>查看单个节点详细信息</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[root@swarm02 ~]# docker node inspect swarm01</span><br></pre></td></tr></table></figure>\n<p>关于Node的其他操作，比如状态变更、添加标签、提权和降权等，详见附录2</p>\n<h1 id=\"4-高可用实践\"><a href=\"#4-高可用实践\" class=\"headerlink\" title=\"4.高可用实践\"></a>4.高可用实践</h1><h2 id=\"4-1-应用服务管理\"><a href=\"#4-1-应用服务管理\" class=\"headerlink\" title=\"4.1 应用服务管理\"></a>4.1 应用服务管理</h2><p>docker swarm 还可以使用 stack 这一模型，用来部署管理和关联不同的服务。stack 功能比service 还要强大，因为它具备多服务编排部署的能力。<strong>stack file</strong> 是一种 yaml 格式的文件，类似于 docker-compose.yml 文件，它定义了一个或多个服务，并定义了服务的环境变量、部署标签、容器数量以及相关的环境特定配置等。 </p>\n<p>以下实验仅仅创建利用stack 创建单一服务。</p>\n<h3 id=\"4-1-1-创建服务\"><a href=\"#4-1-1-创建服务\" class=\"headerlink\" title=\"4.1.1 创建服务\"></a>4.1.1 创建服务</h3><p>服务配置文件docker-swarm.yml ，内容如下</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">version: &apos;3&apos;</span><br><span class=\"line\">services:</span><br><span class=\"line\">  web:</span><br><span class=\"line\">    image: &quot;httpd:v1&quot;</span><br><span class=\"line\">    deploy:</span><br><span class=\"line\">      replicas: 3</span><br><span class=\"line\">      resources:</span><br><span class=\"line\">        limits:</span><br><span class=\"line\">          cpus: &quot;0.5&quot;</span><br><span class=\"line\">          memory: 256M</span><br><span class=\"line\">      restart_policy:</span><br><span class=\"line\">        condition: on-failure</span><br><span class=\"line\">    ports:</span><br><span class=\"line\">     - &quot;7080:80&quot;</span><br><span class=\"line\">    volumes:</span><br><span class=\"line\">     - /var/www/html:/usr/local/apache2/htdocs</span><br></pre></td></tr></table></figure>\n<p>image: “httpd” ：httpd是docker hub 下载的镜像。提供简单的httpd服务，此处用来做服务都高可用验证。</p>\n<p>replicas: 3 表示该服务副本有3个，可对服务副本进行扩缩容，后续会讲到。</p>\n<p>/var/www/html:/usr/local/apache2/htdocs ：表示httpd 首页的内容。各个swarm节点上展示的内容分别是其主机名信息。</p>\n<p>在docker-swarm.yml 同级目录下，执行命令</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[root@swarm01 swarm]# docker stack deploy -c docker-swarm.yml myservice</span><br><span class=\"line\">Creating network myservice_default</span><br><span class=\"line\">Creating service myservice_web</span><br></pre></td></tr></table></figure>\n<h3 id=\"4-1-2-查看服务\"><a href=\"#4-1-2-查看服务\" class=\"headerlink\" title=\"4.1.2 查看服务\"></a>4.1.2 查看服务</h3><p>查看服务创建是否成功。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[root@swarm01 swarm]# docker stack ls</span><br><span class=\"line\">NAME                SERVICES</span><br><span class=\"line\">myservice           1</span><br><span class=\"line\">[root@swarm01 swarm]# docker stack services myservice</span><br><span class=\"line\">ID                  NAME                MODE                REPLICAS            IMAGE               PORTS</span><br><span class=\"line\">kom8x49f7qv2        myservice_web       replicated          3/3                 httpd:latest        *:7080-&gt;80/tcp</span><br><span class=\"line\">[root@swarm01 swarm]# docker stack ps myservice</span><br><span class=\"line\">ID                  NAME                IMAGE               NODE                DESIRED STATE       CURRENT STATE            ERROR               PORTS</span><br><span class=\"line\">sybn1dur1wmw        myservice_web.1     httpd:latest        swarm03             Running             Running 45 seconds ago</span><br><span class=\"line\">mlcb5r9vzv6z        myservice_web.2     httpd:latest        swarm03             Running             Running 45 seconds ago</span><br><span class=\"line\">fw21lcu3zp83        myservice_web.3     httpd:latest        swarm01             Running             Running 45 seconds ago</span><br></pre></td></tr></table></figure>\n<p>可以看到 swarm01 上面运行了1个容器，swarm03上面运行了2个，而swarm02上面一个容器都没有，这是咋回事呢？哦，原来我们在创建集群过程中，对swarm02 做了状态变更的操作。<code>docker node update  --availability drain  swarm02</code>. 查看集群状态。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[root@swarm01 swarm]# docker node ls</span><br><span class=\"line\">ID                            HOSTNAME            STATUS              AVAILABILITY        MANAGER STATUS      ENGINE VERSION</span><br><span class=\"line\">epoic1y0vv830vnwbc6nnacjc *   swarm01             Ready               Active              Leader              18.03.1-ce</span><br><span class=\"line\">nsqqv181e0r7mu77azixtqhzl     swarm02             Ready               Drain               Reachable           18.03.1-ce</span><br><span class=\"line\">mkkxaeqergz8xw21bbkd47q1c     swarm03             Ready               Active              Reachable           18.03.1-ce</span><br></pre></td></tr></table></figure>\n<p>此时打开浏览器，输入HTTP URL。任意swarm集群节点都可以访问httpd服务。</p>\n<p><img src=\"C:\\Users\\jalon\\OneDrive\\01笔记\\Docker\\swarm-web.png\" alt=\"\"></p>\n<p>需要说明的是，由于3个节点的/var/www/html 路径未做共享存储。因此swarm会随机显示一个swarm节点的网页内容。</p>\n<h3 id=\"4-1-3-服务扩缩容\"><a href=\"#4-1-3-服务扩缩容\" class=\"headerlink\" title=\"4.1.3 服务扩缩容\"></a>4.1.3 服务扩缩容</h3><p>修改docker-swarm.yml 配置字段 <code>replicas: 5</code> ，将容器个数提升到5个。由于实验环境资源有限，因此这里将swarm02状态变更为可以状态，用以分配容器。</p>\n<p><code>docker node update  --availability active swarm02</code>.</p>\n<p>重新执行部署命令<code>docker stack deploy -c docker-swarm.yml myservice</code></p>\n<p>也可以通过命令直接对服务进行操作，但是不推荐。 <code>docker service scale 服务ID=服务个数</code></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[root@swarm01 swarm]# docker stack ps myservice</span><br><span class=\"line\">ID                  NAME                IMAGE               NODE                DESIRED STATE       CURRENT STATE                    ERROR               PORTS</span><br><span class=\"line\">unwepav5z9nz        myservice_web.1     httpd:v1            swarm02             Running             Running less than a second ago</span><br><span class=\"line\">3j5o1tktcg1l        myservice_web.2     httpd:v1            swarm01             Running             Preparing 3 seconds ago</span><br><span class=\"line\">wl1unt6lynft        myservice_web.3     httpd:v1            swarm03             Running             Running less than a second ago</span><br><span class=\"line\">[root@swarm01 swarm]# vi docker-swarm.yml</span><br><span class=\"line\">[root@swarm01 swarm]# docker stack deploy -c docker-swarm.yml myservice</span><br><span class=\"line\">Updating service myservice_web (id: 85quzwi5k0btkn5wlht4jbco9)</span><br><span class=\"line\">image httpd:v1 could not be accessed on a registry to record</span><br><span class=\"line\">its digest. Each node will access httpd:v1 independently,</span><br><span class=\"line\">possibly leading to different nodes running different</span><br><span class=\"line\">versions of the image.</span><br><span class=\"line\"></span><br><span class=\"line\">[root@swarm01 swarm]# docker stack ps myservice</span><br><span class=\"line\">ID                  NAME                IMAGE               NODE                DESIRED STATE       CURRENT STATE                    ERROR               PORTS</span><br><span class=\"line\">unwepav5z9nz        myservice_web.1     httpd:v1            swarm02             Running             Running 24 seconds ago</span><br><span class=\"line\">3j5o1tktcg1l        myservice_web.2     httpd:v1            swarm01             Running             Running 31 seconds ago</span><br><span class=\"line\">wl1unt6lynft        myservice_web.3     httpd:v1            swarm03             Running             Running 24 seconds ago</span><br><span class=\"line\">ujedjxbuffxp        myservice_web.4     httpd:v1            swarm01             Running             Running less than a second ago</span><br><span class=\"line\">ml9qvjh2add7        myservice_web.5     httpd:v1            swarm02             Running             Running less than a second ago</span><br></pre></td></tr></table></figure>\n<p>同样，对于服务缩容，也仅需要修改<code>replicas:</code> 配置即可。这里不再演示。</p>\n<h3 id=\"4-1-4-删除服务\"><a href=\"#4-1-4-删除服务\" class=\"headerlink\" title=\"4.1.4 删除服务\"></a>4.1.4 删除服务</h3><p>例如，删除myredis应用服务，执行docker service rm myredis，则应用服务myredis的全部副本都会被删除。 </p>\n<p>本例使用的是stack ，删除stack ，即可删除其下的服务。没错，docker语法非常类似,<code>docker stack rm myservice</code>。</p>\n<p>需要注意的是，对swarm上面服务的操作，都必须在manager节点上运行。</p>\n<h3 id=\"4-1-5-服务升降级\"><a href=\"#4-1-5-服务升降级\" class=\"headerlink\" title=\"4.1.5 服务升降级\"></a>4.1.5 服务升降级</h3><p>服务升降级，对应到docker中，则利用了容器镜像的更新，升级很好理解，直接把tag 标签版本往上提。那么降级，其实也可以理解为另外一种“升级“，只是镜像内容是上一个版本的而已。对于镜像标签的命令应该遵循一套严格的规范，这里不再赘述。</p>\n<p>先介绍利用service 命令行工具的用法。</p>\n<p>服务的滚动更新，这里我参考官网文档的例子说明。在Manager Node上执行如下命令：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">docker service create --replicas 3  --name redis --update-delay 10s redis:3.0.6</span><br></pre></td></tr></table></figure>\n<p>上面通过指定 –update-delay 选项，表示需要进行更新的服务，每次成功部署一个，延迟10秒钟，然后再更新下一个服务。如果某个服务更新失败，则Swarm的调度器就会暂停本次服务的部署更新。</p>\n<p>另外，也可以更新已经部署的服务所在容器中使用的Image的版本，例如执行如下命令：</p>\n<p>将Redis服务对应的Image版本有3.0.6更新为3.0.7，同样，如果更新失败，则暂停本次更新。</p>\n<p>那么，stack file 如何定义滚动更新呢？</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">update_config:</span><br><span class=\"line\">  parallelism: 2</span><br><span class=\"line\">  delay: 10s</span><br></pre></td></tr></table></figure>\n<p>修改docker-swarm.yml 如下</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">version: &apos;3&apos;</span><br><span class=\"line\">services:</span><br><span class=\"line\">  web:</span><br><span class=\"line\">    image: &quot;httpd:v2&quot;</span><br><span class=\"line\">    deploy:</span><br><span class=\"line\">      replicas: 3</span><br><span class=\"line\">      update_config:</span><br><span class=\"line\">        parallelism: 2</span><br><span class=\"line\">        delay: 30s</span><br><span class=\"line\">      resources:</span><br><span class=\"line\">        limits:</span><br><span class=\"line\">          cpus: &quot;0.5&quot;</span><br><span class=\"line\">          memory: 256M</span><br><span class=\"line\">      restart_policy:</span><br><span class=\"line\">        condition: on-failure</span><br><span class=\"line\">    ports:</span><br><span class=\"line\">     - &quot;7080:80&quot;</span><br><span class=\"line\">    volumes:</span><br><span class=\"line\">     - /var/www/html:/usr/local/apache2/htdocs</span><br></pre></td></tr></table></figure>\n<p>parallelism 表示同时升级的个数，delay 则表示间隔多长时间升级。</p>\n<p>同时将httpd 由v1 升级到v2，这里我们使用同一个镜像，只是tag 修改了一下。查看服务状态</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[root@swarm01 swarm]# docker stack ps myservice</span><br><span class=\"line\">ID                  NAME                  IMAGE               NODE                DESIRED STATE       CURRENT STATE                 ERROR               PORTS</span><br><span class=\"line\">qk8pvvc9nmv0        myservice_web.1       httpd:v2            swarm01             Running             Running 56 seconds ago</span><br><span class=\"line\">unwepav5z9nz         \\_ myservice_web.1   httpd:v1            swarm02             Shutdown            Shutdown 48 seconds ago</span><br><span class=\"line\">uyj0pm1df9hl        myservice_web.2       httpd:v2            swarm02             Running             Running 46 seconds ago</span><br><span class=\"line\">3j5o1tktcg1l         \\_ myservice_web.2   httpd:v1            swarm01             Shutdown            Shutdown about a minute ago</span><br><span class=\"line\">qdjov5zf5alb        myservice_web.3       httpd:v2            swarm03             Running             Running 10 seconds ago</span><br><span class=\"line\">wl1unt6lynft         \\_ myservice_web.3   httpd:v1            swarm03             Shutdown            Shutdown 12 seconds ago</span><br></pre></td></tr></table></figure>\n<p>可以看到同时只有2个容器在升级，同时第3个容器也是在间隔了30s 之后，才开始升级。</p>\n<p> –filter 过滤条件<code>docker stack ps  --filter  &quot;desired-state=running&quot;  myservice</code></p>\n<h2 id=\"4-2-应用服务高可用\"><a href=\"#4-2-应用服务高可用\" class=\"headerlink\" title=\"4.2 应用服务高可用\"></a>4.2 应用服务高可用</h2><p>书接上文。在前面创建的myservice的基础上，进行容器应用的高可用验证。</p>\n<h3 id=\"4-2-1-模拟容器故障\"><a href=\"#4-2-1-模拟容器故障\" class=\"headerlink\" title=\"4.2.1 模拟容器故障\"></a>4.2.1 模拟容器故障</h3><p>查看当前服务状态</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[root@swarm01 swarm]# docker stack ps  --filter  &quot;desired-state=running&quot;  myservice</span><br><span class=\"line\">ID                  NAME                IMAGE               NODE                DESIRED STATE       CURRENT STATE            ERROR               PORTS</span><br><span class=\"line\">qk8pvvc9nmv0        myservice_web.1     httpd:v2            swarm01             Running             Running 16 minutes ago</span><br><span class=\"line\">uyj0pm1df9hl        myservice_web.2     httpd:v2            swarm02             Running             Running 16 minutes ago</span><br><span class=\"line\">qdjov5zf5alb        myservice_web.3     httpd:v2            swarm03             Running             Running 15 minutes ago</span><br></pre></td></tr></table></figure>\n<p>swarm01-03 分别运行3个容器。现在登陆swam03节点，将其容器进程Kill 掉。模拟容器意外故障。</p>\n<p>将stack ps 得到的swarm03节点的ID <code>qdjov5zf5alb</code> 代入命令inspect。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[root@swarm01 swarm]# docker inspect qdjov5zf5alb</span><br><span class=\"line\">[</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        &quot;ID&quot;: &quot;qdjov5zf5albdm1o7vhuwoxjt&quot;,</span><br><span class=\"line\">...</span><br><span class=\"line\">        &quot;Spec&quot;: &#123;</span><br><span class=\"line\">            &quot;ContainerSpec&quot;: &#123;</span><br><span class=\"line\">                &quot;Image&quot;: &quot;httpd:v2&quot;,</span><br><span class=\"line\">                &quot;Labels&quot;: &#123;</span><br><span class=\"line\">                    &quot;com.docker.stack.namespace&quot;: &quot;myservice&quot;</span><br><span class=\"line\">                &#125;,</span><br><span class=\"line\">                &quot;Privileges&quot;: &#123;</span><br><span class=\"line\">                    &quot;CredentialSpec&quot;: null,</span><br><span class=\"line\">                    &quot;SELinuxContext&quot;: null</span><br><span class=\"line\">                &#125;,</span><br><span class=\"line\">                &quot;Mounts&quot;: [</span><br><span class=\"line\">                    &#123;</span><br><span class=\"line\">                        &quot;Type&quot;: &quot;bind&quot;,</span><br><span class=\"line\">                        &quot;Source&quot;: &quot;/var/www/html&quot;,</span><br><span class=\"line\">                        &quot;Target&quot;: &quot;/usr/local/apache2/htdocs&quot;</span><br><span class=\"line\">                    &#125;</span><br><span class=\"line\">                ],</span><br><span class=\"line\">                &quot;Isolation&quot;: &quot;default&quot;</span><br><span class=\"line\">            &#125;,</span><br><span class=\"line\">            &quot;Resources&quot;: &#123;</span><br><span class=\"line\">                &quot;Limits&quot;: &#123;</span><br><span class=\"line\">                    &quot;NanoCPUs&quot;: 500000000,</span><br><span class=\"line\">                    &quot;MemoryBytes&quot;: 268435456</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">            &#125;,</span><br><span class=\"line\">....</span><br><span class=\"line\">        &quot;ServiceID&quot;: &quot;85quzwi5k0btkn5wlht4jbco9&quot;,</span><br><span class=\"line\">        &quot;Slot&quot;: 3,</span><br><span class=\"line\">        &quot;NodeID&quot;: &quot;mkkxaeqergz8xw21bbkd47q1c&quot;,</span><br><span class=\"line\">        &quot;Status&quot;: &#123;</span><br><span class=\"line\">            &quot;Timestamp&quot;: &quot;2018-08-07T09:02:25.674376822Z&quot;,</span><br><span class=\"line\">            &quot;State&quot;: &quot;running&quot;,</span><br><span class=\"line\">            &quot;Message&quot;: &quot;started&quot;,</span><br><span class=\"line\">            &quot;ContainerStatus&quot;: &#123;</span><br><span class=\"line\">                &quot;ContainerID&quot;: &quot;fa7d228e0c1981bfc78bca0b81363d7ca44cac99844c2996afe528ae3ae9a129&quot;,</span><br><span class=\"line\">                &quot;PID&quot;: 8852,</span><br><span class=\"line\">                &quot;ExitCode&quot;: 0</span><br><span class=\"line\">            &#125;,</span><br><span class=\"line\">            &quot;PortStatus&quot;: &#123;&#125;</span><br><span class=\"line\">        &#125;,</span><br></pre></td></tr></table></figure>\n<p>得到得到其对于的容器fa7d228e0c1981bfc78bca0b81363d7ca44cac99844c2996afe528ae3ae9a129 （swarm03节点上运行）</p>\n<p>登陆swarm03节点，查看容器fa7d22 对应的进程。得到其进程号，并将其杀死。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[root@swarm03 /]# ps -ef|grep fa7d228e0c19</span><br><span class=\"line\">root      8832  1129  0 05:02 ?        00:00:00 docker-containerd-shim -namespace moby -workdir /var/lib/docker/containerd/daemon/io.containerd.runtime.v1.linux/moby/fa7d228e0c1981bfc78bca0b81363d7ca44cac99844c2996afe528ae3ae9a129 -address /var/run/docker/containerd/docker-containerd.sock -containerd-binary /usr/bin/docker-containerd -runtime-root /var/run/docker/runtime-runc</span><br><span class=\"line\">root      9989  1090  0 05:32 pts/0    00:00:00 grep --color=auto fa7d228e0c19</span><br><span class=\"line\">[root@swarm03 /]# kill -9 8832</span><br></pre></td></tr></table></figure>\n<h3 id=\"4-2-2-故障查看\"><a href=\"#4-2-2-故障查看\" class=\"headerlink\" title=\"4.2.2 故障查看\"></a>4.2.2 故障查看</h3><p>立即查看服务状态。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[root@swarm03 /]# docker stack ps myservice</span><br><span class=\"line\">ID                  NAME                  IMAGE               NODE                DESIRED STATE       CURRENT STATE             ERROR                         PORTS</span><br><span class=\"line\">qk8pvvc9nmv0        myservice_web.1       httpd:v2            swarm01             Running             Running 32 minutes ago</span><br><span class=\"line\">unwepav5z9nz         \\_ myservice_web.1   httpd:v1            swarm02             Shutdown            Shutdown 32 minutes ago</span><br><span class=\"line\">uyj0pm1df9hl        myservice_web.2       httpd:v2            swarm02             Running             Running 32 minutes ago</span><br><span class=\"line\">3j5o1tktcg1l         \\_ myservice_web.2   httpd:v1            swarm01             Shutdown            Shutdown 32 minutes ago</span><br><span class=\"line\">hx741f0kye5a        myservice_web.3       httpd:v2            swarm03             Running             Running 29 seconds ago</span><br><span class=\"line\">qdjov5zf5alb         \\_ myservice_web.3   httpd:v2            swarm03             Shutdown            Failed 36 seconds ago     &quot;task: non-zero exit (137)&quot;</span><br><span class=\"line\">wl1unt6lynft         \\_ myservice_web.3   httpd:v1            swarm03             Shutdown            Shutdown 31 minutes ago</span><br></pre></td></tr></table></figure>\n<p>可以看到，swarm03上面的容器myservice_web.3 立即被重启。故障即刻恢复。</p>\n<h3 id=\"4-2-3-结论\"><a href=\"#4-2-3-结论\" class=\"headerlink\" title=\"4.2.3 结论\"></a>4.2.3 结论</h3><p>通过以上实验我们得出，在docker swarm 集群服务中，任意容器出现故障意外死亡，都会被重启。满足应用服务高可用的需求。但是，可能会有同学有疑问，你这个只是kill掉进程，模拟的是进程级别的故障，如果是主机宕机呢，swarm是否也能满足呢？答案是肯定的！</p>\n<p>在下一个章节，就让我们继续模拟主机级别的故障，验证服务的高可用。同时，由于我们环境配置的均是Manager节点，我们也可以同时验证Swarm Manager 的高可用。让我们拭目以待吧！</p>\n<h2 id=\"4-3-Swarm-Manager-高可用\"><a href=\"#4-3-Swarm-Manager-高可用\" class=\"headerlink\" title=\"4.3 Swarm Manager 高可用\"></a>4.3 Swarm Manager 高可用</h2><p>查看当前swarm集群节点信息</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[root@swarm01 swarm]# docker node ls</span><br><span class=\"line\">ID                            HOSTNAME            STATUS              AVAILABILITY        MANAGER STATUS      ENGINE VERSION</span><br><span class=\"line\">epoic1y0vv830vnwbc6nnacjc *   swarm01             Ready               Active              Leader              18.03.1-ce</span><br><span class=\"line\">nsqqv181e0r7mu77azixtqhzl     swarm02             Ready               Active              Reachable           18.03.1-ce</span><br><span class=\"line\">mkkxaeqergz8xw21bbkd47q1c     swarm03             Ready               Active              Reachable           18.03.1-ce</span><br></pre></td></tr></table></figure>\n<p>以上可知，swarm01 为swarm 集群管理节点。倘若swarm01 突然宕机关机会怎样呢，整个swarm 集群会失效，陷入瘫痪吗？其上运行的容器服务是否会全部挂掉，无法继续提供服务?</p>\n<h3 id=\"4-3-1-模拟主机宕机\"><a href=\"#4-3-1-模拟主机宕机\" class=\"headerlink\" title=\"4.3.1 模拟主机宕机\"></a>4.3.1 模拟主机宕机</h3><p>将虚拟机swarm01 直接关机。</p>\n<h3 id=\"4-3-2-故障查看\"><a href=\"#4-3-2-故障查看\" class=\"headerlink\" title=\"4.3.2 故障查看\"></a>4.3.2 故障查看</h3><p> 登陆swarm03 ，查看swarm集群状态。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[root@swarm03 /]# docker node ls</span><br><span class=\"line\">ID                            HOSTNAME            STATUS              AVAILABILITY        MANAGER STATUS      ENGINE VERSION</span><br><span class=\"line\">epoic1y0vv830vnwbc6nnacjc     swarm01             Unknown             Active              Unreachable         18.03.1-ce</span><br><span class=\"line\">nsqqv181e0r7mu77azixtqhzl     swarm02             Ready               Active              Reachable           18.03.1-ce</span><br><span class=\"line\">mkkxaeqergz8xw21bbkd47q1c *   swarm03             Ready               Active              Leader              18.03.1-ce</span><br></pre></td></tr></table></figure>\n<p>在经历短暂的间隙后，可看到swarm集群认定swarm01 现在处于Unknown 状态，MANAGER STATUS 处于Unreachable状态。同时，swarm集群基于raft 算法，重新推举出新的Leader swarm03。</p>\n<p>再过一段时间，swarm01 状态变成Down</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[root@swarm03 ~]# docker node ls</span><br><span class=\"line\">ID                            HOSTNAME            STATUS              AVAILABILITY        MANAGER STATUS      ENGINE VERSION</span><br><span class=\"line\">epoic1y0vv830vnwbc6nnacjc     swarm01             Down                Active              Unreachable         18.03.1-ce</span><br><span class=\"line\">nsqqv181e0r7mu77azixtqhzl     swarm02             Ready               Active              Reachable           18.03.1-ce</span><br><span class=\"line\">mkkxaeqergz8xw21bbkd47q1c *   swarm03             Ready               Active              Leader              18.03.1-ce</span><br></pre></td></tr></table></figure>\n<p>查看服务状态</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[root@swarm03 /]# docker stack ps myservice</span><br><span class=\"line\">ID                  NAME                  IMAGE               NODE                DESIRED STATE       CURRENT STATE                ERROR                         PORTS</span><br><span class=\"line\">zfbvcz5iu8g7        myservice_web.1       httpd:v2            swarm03             Running             Running 2 minutes ago</span><br><span class=\"line\">qk8pvvc9nmv0         \\_ myservice_web.1   httpd:v2            swarm01             Shutdown            Running about an hour ago</span><br><span class=\"line\">unwepav5z9nz         \\_ myservice_web.1   httpd:v1            swarm02             Shutdown            Shutdown about an hour ago</span><br><span class=\"line\">uyj0pm1df9hl        myservice_web.2       httpd:v2            swarm02             Running             Running 3 minutes ago</span><br><span class=\"line\">3j5o1tktcg1l         \\_ myservice_web.2   httpd:v1            swarm01             Shutdown            Shutdown about an hour ago</span><br><span class=\"line\">hx741f0kye5a        myservice_web.3       httpd:v2            swarm03             Running             Running 3 minutes ago</span><br><span class=\"line\">qdjov5zf5alb         \\_ myservice_web.3   httpd:v2            swarm03             Shutdown            Failed 21 minutes ago        &quot;task: non-zero exit (137)&quot;</span><br><span class=\"line\">wl1unt6lynft         \\_ myservice_web.3   httpd:v1            swarm03             Shutdown            Shutdown about an hour ago</span><br></pre></td></tr></table></figure>\n<p>可以看到容器服务个数依然维持在3个，原先在swarm01上面运行的容器，被分配到swarm02和swarm03上面。现在对服务进行扩容操作，以验证此时swarm集群是否对服务还有管理能力。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[root@swarm03 swarm]# docker stack deploy -c docker-swarm.yml myservice</span><br><span class=\"line\">Updating service myservice_web (id: 85quzwi5k0btkn5wlht4jbco9)</span><br><span class=\"line\">image httpd:v2 could not be accessed on a registry to record</span><br><span class=\"line\">its digest. Each node will access httpd:v2 independently,</span><br><span class=\"line\">possibly leading to different nodes running different</span><br><span class=\"line\">versions of the image.</span><br><span class=\"line\"></span><br><span class=\"line\">[root@swarm03 swarm]# docker stack ps  myservice</span><br><span class=\"line\">ID                  NAME                  IMAGE               NODE                DESIRED STATE       CURRENT STATE                ERROR                         PORTS</span><br><span class=\"line\">zfbvcz5iu8g7        myservice_web.1       httpd:v2            swarm03             Running             Running 6 minutes ago</span><br><span class=\"line\">qk8pvvc9nmv0         \\_ myservice_web.1   httpd:v2            swarm01             Shutdown            Running about an hour ago</span><br><span class=\"line\">unwepav5z9nz         \\_ myservice_web.1   httpd:v1            swarm02             Shutdown            Shutdown about an hour ago</span><br><span class=\"line\">uyj0pm1df9hl        myservice_web.2       httpd:v2            swarm02             Running             Running 6 minutes ago</span><br><span class=\"line\">3j5o1tktcg1l         \\_ myservice_web.2   httpd:v1            swarm01             Shutdown            Shutdown about an hour ago</span><br><span class=\"line\">hx741f0kye5a        myservice_web.3       httpd:v2            swarm03             Running             Running 6 minutes ago</span><br><span class=\"line\">qdjov5zf5alb         \\_ myservice_web.3   httpd:v2            swarm03             Shutdown            Failed 25 minutes ago        &quot;task: non-zero exit (137)&quot;</span><br><span class=\"line\">wl1unt6lynft         \\_ myservice_web.3   httpd:v1            swarm03             Shutdown            Shutdown about an hour ago</span><br><span class=\"line\">6nifzsbysug5        myservice_web.4       httpd:v2            swarm02             Running             Running 20 seconds ago</span><br><span class=\"line\">uwgnl2usv59a        myservice_web.5       httpd:v2            swarm02             Running             Running 20 seconds ago</span><br></pre></td></tr></table></figure>\n<p>可以看到，此时swarm集群依然具有管理能力（服务扩容能力）。也从侧面验证swarm manager的高可用能力。</p>\n<p>此时重启swarm01 ，看看 swarm01节点是否会自动加入集群。</p>\n<p>登陆swarm01 ，查看集群状态</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[root@swarm01 ~]# docker node ls</span><br><span class=\"line\">Error response from daemon: Swarm is encrypted and needs to be unlocked before it can be used. Please use &quot;docker swarm unlock&quot; to unlock it.</span><br></pre></td></tr></table></figure>\n<p>可以看到，此时swarm01 处于被锁的状态。需要解锁。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[root@swarm01 ~]# docker swarm unlock</span><br><span class=\"line\">Please enter unlock key:</span><br><span class=\"line\">Error response from daemon: invalid key string</span><br></pre></td></tr></table></figure>\n<p>那么  unlock key 从哪里取值呢？答案就是，需要在可用的集群管理节点，比如swarm03上面执行命令</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[root@swarm03 ~]# docker swarm unlock-key</span><br><span class=\"line\">To unlock a swarm manager after it restarts, run the `docker swarm unlock`</span><br><span class=\"line\">command and provide the following key:</span><br><span class=\"line\"></span><br><span class=\"line\">    SWMKEY-1-796L7+nU55V/qGCMPgBvKS/5PyotvNaKPS4SJ0AUe+g</span><br><span class=\"line\"></span><br><span class=\"line\">Please remember to store this key in a password manager, since without it you</span><br><span class=\"line\">will not be able to restart the manager.</span><br></pre></td></tr></table></figure>\n<p>通过该 unlock key ： SWMKEY-1-796L7+nU55V/qGCMPgBvKS/5PyotvNaKPS4SJ0AUe+g ，重新加入集群。</p>\n<p>此时在swarm01上面查看集群状态，即可看到已经重新加入集群。</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[root@swarm01 ~]<span class=\"comment\"># docker node ls</span></span><br><span class=\"line\">ID                            HOSTNAME            STATUS              AVAILABILITY        MANAGER STATUS      ENGINE VERSION</span><br><span class=\"line\">epoic1y0vv830vnwbc6nnacjc *   swarm01             Ready               Active              Reachable           18.03.1-ce</span><br><span class=\"line\">nsqqv181e0r7mu77azixtqhzl     swarm02             Ready               Active              Reachable           18.03.1-ce</span><br><span class=\"line\">mkkxaeqergz8xw21bbkd47q1c     swarm03             Ready               Active              Leader              18.03.1-ce</span><br></pre></td></tr></table></figure>\n<p>但是如果再把swarm03关掉，即同时宕掉2个节点，则此时3个管理节点，只剩下一个节点，不满足<a href=\"https://raft.github.io/raft.pdf\" target=\"_blank\" rel=\"noopener\">Raft</a> 协议的节点个数要求（&gt;N/2），集群功能就会失效，不能再管理集群，但是集群上的服务还能够继续运行。</p>\n<p>继续我们的脚步，将swarm03,swarm01 都关机。</p>\n<p>登陆swarm02，查看集群状态</p>\n <figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[root@swarm02 ~]# docker node ls</span><br><span class=\"line\">Error response from daemon: rpc error: code = Unknown desc = The swarm does not have a leader. It&apos;s possible that too few managers are online. Make sure more than half of the managers are online.</span><br><span class=\"line\">[root@swarm02 ~]# docker stack ps myservice</span><br><span class=\"line\">Error response from daemon: rpc error: code = Unknown desc = The swarm does not have a leader. It&apos;s possible that too few managers are online. Make sure more than half of the managers are online.</span><br></pre></td></tr></table></figure>\n<p>可以看到，此时集群功能已经失效，不能通过swarm 命令查看集群和管理集群了。那么运行其上的任务呢？</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[root@swarm02 ~]# docker container ls</span><br><span class=\"line\">CONTAINER ID        IMAGE                COMMAND              CREATED             STATUS              PORTS                    NAMES</span><br><span class=\"line\">2cc1b76be2cf        httpd:v2             &quot;httpd-foreground&quot;   2 hours ago         Up 2 hours          80/tcp                   myservice_web.5.uwgnl2usv59aygb5e9a8cig2y</span><br><span class=\"line\">96034bff8902        httpd:v2             &quot;httpd-foreground&quot;   2 hours ago         Up 2 hours          80/tcp                   myservice_web.4.6nifzsbysug5medburznrrr7h</span><br><span class=\"line\">82beb10e4cac        httpd:v2             &quot;httpd-foreground&quot;   3 hours ago         Up 3 hours          80/tcp                   myservice_web.2.uyj0pm1df9hlx9vdvck36ilbl</span><br><span class=\"line\">ea9f879cdf12        composetest_web:v1   &quot;python app.py&quot;      3 weeks ago         Up 5 hours          0.0.0.0:5000-&gt;5000/tcp   elegant_borg</span><br></pre></td></tr></table></figure>\n<p>还好还好，通过 <code>docker container ls</code>  查看swarm02节点上的容器，可以看到仍然有3个容器在运行，这些都是之前已经分配好的。显然，swarm集群即使失效了，也不会影响其上运行的容器服务正常运行，只是失去了管理和调度的能力了。</p>\n<p>此时，如果再次打开swarm01 和 swarm03的电源，他们会重新组建集群吗？从上文中我们知道，离线的manager节点要重新加入集群需要解锁，而钥匙需要通过原集群运行<code>docker swarm unlock-key</code> 获取。那么显然，原集群已经失效了。那此时会怎样呢？</p>\n<p>不幸的是，确实swarm01 和 swarm03 都无法自动加入集群，确实需要通过<code>docker swarm unlock</code>进行解锁。但是幸运的是，<strong>unlock key 是不变的</strong>，跟刚才swarm01宕机时重新加入的key 是一样的。分别在swarm01 和swarm03  执行解锁操作。集群最终恢复正常。因此需要在创建集群后，第一时间保存unlock key 。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[root@swarm03 ~]# docker node ls</span><br><span class=\"line\">ID                            HOSTNAME            STATUS              AVAILABILITY        MANAGER STATUS      ENGINE VERSION</span><br><span class=\"line\">epoic1y0vv830vnwbc6nnacjc     swarm01             Ready               Active              Reachable           18.03.1-ce</span><br><span class=\"line\">nsqqv181e0r7mu77azixtqhzl     swarm02             Ready               Active              Leader              18.03.1-ce</span><br><span class=\"line\">mkkxaeqergz8xw21bbkd47q1c *   swarm03             Ready               Active              Reachable           18.03.1-ce</span><br></pre></td></tr></table></figure>\n<p>注：在创建集群的时候，可设置自动解锁，即主机重启时，不用输入密钥，即可重新加入集群</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">docker swarm init --autolock=false</span><br><span class=\"line\">docker swarm update --autolock=false</span><br></pre></td></tr></table></figure>\n<h3 id=\"4-3-3-结论\"><a href=\"#4-3-3-结论\" class=\"headerlink\" title=\"4.3.3 结论\"></a>4.3.3 结论</h3><p>从以上实验中也可以看出，只要管理节点正常，服务的高可用就能达到，但是管理节点的个数必须保持在总的管理节点个数的一半以上，即3个只允许宕机一台，5台只允许宕机2台。而总的管理节点的个数一般不超过7个，如果太多的话，集群内管理节点通信消耗会比较大。由于偶数性价比不高（因为4台也只能宕机掉1台跟3台时是一样的），所以管理节点的个数一般都是奇数。</p>\n<p>管理节点的个数以及允许宕机的个数如下：</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">mannager node</th>\n<th style=\"text-align:center\">允许宕机个数</th>\n<th style=\"text-align:center\">服务运行状态</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">3</td>\n<td style=\"text-align:center\">1</td>\n<td style=\"text-align:center\">正常</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">5</td>\n<td style=\"text-align:center\">2</td>\n<td style=\"text-align:center\">正常</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">7</td>\n<td style=\"text-align:center\">3</td>\n<td style=\"text-align:center\">正常</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"4-4-网络故障转移\"><a href=\"#4-4-网络故障转移\" class=\"headerlink\" title=\"4.4 网络故障转移\"></a>4.4 网络故障转移</h2><h3 id=\"4-4-1-模拟网络分区\"><a href=\"#4-4-1-模拟网络分区\" class=\"headerlink\" title=\"4.4.1 模拟网络分区\"></a>4.4.1 模拟网络分区</h3><p>环境说明：仍然是3台虚拟机组成的swarm管理集群 </p>\n<p>hd_hd-1服务：运行4个副本，分别swarm01/swarm02 各一个容器，swarm03运行2个容器</p>\n<p>将swarm02网卡拔掉，模拟网络故障分区。</p>\n<p>备注使用 –autolock=false 初始化集群：docker swarm init –autolock=false  –advertise-addr 172.21.111.56 –listen-addr 172.21.111.56:2377  </p>\n<h3 id=\"4-4-2-观察swarm-node信息\"><a href=\"#4-4-2-观察swarm-node信息\" class=\"headerlink\" title=\"4.4.2 观察swarm node信息\"></a>4.4.2 观察swarm node信息</h3><p>查看集群信息和服务状态</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[root@swarm01 httpd]# docker node ls</span><br><span class=\"line\">ID                            HOSTNAME            STATUS              AVAILABILITY        MANAGER STATUS      ENGINE VERSION</span><br><span class=\"line\">wk8jaypu23vr63082arpsl4ut *   swarm01             Ready               Active              Leader              18.03.1-ce</span><br><span class=\"line\">q9lcw91vus73wyly174iy72h3     swarm02             Down                Active              Unreachable         18.03.1-ce</span><br><span class=\"line\">1xelzfyr5kbn7kk70le45anei     swarm03             Ready               Active              Reachable           18.06.1-ce</span><br><span class=\"line\">[root@swarm01 httpd]# docker service ps hd_hd-1</span><br><span class=\"line\">ID                  NAME                IMAGE               NODE                DESIRED STATE       CURRENT STATE           ERROR               PORTS</span><br><span class=\"line\">t604bqtcj588        hd_hd-1.1           httpd:latest        swarm01             Running             Running 2 minutes ago</span><br><span class=\"line\">xfajbfz51nd9         \\_ hd_hd-1.1       httpd:latest        swarm02             Shutdown            Running 8 minutes ago</span><br><span class=\"line\">8clt10jl1xyf        hd_hd-1.2           httpd:latest        swarm01             Running             Running 6 minutes ago</span><br><span class=\"line\">py5cavsog5mo        hd_hd-1.3           httpd:latest        swarm03             Running             Running 7 minutes ago</span><br><span class=\"line\">oigopryy1kro        hd_hd-1.4           httpd:latest        swarm03             Running             Running 7 minutes ago</span><br></pre></td></tr></table></figure>\n<p>我们可以发现，集群认为swarm02现在已经失联（不论是宕机还是网络端开）。现在swarm02上面的容器被迁移到swarm01上面（调度策略应该是swarm01上面目前运行的容器个数比swarm03少）。</p>\n<p>通过虚拟机控制台登陆swarm02，验证集群功能和容器是否在运行</p>\n<p><img src=\"C:\\Users\\jalon\\OneDrive\\01笔记\\Docker\\swarm-network.png\" alt=\"1537414250767\"></p>\n<p>我们可以发现，swarm02由于当前是一个节点，根据raft算法，处于swarm集群功能不可用状态。docker ps查看容器，发现容器仍然在正常运行。得出结论，swarm集群功能失效，但是不影响原先已经运行的docker容器。</p>\n<p>综合swarm01/swarm03上面的容器个数和swarm02上面的容器个数，现在已经有5个容器了。而我们预设的容器个数是4个。那么当swarm02重新加入集群，会发生什么事情呢？</p>\n<h3 id=\"4-4-3-网络故障恢复\"><a href=\"#4-4-3-网络故障恢复\" class=\"headerlink\" title=\"4.4.3 网络故障恢复\"></a>4.4.3 网络故障恢复</h3><p>把swarm02网卡重新接上。swarm02 会自动加入集群。查看集群信息和服务信息。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[root@swarm01 httpd]# docker node ls</span><br><span class=\"line\">ID                            HOSTNAME            STATUS              AVAILABILITY        MANAGER STATUS      ENGINE VERSION</span><br><span class=\"line\">lkivi02vx5z939z0x7lacuarl *   swarm01             Ready               Active              Leader              18.03.1-ce</span><br><span class=\"line\">pdp0awtf34o4x60x812h07a6e     swarm02             Ready               Active              Reachable           18.03.1-ce</span><br><span class=\"line\">qavrh9e6pjdq2aoxpucl6nohm     swarm03             Ready               Active              Reachable           18.06.1-ce</span><br><span class=\"line\">[root@swarm01 httpd]# docker service ps hd_hd-1</span><br><span class=\"line\">ID                  NAME                IMAGE               NODE                DESIRED STATE       CURRENT STATE            ERROR               PORTS</span><br><span class=\"line\">pepxi7bmzi8r        hd_hd-1.1           httpd:latest        swarm01             Running             Running 2 minutes ago</span><br><span class=\"line\">yf42dr6fqvl5         \\_ hd_hd-1.1       httpd:latest        swarm02             Shutdown            Shutdown 2 minutes ago</span><br><span class=\"line\">j6iq7tpsnwb0        hd_hd-1.2           httpd:latest        swarm03             Running             Running 2 minutes ago</span><br><span class=\"line\">k23a3ezbocit        hd_hd-1.3           httpd:latest        swarm03             Running             Running 2 minutes ago</span><br><span class=\"line\">bqra1k9m0ojc        hd_hd-1.4           httpd:latest        swarm01             Running             Running 2 minutes ago</span><br></pre></td></tr></table></figure>\n<p>我们发现swarm02已经正常加入集群。登陆swarm02，查看swarm02上面原先的容器也已经停掉了。符合我们的预期。</p>\n<p><img src=\"C:\\Users\\jalon\\OneDrive\\01笔记\\Docker\\swarm-network-2.png\" alt=\"1537419010126\"></p>\n<h1 id=\"5-附录\"><a href=\"#5-附录\" class=\"headerlink\" title=\"5.附录\"></a>5.附录</h1><h2 id=\"5-1参考文章：\"><a href=\"#5-1参考文章：\" class=\"headerlink\" title=\"5.1参考文章：\"></a>5.1参考文章：</h2><p>docker swarm介绍：</p>\n<p><a href=\"http://shiyanjun.cn/archives/1625.html\" target=\"_blank\" rel=\"noopener\">http://shiyanjun.cn/archives/1625.html</a></p>\n<p><a href=\"https://jiayi.space/post/docker-swarmrong-qi-ji-qun-guan-li-gong-ju\" target=\"_blank\" rel=\"noopener\">https://jiayi.space/post/docker-swarmrong-qi-ji-qun-guan-li-gong-ju</a></p>\n<p><a href=\"http://www.shangyang.me/2018/02/01/docker-swarm-03-architect/\" target=\"_blank\" rel=\"noopener\">http://www.shangyang.me/2018/02/01/docker-swarm-03-architect/</a></p>\n<p><a href=\"http://blog.51cto.com/cloudman/1952873\" target=\"_blank\" rel=\"noopener\">http://blog.51cto.com/cloudman/1952873</a></p>\n<p><a href=\"https://www.cnblogs.com/bigberg/p/8761047.html\" target=\"_blank\" rel=\"noopener\">https://www.cnblogs.com/bigberg/p/8761047.html</a></p>\n<p>服务发现：<a href=\"https://success.docker.com/article/ucp-service-discovery\" target=\"_blank\" rel=\"noopener\">https://success.docker.com/article/ucp-service-discovery</a></p>\n<p>raft协议：<a href=\"http://thesecretlivesofdata.com/raft/\" target=\"_blank\" rel=\"noopener\">http://thesecretlivesofdata.com/raft/</a></p>\n<h2 id=\"5-2-Swarm-Node-节点操作\"><a href=\"#5-2-Swarm-Node-节点操作\" class=\"headerlink\" title=\"5.2 Swarm Node 节点操作\"></a>5.2 Swarm Node 节点操作</h2><h3 id=\"5-2-1-状态变更\"><a href=\"#5-2-1-状态变更\" class=\"headerlink\" title=\"5.2.1 状态变更\"></a>5.2.1 状态变更</h3><p>前面我们已经提到过，Node的AVAILABILITY有三种状态：Active、Pause、Drain，对某个Node进行变更，可以将其AVAILABILITY值通过Docker CLI修改为对应的状态即可，下面是常见的变更操作：</p>\n<ul>\n<li>设置Manager Node只具有管理功能</li>\n<li>对服务进行停机维护，可以修改AVAILABILITY为Drain状态</li>\n<li>暂停一个Node，然后该Node就不再接收新的Task</li>\n<li>恢复一个不可用或者暂停的Node</li>\n</ul>\n<p>例如，将Manager Node的AVAILABILITY值修改为Drain状态，使其只具备管理功能，执行如下命令：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[root@swarm02 ~]# docker node update  --availability drain  swarm02</span><br><span class=\"line\">swarm02</span><br><span class=\"line\">[root@swarm02 ~]# docker node ls</span><br><span class=\"line\">ID                            HOSTNAME            STATUS              AVAILABILITY        MANAGER STATUS      ENGINE VERSION</span><br><span class=\"line\">epoic1y0vv830vnwbc6nnacjc     swarm01             Ready               Active              Leader              18.03.1-ce</span><br><span class=\"line\">nsqqv181e0r7mu77azixtqhzl *   swarm02             Ready               Drain               Reachable           18.03.1-ce</span><br><span class=\"line\">mkkxaeqergz8xw21bbkd47q1c     swarm03             Ready               Active              Reachable           18.03.1-ce</span><br></pre></td></tr></table></figure>\n<p>这样，Manager Node不能被指派Task，也就是不能部署实际的Docker容器来运行服务，而只是作为管理Node的角色。 </p>\n<h3 id=\"5-2-2-添加标签\"><a href=\"#5-2-2-添加标签\" class=\"headerlink\" title=\"5.2.2 添加标签\"></a>5.2.2 添加标签</h3><p>每个Node的主机配置情况可能不同，比如有的适合运行CPU密集型应用，有的适合运行IO密集型应用，Swarm支持给每个Node添加标签元数据，这样可以根据Node的标签，来选择性地调度某个服务部署到期望的一组Node上。 给SWarm集群中的某个Worker Node添加标签，执行如下命令格式如下： </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[root@swarm02 ~]# docker node update  --label-add  app=dev swarm02</span><br><span class=\"line\">swarm02</span><br></pre></td></tr></table></figure>\n<h3 id=\"5-2-3-提权-降权\"><a href=\"#5-2-3-提权-降权\" class=\"headerlink\" title=\"5.2.3 提权/降权\"></a>5.2.3 提权/降权</h3><p>改变Node的角色，Worker Node可以变为Manager Node，这样实际Worker Node有工作Node变成了管理Node，对应操作分别是： </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">docker node promote swarm02</span><br><span class=\"line\">docker node demote swarm02</span><br></pre></td></tr></table></figure>\n<h2 id=\"5-3-网络管理\"><a href=\"#5-3-网络管理\" class=\"headerlink\" title=\"5.3 网络管理\"></a>5.3 网络管理</h2><h3 id=\"5-3-1-添加overlay-网络\"><a href=\"#5-3-1-添加overlay-网络\" class=\"headerlink\" title=\"5.3.1 添加overlay 网络\"></a>5.3.1 添加overlay 网络</h3><p>overlay网络是swarm中默认的跨主机网络模型。在Swarm集群中可以使用Overlay网络来连接到一个或多个服务。具体添加Overlay网络，首先，我们需要创建在Manager Node上创建一个Overlay网络，执行如下命令： </p>\n<p><code>docker network create --driver overlay my-network</code></p>\n<p>创建完Overlay网络my-network以后，Swarm集群中所有的Manager Node都可以访问该网络。然后，我们在创建服务的时候，只需要指定使用的网络为已存在的Overlay网络即可，如下命令所示： </p>\n<p><code>docker service create --replicas 3 --network my-network --name myweb  nginx</code></p>\n<p>这样，如果Swarm集群中其他Node上的Docker容器也使用my-network这个网络，那么处于该Overlay网络中的所有容器之间，通过网络可以连通。  </p>\n<h2 id=\"5-4-API\"><a href=\"#5-4-API\" class=\"headerlink\" title=\"5.4 API\"></a>5.4 API</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ curl --unix-socket /var/run/docker.sock -H &quot;Content-Type: application/json&quot; \\</span><br><span class=\"line\">  -d &apos;&#123;&quot;Image&quot;: &quot;alpine&quot;, &quot;Cmd&quot;: [&quot;echo&quot;, &quot;hello world&quot;]&#125;&apos; \\</span><br><span class=\"line\">  -X POST http:/v1.24/containers/create</span><br><span class=\"line\">&#123;&quot;Id&quot;:&quot;1c6594faf5&quot;,&quot;Warnings&quot;:null&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">$ curl --unix-socket /var/run/docker.sock -X POST http:/v1.24/containers/1c6594faf5/start</span><br><span class=\"line\"></span><br><span class=\"line\">$ curl --unix-socket /var/run/docker.sock -X POST http:/v1.24/containers/1c6594faf5/wait</span><br><span class=\"line\">&#123;&quot;StatusCode&quot;:0&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">$ curl --unix-socket /var/run/docker.sock &quot;http:/v1.24/containers/1c6594faf5/logs?stdout=1&quot;</span><br><span class=\"line\">hello world</span><br></pre></td></tr></table></figure>\n<p><a href=\"https://docs.docker.com/develop/sdk/#sdk-and-api-quickstart\" target=\"_blank\" rel=\"noopener\">https://docs.docker.com/develop/sdk/#sdk-and-api-quickstart</a></p>\n<p><a href=\"https://www.programmableweb.com/api/docker-swarm\" target=\"_blank\" rel=\"noopener\">https://www.programmableweb.com/api/docker-swarm</a></p>\n<h2 id=\"5-5-Docker-三剑客\"><a href=\"#5-5-Docker-三剑客\" class=\"headerlink\" title=\"5.5 Docker 三剑客\"></a>5.5 Docker 三剑客</h2><ul>\n<li>docker-compose 负责组织应用，应用由哪些服务组成，每个服务由多少个容器，均在该配置文件里面配置。</li>\n<li>docker-machine 负责提供虚拟机，该虚拟机具备docker daemon服务能力。创建的虚拟机，可以用来组成docker swarm集群。docker-machine 并不是必须的功能。完全可以手动安装docker 服务。</li>\n<li>docker swarm 负责组织docker集群，组织跨主机的docker集群管理能力。</li>\n</ul>"},{"title":"Docker Swarm服务调度分析","date":"2018-10-20T08:37:53.000Z","_content":"\n\n本文主要介绍docker swarm 对于容器应用的调度管理分析。Docker swarm 1.12.0  是一个非常重要的版本，1.12.0之前的版本 docker swarm 是一个独立的项目，需要额外下载swarm镜像进行安装。1.12.0  之后的版本，swarm功能直接继承到docker engine 当中。直接使用docker CLI 可创建一个swarm 集群、可在swarm 集群中部署应用程序，同时具备管理能力。 \n\n关于swarm的详细介绍，可参考官方文档：https://docs.docker.com/engine/swarm/\n\n# 1.Docker Swarm 集群架构\n\nSwarm 集群中，有两种类型的节点：Manager和Worker。\n\n要部署服务到swarm，需要将服务定义提交给Manager节点。Manager节点将称为work的工作单元分派 给Worker节点。\n\n![](\\img\\swarm-node.png)\n\n<!-- more -->\n\n## 1.1 Manager Node\n\nManager节点处理集群管理任务：\n\n- 维护集群状态\n- 调度服务\n- 为 Swarm 提供外部可调用的 API 接口\n\nManager 节点需要时刻维护和保存当前 Swarm 集群中各个节点的一致性状态，这里主要是指各个 Tasks 的执行的状态和其它节点的状态。因为 Swarm 集群是一个典型的分布式集群，在保证一致性上，Manager 节点采用 [Raft](https://raft.github.io/raft.pdf) 协议来保证分布式场景下的数据一致性。\n\n通常为了保证 Manager 节点的高可用，Docker 建议采用奇数个 Manager 节点，这样的话，你可以在 Manager 失败的时候不用关机维护，Docker 官方给出如下的建议：\n\n- 3 个 Manager 节点最多可以同时容忍 1 个 Manager 节点失效的情况下保证高可用；\n- 5 个 Manager 节点最多可以同时容忍 2 个 Manager 节点失效的情况下保证高可用；\n- N 个 Manager 节点最多可以同时容忍 (N−1)/2个 Manager 节点失效的情况下保证高可用；\n- 最多最多的情况下，使用 7 个 Manager 节点就够了，否则反而会降低集群的性能了。\n\n> **重要说明**：添加更多管理器并不意味着可扩展性更高或性能更高。一般而言，情况正好相反。\n\n| 群体大小 | 多数 | 容错  |\n| -------- | ---- | ----- |\n| 1        | 1    | 0     |\n| 2        | 2    | 0     |\n| **3**    | 2    | **1** |\n| 4        | 3    | 1     |\n| **5**    | 3    | **2** |\n| 6        | 4    | 2     |\n| **7**    | 4    | **3** |\n| 8        | 5    | 3     |\n\n## 1.2 Worker Node \n\nWorker节点接收并执行从Manager节点分派的任务。默认情况下，Manager节点还将服务作为Worker节点运行，但也可以将它们配置为仅运行Manage任务。 可将Manager可用性设置为`Drain` ，这样服务就不会被分配到Manager节点上运行了。\n\nWorker节点不参与Raft分布状态，做出调度决策或提供群模式HTTP API。\n\n通过 docker node promote 命令将一个 Worker 节点提升为 Manager 节点。通常情况下，该命令使用在维护的过程中，需要将 Manager 节点占时下线进行维护操作。同样可以使用 docker node demote 将某个 manager 节点降级为 worker 节点。 \n\n## 1.3 小结  \n\n综上可知， Swarm 集群的管理工作是由manager节点实现。如上图所示，manager节点实现的功能主要包括：node discovery，scheduler,cluster管理等。同时，为了保证Manager 节点的高可用，Manager 节点需要时刻维护和保存当前 Swarm 集群中各个节点的一致性状态。在保证一致性上，Manager 节点采用 [Raft](https://raft.github.io/raft.pdf) 协议来保证分布式场景下的数据一致性；\n\nDocker Swarm内置了Raft一致性算法，可以保证分布式系统的数据保持一致性同步。Etcd, Consul等高可用键值存储系统也是采用了这种算法。这个算法的作用简单点说就是随时保证集群中有一个Leader，由Leader接收数据更新，再同步到其他各个Follower节点。在Swarm中的作用表现为当一个Leader 节点 down掉时，系统会立即选取出另一个Leader节点，由于这个节点同步了之前节点的所有数据，所以可以无缝地管理集群。\n\nRaft的详细解释可以参考[《The Secret Lives of Data--Raft: Understandable Distributed Consensus》](http://thesecretlivesofdata.com/raft/)。\n\n为保证Magnger高可用，一般部署3-7个Manager 节点。而在云桌面的场景下，可能一个集群就只有两台服务器。显然此时是无法满足Manager 3个节点的高可用要求。因此我们面对的挑战就是如何修改swarm Manager Raft相关部分代码，使其在只有2个节点的情况下，也可以保证一个节点宕机的高可用。\n\n# 2.Docker Swarm 网络架构\n\n## 2.1 网络概述\n\n我们知道，Docker 的几种网络方案：none、host、bridge 和 joined 容器，它们解决了单个 Docker Host 内容器通信的问题。那么跨主机容器间通信的方案又有哪些呢？\n\n跨主机网络方案包括：\n\n1. docker 原生的 overlay 和 macvlan。\n2. 第三方方案：常用的包括 flannel、weave 和 calico。\n\nDocker 网络是一个非常活跃的技术领域，不断有新的方案开发出来，那么要问个非常重要的问题了： 如此众多的方案是如何与 docker 集成在一起的？答案是：libnetwork 以及 CNM。\n\nlibnetwork 是 docker 容器网络库，最核心的内容是其定义的 Container Network Model (CNM)，这个模型对容器网络进行了抽象，由以下三类组件组成：\n\n1. Sandbox：Sandbox 是容器的网络栈，包含容器的 interface、路由表和 DNS 设置。 Linux Network Namespace 是 Sandbox 的标准实现。Sandbox 可以包含来自不同 Network 的 Endpoint。\n2. Endpoint：Endpoint 的作用是将 Sandbox 接入 Network。Endpoint 的典型实现是 veth pair，后面我们会举例。一个 Endpoint 只能属于一个网络，也只能属于一个 Sandbox。\n3. Network：Network 包含一组 Endpoint，同一 Network 的 Endpoint 可以直接通信。Network 的实现可以是 Linux Bridge、VLAN 等。\n\n如图所示两个容器，一个容器一个 Sandbox，每个 Sandbox 都有一个 Endpoint 连接到 Network 1，第二个 Sandbox 还有一个 Endpoint 将其接入 Network 2.\n\nlibnetwork CNM 定义了 docker 容器的网络模型，按照该模型开发出的 driver 就能与 docker daemon 协同工作，实现容器网络。docker 原生的 driver 包括 none、bridge、overlay 和 macvlan，第三方 driver 包括 flannel、weave、calico 等。\n\n## 2.2 Overlay网络\n\nDocker Swarm 内置的跨主机容器通信方案是overlay网络，这是一个基于VxLAN 协议的网络实现。VxLAN 可将二层数据封装到 UDP 进行传输，VxLAN 提供与 VLAN 相同的以太网二层服务，但是拥有更强的扩展性和灵活性。 overlay 通过虚拟出一个子网，让处于不同主机的容器能透明地使用这个子网。所以跨主机的容器通信就变成了在同一个子网下的容器通信，看上去就像是同一主机下的bridge网络通信。\n\n![1534939506173](\\img\\overlay-vxlan.png)\n\n根据vxlan的作用知道，它是要在三层网络中虚拟出二层网络，即跨网段建立虚拟子网。简单的理解就是把发送到虚拟子网地址`10.0.0.3`的报文封装为发送到真实IP`192.168.1.3`的报文。这必然会有更大的数据开销，但却简化了集群的网络连接，让分布在不同主机的容器好像都在同一个主机上一样 。\n\n`overlay`网络会创建多个Docker主机之间的分布式网络。该网络位于（覆盖）特定于主机的网络之上，允许连接到它的容器（包括群集服务容器）安全地进行通信。Docker透明地处理每个数据包与正确的Docker守护程序主机和正确的目标容器的路由。\n\n初始化swarm或将Docker主机加入现有swarm时，会在该Docker主机上创建两个新网络：\n\n- ingress overlay 网络，处理与swarm集群服务相关的控制和数据流量。创建群组服务并且不将其连接到用户定义的覆盖网络时，服务将默认连接到ingress overlay网络。集群中只能有一个ingress overlay 网络。\n- docker_gwbridge 桥接网络，它将各个Docker守护程序连接到参与该群集的其他Docker守护进程。同时该docker_gwbridge 网络将为主机上的容器提供访问外网的能力。\n\n服务或容器一次可以连接到多个网络。服务或容器只能通过它们各自连接的网络进行通信。\n\n### 2.2.1 创建overlay网络\n\n> **先决条件**：\n>\n> - 使用overlay 网络的Docker守护程序的防火墙规则\n>\n>   您需要以下端口打开来往于overlay 网络上的每个Docker主机的流量：\n>\n>   - 用于集群管理通信的TCP端口2377\n>   - TCP和UDP端口7946用于节点之间的通信\n>   - UDP端口4789用于覆盖网络流量\n>\n> - 在创建overlay 网络之前，您需要将Docker守护程序初始化为swarm管理器，`docker swarm init`或者使用它将其连接到现有的swarm `docker swarm join`。这些中的任何一个都会创建默认ingress overlay 网络，默认情况下 由群服务使用。即使您从未计划使用群组服务，也需要执行此操作。之后，您可以创建其他用户定义的overlay 网络。\n\n要创建用于swarm服务的覆盖网络，请使用如下命令：\n\n```\n$ docker network create -d overlay my-overlay\n```\n\n要创建可由群集服务或独立容器用于与在其他Docker守护程序上运行的其他独立容器通信的覆盖网络，请添加`--attachable`标志：\n\n```\n$ docker network create -d overlay --attachable my-attachable-overlay\n```\n\n同时可以指定IP地址范围，子网，网关和其他选项。详情`docker network create --help`请见\n\n### 2.2.2 加密overlay网络\n\n默认情况下，使用GCM模式下的AES算法加密所有swarm群集服务管理流量 。集群中的管理器节点每隔12小时轮换用于加密数据的密钥。\n\n要加密应用程序数据，请`--opt encrypted`在创建覆盖网络时添加。这样可以在vxlan级别启用IPSEC加密。此加密会产生不可忽视的性能损失，因此您应该在生产中使用此选项之前对其进行测试。\n\n启用覆盖加密后，Docker会在所有节点之间创建IPSEC隧道，在这些节点上为连接到覆盖网络的服务安排任务。这些隧道还在GCM模式下使用AES算法，管理器节点每12小时自动旋转密钥。\n\n```\ndocker network create --opt encrypted --driver overlay my-encrypted-network\n```\n\n### 2.2.3 暴露服务端口\n\n对于连接到同一个swarm集群的服务来说，它们之间所有的端口都是互相暴露的。对于可在服务外部访问的端口，必须使用 -p ( 或者 --publish) 发布该端口。\n\n| Flag value                                                   | Description                                         |\n| ------------------------------------------------------------ | --------------------------------------------------- |\n| `-p 8080:80` or `-p published=8080,target=80`                | 将服务上的TCP端口80映射到routing mesh上的端口8080。 |\n| `-p 8080:80/udp` or `-p published=8080,target=80,protocol=udp` | 将服务上的UDP端口80映射到routing mesh上的端口8080。 |\n\n\n\n### 2.2.4 实现原理\n\n下面我们讨论下overlay 网络的具体实现：\n\ndocker 会为每个 overlay 网络创建一个独立的 network namespace，其中会有一个 linux bridge br0，endpoint 还是由 veth pair 实现，一端连接到容器中（即 eth0），另一端连接到 namespace 的 br0 上。\n\nbr0 除了连接所有的 endpoint，还会连接一个 vxlan 设备，用于与其他 host 建立 vxlan tunnel。容器之间的数据就是通过这个 tunnel 通信的。逻辑网络拓扑结构如图所示：\n\n![](./overlay.jpg)\n\n\n\n## 2.3 Ingress Routing Mesh\n\n上文暴露服务端口中有讲到routing mesh，现在来详细介绍这一网络功能。\n\n默认情况下，发布端口的swarm服务使用routing mesh来实现。当客户端连接到任何swarm节点上的已发布端口（无论它是否正在运行给定服务）时，客户端请求将被透明地重定向到正在运行该服务的worker。实际上，Docker充当集群服务的负载均衡器。使用routing mesh的服务以*虚拟IP（VIP）模式运行*。即使在每个节点上运行的服务（通过`--global`标志）也使用routing mesh。使用routing mesh时，无法保证哪个Docker节点服务客户端请求。\n\n要在群集中使用 ingress 网络，您需要在启用群集模式之前在群集节点之间打开以下端口：\n\n- 端口`7946`  TCP / UDP用于容器网络发现。\n- 端口`4789`  UDP 用于容器 ingress 网络。\n\n例如，以下命令将nginx容器中的端口80发布到群集中任何节点的端口8080：\n\n```\n$ docker service create \\\n  --name my-web \\\n  --publish published=8080,target=80 \\\n  --replicas 2 \\\n  nginx\n```\n\n当您在任何节点上访问端口8080时，Docker会将您的请求路由到活动容器。在群集节点本身上，端口8080实际上可能不受约束，但routing mesh 知道如何路由流量并防止发生任何端口冲突。\n\nrouting mesh在已发布的端口上侦听分配给该节点的任何IP地址。对于可外部路由的IP地址，该端口可从主机外部获得。对于所有其他IP地址，只能从主机内访问。\n\n![](\\img\\ingress-routing-mesh.png)\n\n如果想要绕过routing mesh，可以使用*DNS循环（DNSRR）模式*启动服务，方法是将`--endpoint-mode`标志设置为`dnsrr`。这样就必须在服务前运行自己的负载均衡器。Docker主机上的服务名称的DNS查询返回运行该服务的节点的IP地址列表。配置负载均衡器以使用此列表并平衡节点之间的流量。\n\n对于我们云桌面应用场景来说，直接利用routing mesh 功能即可，不需要额外搭建负载均衡器。\n\n## 2.4 服务发现\n\nDocker Swarm 原生就提供了服务发现的功能。要使用服务发现，需要相互通信的 services 必须属于同一个 overlay 网络，所以我们先得创建一个新的 overlay 网络。 直接使用 `ingress` 行不行？很遗憾，目前 `ingress` 没有提供服务发现，必须创建自己的 overlay 网络。\n\nDocker Swarm mode下会为每个节点的Docker engine内置一个DNS server，各个节点间的DNS server通过control plane的gossip协议互相交互信息。此处DNS server用于容器间的服务发现。swarm mode会为每个 --net=自定义网络  的service分配一个DNS entry。目前必须是自定义网络，比如overaly。而bridge和routing mesh 的service，是不会分配DNS的。\n\n那么，下面就来详细介绍服务发现的原理。\n\n每个Docker容器都有一个DNS解析器，它将DNS查询转发到docker engine，该引擎充当DNS服务器。docker 引擎收到请求后就会在发出请求的容器所在的所有网络中，检查域名对应的是不是一个容器或者是服务，如果是，docker引擎就会从存储的key-value建值对中查找这个容器名、任务名、或者服务名对应的IP地址，并把这个IP地址或者是服务的虚拟IP地址返回给发起请求的域名解析器。  \n\n由上可知，docker的服务发现的作用范围是网络级别，也就意味着只有在同一个网络上的容器或任务才能利用内嵌的DNS服务来相互发现，不在同一个网络里面的服务是不能解析名称的，另外，为了安全和性能只有当一个节点上有容器或任务在某个网络里面时，这个节点才会存储那个网络里面的DNS记录。\n\n如果目的容器或服务和源容器不在同一个网络里面，Docker引擎会把这个DNS查询转发到配置的默认DNS服务  。\n\n![](\\img\\service-dns.png)\n\n在上面的例子中，总共有两个服务myservice和client，其中myservice有两个容器，这两个服务在同一个网里面。在client里针对docker.com和myservice各执行了一个curl操作，下面时执行的流程： \n\n- 为了client解析docker.com和myservice，DNS查询进行初始化\n- 容器内建的解析器在127.0.0.11:53拦截到这个DNS查询请求，并把请求转发到docker引擎的DNS服务\n- myservice 被解析成服务对应的虚拟IP（10.0.0.3）。在接下来的内部负载均衡阶段再被解析成一个具体任务容器的IP地址。如果是容器名称这一步直接解析成容器对应的IP地址（10.0.0.4或者10.0.0.5）。\n- docker.com 在docker引擎或者mynet网络上不能被解析成服务，所以这个请求被转发到外部DNS，例如配置好的默认DNS服务器（8.8.8.8）上。\n\nDocker1.12+的服务发现和负载均衡是结合到一起的， 实现办法有两种，DNS轮询和IPVS。 配置参数 --endpoint-mode  (vip or dnsrr) ，默认使用VIP方式。DNS轮询有一些缺点，比如一些应用可能缓存DNS请求， 导致容器变化之后DNS不能实时更新；DNS生效时间也导致不能实时反映服务变化情况。VIP和IPVS原理上比较容易理解， 就是Docker为每个服务分配了一个VIP，DNS解析服务名称或者自定义的别名到这个VIP上。由于VIP本身没有容器提供服务，Docker把到VIP的请求通过IPVS技术负载到后面的容器上。\n\n因此正常情况下，使用默认的VIP方式即可。\n\n## 3.5 负载均衡\n\n负载均衡分为两种：\n\nSwarm集群内的service之间的相互访问需要做负载均衡，称为内部负载均衡（Internal LB）；\n\n从Swarm集群外部访问服务的公开端口，也需要做负载均衡，称外部负载均衡(Exteral LB or Ingress LB)。\n\n### 3.5.1 Internal LB\n\n内部负载均衡就是我们在上一段提到的服务发现，集群内部通过DNS访问service时，Swarm默认通过VIP（virtual IP）、iptables、IPVS转发到某个容器。 \n\n![](\\img\\intelnal-lb.jpg)\n\n当在docker swarm集群模式下创建一个服务时，会自动在服务所属的网络上给服务额外的分配一个虚拟IP，当解析服务名字时就会返回这个虚拟IP。对虚拟IP的请求会通过overlay网络自动的负载到这个服务所有的健康任务上。这个方式也避免了客户端的负载均衡，因为只有单独的一个IP会返回到客户端，docker会处理虚拟IP到具体任务的路由，并把请求平均的分配给所有的健康任务。  \n\n![](.\\internal-lb-2.png)\n\n```\n# 创建overlay网络：mynet \n$ docker network create -d overlay mynet  \na59umzkdj2r0ua7x8jxd84dhr \n# 利用mynet网络创建myservice服务，并复制两份  \n$ docker service create --network mynet --name myservice --replicas 2 busybox ping localhost  \n78t5r8cr0f0h6k2c3k7ih4l6f5\n# 通过下面的命令查看myservice对应的虚拟IP \n$ docker service inspect myservice  \n...\n\"VirtualIPs\": [ \n\t{  \n     \"NetworkID\": \"a59umzkdj2r0ua7x8jxd84dhr\",  \n     \t\t\t\"Addr\": \"10.0.0.3/24\"  \n      },  \n]  \t\n```\n\n注：swarm中服务还有另外一种负载均衡技术可选DNS round robin (DNS RR) （在创建服务时通过--endpoint-mode配置项指定），在DNSRR模式下，docker不再为服务创建VIP，docker DNS服务直接利用轮询的策略把服务名称直接解析成一个容器的IP地址。 \n\n### 3.5.2 Exteral LB\n\nExteral LB(Ingress LB 或者 Swarm Mode Routing Mesh) 看名字就知道，这个负载均衡方式和前面提到的Ingress网络有关。Swarm网络要提供对外访问的服务就需要打开公开端口，并映射到宿主机。Exteral LB就是外部通过公开端口访问集群时做的负载均衡。\n\n当创建或更新一个服务时，你可以利用--publish选项把一个服务暴露到外部，在docker swarm模式下发布一个端口意味着在集群中的所有节点都会监听这个端口，这时当访问一个监听了端口但是并没有对应服务运行在其上的节点会发生什么呢？接下来就该我们的路由网（routing mesh）出场了，路由网时docker1.12引入的一个新特性，它结合了IPVS和iptables创建了一个强大的集群范围的L4层负载均衡，它使所有节点接收服务暴露端口的请求成为可能。当任意节点接收到针对某个服务暴露的TCP/UDP端口的请求时，这个节点会利用预先定义过的Ingress overlay网络，把请求转发给服务对应的虚拟IP。ingress网络和其他的overlay网络一样，只是它的目的是为了转换来自客户端到集群的请求，它也是利用我们前一小节介绍过的基于VIP的负载均衡技术。 \n\n启动服务后，你可以为应用程序创建外部DNS记录，并将其映射到任何或所有Docker swarm节点。你无需担心你的容器具体运行在那个节点上，因为有了路由网这个特性后，你的集群看起来就像是单独的一个节点一样。  \n\n```\n#在集群中创建一个复制两份的服务，并暴露在8000端口  \n$ docker service create --name app --replicas 2 --network appnet --publish 8000:80 nginx  \n```\n\n![](\\img\\external-routing-mesh.png)\n\n上面这个图表明了路由网是怎么工作的： \n\n- 服务（app）拥有两份复制，并把端口映射到外部端口的8000\n- 路由网在集群中的所有节点上都暴露出8000\n- 外部对服务app的请求可以是任意节点，在本例子中外部的负载均衡器将请求转发到了没有app服务的主机上\n- docker swarm的IPVS利用ingress overlay网路将请求重新转发到运行着app服务的节点的容器中\n\n注：以上服务发现和负载均衡参考文档 https://success.docker.com/article/ucp-service-discovery\n\n\n\n# 3.Docker Swarm 存储架构\n\n从业务数据的角度看，容器可以分为两类：无状态（stateless）容器和有状态（stateful）容器。\n\n无状态是指容器在运行过程中不需要保存数据，每次访问的结果不依赖上一次访问，比如提供静态页面的 web 服务器。有状态是指容器需要保存数据，而且数据会发生变化，访问的结果依赖之前请求的处理结果，最典型的就是数据库服务器。\n\n简单来讲，状态（state）就是数据，如果容器需要处理并存储数据，它就是有状态的，反之则无状态。\n\n对于有状态的容器，如何保存数据呢？\n\n## 3.1 Storage Driver\n\nDocker在启动容器的时候，需要创建文件系统，为rootfs提供挂载点。最底层的引导文件系统bootfs主要包含 bootloader和kernel，bootloader主要是引导加载kernel，当kernel被加载到内存中后 bootfs就被umount了。 rootfs包含的就是典型 Linux 系统中的/dev，/proc，/bin，/etc等标准目录和文件。\n\nDocker 模型的核心部分是有效利用分层镜像机制，镜像可以通过分层来进行继承，基于基础镜像（没有父镜像），可以制作各种具体的应用镜像。Docker1.10引入新的可寻址存储模型，使用安全内容哈希代替随机的UUID管理镜像。同时，Docker提供了迁移工具，将已经存在的镜像迁移到新模型上。不同 Docker 容器就可以共享一些基础的文件系统层，同时再加上自己独有的可读写层，大大提高了存储的效率。其中主要的机制就是分层模型和将不同目录挂载到同一个虚拟文件系统。\n\nDocker存储方式提供管理分层镜像和容器的可读写层的具体实现。最初Docker仅能在支持AUFS文件系统的ubuntu发行版上运行，但是由于AUFS未能加入Linux内核，为了寻求兼容性、扩展性，Docker在内部通过graphdriver机制这种可扩展的方式来实现对不同文件系统的支持。\n\n![](.\\storage-driver.png)\n\n如上图所示，容器由最上面一个可写的容器层，以及若干只读的镜像层组成，容器的数据就存放在这些层中。这样的分层结构最大的特性是 Copy-on-Write：\n\n1. 新数据会直接存放在最上面的容器层。\n2. 修改现有数据会先从镜像层将数据复制到容器层，修改后的数据直接保存在容器层中，镜像层保持不变。\n3. 如果多个层中有命名相同的文件，用户只能看到最上面那层中的文件。\n\n分层结构使镜像和容器的创建、共享以及分发变得非常高效，而这些都要归功于 Docker storage driver。正是 storage driver 实现了多层数据的堆叠并为用户提供一个单一的合并之后的统一视图。\n\nDocker 支持多种 storage driver，有 AUFS、Device Mapper、Btrfs、OverlayFS、VFS 和 ZFS。它们都能实现分层的架构，同时又有各自的特性。关于这些方案的对比可以查阅相关文档（http://dockone.io/article/1513）。对于 Docker 用户来说，具体选择使用哪个 storage driver 是一个难题，不过 Docker 官方给出了一个简单的答案： **优先使用 Linux 发行版默认的 storage driver**。 使用`docker info `查看系统默认的storage driver。\n\n使用的storage driver是与主机上的Backing Filesystem有关的。storage driver 是专门用来存放docker容器和镜像的，Backing Filesystem指的是主机的文件系统。默认的centos7 driver 是overlay2，对应的Backing Filesystem 是ext4。这个应该是没有问题的。storage driver 和Backing Filesystem 对应关系见下表。 \n\n![1534987414895](.\\storage_backfing.png)\n\n对于某些容器，直接将数据放在由 storage driver 维护的层中是很好的选择，比如那些无状态的应用。无状态意味着容器没有需要持久化的数据，随时可以从镜像直接创建。\n\n比如 busybox，它是一个工具箱，我们启动 busybox 是为了执行诸如 wget，ping 之类的命令，不需要保存数据供以后使用，使用完直接退出，容器删除时存放在容器层中的工作数据也一起被删除，这没问题，下次再启动新容器即可。\n\n但对于另一类应用这种方式就不合适了，它们有持久化数据的需求，容器启动时需要加载已有的数据，容器销毁时希望保留产生的新数据，也就是说，这类容器是有状态的。\n\n## 3.2 数据持久化方案\n\n那么对于有状态的容器，如何保存数据呢？\n\n选项一：打包在容器里。\n\n显然不行。除非数据不会发生变化，否则，如何在多个副本直接保持同步呢？\n\n选项二：数据放在 Docker 主机的本地目录中，通过 volume 映射到容器里。\n\n位于同一个主机的副本倒是能够共享这个 volume，但不同主机中的副本如何同步呢？\n\n选项三：利用 Docker 的 volume driver，由外部 storage provider 管理和提供 volume，所有 Docker 主机 volume 将挂载到各个副本。\n\n这是目前最佳的方案。volume 不依赖 Docker 主机和容器，生命周期由 storage provider 管理，volume 的高可用和数据有效性也全权由 provider 负责，Docker 只管使用。\n\n我们知道Docker 单机的存储方案里面，通过 data volume 可以存储容器的状态。不过其本质是 Docker 主机本地的目录。 本地目录就存在一个隐患：如果 Docker Host 宕机了，如何恢复容器？ 一个办法就是定期备份数据，但这种方案还是会丢失从上次备份到宕机这段时间的数据。更好的方案是由专门的 storage provider 提供 volume，Docker 从 provider 那里获取 volume 并挂载到容器。这样即使 Host 挂了，也可以立刻在其他可用 Host 上启动相同镜像的容器，挂载之前使用的 volume，这样就不会有数据丢失。\n\n还有一个容易混淆的存储方案，是通过在Docker主机上挂载共享文件系统（例如Ceph，GlusterFS，NFS）。如下图所示：\n\n![](.\\Chart_Multi-Host-Persistence-Shared-Among-Containers.png)\n\n在运行Docker容器的每个主机上配置分布式文件系统。通过创建一致的命名约定和统一命名空间，所有正在运行的容器都可以访问底层的共享存储后端。共享文件系统存储方案利用分布式文件系统与显式存储技术相结合。由于挂载点在所有节点上都可用，因此可以利用它在容器之间创建共享挂载点。这种方案其实是属于选项二的一种变种。\n\n尽管该方案对于特定用例而言，是Docker 存储方案的一个有价值的补充，但它具有限制容器到特定主机的可移植性的显着缺点。它也没有利用针对数据密集型工作负载优化的专用存储后端。为了解决这些限制，volume plugins 被添加到Docker中，将容器的功能扩展到各种存储后端，并且而无需强制更改应用程序设计或部署体系结构。 这就是我们下节将详细介绍的 Volume Driver。\n\n##  3.3 Volume Driver\n\n假设有两个 Dokcer 主机，Host1 运行了一个 MySQL 容器，为了保护数据，data volume 由 storage provider 提供，如下图所示。 \n\n![](.\\storage-provider-1.png)\n\n当 Host1 发生故障，我们会在 Host2 上启动相同的 MySQL 镜像，并挂载 data volume。 \n\n![](.\\storage-provider-2.png)\n\nDocker 是如何实现这个跨主机管理 data volume 方案的呢？\n\n**答案是 volume driver。**\n\n利用 Docker 的 volume driver，由外部 storage provider 管理和提供 volume，所有 Docker 主机 volume 将挂载到各个副本。任何一个 data volume 都是由 driver 管理的，创建 volume 时如果不特别指定，将使用 `local` 类型的 driver，即从 Docker Host 的本地目录中分配存储空间。如果要支持跨主机的 volume，则需要使用第三方 driver。\n\n目前已经有很多可用的 driver，比如使用 Azure File Storage 的 driver，使用 GlusterFS 的 driver，完整的列表可参考 https://docs.docker.com/engine/extend/legacy_plugins/#volume-plugins \n\n![](.\\Docker-Volume-Plugin-Architecture.png)\n\nVolume drivers 让我们将应用程序与底层存储系统隔离。例如，如果原先服务使用一个 NFS驱动器，现在可以在不改变应用程序的逻辑下，直接更新服务使用不同的驱动程序，例如将数据存储在云端,。 \n\n通过`docker volume create ` 命令可以直接创建volume，同时在参数中指定driver 类型。\t下面的例子使用vieux/sshfs  这种类型的Volume drivers。首先创建一个单独的 volume，然后创建容器引用该volume。\n\n### 3.3.1 安装driver插件 \n\n这个示例假设有两个节点，两个节点直接可以通过ssh互相连接。在Docker 主机上，安装`vieux/sshfs` 插件。\n\n```docker plugin install --grant-all-permissions vieux/sshfs\ndocker plugin install --grant-all-permissions vieux/sshfs\n```\n\n### 3.3.2 创建volume\n\n本例中指定一个SSH密码，但如果两个主机配置了共享密钥，可以省略的密码。每个volume driver 可以有零个或多个可配置选项，每一个都指定使用 -o 标志。 \n\n```\ndocker volume create --driver vieux/sshfs \\\n  -o sshcmd=test@node2:/home/test \\\n  -o password=testpassword \\\n  sshvolume\n```\n\n### 3.3.3 使用volume\n\n本例中指定一个SSH密码,但如果两个主机配置了共享密钥,您可以省略的密码。每个卷司机可以有零个或多个可配置的选项。如果 **volume driver**  需要传递参数选项,你必须使用 **--mount**  挂载卷,而不是使用 -v。 \n\n```\n$ docker run -d \\\n  --name sshfs-container \\\n  --volume-driver vieux/sshfs \\\n  --mount src=sshvolume,target=/app,volume-opt=sshcmd=test@node2:/home/test,volume-opt=password=testpassword \\\n  nginx:latest\n```\n\n关于volume driver更多的介绍，可以查看官方文档：https://docs.docker.com/storage/volumes/#use-a-volume-driver\n\nhttps://docs.docker.com/storage/volumes/\n\n# 4.Docker Swarm 服务架构\n\n我们知道，Docker 场景下，应用是以容器的形式运行并提供服务的。而在Swarm场景下，任务是一个个具体的容器，通过抽象出服务的概念，来管理具有一定关联关系的任务容器。 \n\n为了将某一个服务封装为 Service 在Swarm 中部署执行，我们需要通过指定容器 image 以及需要在容器中执行的 Commands 来创建你的 Service，除此之外，还需要配置如下选项，\n\n- 指定可以在 Swarm 之外可以被访问的服务端口号 port\n- 指定加入某个 Overlay 网络以便 Service 与 Service 之间可以建立连接并通讯，\n- 指定该 Service 所要使用的 CPU 和 内存的大小，\n- 指定一个滚动更新的策略 (Rolling Update Policy)\n- 指定多少个 Task 的副本 (replicas) 在 Swarm 集群中同时存在\n\n如下所示，`docker-swarm.yml`文件是一个YAML文件，它定义了如何Docker容器在生产中应表现。\n\n```\nversion: \"3\"\nservices:\n  web:\n    # replace username/repo:tag with your name and image details\n    image: username/repo:tag\n    deploy:\n      replicas: 5\n      resources:\n        limits:\n          cpus: \"0.1\"\n          memory: 50M\n      restart_policy:\n        condition: on-failure\n    ports:\n      - \"4000:80\"\n    networks:\n      - webnet\nnetworks:\n  webnet:\n```\n\n该`docker-swarm.yml`文件告诉Docker执行以下操作：\n\n- 拉username/repo 仓库中拉取镜像。\n- 将该映像的5个实例作为一个被调用的服务运行`web`，限制每个实例使用，最多10％的CPU（跨所有内核）和50MB的RAM。\n- 如果一个失败，立即重启容器。\n- 将主机上的端口4000映射到`web`端口80。\n- 指示`web`容器通过称为负载平衡的网络共享端口80 `webnet`。（在内部，容器本身`web`在短暂的端口发布到 80端口。）\n- `webnet`使用默认设置（负载平衡的覆盖网络）定义网络。\n\n## 4.1 服务和任务\n\n我们一般使用如下命令来部署一个服务到swarm中：\n\n`docker stack deploy -c docker-swarm.yml myservice。`\n\ndocker-swarm.yml中定义了服务所需要达到的状态。我们不需要告诉swarm如何做，我们只需要告诉swarm 我们想要的服务状态是什么，swarm最终会帮我们达到这个状态。\n\nswarm管理器将服务定义为服务的所需状态。swarm将群中节点上的服务调度为一个或多个副本任务。任务在群集中的节点上彼此独立地运行。\n\n例如，想要在HTTP侦听器的三个实例之间进行负载平衡。下图显示了具有三个副本的HTTP侦听器服务。监听器的三个实例中的每一个都是集群中的任务。容器是一个孤立的过程。在swarm模式模型中，每个任务只调用一个容器。任务类似于调度程序放置容器的“槽”。容器生效后，调度程序会识别该任务处于运行状态。如果容器未通过运行状况检查或终止，则任务将终止。\n\n![](./services-diagram.png)\n\nServices，Tasks 和 Containers 之间的关系可以用上面这张图来描述。来看这张图的逻辑，表示用户想要通过 Manager 节点部署一个有 3 个副本的 Nginx 的 Service，Manager 节点接收到用户的 Service definition 后，便开始对该 Service 进行调度，将会在当前可用的Worker（或者Manager ）节点中启动相应的 Tasks 以及相关的副本；所以可以看到，Service 实际上是 Task 的定义，而 Task 则是执行在节点上的程序。\n\nTask 是什么呢？其实就是一个 Container，只是，在 Swarm 中，每个 Task 都有自己的名字和编号，如图，比如 nginx.1、nginx.2 和 nginx.3，这些 Container 各自运行在各自的 node 上，当然，一个 node 上可以运行多个 Container；\n\n## 4.2 任务调度\n\n任务是swarm集群内调度的原子单位。在创建或更新服务时，声明定义服务状态，协调器通过调度任务来实现所需的状态。例如上例的服务，该服务指示协调器始终保持三个HTTP侦听器实例的运行。协调器通过创建三个任务来响应。每个任务都是调度程序通过生成容器来填充的插槽。容器是任务的实例化。如果HTTP侦听器任务随后未通过其运行状况检查或崩溃，则协调器会创建一个新的副本任务，该任务会生成一个新容器。\n\n任务是单向机制。它通过一系列状态单调进行：已分配，准备，运行等。如果任务失败，则协调器将删除任务及其容器，然后根据服务指定的所需状态创建新任务以替换它。\n\nDocker swarm mode 模式的底层技术实际上就是指的是调度器( scheduler )和编排器( orchestrator )；下面这张图展示了 Swarm mode 如何从一个 Service 的创建请求并且成功将该 Service 分发到 worker 节点上执行的过程。\n\n![](.\\swarm-service-lifecycle.png)\n\n\n\n- 首先，看上半部分Swarm manager \n  1. 用户通过 Docker Engine Client 使用命令 docker service create 提交 Service definition，\n  2. 根据 Service definition 创建相应的 Task，\n  3. 为 Task 分配 IP 地址，\n     注意，这是分配运行在 Swarm 集群中 Container 的 IP 地址，该 IP 地址最佳的分配地点是在这里，因为 Manager 节点上保存得有最新最全的 Tasks 的状态信息，为了保证不与其他的 Task 分配到相同的 IP，所以在这里就将 IP 地址给初始化好；\n  4. 将 Task 分发到 Node 上，可以是 Manager 节点也可以使 Worker 节点，\n  5. 对 Worker 节点进行相应的初始化使得它可以执行 Task\n- 接着，看下半部分Swarm  Work\n  1. 首先连接 manager 的分配器( scheduler)检查该 task\n  2. 验证通过以后，便开始通过 Worker 节点上的执行器( executor )执行；\n\n注意，上述 task 的执行过程是一种单向机制，比如它会按顺序的依次经历 assigned, prepared 和 running 等执行状态，不过在某些特殊情况下，在执行过程中，某个 task 失败了( fails )，编排器( orchestrator )直接将该 task 以及它的 container 给删除掉，然后在其它节点上另外创建并执行该 task；\n\n## 4.3 任务状态\n\n通过运行`docker service ps <service-name>`以获取任务的状态。该 `CURRENT STATE`字段显示任务的状态以及任务的持续时间 。\n\n```\n[root@swarm01 ~]# docker service ps myweb_web\nID                  NAME                IMAGE               NODE                DESIRED STATE       CURRENT STATE            ERROR                         PORTS\n52lolsfykj4t        myweb_web.1         httpd:v2            swarm03             Running             Running 16 minutes ago\nzwpufyfacagy         \\_ myweb_web.1     httpd:v2            swarm03             Shutdown            Failed 16 minutes ago    \"task: non-zero exit (255)\"\n00fhp9k1byf6        myweb_web.2         httpd:v2            swarm02             Running             Running 16 minutes ago\nkgtiyw39xpcc         \\_ myweb_web.2     httpd:v2            swarm02             Shutdown            Failed 17 minutes ago    \"task: non-zero exit (255)\"\nvefk4yn0b00m        myweb_web.3         httpd:v2            swarm01             Running             Running 16 minutes ago\npnj4z0j281ng         \\_ myweb_web.3     httpd:v2            swarm01             Shutdown            Failed 17 minutes ago    \"task: non-zero exit (255)\"\n```\n\n任务具有如下几种状态：\n\n| 任务状态    | 描述                                                         |\n| ----------- | ------------------------------------------------------------ |\n| `NEW`       | 任务已初始化。                                               |\n| `PENDING`   | 分配了任务的资源。                                           |\n| `ASSIGNED`  | Docker将任务分配给节点。                                     |\n| `ACCEPTED`  | 该任务被工作节点接受。如果工作节点拒绝该任务，则状态将更改为`REJECTED`。 |\n| `PREPARING` | Docker正在准备任务。                                         |\n| `STARTING`  | Docker正在开始这项任务。                                     |\n| `RUNNING`   | 任务正在执行。                                               |\n| `COMPLETE`  | 任务退出时没有错误代码。                                     |\n| `FAILED`    | 任务退出并显示错误代码。                                     |\n| `SHUTDOWN`  | Docker请求关闭任务。                                         |\n| `REJECTED`  | 工作节点拒绝了该任务。                                       |\n| `ORPHANED`  | 该节点停机时间过长。                                         |\n| `REMOVE`    | 该任务不是终端，但关联的服务已被删除或缩小。                 |\n\n如果在某个 service 的[调度过程](http://www.shangyang.me/2018/02/01/docker-swarm-03-architect/#%E8%B0%83%E5%BA%A6%E8%BF%87%E7%A8%8B)中，发现当前没有可用的 node 资源可以执行该 service，这个时候，该 service 的状态将会保持为 pending 的状态；下面，我们来看一些例子可能使得 service 维持在 pending 状态，\n\n- 如果当所有的节点都被停止或者进入了 drained 状态，这个时候，你试图创建一个 service，该 service 将会一直保持 pending 状态直到当前某个节点可用为止；不过要注意的是，第一个恢复的 node 将会得到所有的 task 调度请求，接收并执行，因此，这种情况在 production 环境上要尽量避免；\n- 你可以为你的 service 设置执行所需的内存大小，在调度的过程当中，发现没有一个 node 能够满足你的内存请求，那么你当前的 service 将会一直处于 pending 状态直到有满足需求的 node 出现；\n- 可以对服务施加放置约束，并且可能无法在给定时间遵守约束。 \n\n上面的例子都表明一个事实，就是你期望的执行条件与 Swarm 现有的可用资源的情况不匹配，不吻合，因此，作为 Swarm 的管理者，应该考虑对 Swarm 集群整体进行扩容；\n\n## 4.4 服务副本与全局服务\n\nDocker Swarm 有两种类型的服务部署模式：复制和全局。\n\nSwarm通过`--mode`选项设置服务类型，提供了两种模式：一种是replicated，我们可以指定服务Task的个数（也就是需要创建几个冗余副本），这也是Swarm默认使用的服务类型；另一种是global，这样会在Swarm集群的每个Node上都创建一个服务。如下图所示（出自Docker官网），是一个包含replicated和global模式的Swarm集群： \n\n![](.\\docker-swarm-replicated-vs-global.png)\n\n\n\n上图中，黄色表示的replicated模式下的Service Replicas，灰色表示global模式下Service的分布。 \n\n在Swarm mode下使用Docker，可以实现部署运行服务、服务扩容缩容、删除服务、滚动更新等功能。\n\n## 4.5 调度策略\n\nSwarm提供了几种不同的方法来控制服务任务可以在哪些节点上运行。\n\n- 可以指定服务是需要运行特定数量的副本还是应该在每个工作节点上全局运行。请参阅2.4服务副本与全局服务。\n\n- 您可以配置服务的 CPU或内存要求，该服务仅在满足这些要求的节点上运行。\n\n- 通过设置约束标签 --constraint ，您可以将服务配置为仅在具有特定（任意）元数据集的节点上运行，并且如果不存在适当的节点，则会导致部署失败。例如，您可以指定您的服务只应在任意标签`pci_compliant`设置为的节点上运行 `true`。\n\n- 通过设置首选项 --preferences，您可以将具有一系列值的任意标签应用于每个节点，并使用算法将服务的任务分布到这些节点上。目前，唯一支持的算法是`spread`，该算法将尝试均匀放置它们。例如，如果`rack`使用值为1-10 的标签标记每个节点，然后指定键入的放置首选项`rack`，服务任务将尽可能均匀地放置在具有标签的所有节点上。如果具有`rack`标签的节点个数，大于任务个数，那么则会根据rack 值的大小排序，然后优先选择排名靠前的节点。\n\n  与约束不同，放置首选项是尽力而为，如果没有节点可以满足首选项，则服务不会失败。如果为服务指定放置首选项，则当集群管理器决定哪些节点应运行服务任务时，与该首选项匹配的节点的排名会更高。其他因素，例如服务的高可用性，也是计划节点运行服务任务的因素。例如，如果您有N个节点带有机架标签（然后是其他一些节点），并且您的服务配置为运行N + 1个副本，那么+1将在一个尚未拥有该服务的节点上进行调度，如果无论该节点是否具有`rack`标签。\n\n\n\n## 4.6 服务管理\n\ndocker swarm 还可以使用 stack 这一模型，用来部署管理和关联不同的服务。stack 功能比service 还要强大，因为它具备多服务编排部署的能力。**stack file** 是一种 yaml 格式的文件，类似于 docker-compose.yml 文件，它定义了一个或多个服务，并定义了服务的环境变量、部署标签、容器数量以及相关的环境特定配置等。 \n\n以下实验仅仅创建利用stack 创建单一服务。\n\n### 4.6.1 创建服务\n\n服务配置文件docker-swarm.yml ，内容如下\n\n```\nversion: '3'\nservices:\n  web:\n    image: \"httpd:v1\"\n    deploy:\n      replicas: 3\n      resources:\n        limits:\n          cpus: \"0.5\"\n          memory: 256M\n      restart_policy:\n        condition: on-failure\n    ports:\n     - \"7080:80\"\n    volumes:\n     - /var/www/html:/usr/local/apache2/htdocs\n```\n\nimage: \"httpd\" ：httpd是docker hub 下载的镜像。提供简单的httpd服务，此处用来做服务都高可用验证。\n\nreplicas: 3 表示该服务副本有3个，可对服务副本进行扩缩容，后续会讲到。\n\n/var/www/html:/usr/local/apache2/htdocs ：表示httpd 首页的内容。各个swarm节点上展示的内容分别是其主机名信息。\n\n在docker-swarm.yml 同级目录下，执行命令\n\n```\n[root@swarm01 swarm]# docker stack deploy -c docker-swarm.yml myservice\nCreating network myservice_default\nCreating service myservice_web\n```\n\n### 4.6.2 查看服务\n\n查看服务创建是否成功。\n\n```\n[root@swarm01 swarm]# docker stack ls\nNAME                SERVICES\nmyservice           1\n[root@swarm01 swarm]# docker stack services myservice\nID                  NAME                MODE                REPLICAS            IMAGE               PORTS\nkom8x49f7qv2        myservice_web       replicated          3/3                 httpd:latest        *:7080->80/tcp\n[root@swarm01 swarm]# docker stack ps myservice\nID                  NAME                IMAGE               NODE                DESIRED STATE       CURRENT STATE            ERROR               PORTS\nsybn1dur1wmw        myservice_web.1     httpd:latest        swarm03             Running             Running 45 seconds ago\nmlcb5r9vzv6z        myservice_web.2     httpd:latest        swarm03             Running             Running 45 seconds ago\nfw21lcu3zp83        myservice_web.3     httpd:latest        swarm01             Running             Running 45 seconds ago\n```\n\n可以看到 swarm01 上面运行了1个容器，swarm03上面运行了2个，而swarm02上面一个容器都没有，这是咋回事呢？哦，原来我们在创建集群过程中，对swarm02 做了状态变更的操作，将其设置为不运行任务。` docker node update  --availability drain  swarm02`. 查看集群状态。\n\n```\n[root@swarm01 swarm]# docker node ls\nID                            HOSTNAME            STATUS              AVAILABILITY        MANAGER STATUS      ENGINE VERSION\nepoic1y0vv830vnwbc6nnacjc *   swarm01             Ready               Active              Leader              18.03.1-ce\nnsqqv181e0r7mu77azixtqhzl     swarm02             Ready               Drain               Reachable           18.03.1-ce\nmkkxaeqergz8xw21bbkd47q1c     swarm03             Ready               Active              Reachable           18.03.1-ce\n```\n\n此时打开浏览器，输入HTTP URL。任意swarm集群节点都可以访问httpd服务。\n\n![](.\\swarm-web.png)\n\n需要说明的是，由于3个节点的/var/www/html 路径未做共享存储。因此swarm会随机显示一个swarm节点的网页内容。\n\n### 4.6.3 服务扩缩容\n\n修改docker-swarm.yml 配置字段 `replicas: 5` ，将容器个数提升到5个。由于实验环境资源有限，因此这里将swarm02状态变更为可以状态，用以分配容器。\n\n` docker node update  --availability active swarm02`.\n\n重新执行部署命令`docker stack deploy -c docker-swarm.yml myservice`\n\n也可以通过命令直接对服务进行操作，但是不推荐。 `docker service scale 服务ID=服务个数`\n\n```\n[root@swarm01 swarm]# docker stack ps myservice\nID                  NAME                IMAGE               NODE                DESIRED STATE       CURRENT STATE                    ERROR               PORTS\nunwepav5z9nz        myservice_web.1     httpd:v1            swarm02             Running             Running less than a second ago\n3j5o1tktcg1l        myservice_web.2     httpd:v1            swarm01             Running             Preparing 3 seconds ago\nwl1unt6lynft        myservice_web.3     httpd:v1            swarm03             Running             Running less than a second ago\n[root@swarm01 swarm]# vi docker-swarm.yml\n[root@swarm01 swarm]# docker stack deploy -c docker-swarm.yml myservice\nUpdating service myservice_web (id: 85quzwi5k0btkn5wlht4jbco9)\nimage httpd:v1 could not be accessed on a registry to record\nits digest. Each node will access httpd:v1 independently,\npossibly leading to different nodes running different\nversions of the image.\n\n[root@swarm01 swarm]# docker stack ps myservice\nID                  NAME                IMAGE               NODE                DESIRED STATE       CURRENT STATE                    ERROR               PORTS\nunwepav5z9nz        myservice_web.1     httpd:v1            swarm02             Running             Running 24 seconds ago\n3j5o1tktcg1l        myservice_web.2     httpd:v1            swarm01             Running             Running 31 seconds ago\nwl1unt6lynft        myservice_web.3     httpd:v1            swarm03             Running             Running 24 seconds ago\nujedjxbuffxp        myservice_web.4     httpd:v1            swarm01             Running             Running less than a second ago\nml9qvjh2add7        myservice_web.5     httpd:v1            swarm02             Running             Running less than a second ago\n```\n\n同样，对于服务缩容，也仅需要修改`replicas:` 配置即可。这里不再演示。\n\n### 4.6.4 删除服务\n\n例如，删除myredis应用服务，执行docker service rm myredis，则应用服务myredis的全部副本都会被删除。 \n\n本例使用的是stack ，删除stack ，即可删除其下的服务。没错，docker语法非常类似,`docker stack rm myservice`。\n\n需要注意的是，对swarm上面服务的操作，都必须在manager节点上运行。\n\n### 4.6.5 服务升降级\n\n服务升降级，对应到docker中，则利用了容器镜像的更新，升级很好理解，直接把tag 标签版本往上提。那么降级，其实也可以理解为另外一种“升级“，只是镜像内容是上一个版本的而已。对于镜像标签的命令应该遵循一套严格的规范，这里不再赘述。\n\n先介绍利用service 命令行工具的用法。\n\n服务的滚动更新，这里我参考官网文档的例子说明。在Manager Node上执行如下命令：\n\n```\ndocker service create --replicas 3  --name redis --update-delay 10s redis:3.0.6\n```\n\n上面通过指定 --update-delay 选项，表示需要进行更新的服务，每次成功部署一个，延迟10秒钟，然后再更新下一个服务。如果某个服务更新失败，则Swarm的调度器就会暂停本次服务的部署更新。\n\n另外，也可以更新已经部署的服务所在容器中使用的Image的版本，例如执行如下命令：\n\n将Redis服务对应的Image版本有3.0.6更新为3.0.7，同样，如果更新失败，则暂停本次更新。\n\n那么，stack file 如何定义滚动更新呢？\n\n```\n      update_config:\n        parallelism: 2\n        delay: 10s\n```\n\n修改docker-swarm.yml 如下\n\n```\nversion: '3'\nservices:\n  web:\n    image: \"httpd:v2\"\n    deploy:\n      replicas: 3\n      update_config:\n        parallelism: 2\n        delay: 30s\n      resources:\n        limits:\n          cpus: \"0.5\"\n          memory: 256M\n      restart_policy:\n        condition: on-failure\n    ports:\n     - \"7080:80\"\n    volumes:\n     - /var/www/html:/usr/local/apache2/htdocs\n```\n\nparallelism 表示同时升级的个数，delay 则表示间隔多长时间升级。\n\n同时将httpd 由v1 升级到v2，这里我们使用同一个镜像，只是tag 修改了一下。查看服务状态\n\n```\n[root@swarm01 swarm]# docker stack ps myservice\nID                  NAME                  IMAGE               NODE                DESIRED STATE       CURRENT STATE                 ERROR               PORTS\nqk8pvvc9nmv0        myservice_web.1       httpd:v2            swarm01             Running             Running 56 seconds ago\nunwepav5z9nz         \\_ myservice_web.1   httpd:v1            swarm02             Shutdown            Shutdown 48 seconds ago\nuyj0pm1df9hl        myservice_web.2       httpd:v2            swarm02             Running             Running 46 seconds ago\n3j5o1tktcg1l         \\_ myservice_web.2   httpd:v1            swarm01             Shutdown            Shutdown about a minute ago\nqdjov5zf5alb        myservice_web.3       httpd:v2            swarm03             Running             Running 10 seconds ago\nwl1unt6lynft         \\_ myservice_web.3   httpd:v1            swarm03             Shutdown            Shutdown 12 seconds ago\n```\n\n可以看到同时只有2个容器在升级，同时第3个容器也是在间隔了30s 之后，才开始升级。\n\n --filter 过滤条件`docker stack ps  --filter  \"desired-state=running\"  myservice`\n\n### 4.6.6 服务健康检查\n\n对于容器而言，最简单的健康检查是进程级的健康检查，即检验进程是否存活。Docker Daemon会自动监控容器中的PID1进程，如果docker run命令中指明了restart policy，可以根据策略自动重启已结束的容器。在很多实际场景下，仅使用进程级健康检查机制还远远不够。比如，容器进程虽然依旧运行却由于应用死锁无法继续响应用户请求，这样的问题是无法通过进程监控发现的 。\n\n下面先介绍Docker容器健康检查机制，之后再介绍Docker Swarm mode的新特性。\n\n**Docker 原生健康检查能力**\n\n而自 1.12 版本之后，Docker 引入了原生的健康检查实现，可以在Dockerfile中声明应用自身的健康检测配置。HEALTHCHECK 指令声明了健康检测命令，用这个命令来判断容器主进程的服务状态是否正常，从而比较真实的反应容器实际状态。\n\nHEALTHCHECK 指令格式：\n\n- HEALTHCHECK [选项] CMD <命令>：设置检查容器健康状况的命令\n- HEALTHCHECK NONE：如果基础镜像有健康检查指令，使用这行可以屏蔽掉\n\n注：在Dockerfile中 HEALTHCHECK 只可以出现一次，如果写了多个，只有最后一个生效。\n\n使用包含 HEALTHCHECK 指令的dockerfile构建出来的镜像，在实例化Docker容器的时候，就具备了健康状态检查的功能。启动容器后会自动进行健康检查。\n\n> HEALTHCHECK 支持下列选项：\n>\n> - interval=<间隔>：两次健康检查的间隔，默认为 30 秒;\n> - timeout=<间隔>：健康检查命令运行超时时间，如果超过这个时间，本次健康检查就被视为失败，默认 30 秒;\n> - retries=<次数>：当连续失败指定次数后，则将容器状态视为 unhealthy，默认 3 次。\n> - start-period=<间隔>: 应用的启动的初始化时间，在启动过程中的健康检查失效不会计入，默认 0 秒; (从17.05)引入\n\n在 HEALTHCHECK [选项] CMD 后面的命令，格式和 ENTRYPOINT 一样，分为 shell 格式，和 exec 格式。命令的返回值决定了该次健康检查的成功与否：\n\n- 0：成功;\n- 1：失败;\n- 2：保留值\n\n容器启动之后，初始状态会为 starting (启动中)。Docker Engine会等待 interval 时间，开始执行健康检查命令，并周期性执行。如果单次检查返回值非0或者运行需要比指定 timeout 时间还长，则本次检查被认为失败。如果健康检查连续失败超过了 retries 重试次数，状态就会变为 unhealthy (不健康)。\n\n注：\n\n- 一旦有一次健康检查成功，Docker会将容器置回 healthy (健康)状态\n- 当容器的健康状态发生变化时，Docker Engine会发出一个 health_status 事件。\n\n假设我们有个镜像是个最简单的 Web 服务，我们希望增加健康检查来判断其 Web 服务是否在正常工作，我们可以用 curl来帮助判断，其 Dockerfile 的 HEALTHCHECK 可以这么写：\n\n```\nFROM elasticsearch:5.5 \n \nHEALTHCHECK --interval=5s --timeout=2s --retries=12 \\ \n  CMD curl --silent --fail localhost:9200/_cluster/health || exit 1\n```\n\n然后编译镜像，并运行\n\n```\ndocker build -t test/elasticsearch:5.5 . \ndocker run --rm -d \\ \n    --name=elasticsearch \\ \n    test/elasticsearch:5.5 \n```\n\n执行 docker ps容器检查命令，发现过了几秒之后，Elasticsearch容器从 starting 状态进入了 healthy 状态\n\n```\n$ docker ps \nCONTAINER ID        IMAGE                    COMMAND                  CREATED             STATUS                            PORTS                NAMES \nc9a6e68d4a7f        test/elasticsearch:5.5   \"/docker-entrypoin...\"   2 seconds ago       Up 2 seconds (health: starting)   9200/tcp, 9300/tcp   elasticsearch \n$ docker ps \nCONTAINER ID        IMAGE                    COMMAND                  CREATED             STATUS                    PORTS                NAMES \nc9a6e68d4a7f        test/elasticsearch:5.5   \"/docker-entrypoin...\"   14 seconds ago      Up 13 seconds (healthy)   9200/tcp, 9300/tcp   elasticsearch \n```\n\n**Docker Swarm健康检查能力**\n\n在Docker 1.13之后，在Docker Swarm mode中提供了对健康检查策略的支持。\n\n可以在 `docker service create` 命令中指明健康检查策略\n\n```\n$ docker service create -d \\\n    --name=elasticsearch \\\n    --health-cmd=\"curl --silent --fail localhost:9200/_cluster/health || exit 1\" \\\n    --health-interval=5s \\\n    --health-retries=12 \\\n    --health-timeout=2s \\\n    elasticsearch\n```\n\n在Swarm模式下，Swarm manager会监控服务task的健康状态，如果容器进入 `unhealthy` 状态，它会停止容器并且重新启动一个新容器来取代它。这个过程中会自动更新服务的 load balancer (routing mesh) 后端或者 DNS记录，可以保障服务的可用性。\n\n在1.13版本之后，在服务更新阶段也增加了对健康检查的支持，这样在新容器完全启动成功并进入健康状态之前，load balancer/DNS解析不会将请求发送给它。这样可以保证应用在更新过程中请求不会中断。\n\n![](.\\healthcheck.png)\n\ndocker 官方网站有些常用容器的健康检查的例子：https://github.com/docker-library/healthcheck\n\n# 5.附录\n\n## 5.1 参考文档\n\n官网文档：\n\nhttps://docs.docker.com/engine/swarm/\n\nhttps://docs.docker.com/get-started/part4/\n\nhttps://docs.docker.com/network/overlay/#publish-ports-on-an-overlay-network\n\n技术博客：\n\nhttp://www.dockerinfo.net/2552.html\n\nhttps://www.cnblogs.com/bigberg/p/8761047.html\n\nhttp://blog.51cto.com/cloudman/1968287 \n\nhttps://neuvector.com/network-security/docker-swarm-container-networking/\n\nhttp://www.uml.org.cn/yunjisuan/201708282.asp","source":"_posts/docker-swarm-scheduler.md","raw":"---\ntitle: Docker Swarm服务调度分析\ndate: 2018-10-20 16:37:53\ntags: \n - Docker Swarm\n - Service\ncategory: 技术 \n---\n\n\n本文主要介绍docker swarm 对于容器应用的调度管理分析。Docker swarm 1.12.0  是一个非常重要的版本，1.12.0之前的版本 docker swarm 是一个独立的项目，需要额外下载swarm镜像进行安装。1.12.0  之后的版本，swarm功能直接继承到docker engine 当中。直接使用docker CLI 可创建一个swarm 集群、可在swarm 集群中部署应用程序，同时具备管理能力。 \n\n关于swarm的详细介绍，可参考官方文档：https://docs.docker.com/engine/swarm/\n\n# 1.Docker Swarm 集群架构\n\nSwarm 集群中，有两种类型的节点：Manager和Worker。\n\n要部署服务到swarm，需要将服务定义提交给Manager节点。Manager节点将称为work的工作单元分派 给Worker节点。\n\n![](\\img\\swarm-node.png)\n\n<!-- more -->\n\n## 1.1 Manager Node\n\nManager节点处理集群管理任务：\n\n- 维护集群状态\n- 调度服务\n- 为 Swarm 提供外部可调用的 API 接口\n\nManager 节点需要时刻维护和保存当前 Swarm 集群中各个节点的一致性状态，这里主要是指各个 Tasks 的执行的状态和其它节点的状态。因为 Swarm 集群是一个典型的分布式集群，在保证一致性上，Manager 节点采用 [Raft](https://raft.github.io/raft.pdf) 协议来保证分布式场景下的数据一致性。\n\n通常为了保证 Manager 节点的高可用，Docker 建议采用奇数个 Manager 节点，这样的话，你可以在 Manager 失败的时候不用关机维护，Docker 官方给出如下的建议：\n\n- 3 个 Manager 节点最多可以同时容忍 1 个 Manager 节点失效的情况下保证高可用；\n- 5 个 Manager 节点最多可以同时容忍 2 个 Manager 节点失效的情况下保证高可用；\n- N 个 Manager 节点最多可以同时容忍 (N−1)/2个 Manager 节点失效的情况下保证高可用；\n- 最多最多的情况下，使用 7 个 Manager 节点就够了，否则反而会降低集群的性能了。\n\n> **重要说明**：添加更多管理器并不意味着可扩展性更高或性能更高。一般而言，情况正好相反。\n\n| 群体大小 | 多数 | 容错  |\n| -------- | ---- | ----- |\n| 1        | 1    | 0     |\n| 2        | 2    | 0     |\n| **3**    | 2    | **1** |\n| 4        | 3    | 1     |\n| **5**    | 3    | **2** |\n| 6        | 4    | 2     |\n| **7**    | 4    | **3** |\n| 8        | 5    | 3     |\n\n## 1.2 Worker Node \n\nWorker节点接收并执行从Manager节点分派的任务。默认情况下，Manager节点还将服务作为Worker节点运行，但也可以将它们配置为仅运行Manage任务。 可将Manager可用性设置为`Drain` ，这样服务就不会被分配到Manager节点上运行了。\n\nWorker节点不参与Raft分布状态，做出调度决策或提供群模式HTTP API。\n\n通过 docker node promote 命令将一个 Worker 节点提升为 Manager 节点。通常情况下，该命令使用在维护的过程中，需要将 Manager 节点占时下线进行维护操作。同样可以使用 docker node demote 将某个 manager 节点降级为 worker 节点。 \n\n## 1.3 小结  \n\n综上可知， Swarm 集群的管理工作是由manager节点实现。如上图所示，manager节点实现的功能主要包括：node discovery，scheduler,cluster管理等。同时，为了保证Manager 节点的高可用，Manager 节点需要时刻维护和保存当前 Swarm 集群中各个节点的一致性状态。在保证一致性上，Manager 节点采用 [Raft](https://raft.github.io/raft.pdf) 协议来保证分布式场景下的数据一致性；\n\nDocker Swarm内置了Raft一致性算法，可以保证分布式系统的数据保持一致性同步。Etcd, Consul等高可用键值存储系统也是采用了这种算法。这个算法的作用简单点说就是随时保证集群中有一个Leader，由Leader接收数据更新，再同步到其他各个Follower节点。在Swarm中的作用表现为当一个Leader 节点 down掉时，系统会立即选取出另一个Leader节点，由于这个节点同步了之前节点的所有数据，所以可以无缝地管理集群。\n\nRaft的详细解释可以参考[《The Secret Lives of Data--Raft: Understandable Distributed Consensus》](http://thesecretlivesofdata.com/raft/)。\n\n为保证Magnger高可用，一般部署3-7个Manager 节点。而在云桌面的场景下，可能一个集群就只有两台服务器。显然此时是无法满足Manager 3个节点的高可用要求。因此我们面对的挑战就是如何修改swarm Manager Raft相关部分代码，使其在只有2个节点的情况下，也可以保证一个节点宕机的高可用。\n\n# 2.Docker Swarm 网络架构\n\n## 2.1 网络概述\n\n我们知道，Docker 的几种网络方案：none、host、bridge 和 joined 容器，它们解决了单个 Docker Host 内容器通信的问题。那么跨主机容器间通信的方案又有哪些呢？\n\n跨主机网络方案包括：\n\n1. docker 原生的 overlay 和 macvlan。\n2. 第三方方案：常用的包括 flannel、weave 和 calico。\n\nDocker 网络是一个非常活跃的技术领域，不断有新的方案开发出来，那么要问个非常重要的问题了： 如此众多的方案是如何与 docker 集成在一起的？答案是：libnetwork 以及 CNM。\n\nlibnetwork 是 docker 容器网络库，最核心的内容是其定义的 Container Network Model (CNM)，这个模型对容器网络进行了抽象，由以下三类组件组成：\n\n1. Sandbox：Sandbox 是容器的网络栈，包含容器的 interface、路由表和 DNS 设置。 Linux Network Namespace 是 Sandbox 的标准实现。Sandbox 可以包含来自不同 Network 的 Endpoint。\n2. Endpoint：Endpoint 的作用是将 Sandbox 接入 Network。Endpoint 的典型实现是 veth pair，后面我们会举例。一个 Endpoint 只能属于一个网络，也只能属于一个 Sandbox。\n3. Network：Network 包含一组 Endpoint，同一 Network 的 Endpoint 可以直接通信。Network 的实现可以是 Linux Bridge、VLAN 等。\n\n如图所示两个容器，一个容器一个 Sandbox，每个 Sandbox 都有一个 Endpoint 连接到 Network 1，第二个 Sandbox 还有一个 Endpoint 将其接入 Network 2.\n\nlibnetwork CNM 定义了 docker 容器的网络模型，按照该模型开发出的 driver 就能与 docker daemon 协同工作，实现容器网络。docker 原生的 driver 包括 none、bridge、overlay 和 macvlan，第三方 driver 包括 flannel、weave、calico 等。\n\n## 2.2 Overlay网络\n\nDocker Swarm 内置的跨主机容器通信方案是overlay网络，这是一个基于VxLAN 协议的网络实现。VxLAN 可将二层数据封装到 UDP 进行传输，VxLAN 提供与 VLAN 相同的以太网二层服务，但是拥有更强的扩展性和灵活性。 overlay 通过虚拟出一个子网，让处于不同主机的容器能透明地使用这个子网。所以跨主机的容器通信就变成了在同一个子网下的容器通信，看上去就像是同一主机下的bridge网络通信。\n\n![1534939506173](\\img\\overlay-vxlan.png)\n\n根据vxlan的作用知道，它是要在三层网络中虚拟出二层网络，即跨网段建立虚拟子网。简单的理解就是把发送到虚拟子网地址`10.0.0.3`的报文封装为发送到真实IP`192.168.1.3`的报文。这必然会有更大的数据开销，但却简化了集群的网络连接，让分布在不同主机的容器好像都在同一个主机上一样 。\n\n`overlay`网络会创建多个Docker主机之间的分布式网络。该网络位于（覆盖）特定于主机的网络之上，允许连接到它的容器（包括群集服务容器）安全地进行通信。Docker透明地处理每个数据包与正确的Docker守护程序主机和正确的目标容器的路由。\n\n初始化swarm或将Docker主机加入现有swarm时，会在该Docker主机上创建两个新网络：\n\n- ingress overlay 网络，处理与swarm集群服务相关的控制和数据流量。创建群组服务并且不将其连接到用户定义的覆盖网络时，服务将默认连接到ingress overlay网络。集群中只能有一个ingress overlay 网络。\n- docker_gwbridge 桥接网络，它将各个Docker守护程序连接到参与该群集的其他Docker守护进程。同时该docker_gwbridge 网络将为主机上的容器提供访问外网的能力。\n\n服务或容器一次可以连接到多个网络。服务或容器只能通过它们各自连接的网络进行通信。\n\n### 2.2.1 创建overlay网络\n\n> **先决条件**：\n>\n> - 使用overlay 网络的Docker守护程序的防火墙规则\n>\n>   您需要以下端口打开来往于overlay 网络上的每个Docker主机的流量：\n>\n>   - 用于集群管理通信的TCP端口2377\n>   - TCP和UDP端口7946用于节点之间的通信\n>   - UDP端口4789用于覆盖网络流量\n>\n> - 在创建overlay 网络之前，您需要将Docker守护程序初始化为swarm管理器，`docker swarm init`或者使用它将其连接到现有的swarm `docker swarm join`。这些中的任何一个都会创建默认ingress overlay 网络，默认情况下 由群服务使用。即使您从未计划使用群组服务，也需要执行此操作。之后，您可以创建其他用户定义的overlay 网络。\n\n要创建用于swarm服务的覆盖网络，请使用如下命令：\n\n```\n$ docker network create -d overlay my-overlay\n```\n\n要创建可由群集服务或独立容器用于与在其他Docker守护程序上运行的其他独立容器通信的覆盖网络，请添加`--attachable`标志：\n\n```\n$ docker network create -d overlay --attachable my-attachable-overlay\n```\n\n同时可以指定IP地址范围，子网，网关和其他选项。详情`docker network create --help`请见\n\n### 2.2.2 加密overlay网络\n\n默认情况下，使用GCM模式下的AES算法加密所有swarm群集服务管理流量 。集群中的管理器节点每隔12小时轮换用于加密数据的密钥。\n\n要加密应用程序数据，请`--opt encrypted`在创建覆盖网络时添加。这样可以在vxlan级别启用IPSEC加密。此加密会产生不可忽视的性能损失，因此您应该在生产中使用此选项之前对其进行测试。\n\n启用覆盖加密后，Docker会在所有节点之间创建IPSEC隧道，在这些节点上为连接到覆盖网络的服务安排任务。这些隧道还在GCM模式下使用AES算法，管理器节点每12小时自动旋转密钥。\n\n```\ndocker network create --opt encrypted --driver overlay my-encrypted-network\n```\n\n### 2.2.3 暴露服务端口\n\n对于连接到同一个swarm集群的服务来说，它们之间所有的端口都是互相暴露的。对于可在服务外部访问的端口，必须使用 -p ( 或者 --publish) 发布该端口。\n\n| Flag value                                                   | Description                                         |\n| ------------------------------------------------------------ | --------------------------------------------------- |\n| `-p 8080:80` or `-p published=8080,target=80`                | 将服务上的TCP端口80映射到routing mesh上的端口8080。 |\n| `-p 8080:80/udp` or `-p published=8080,target=80,protocol=udp` | 将服务上的UDP端口80映射到routing mesh上的端口8080。 |\n\n\n\n### 2.2.4 实现原理\n\n下面我们讨论下overlay 网络的具体实现：\n\ndocker 会为每个 overlay 网络创建一个独立的 network namespace，其中会有一个 linux bridge br0，endpoint 还是由 veth pair 实现，一端连接到容器中（即 eth0），另一端连接到 namespace 的 br0 上。\n\nbr0 除了连接所有的 endpoint，还会连接一个 vxlan 设备，用于与其他 host 建立 vxlan tunnel。容器之间的数据就是通过这个 tunnel 通信的。逻辑网络拓扑结构如图所示：\n\n![](./overlay.jpg)\n\n\n\n## 2.3 Ingress Routing Mesh\n\n上文暴露服务端口中有讲到routing mesh，现在来详细介绍这一网络功能。\n\n默认情况下，发布端口的swarm服务使用routing mesh来实现。当客户端连接到任何swarm节点上的已发布端口（无论它是否正在运行给定服务）时，客户端请求将被透明地重定向到正在运行该服务的worker。实际上，Docker充当集群服务的负载均衡器。使用routing mesh的服务以*虚拟IP（VIP）模式运行*。即使在每个节点上运行的服务（通过`--global`标志）也使用routing mesh。使用routing mesh时，无法保证哪个Docker节点服务客户端请求。\n\n要在群集中使用 ingress 网络，您需要在启用群集模式之前在群集节点之间打开以下端口：\n\n- 端口`7946`  TCP / UDP用于容器网络发现。\n- 端口`4789`  UDP 用于容器 ingress 网络。\n\n例如，以下命令将nginx容器中的端口80发布到群集中任何节点的端口8080：\n\n```\n$ docker service create \\\n  --name my-web \\\n  --publish published=8080,target=80 \\\n  --replicas 2 \\\n  nginx\n```\n\n当您在任何节点上访问端口8080时，Docker会将您的请求路由到活动容器。在群集节点本身上，端口8080实际上可能不受约束，但routing mesh 知道如何路由流量并防止发生任何端口冲突。\n\nrouting mesh在已发布的端口上侦听分配给该节点的任何IP地址。对于可外部路由的IP地址，该端口可从主机外部获得。对于所有其他IP地址，只能从主机内访问。\n\n![](\\img\\ingress-routing-mesh.png)\n\n如果想要绕过routing mesh，可以使用*DNS循环（DNSRR）模式*启动服务，方法是将`--endpoint-mode`标志设置为`dnsrr`。这样就必须在服务前运行自己的负载均衡器。Docker主机上的服务名称的DNS查询返回运行该服务的节点的IP地址列表。配置负载均衡器以使用此列表并平衡节点之间的流量。\n\n对于我们云桌面应用场景来说，直接利用routing mesh 功能即可，不需要额外搭建负载均衡器。\n\n## 2.4 服务发现\n\nDocker Swarm 原生就提供了服务发现的功能。要使用服务发现，需要相互通信的 services 必须属于同一个 overlay 网络，所以我们先得创建一个新的 overlay 网络。 直接使用 `ingress` 行不行？很遗憾，目前 `ingress` 没有提供服务发现，必须创建自己的 overlay 网络。\n\nDocker Swarm mode下会为每个节点的Docker engine内置一个DNS server，各个节点间的DNS server通过control plane的gossip协议互相交互信息。此处DNS server用于容器间的服务发现。swarm mode会为每个 --net=自定义网络  的service分配一个DNS entry。目前必须是自定义网络，比如overaly。而bridge和routing mesh 的service，是不会分配DNS的。\n\n那么，下面就来详细介绍服务发现的原理。\n\n每个Docker容器都有一个DNS解析器，它将DNS查询转发到docker engine，该引擎充当DNS服务器。docker 引擎收到请求后就会在发出请求的容器所在的所有网络中，检查域名对应的是不是一个容器或者是服务，如果是，docker引擎就会从存储的key-value建值对中查找这个容器名、任务名、或者服务名对应的IP地址，并把这个IP地址或者是服务的虚拟IP地址返回给发起请求的域名解析器。  \n\n由上可知，docker的服务发现的作用范围是网络级别，也就意味着只有在同一个网络上的容器或任务才能利用内嵌的DNS服务来相互发现，不在同一个网络里面的服务是不能解析名称的，另外，为了安全和性能只有当一个节点上有容器或任务在某个网络里面时，这个节点才会存储那个网络里面的DNS记录。\n\n如果目的容器或服务和源容器不在同一个网络里面，Docker引擎会把这个DNS查询转发到配置的默认DNS服务  。\n\n![](\\img\\service-dns.png)\n\n在上面的例子中，总共有两个服务myservice和client，其中myservice有两个容器，这两个服务在同一个网里面。在client里针对docker.com和myservice各执行了一个curl操作，下面时执行的流程： \n\n- 为了client解析docker.com和myservice，DNS查询进行初始化\n- 容器内建的解析器在127.0.0.11:53拦截到这个DNS查询请求，并把请求转发到docker引擎的DNS服务\n- myservice 被解析成服务对应的虚拟IP（10.0.0.3）。在接下来的内部负载均衡阶段再被解析成一个具体任务容器的IP地址。如果是容器名称这一步直接解析成容器对应的IP地址（10.0.0.4或者10.0.0.5）。\n- docker.com 在docker引擎或者mynet网络上不能被解析成服务，所以这个请求被转发到外部DNS，例如配置好的默认DNS服务器（8.8.8.8）上。\n\nDocker1.12+的服务发现和负载均衡是结合到一起的， 实现办法有两种，DNS轮询和IPVS。 配置参数 --endpoint-mode  (vip or dnsrr) ，默认使用VIP方式。DNS轮询有一些缺点，比如一些应用可能缓存DNS请求， 导致容器变化之后DNS不能实时更新；DNS生效时间也导致不能实时反映服务变化情况。VIP和IPVS原理上比较容易理解， 就是Docker为每个服务分配了一个VIP，DNS解析服务名称或者自定义的别名到这个VIP上。由于VIP本身没有容器提供服务，Docker把到VIP的请求通过IPVS技术负载到后面的容器上。\n\n因此正常情况下，使用默认的VIP方式即可。\n\n## 3.5 负载均衡\n\n负载均衡分为两种：\n\nSwarm集群内的service之间的相互访问需要做负载均衡，称为内部负载均衡（Internal LB）；\n\n从Swarm集群外部访问服务的公开端口，也需要做负载均衡，称外部负载均衡(Exteral LB or Ingress LB)。\n\n### 3.5.1 Internal LB\n\n内部负载均衡就是我们在上一段提到的服务发现，集群内部通过DNS访问service时，Swarm默认通过VIP（virtual IP）、iptables、IPVS转发到某个容器。 \n\n![](\\img\\intelnal-lb.jpg)\n\n当在docker swarm集群模式下创建一个服务时，会自动在服务所属的网络上给服务额外的分配一个虚拟IP，当解析服务名字时就会返回这个虚拟IP。对虚拟IP的请求会通过overlay网络自动的负载到这个服务所有的健康任务上。这个方式也避免了客户端的负载均衡，因为只有单独的一个IP会返回到客户端，docker会处理虚拟IP到具体任务的路由，并把请求平均的分配给所有的健康任务。  \n\n![](.\\internal-lb-2.png)\n\n```\n# 创建overlay网络：mynet \n$ docker network create -d overlay mynet  \na59umzkdj2r0ua7x8jxd84dhr \n# 利用mynet网络创建myservice服务，并复制两份  \n$ docker service create --network mynet --name myservice --replicas 2 busybox ping localhost  \n78t5r8cr0f0h6k2c3k7ih4l6f5\n# 通过下面的命令查看myservice对应的虚拟IP \n$ docker service inspect myservice  \n...\n\"VirtualIPs\": [ \n\t{  \n     \"NetworkID\": \"a59umzkdj2r0ua7x8jxd84dhr\",  \n     \t\t\t\"Addr\": \"10.0.0.3/24\"  \n      },  \n]  \t\n```\n\n注：swarm中服务还有另外一种负载均衡技术可选DNS round robin (DNS RR) （在创建服务时通过--endpoint-mode配置项指定），在DNSRR模式下，docker不再为服务创建VIP，docker DNS服务直接利用轮询的策略把服务名称直接解析成一个容器的IP地址。 \n\n### 3.5.2 Exteral LB\n\nExteral LB(Ingress LB 或者 Swarm Mode Routing Mesh) 看名字就知道，这个负载均衡方式和前面提到的Ingress网络有关。Swarm网络要提供对外访问的服务就需要打开公开端口，并映射到宿主机。Exteral LB就是外部通过公开端口访问集群时做的负载均衡。\n\n当创建或更新一个服务时，你可以利用--publish选项把一个服务暴露到外部，在docker swarm模式下发布一个端口意味着在集群中的所有节点都会监听这个端口，这时当访问一个监听了端口但是并没有对应服务运行在其上的节点会发生什么呢？接下来就该我们的路由网（routing mesh）出场了，路由网时docker1.12引入的一个新特性，它结合了IPVS和iptables创建了一个强大的集群范围的L4层负载均衡，它使所有节点接收服务暴露端口的请求成为可能。当任意节点接收到针对某个服务暴露的TCP/UDP端口的请求时，这个节点会利用预先定义过的Ingress overlay网络，把请求转发给服务对应的虚拟IP。ingress网络和其他的overlay网络一样，只是它的目的是为了转换来自客户端到集群的请求，它也是利用我们前一小节介绍过的基于VIP的负载均衡技术。 \n\n启动服务后，你可以为应用程序创建外部DNS记录，并将其映射到任何或所有Docker swarm节点。你无需担心你的容器具体运行在那个节点上，因为有了路由网这个特性后，你的集群看起来就像是单独的一个节点一样。  \n\n```\n#在集群中创建一个复制两份的服务，并暴露在8000端口  \n$ docker service create --name app --replicas 2 --network appnet --publish 8000:80 nginx  \n```\n\n![](\\img\\external-routing-mesh.png)\n\n上面这个图表明了路由网是怎么工作的： \n\n- 服务（app）拥有两份复制，并把端口映射到外部端口的8000\n- 路由网在集群中的所有节点上都暴露出8000\n- 外部对服务app的请求可以是任意节点，在本例子中外部的负载均衡器将请求转发到了没有app服务的主机上\n- docker swarm的IPVS利用ingress overlay网路将请求重新转发到运行着app服务的节点的容器中\n\n注：以上服务发现和负载均衡参考文档 https://success.docker.com/article/ucp-service-discovery\n\n\n\n# 3.Docker Swarm 存储架构\n\n从业务数据的角度看，容器可以分为两类：无状态（stateless）容器和有状态（stateful）容器。\n\n无状态是指容器在运行过程中不需要保存数据，每次访问的结果不依赖上一次访问，比如提供静态页面的 web 服务器。有状态是指容器需要保存数据，而且数据会发生变化，访问的结果依赖之前请求的处理结果，最典型的就是数据库服务器。\n\n简单来讲，状态（state）就是数据，如果容器需要处理并存储数据，它就是有状态的，反之则无状态。\n\n对于有状态的容器，如何保存数据呢？\n\n## 3.1 Storage Driver\n\nDocker在启动容器的时候，需要创建文件系统，为rootfs提供挂载点。最底层的引导文件系统bootfs主要包含 bootloader和kernel，bootloader主要是引导加载kernel，当kernel被加载到内存中后 bootfs就被umount了。 rootfs包含的就是典型 Linux 系统中的/dev，/proc，/bin，/etc等标准目录和文件。\n\nDocker 模型的核心部分是有效利用分层镜像机制，镜像可以通过分层来进行继承，基于基础镜像（没有父镜像），可以制作各种具体的应用镜像。Docker1.10引入新的可寻址存储模型，使用安全内容哈希代替随机的UUID管理镜像。同时，Docker提供了迁移工具，将已经存在的镜像迁移到新模型上。不同 Docker 容器就可以共享一些基础的文件系统层，同时再加上自己独有的可读写层，大大提高了存储的效率。其中主要的机制就是分层模型和将不同目录挂载到同一个虚拟文件系统。\n\nDocker存储方式提供管理分层镜像和容器的可读写层的具体实现。最初Docker仅能在支持AUFS文件系统的ubuntu发行版上运行，但是由于AUFS未能加入Linux内核，为了寻求兼容性、扩展性，Docker在内部通过graphdriver机制这种可扩展的方式来实现对不同文件系统的支持。\n\n![](.\\storage-driver.png)\n\n如上图所示，容器由最上面一个可写的容器层，以及若干只读的镜像层组成，容器的数据就存放在这些层中。这样的分层结构最大的特性是 Copy-on-Write：\n\n1. 新数据会直接存放在最上面的容器层。\n2. 修改现有数据会先从镜像层将数据复制到容器层，修改后的数据直接保存在容器层中，镜像层保持不变。\n3. 如果多个层中有命名相同的文件，用户只能看到最上面那层中的文件。\n\n分层结构使镜像和容器的创建、共享以及分发变得非常高效，而这些都要归功于 Docker storage driver。正是 storage driver 实现了多层数据的堆叠并为用户提供一个单一的合并之后的统一视图。\n\nDocker 支持多种 storage driver，有 AUFS、Device Mapper、Btrfs、OverlayFS、VFS 和 ZFS。它们都能实现分层的架构，同时又有各自的特性。关于这些方案的对比可以查阅相关文档（http://dockone.io/article/1513）。对于 Docker 用户来说，具体选择使用哪个 storage driver 是一个难题，不过 Docker 官方给出了一个简单的答案： **优先使用 Linux 发行版默认的 storage driver**。 使用`docker info `查看系统默认的storage driver。\n\n使用的storage driver是与主机上的Backing Filesystem有关的。storage driver 是专门用来存放docker容器和镜像的，Backing Filesystem指的是主机的文件系统。默认的centos7 driver 是overlay2，对应的Backing Filesystem 是ext4。这个应该是没有问题的。storage driver 和Backing Filesystem 对应关系见下表。 \n\n![1534987414895](.\\storage_backfing.png)\n\n对于某些容器，直接将数据放在由 storage driver 维护的层中是很好的选择，比如那些无状态的应用。无状态意味着容器没有需要持久化的数据，随时可以从镜像直接创建。\n\n比如 busybox，它是一个工具箱，我们启动 busybox 是为了执行诸如 wget，ping 之类的命令，不需要保存数据供以后使用，使用完直接退出，容器删除时存放在容器层中的工作数据也一起被删除，这没问题，下次再启动新容器即可。\n\n但对于另一类应用这种方式就不合适了，它们有持久化数据的需求，容器启动时需要加载已有的数据，容器销毁时希望保留产生的新数据，也就是说，这类容器是有状态的。\n\n## 3.2 数据持久化方案\n\n那么对于有状态的容器，如何保存数据呢？\n\n选项一：打包在容器里。\n\n显然不行。除非数据不会发生变化，否则，如何在多个副本直接保持同步呢？\n\n选项二：数据放在 Docker 主机的本地目录中，通过 volume 映射到容器里。\n\n位于同一个主机的副本倒是能够共享这个 volume，但不同主机中的副本如何同步呢？\n\n选项三：利用 Docker 的 volume driver，由外部 storage provider 管理和提供 volume，所有 Docker 主机 volume 将挂载到各个副本。\n\n这是目前最佳的方案。volume 不依赖 Docker 主机和容器，生命周期由 storage provider 管理，volume 的高可用和数据有效性也全权由 provider 负责，Docker 只管使用。\n\n我们知道Docker 单机的存储方案里面，通过 data volume 可以存储容器的状态。不过其本质是 Docker 主机本地的目录。 本地目录就存在一个隐患：如果 Docker Host 宕机了，如何恢复容器？ 一个办法就是定期备份数据，但这种方案还是会丢失从上次备份到宕机这段时间的数据。更好的方案是由专门的 storage provider 提供 volume，Docker 从 provider 那里获取 volume 并挂载到容器。这样即使 Host 挂了，也可以立刻在其他可用 Host 上启动相同镜像的容器，挂载之前使用的 volume，这样就不会有数据丢失。\n\n还有一个容易混淆的存储方案，是通过在Docker主机上挂载共享文件系统（例如Ceph，GlusterFS，NFS）。如下图所示：\n\n![](.\\Chart_Multi-Host-Persistence-Shared-Among-Containers.png)\n\n在运行Docker容器的每个主机上配置分布式文件系统。通过创建一致的命名约定和统一命名空间，所有正在运行的容器都可以访问底层的共享存储后端。共享文件系统存储方案利用分布式文件系统与显式存储技术相结合。由于挂载点在所有节点上都可用，因此可以利用它在容器之间创建共享挂载点。这种方案其实是属于选项二的一种变种。\n\n尽管该方案对于特定用例而言，是Docker 存储方案的一个有价值的补充，但它具有限制容器到特定主机的可移植性的显着缺点。它也没有利用针对数据密集型工作负载优化的专用存储后端。为了解决这些限制，volume plugins 被添加到Docker中，将容器的功能扩展到各种存储后端，并且而无需强制更改应用程序设计或部署体系结构。 这就是我们下节将详细介绍的 Volume Driver。\n\n##  3.3 Volume Driver\n\n假设有两个 Dokcer 主机，Host1 运行了一个 MySQL 容器，为了保护数据，data volume 由 storage provider 提供，如下图所示。 \n\n![](.\\storage-provider-1.png)\n\n当 Host1 发生故障，我们会在 Host2 上启动相同的 MySQL 镜像，并挂载 data volume。 \n\n![](.\\storage-provider-2.png)\n\nDocker 是如何实现这个跨主机管理 data volume 方案的呢？\n\n**答案是 volume driver。**\n\n利用 Docker 的 volume driver，由外部 storage provider 管理和提供 volume，所有 Docker 主机 volume 将挂载到各个副本。任何一个 data volume 都是由 driver 管理的，创建 volume 时如果不特别指定，将使用 `local` 类型的 driver，即从 Docker Host 的本地目录中分配存储空间。如果要支持跨主机的 volume，则需要使用第三方 driver。\n\n目前已经有很多可用的 driver，比如使用 Azure File Storage 的 driver，使用 GlusterFS 的 driver，完整的列表可参考 https://docs.docker.com/engine/extend/legacy_plugins/#volume-plugins \n\n![](.\\Docker-Volume-Plugin-Architecture.png)\n\nVolume drivers 让我们将应用程序与底层存储系统隔离。例如，如果原先服务使用一个 NFS驱动器，现在可以在不改变应用程序的逻辑下，直接更新服务使用不同的驱动程序，例如将数据存储在云端,。 \n\n通过`docker volume create ` 命令可以直接创建volume，同时在参数中指定driver 类型。\t下面的例子使用vieux/sshfs  这种类型的Volume drivers。首先创建一个单独的 volume，然后创建容器引用该volume。\n\n### 3.3.1 安装driver插件 \n\n这个示例假设有两个节点，两个节点直接可以通过ssh互相连接。在Docker 主机上，安装`vieux/sshfs` 插件。\n\n```docker plugin install --grant-all-permissions vieux/sshfs\ndocker plugin install --grant-all-permissions vieux/sshfs\n```\n\n### 3.3.2 创建volume\n\n本例中指定一个SSH密码，但如果两个主机配置了共享密钥，可以省略的密码。每个volume driver 可以有零个或多个可配置选项，每一个都指定使用 -o 标志。 \n\n```\ndocker volume create --driver vieux/sshfs \\\n  -o sshcmd=test@node2:/home/test \\\n  -o password=testpassword \\\n  sshvolume\n```\n\n### 3.3.3 使用volume\n\n本例中指定一个SSH密码,但如果两个主机配置了共享密钥,您可以省略的密码。每个卷司机可以有零个或多个可配置的选项。如果 **volume driver**  需要传递参数选项,你必须使用 **--mount**  挂载卷,而不是使用 -v。 \n\n```\n$ docker run -d \\\n  --name sshfs-container \\\n  --volume-driver vieux/sshfs \\\n  --mount src=sshvolume,target=/app,volume-opt=sshcmd=test@node2:/home/test,volume-opt=password=testpassword \\\n  nginx:latest\n```\n\n关于volume driver更多的介绍，可以查看官方文档：https://docs.docker.com/storage/volumes/#use-a-volume-driver\n\nhttps://docs.docker.com/storage/volumes/\n\n# 4.Docker Swarm 服务架构\n\n我们知道，Docker 场景下，应用是以容器的形式运行并提供服务的。而在Swarm场景下，任务是一个个具体的容器，通过抽象出服务的概念，来管理具有一定关联关系的任务容器。 \n\n为了将某一个服务封装为 Service 在Swarm 中部署执行，我们需要通过指定容器 image 以及需要在容器中执行的 Commands 来创建你的 Service，除此之外，还需要配置如下选项，\n\n- 指定可以在 Swarm 之外可以被访问的服务端口号 port\n- 指定加入某个 Overlay 网络以便 Service 与 Service 之间可以建立连接并通讯，\n- 指定该 Service 所要使用的 CPU 和 内存的大小，\n- 指定一个滚动更新的策略 (Rolling Update Policy)\n- 指定多少个 Task 的副本 (replicas) 在 Swarm 集群中同时存在\n\n如下所示，`docker-swarm.yml`文件是一个YAML文件，它定义了如何Docker容器在生产中应表现。\n\n```\nversion: \"3\"\nservices:\n  web:\n    # replace username/repo:tag with your name and image details\n    image: username/repo:tag\n    deploy:\n      replicas: 5\n      resources:\n        limits:\n          cpus: \"0.1\"\n          memory: 50M\n      restart_policy:\n        condition: on-failure\n    ports:\n      - \"4000:80\"\n    networks:\n      - webnet\nnetworks:\n  webnet:\n```\n\n该`docker-swarm.yml`文件告诉Docker执行以下操作：\n\n- 拉username/repo 仓库中拉取镜像。\n- 将该映像的5个实例作为一个被调用的服务运行`web`，限制每个实例使用，最多10％的CPU（跨所有内核）和50MB的RAM。\n- 如果一个失败，立即重启容器。\n- 将主机上的端口4000映射到`web`端口80。\n- 指示`web`容器通过称为负载平衡的网络共享端口80 `webnet`。（在内部，容器本身`web`在短暂的端口发布到 80端口。）\n- `webnet`使用默认设置（负载平衡的覆盖网络）定义网络。\n\n## 4.1 服务和任务\n\n我们一般使用如下命令来部署一个服务到swarm中：\n\n`docker stack deploy -c docker-swarm.yml myservice。`\n\ndocker-swarm.yml中定义了服务所需要达到的状态。我们不需要告诉swarm如何做，我们只需要告诉swarm 我们想要的服务状态是什么，swarm最终会帮我们达到这个状态。\n\nswarm管理器将服务定义为服务的所需状态。swarm将群中节点上的服务调度为一个或多个副本任务。任务在群集中的节点上彼此独立地运行。\n\n例如，想要在HTTP侦听器的三个实例之间进行负载平衡。下图显示了具有三个副本的HTTP侦听器服务。监听器的三个实例中的每一个都是集群中的任务。容器是一个孤立的过程。在swarm模式模型中，每个任务只调用一个容器。任务类似于调度程序放置容器的“槽”。容器生效后，调度程序会识别该任务处于运行状态。如果容器未通过运行状况检查或终止，则任务将终止。\n\n![](./services-diagram.png)\n\nServices，Tasks 和 Containers 之间的关系可以用上面这张图来描述。来看这张图的逻辑，表示用户想要通过 Manager 节点部署一个有 3 个副本的 Nginx 的 Service，Manager 节点接收到用户的 Service definition 后，便开始对该 Service 进行调度，将会在当前可用的Worker（或者Manager ）节点中启动相应的 Tasks 以及相关的副本；所以可以看到，Service 实际上是 Task 的定义，而 Task 则是执行在节点上的程序。\n\nTask 是什么呢？其实就是一个 Container，只是，在 Swarm 中，每个 Task 都有自己的名字和编号，如图，比如 nginx.1、nginx.2 和 nginx.3，这些 Container 各自运行在各自的 node 上，当然，一个 node 上可以运行多个 Container；\n\n## 4.2 任务调度\n\n任务是swarm集群内调度的原子单位。在创建或更新服务时，声明定义服务状态，协调器通过调度任务来实现所需的状态。例如上例的服务，该服务指示协调器始终保持三个HTTP侦听器实例的运行。协调器通过创建三个任务来响应。每个任务都是调度程序通过生成容器来填充的插槽。容器是任务的实例化。如果HTTP侦听器任务随后未通过其运行状况检查或崩溃，则协调器会创建一个新的副本任务，该任务会生成一个新容器。\n\n任务是单向机制。它通过一系列状态单调进行：已分配，准备，运行等。如果任务失败，则协调器将删除任务及其容器，然后根据服务指定的所需状态创建新任务以替换它。\n\nDocker swarm mode 模式的底层技术实际上就是指的是调度器( scheduler )和编排器( orchestrator )；下面这张图展示了 Swarm mode 如何从一个 Service 的创建请求并且成功将该 Service 分发到 worker 节点上执行的过程。\n\n![](.\\swarm-service-lifecycle.png)\n\n\n\n- 首先，看上半部分Swarm manager \n  1. 用户通过 Docker Engine Client 使用命令 docker service create 提交 Service definition，\n  2. 根据 Service definition 创建相应的 Task，\n  3. 为 Task 分配 IP 地址，\n     注意，这是分配运行在 Swarm 集群中 Container 的 IP 地址，该 IP 地址最佳的分配地点是在这里，因为 Manager 节点上保存得有最新最全的 Tasks 的状态信息，为了保证不与其他的 Task 分配到相同的 IP，所以在这里就将 IP 地址给初始化好；\n  4. 将 Task 分发到 Node 上，可以是 Manager 节点也可以使 Worker 节点，\n  5. 对 Worker 节点进行相应的初始化使得它可以执行 Task\n- 接着，看下半部分Swarm  Work\n  1. 首先连接 manager 的分配器( scheduler)检查该 task\n  2. 验证通过以后，便开始通过 Worker 节点上的执行器( executor )执行；\n\n注意，上述 task 的执行过程是一种单向机制，比如它会按顺序的依次经历 assigned, prepared 和 running 等执行状态，不过在某些特殊情况下，在执行过程中，某个 task 失败了( fails )，编排器( orchestrator )直接将该 task 以及它的 container 给删除掉，然后在其它节点上另外创建并执行该 task；\n\n## 4.3 任务状态\n\n通过运行`docker service ps <service-name>`以获取任务的状态。该 `CURRENT STATE`字段显示任务的状态以及任务的持续时间 。\n\n```\n[root@swarm01 ~]# docker service ps myweb_web\nID                  NAME                IMAGE               NODE                DESIRED STATE       CURRENT STATE            ERROR                         PORTS\n52lolsfykj4t        myweb_web.1         httpd:v2            swarm03             Running             Running 16 minutes ago\nzwpufyfacagy         \\_ myweb_web.1     httpd:v2            swarm03             Shutdown            Failed 16 minutes ago    \"task: non-zero exit (255)\"\n00fhp9k1byf6        myweb_web.2         httpd:v2            swarm02             Running             Running 16 minutes ago\nkgtiyw39xpcc         \\_ myweb_web.2     httpd:v2            swarm02             Shutdown            Failed 17 minutes ago    \"task: non-zero exit (255)\"\nvefk4yn0b00m        myweb_web.3         httpd:v2            swarm01             Running             Running 16 minutes ago\npnj4z0j281ng         \\_ myweb_web.3     httpd:v2            swarm01             Shutdown            Failed 17 minutes ago    \"task: non-zero exit (255)\"\n```\n\n任务具有如下几种状态：\n\n| 任务状态    | 描述                                                         |\n| ----------- | ------------------------------------------------------------ |\n| `NEW`       | 任务已初始化。                                               |\n| `PENDING`   | 分配了任务的资源。                                           |\n| `ASSIGNED`  | Docker将任务分配给节点。                                     |\n| `ACCEPTED`  | 该任务被工作节点接受。如果工作节点拒绝该任务，则状态将更改为`REJECTED`。 |\n| `PREPARING` | Docker正在准备任务。                                         |\n| `STARTING`  | Docker正在开始这项任务。                                     |\n| `RUNNING`   | 任务正在执行。                                               |\n| `COMPLETE`  | 任务退出时没有错误代码。                                     |\n| `FAILED`    | 任务退出并显示错误代码。                                     |\n| `SHUTDOWN`  | Docker请求关闭任务。                                         |\n| `REJECTED`  | 工作节点拒绝了该任务。                                       |\n| `ORPHANED`  | 该节点停机时间过长。                                         |\n| `REMOVE`    | 该任务不是终端，但关联的服务已被删除或缩小。                 |\n\n如果在某个 service 的[调度过程](http://www.shangyang.me/2018/02/01/docker-swarm-03-architect/#%E8%B0%83%E5%BA%A6%E8%BF%87%E7%A8%8B)中，发现当前没有可用的 node 资源可以执行该 service，这个时候，该 service 的状态将会保持为 pending 的状态；下面，我们来看一些例子可能使得 service 维持在 pending 状态，\n\n- 如果当所有的节点都被停止或者进入了 drained 状态，这个时候，你试图创建一个 service，该 service 将会一直保持 pending 状态直到当前某个节点可用为止；不过要注意的是，第一个恢复的 node 将会得到所有的 task 调度请求，接收并执行，因此，这种情况在 production 环境上要尽量避免；\n- 你可以为你的 service 设置执行所需的内存大小，在调度的过程当中，发现没有一个 node 能够满足你的内存请求，那么你当前的 service 将会一直处于 pending 状态直到有满足需求的 node 出现；\n- 可以对服务施加放置约束，并且可能无法在给定时间遵守约束。 \n\n上面的例子都表明一个事实，就是你期望的执行条件与 Swarm 现有的可用资源的情况不匹配，不吻合，因此，作为 Swarm 的管理者，应该考虑对 Swarm 集群整体进行扩容；\n\n## 4.4 服务副本与全局服务\n\nDocker Swarm 有两种类型的服务部署模式：复制和全局。\n\nSwarm通过`--mode`选项设置服务类型，提供了两种模式：一种是replicated，我们可以指定服务Task的个数（也就是需要创建几个冗余副本），这也是Swarm默认使用的服务类型；另一种是global，这样会在Swarm集群的每个Node上都创建一个服务。如下图所示（出自Docker官网），是一个包含replicated和global模式的Swarm集群： \n\n![](.\\docker-swarm-replicated-vs-global.png)\n\n\n\n上图中，黄色表示的replicated模式下的Service Replicas，灰色表示global模式下Service的分布。 \n\n在Swarm mode下使用Docker，可以实现部署运行服务、服务扩容缩容、删除服务、滚动更新等功能。\n\n## 4.5 调度策略\n\nSwarm提供了几种不同的方法来控制服务任务可以在哪些节点上运行。\n\n- 可以指定服务是需要运行特定数量的副本还是应该在每个工作节点上全局运行。请参阅2.4服务副本与全局服务。\n\n- 您可以配置服务的 CPU或内存要求，该服务仅在满足这些要求的节点上运行。\n\n- 通过设置约束标签 --constraint ，您可以将服务配置为仅在具有特定（任意）元数据集的节点上运行，并且如果不存在适当的节点，则会导致部署失败。例如，您可以指定您的服务只应在任意标签`pci_compliant`设置为的节点上运行 `true`。\n\n- 通过设置首选项 --preferences，您可以将具有一系列值的任意标签应用于每个节点，并使用算法将服务的任务分布到这些节点上。目前，唯一支持的算法是`spread`，该算法将尝试均匀放置它们。例如，如果`rack`使用值为1-10 的标签标记每个节点，然后指定键入的放置首选项`rack`，服务任务将尽可能均匀地放置在具有标签的所有节点上。如果具有`rack`标签的节点个数，大于任务个数，那么则会根据rack 值的大小排序，然后优先选择排名靠前的节点。\n\n  与约束不同，放置首选项是尽力而为，如果没有节点可以满足首选项，则服务不会失败。如果为服务指定放置首选项，则当集群管理器决定哪些节点应运行服务任务时，与该首选项匹配的节点的排名会更高。其他因素，例如服务的高可用性，也是计划节点运行服务任务的因素。例如，如果您有N个节点带有机架标签（然后是其他一些节点），并且您的服务配置为运行N + 1个副本，那么+1将在一个尚未拥有该服务的节点上进行调度，如果无论该节点是否具有`rack`标签。\n\n\n\n## 4.6 服务管理\n\ndocker swarm 还可以使用 stack 这一模型，用来部署管理和关联不同的服务。stack 功能比service 还要强大，因为它具备多服务编排部署的能力。**stack file** 是一种 yaml 格式的文件，类似于 docker-compose.yml 文件，它定义了一个或多个服务，并定义了服务的环境变量、部署标签、容器数量以及相关的环境特定配置等。 \n\n以下实验仅仅创建利用stack 创建单一服务。\n\n### 4.6.1 创建服务\n\n服务配置文件docker-swarm.yml ，内容如下\n\n```\nversion: '3'\nservices:\n  web:\n    image: \"httpd:v1\"\n    deploy:\n      replicas: 3\n      resources:\n        limits:\n          cpus: \"0.5\"\n          memory: 256M\n      restart_policy:\n        condition: on-failure\n    ports:\n     - \"7080:80\"\n    volumes:\n     - /var/www/html:/usr/local/apache2/htdocs\n```\n\nimage: \"httpd\" ：httpd是docker hub 下载的镜像。提供简单的httpd服务，此处用来做服务都高可用验证。\n\nreplicas: 3 表示该服务副本有3个，可对服务副本进行扩缩容，后续会讲到。\n\n/var/www/html:/usr/local/apache2/htdocs ：表示httpd 首页的内容。各个swarm节点上展示的内容分别是其主机名信息。\n\n在docker-swarm.yml 同级目录下，执行命令\n\n```\n[root@swarm01 swarm]# docker stack deploy -c docker-swarm.yml myservice\nCreating network myservice_default\nCreating service myservice_web\n```\n\n### 4.6.2 查看服务\n\n查看服务创建是否成功。\n\n```\n[root@swarm01 swarm]# docker stack ls\nNAME                SERVICES\nmyservice           1\n[root@swarm01 swarm]# docker stack services myservice\nID                  NAME                MODE                REPLICAS            IMAGE               PORTS\nkom8x49f7qv2        myservice_web       replicated          3/3                 httpd:latest        *:7080->80/tcp\n[root@swarm01 swarm]# docker stack ps myservice\nID                  NAME                IMAGE               NODE                DESIRED STATE       CURRENT STATE            ERROR               PORTS\nsybn1dur1wmw        myservice_web.1     httpd:latest        swarm03             Running             Running 45 seconds ago\nmlcb5r9vzv6z        myservice_web.2     httpd:latest        swarm03             Running             Running 45 seconds ago\nfw21lcu3zp83        myservice_web.3     httpd:latest        swarm01             Running             Running 45 seconds ago\n```\n\n可以看到 swarm01 上面运行了1个容器，swarm03上面运行了2个，而swarm02上面一个容器都没有，这是咋回事呢？哦，原来我们在创建集群过程中，对swarm02 做了状态变更的操作，将其设置为不运行任务。` docker node update  --availability drain  swarm02`. 查看集群状态。\n\n```\n[root@swarm01 swarm]# docker node ls\nID                            HOSTNAME            STATUS              AVAILABILITY        MANAGER STATUS      ENGINE VERSION\nepoic1y0vv830vnwbc6nnacjc *   swarm01             Ready               Active              Leader              18.03.1-ce\nnsqqv181e0r7mu77azixtqhzl     swarm02             Ready               Drain               Reachable           18.03.1-ce\nmkkxaeqergz8xw21bbkd47q1c     swarm03             Ready               Active              Reachable           18.03.1-ce\n```\n\n此时打开浏览器，输入HTTP URL。任意swarm集群节点都可以访问httpd服务。\n\n![](.\\swarm-web.png)\n\n需要说明的是，由于3个节点的/var/www/html 路径未做共享存储。因此swarm会随机显示一个swarm节点的网页内容。\n\n### 4.6.3 服务扩缩容\n\n修改docker-swarm.yml 配置字段 `replicas: 5` ，将容器个数提升到5个。由于实验环境资源有限，因此这里将swarm02状态变更为可以状态，用以分配容器。\n\n` docker node update  --availability active swarm02`.\n\n重新执行部署命令`docker stack deploy -c docker-swarm.yml myservice`\n\n也可以通过命令直接对服务进行操作，但是不推荐。 `docker service scale 服务ID=服务个数`\n\n```\n[root@swarm01 swarm]# docker stack ps myservice\nID                  NAME                IMAGE               NODE                DESIRED STATE       CURRENT STATE                    ERROR               PORTS\nunwepav5z9nz        myservice_web.1     httpd:v1            swarm02             Running             Running less than a second ago\n3j5o1tktcg1l        myservice_web.2     httpd:v1            swarm01             Running             Preparing 3 seconds ago\nwl1unt6lynft        myservice_web.3     httpd:v1            swarm03             Running             Running less than a second ago\n[root@swarm01 swarm]# vi docker-swarm.yml\n[root@swarm01 swarm]# docker stack deploy -c docker-swarm.yml myservice\nUpdating service myservice_web (id: 85quzwi5k0btkn5wlht4jbco9)\nimage httpd:v1 could not be accessed on a registry to record\nits digest. Each node will access httpd:v1 independently,\npossibly leading to different nodes running different\nversions of the image.\n\n[root@swarm01 swarm]# docker stack ps myservice\nID                  NAME                IMAGE               NODE                DESIRED STATE       CURRENT STATE                    ERROR               PORTS\nunwepav5z9nz        myservice_web.1     httpd:v1            swarm02             Running             Running 24 seconds ago\n3j5o1tktcg1l        myservice_web.2     httpd:v1            swarm01             Running             Running 31 seconds ago\nwl1unt6lynft        myservice_web.3     httpd:v1            swarm03             Running             Running 24 seconds ago\nujedjxbuffxp        myservice_web.4     httpd:v1            swarm01             Running             Running less than a second ago\nml9qvjh2add7        myservice_web.5     httpd:v1            swarm02             Running             Running less than a second ago\n```\n\n同样，对于服务缩容，也仅需要修改`replicas:` 配置即可。这里不再演示。\n\n### 4.6.4 删除服务\n\n例如，删除myredis应用服务，执行docker service rm myredis，则应用服务myredis的全部副本都会被删除。 \n\n本例使用的是stack ，删除stack ，即可删除其下的服务。没错，docker语法非常类似,`docker stack rm myservice`。\n\n需要注意的是，对swarm上面服务的操作，都必须在manager节点上运行。\n\n### 4.6.5 服务升降级\n\n服务升降级，对应到docker中，则利用了容器镜像的更新，升级很好理解，直接把tag 标签版本往上提。那么降级，其实也可以理解为另外一种“升级“，只是镜像内容是上一个版本的而已。对于镜像标签的命令应该遵循一套严格的规范，这里不再赘述。\n\n先介绍利用service 命令行工具的用法。\n\n服务的滚动更新，这里我参考官网文档的例子说明。在Manager Node上执行如下命令：\n\n```\ndocker service create --replicas 3  --name redis --update-delay 10s redis:3.0.6\n```\n\n上面通过指定 --update-delay 选项，表示需要进行更新的服务，每次成功部署一个，延迟10秒钟，然后再更新下一个服务。如果某个服务更新失败，则Swarm的调度器就会暂停本次服务的部署更新。\n\n另外，也可以更新已经部署的服务所在容器中使用的Image的版本，例如执行如下命令：\n\n将Redis服务对应的Image版本有3.0.6更新为3.0.7，同样，如果更新失败，则暂停本次更新。\n\n那么，stack file 如何定义滚动更新呢？\n\n```\n      update_config:\n        parallelism: 2\n        delay: 10s\n```\n\n修改docker-swarm.yml 如下\n\n```\nversion: '3'\nservices:\n  web:\n    image: \"httpd:v2\"\n    deploy:\n      replicas: 3\n      update_config:\n        parallelism: 2\n        delay: 30s\n      resources:\n        limits:\n          cpus: \"0.5\"\n          memory: 256M\n      restart_policy:\n        condition: on-failure\n    ports:\n     - \"7080:80\"\n    volumes:\n     - /var/www/html:/usr/local/apache2/htdocs\n```\n\nparallelism 表示同时升级的个数，delay 则表示间隔多长时间升级。\n\n同时将httpd 由v1 升级到v2，这里我们使用同一个镜像，只是tag 修改了一下。查看服务状态\n\n```\n[root@swarm01 swarm]# docker stack ps myservice\nID                  NAME                  IMAGE               NODE                DESIRED STATE       CURRENT STATE                 ERROR               PORTS\nqk8pvvc9nmv0        myservice_web.1       httpd:v2            swarm01             Running             Running 56 seconds ago\nunwepav5z9nz         \\_ myservice_web.1   httpd:v1            swarm02             Shutdown            Shutdown 48 seconds ago\nuyj0pm1df9hl        myservice_web.2       httpd:v2            swarm02             Running             Running 46 seconds ago\n3j5o1tktcg1l         \\_ myservice_web.2   httpd:v1            swarm01             Shutdown            Shutdown about a minute ago\nqdjov5zf5alb        myservice_web.3       httpd:v2            swarm03             Running             Running 10 seconds ago\nwl1unt6lynft         \\_ myservice_web.3   httpd:v1            swarm03             Shutdown            Shutdown 12 seconds ago\n```\n\n可以看到同时只有2个容器在升级，同时第3个容器也是在间隔了30s 之后，才开始升级。\n\n --filter 过滤条件`docker stack ps  --filter  \"desired-state=running\"  myservice`\n\n### 4.6.6 服务健康检查\n\n对于容器而言，最简单的健康检查是进程级的健康检查，即检验进程是否存活。Docker Daemon会自动监控容器中的PID1进程，如果docker run命令中指明了restart policy，可以根据策略自动重启已结束的容器。在很多实际场景下，仅使用进程级健康检查机制还远远不够。比如，容器进程虽然依旧运行却由于应用死锁无法继续响应用户请求，这样的问题是无法通过进程监控发现的 。\n\n下面先介绍Docker容器健康检查机制，之后再介绍Docker Swarm mode的新特性。\n\n**Docker 原生健康检查能力**\n\n而自 1.12 版本之后，Docker 引入了原生的健康检查实现，可以在Dockerfile中声明应用自身的健康检测配置。HEALTHCHECK 指令声明了健康检测命令，用这个命令来判断容器主进程的服务状态是否正常，从而比较真实的反应容器实际状态。\n\nHEALTHCHECK 指令格式：\n\n- HEALTHCHECK [选项] CMD <命令>：设置检查容器健康状况的命令\n- HEALTHCHECK NONE：如果基础镜像有健康检查指令，使用这行可以屏蔽掉\n\n注：在Dockerfile中 HEALTHCHECK 只可以出现一次，如果写了多个，只有最后一个生效。\n\n使用包含 HEALTHCHECK 指令的dockerfile构建出来的镜像，在实例化Docker容器的时候，就具备了健康状态检查的功能。启动容器后会自动进行健康检查。\n\n> HEALTHCHECK 支持下列选项：\n>\n> - interval=<间隔>：两次健康检查的间隔，默认为 30 秒;\n> - timeout=<间隔>：健康检查命令运行超时时间，如果超过这个时间，本次健康检查就被视为失败，默认 30 秒;\n> - retries=<次数>：当连续失败指定次数后，则将容器状态视为 unhealthy，默认 3 次。\n> - start-period=<间隔>: 应用的启动的初始化时间，在启动过程中的健康检查失效不会计入，默认 0 秒; (从17.05)引入\n\n在 HEALTHCHECK [选项] CMD 后面的命令，格式和 ENTRYPOINT 一样，分为 shell 格式，和 exec 格式。命令的返回值决定了该次健康检查的成功与否：\n\n- 0：成功;\n- 1：失败;\n- 2：保留值\n\n容器启动之后，初始状态会为 starting (启动中)。Docker Engine会等待 interval 时间，开始执行健康检查命令，并周期性执行。如果单次检查返回值非0或者运行需要比指定 timeout 时间还长，则本次检查被认为失败。如果健康检查连续失败超过了 retries 重试次数，状态就会变为 unhealthy (不健康)。\n\n注：\n\n- 一旦有一次健康检查成功，Docker会将容器置回 healthy (健康)状态\n- 当容器的健康状态发生变化时，Docker Engine会发出一个 health_status 事件。\n\n假设我们有个镜像是个最简单的 Web 服务，我们希望增加健康检查来判断其 Web 服务是否在正常工作，我们可以用 curl来帮助判断，其 Dockerfile 的 HEALTHCHECK 可以这么写：\n\n```\nFROM elasticsearch:5.5 \n \nHEALTHCHECK --interval=5s --timeout=2s --retries=12 \\ \n  CMD curl --silent --fail localhost:9200/_cluster/health || exit 1\n```\n\n然后编译镜像，并运行\n\n```\ndocker build -t test/elasticsearch:5.5 . \ndocker run --rm -d \\ \n    --name=elasticsearch \\ \n    test/elasticsearch:5.5 \n```\n\n执行 docker ps容器检查命令，发现过了几秒之后，Elasticsearch容器从 starting 状态进入了 healthy 状态\n\n```\n$ docker ps \nCONTAINER ID        IMAGE                    COMMAND                  CREATED             STATUS                            PORTS                NAMES \nc9a6e68d4a7f        test/elasticsearch:5.5   \"/docker-entrypoin...\"   2 seconds ago       Up 2 seconds (health: starting)   9200/tcp, 9300/tcp   elasticsearch \n$ docker ps \nCONTAINER ID        IMAGE                    COMMAND                  CREATED             STATUS                    PORTS                NAMES \nc9a6e68d4a7f        test/elasticsearch:5.5   \"/docker-entrypoin...\"   14 seconds ago      Up 13 seconds (healthy)   9200/tcp, 9300/tcp   elasticsearch \n```\n\n**Docker Swarm健康检查能力**\n\n在Docker 1.13之后，在Docker Swarm mode中提供了对健康检查策略的支持。\n\n可以在 `docker service create` 命令中指明健康检查策略\n\n```\n$ docker service create -d \\\n    --name=elasticsearch \\\n    --health-cmd=\"curl --silent --fail localhost:9200/_cluster/health || exit 1\" \\\n    --health-interval=5s \\\n    --health-retries=12 \\\n    --health-timeout=2s \\\n    elasticsearch\n```\n\n在Swarm模式下，Swarm manager会监控服务task的健康状态，如果容器进入 `unhealthy` 状态，它会停止容器并且重新启动一个新容器来取代它。这个过程中会自动更新服务的 load balancer (routing mesh) 后端或者 DNS记录，可以保障服务的可用性。\n\n在1.13版本之后，在服务更新阶段也增加了对健康检查的支持，这样在新容器完全启动成功并进入健康状态之前，load balancer/DNS解析不会将请求发送给它。这样可以保证应用在更新过程中请求不会中断。\n\n![](.\\healthcheck.png)\n\ndocker 官方网站有些常用容器的健康检查的例子：https://github.com/docker-library/healthcheck\n\n# 5.附录\n\n## 5.1 参考文档\n\n官网文档：\n\nhttps://docs.docker.com/engine/swarm/\n\nhttps://docs.docker.com/get-started/part4/\n\nhttps://docs.docker.com/network/overlay/#publish-ports-on-an-overlay-network\n\n技术博客：\n\nhttp://www.dockerinfo.net/2552.html\n\nhttps://www.cnblogs.com/bigberg/p/8761047.html\n\nhttp://blog.51cto.com/cloudman/1968287 \n\nhttps://neuvector.com/network-security/docker-swarm-container-networking/\n\nhttp://www.uml.org.cn/yunjisuan/201708282.asp","slug":"docker-swarm-scheduler","published":1,"updated":"2018-10-20T08:39:42.664Z","_id":"cjnh6x618000164b2knb5mzjw","comments":1,"layout":"post","photos":[],"link":"","content":"<p>本文主要介绍docker swarm 对于容器应用的调度管理分析。Docker swarm 1.12.0  是一个非常重要的版本，1.12.0之前的版本 docker swarm 是一个独立的项目，需要额外下载swarm镜像进行安装。1.12.0  之后的版本，swarm功能直接继承到docker engine 当中。直接使用docker CLI 可创建一个swarm 集群、可在swarm 集群中部署应用程序，同时具备管理能力。 </p>\n<p>关于swarm的详细介绍，可参考官方文档：<a href=\"https://docs.docker.com/engine/swarm/\" target=\"_blank\" rel=\"noopener\">https://docs.docker.com/engine/swarm/</a></p>\n<h1 id=\"1-Docker-Swarm-集群架构\"><a href=\"#1-Docker-Swarm-集群架构\" class=\"headerlink\" title=\"1.Docker Swarm 集群架构\"></a>1.Docker Swarm 集群架构</h1><p>Swarm 集群中，有两种类型的节点：Manager和Worker。</p>\n<p>要部署服务到swarm，需要将服务定义提交给Manager节点。Manager节点将称为work的工作单元分派 给Worker节点。</p>\n<p><img src=\"\\img\\swarm-node.png\" alt=\"\"></p>\n<a id=\"more\"></a>\n<h2 id=\"1-1-Manager-Node\"><a href=\"#1-1-Manager-Node\" class=\"headerlink\" title=\"1.1 Manager Node\"></a>1.1 Manager Node</h2><p>Manager节点处理集群管理任务：</p>\n<ul>\n<li>维护集群状态</li>\n<li>调度服务</li>\n<li>为 Swarm 提供外部可调用的 API 接口</li>\n</ul>\n<p>Manager 节点需要时刻维护和保存当前 Swarm 集群中各个节点的一致性状态，这里主要是指各个 Tasks 的执行的状态和其它节点的状态。因为 Swarm 集群是一个典型的分布式集群，在保证一致性上，Manager 节点采用 <a href=\"https://raft.github.io/raft.pdf\" target=\"_blank\" rel=\"noopener\">Raft</a> 协议来保证分布式场景下的数据一致性。</p>\n<p>通常为了保证 Manager 节点的高可用，Docker 建议采用奇数个 Manager 节点，这样的话，你可以在 Manager 失败的时候不用关机维护，Docker 官方给出如下的建议：</p>\n<ul>\n<li>3 个 Manager 节点最多可以同时容忍 1 个 Manager 节点失效的情况下保证高可用；</li>\n<li>5 个 Manager 节点最多可以同时容忍 2 个 Manager 节点失效的情况下保证高可用；</li>\n<li>N 个 Manager 节点最多可以同时容忍 (N−1)/2个 Manager 节点失效的情况下保证高可用；</li>\n<li>最多最多的情况下，使用 7 个 Manager 节点就够了，否则反而会降低集群的性能了。</li>\n</ul>\n<blockquote>\n<p><strong>重要说明</strong>：添加更多管理器并不意味着可扩展性更高或性能更高。一般而言，情况正好相反。</p>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>群体大小</th>\n<th>多数</th>\n<th>容错</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>1</td>\n<td>1</td>\n<td>0</td>\n</tr>\n<tr>\n<td>2</td>\n<td>2</td>\n<td>0</td>\n</tr>\n<tr>\n<td><strong>3</strong></td>\n<td>2</td>\n<td><strong>1</strong></td>\n</tr>\n<tr>\n<td>4</td>\n<td>3</td>\n<td>1</td>\n</tr>\n<tr>\n<td><strong>5</strong></td>\n<td>3</td>\n<td><strong>2</strong></td>\n</tr>\n<tr>\n<td>6</td>\n<td>4</td>\n<td>2</td>\n</tr>\n<tr>\n<td><strong>7</strong></td>\n<td>4</td>\n<td><strong>3</strong></td>\n</tr>\n<tr>\n<td>8</td>\n<td>5</td>\n<td>3</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"1-2-Worker-Node\"><a href=\"#1-2-Worker-Node\" class=\"headerlink\" title=\"1.2 Worker Node\"></a>1.2 Worker Node</h2><p>Worker节点接收并执行从Manager节点分派的任务。默认情况下，Manager节点还将服务作为Worker节点运行，但也可以将它们配置为仅运行Manage任务。 可将Manager可用性设置为<code>Drain</code> ，这样服务就不会被分配到Manager节点上运行了。</p>\n<p>Worker节点不参与Raft分布状态，做出调度决策或提供群模式HTTP API。</p>\n<p>通过 docker node promote 命令将一个 Worker 节点提升为 Manager 节点。通常情况下，该命令使用在维护的过程中，需要将 Manager 节点占时下线进行维护操作。同样可以使用 docker node demote 将某个 manager 节点降级为 worker 节点。 </p>\n<h2 id=\"1-3-小结\"><a href=\"#1-3-小结\" class=\"headerlink\" title=\"1.3 小结\"></a>1.3 小结</h2><p>综上可知， Swarm 集群的管理工作是由manager节点实现。如上图所示，manager节点实现的功能主要包括：node discovery，scheduler,cluster管理等。同时，为了保证Manager 节点的高可用，Manager 节点需要时刻维护和保存当前 Swarm 集群中各个节点的一致性状态。在保证一致性上，Manager 节点采用 <a href=\"https://raft.github.io/raft.pdf\" target=\"_blank\" rel=\"noopener\">Raft</a> 协议来保证分布式场景下的数据一致性；</p>\n<p>Docker Swarm内置了Raft一致性算法，可以保证分布式系统的数据保持一致性同步。Etcd, Consul等高可用键值存储系统也是采用了这种算法。这个算法的作用简单点说就是随时保证集群中有一个Leader，由Leader接收数据更新，再同步到其他各个Follower节点。在Swarm中的作用表现为当一个Leader 节点 down掉时，系统会立即选取出另一个Leader节点，由于这个节点同步了之前节点的所有数据，所以可以无缝地管理集群。</p>\n<p>Raft的详细解释可以参考<a href=\"http://thesecretlivesofdata.com/raft/\" target=\"_blank\" rel=\"noopener\">《The Secret Lives of Data–Raft: Understandable Distributed Consensus》</a>。</p>\n<p>为保证Magnger高可用，一般部署3-7个Manager 节点。而在云桌面的场景下，可能一个集群就只有两台服务器。显然此时是无法满足Manager 3个节点的高可用要求。因此我们面对的挑战就是如何修改swarm Manager Raft相关部分代码，使其在只有2个节点的情况下，也可以保证一个节点宕机的高可用。</p>\n<h1 id=\"2-Docker-Swarm-网络架构\"><a href=\"#2-Docker-Swarm-网络架构\" class=\"headerlink\" title=\"2.Docker Swarm 网络架构\"></a>2.Docker Swarm 网络架构</h1><h2 id=\"2-1-网络概述\"><a href=\"#2-1-网络概述\" class=\"headerlink\" title=\"2.1 网络概述\"></a>2.1 网络概述</h2><p>我们知道，Docker 的几种网络方案：none、host、bridge 和 joined 容器，它们解决了单个 Docker Host 内容器通信的问题。那么跨主机容器间通信的方案又有哪些呢？</p>\n<p>跨主机网络方案包括：</p>\n<ol>\n<li>docker 原生的 overlay 和 macvlan。</li>\n<li>第三方方案：常用的包括 flannel、weave 和 calico。</li>\n</ol>\n<p>Docker 网络是一个非常活跃的技术领域，不断有新的方案开发出来，那么要问个非常重要的问题了： 如此众多的方案是如何与 docker 集成在一起的？答案是：libnetwork 以及 CNM。</p>\n<p>libnetwork 是 docker 容器网络库，最核心的内容是其定义的 Container Network Model (CNM)，这个模型对容器网络进行了抽象，由以下三类组件组成：</p>\n<ol>\n<li>Sandbox：Sandbox 是容器的网络栈，包含容器的 interface、路由表和 DNS 设置。 Linux Network Namespace 是 Sandbox 的标准实现。Sandbox 可以包含来自不同 Network 的 Endpoint。</li>\n<li>Endpoint：Endpoint 的作用是将 Sandbox 接入 Network。Endpoint 的典型实现是 veth pair，后面我们会举例。一个 Endpoint 只能属于一个网络，也只能属于一个 Sandbox。</li>\n<li>Network：Network 包含一组 Endpoint，同一 Network 的 Endpoint 可以直接通信。Network 的实现可以是 Linux Bridge、VLAN 等。</li>\n</ol>\n<p>如图所示两个容器，一个容器一个 Sandbox，每个 Sandbox 都有一个 Endpoint 连接到 Network 1，第二个 Sandbox 还有一个 Endpoint 将其接入 Network 2.</p>\n<p>libnetwork CNM 定义了 docker 容器的网络模型，按照该模型开发出的 driver 就能与 docker daemon 协同工作，实现容器网络。docker 原生的 driver 包括 none、bridge、overlay 和 macvlan，第三方 driver 包括 flannel、weave、calico 等。</p>\n<h2 id=\"2-2-Overlay网络\"><a href=\"#2-2-Overlay网络\" class=\"headerlink\" title=\"2.2 Overlay网络\"></a>2.2 Overlay网络</h2><p>Docker Swarm 内置的跨主机容器通信方案是overlay网络，这是一个基于VxLAN 协议的网络实现。VxLAN 可将二层数据封装到 UDP 进行传输，VxLAN 提供与 VLAN 相同的以太网二层服务，但是拥有更强的扩展性和灵活性。 overlay 通过虚拟出一个子网，让处于不同主机的容器能透明地使用这个子网。所以跨主机的容器通信就变成了在同一个子网下的容器通信，看上去就像是同一主机下的bridge网络通信。</p>\n<p><img src=\"\\img\\overlay-vxlan.png\" alt=\"1534939506173\"></p>\n<p>根据vxlan的作用知道，它是要在三层网络中虚拟出二层网络，即跨网段建立虚拟子网。简单的理解就是把发送到虚拟子网地址<code>10.0.0.3</code>的报文封装为发送到真实IP<code>192.168.1.3</code>的报文。这必然会有更大的数据开销，但却简化了集群的网络连接，让分布在不同主机的容器好像都在同一个主机上一样 。</p>\n<p><code>overlay</code>网络会创建多个Docker主机之间的分布式网络。该网络位于（覆盖）特定于主机的网络之上，允许连接到它的容器（包括群集服务容器）安全地进行通信。Docker透明地处理每个数据包与正确的Docker守护程序主机和正确的目标容器的路由。</p>\n<p>初始化swarm或将Docker主机加入现有swarm时，会在该Docker主机上创建两个新网络：</p>\n<ul>\n<li>ingress overlay 网络，处理与swarm集群服务相关的控制和数据流量。创建群组服务并且不将其连接到用户定义的覆盖网络时，服务将默认连接到ingress overlay网络。集群中只能有一个ingress overlay 网络。</li>\n<li>docker_gwbridge 桥接网络，它将各个Docker守护程序连接到参与该群集的其他Docker守护进程。同时该docker_gwbridge 网络将为主机上的容器提供访问外网的能力。</li>\n</ul>\n<p>服务或容器一次可以连接到多个网络。服务或容器只能通过它们各自连接的网络进行通信。</p>\n<h3 id=\"2-2-1-创建overlay网络\"><a href=\"#2-2-1-创建overlay网络\" class=\"headerlink\" title=\"2.2.1 创建overlay网络\"></a>2.2.1 创建overlay网络</h3><blockquote>\n<p><strong>先决条件</strong>：</p>\n<ul>\n<li><p>使用overlay 网络的Docker守护程序的防火墙规则</p>\n<p>您需要以下端口打开来往于overlay 网络上的每个Docker主机的流量：</p>\n<ul>\n<li>用于集群管理通信的TCP端口2377</li>\n<li>TCP和UDP端口7946用于节点之间的通信</li>\n<li>UDP端口4789用于覆盖网络流量</li>\n</ul>\n</li>\n<li><p>在创建overlay 网络之前，您需要将Docker守护程序初始化为swarm管理器，<code>docker swarm init</code>或者使用它将其连接到现有的swarm <code>docker swarm join</code>。这些中的任何一个都会创建默认ingress overlay 网络，默认情况下 由群服务使用。即使您从未计划使用群组服务，也需要执行此操作。之后，您可以创建其他用户定义的overlay 网络。</p>\n</li>\n</ul>\n</blockquote>\n<p>要创建用于swarm服务的覆盖网络，请使用如下命令：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker network create -d overlay my-overlay</span><br></pre></td></tr></table></figure>\n<p>要创建可由群集服务或独立容器用于与在其他Docker守护程序上运行的其他独立容器通信的覆盖网络，请添加<code>--attachable</code>标志：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker network create -d overlay --attachable my-attachable-overlay</span><br></pre></td></tr></table></figure>\n<p>同时可以指定IP地址范围，子网，网关和其他选项。详情<code>docker network create --help</code>请见</p>\n<h3 id=\"2-2-2-加密overlay网络\"><a href=\"#2-2-2-加密overlay网络\" class=\"headerlink\" title=\"2.2.2 加密overlay网络\"></a>2.2.2 加密overlay网络</h3><p>默认情况下，使用GCM模式下的AES算法加密所有swarm群集服务管理流量 。集群中的管理器节点每隔12小时轮换用于加密数据的密钥。</p>\n<p>要加密应用程序数据，请<code>--opt encrypted</code>在创建覆盖网络时添加。这样可以在vxlan级别启用IPSEC加密。此加密会产生不可忽视的性能损失，因此您应该在生产中使用此选项之前对其进行测试。</p>\n<p>启用覆盖加密后，Docker会在所有节点之间创建IPSEC隧道，在这些节点上为连接到覆盖网络的服务安排任务。这些隧道还在GCM模式下使用AES算法，管理器节点每12小时自动旋转密钥。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">docker network create --opt encrypted --driver overlay my-encrypted-network</span><br></pre></td></tr></table></figure>\n<h3 id=\"2-2-3-暴露服务端口\"><a href=\"#2-2-3-暴露服务端口\" class=\"headerlink\" title=\"2.2.3 暴露服务端口\"></a>2.2.3 暴露服务端口</h3><p>对于连接到同一个swarm集群的服务来说，它们之间所有的端口都是互相暴露的。对于可在服务外部访问的端口，必须使用 -p ( 或者 –publish) 发布该端口。</p>\n<table>\n<thead>\n<tr>\n<th>Flag value</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><code>-p 8080:80</code> or <code>-p published=8080,target=80</code></td>\n<td>将服务上的TCP端口80映射到routing mesh上的端口8080。</td>\n</tr>\n<tr>\n<td><code>-p 8080:80/udp</code> or <code>-p published=8080,target=80,protocol=udp</code></td>\n<td>将服务上的UDP端口80映射到routing mesh上的端口8080。</td>\n</tr>\n</tbody>\n</table>\n<h3 id=\"2-2-4-实现原理\"><a href=\"#2-2-4-实现原理\" class=\"headerlink\" title=\"2.2.4 实现原理\"></a>2.2.4 实现原理</h3><p>下面我们讨论下overlay 网络的具体实现：</p>\n<p>docker 会为每个 overlay 网络创建一个独立的 network namespace，其中会有一个 linux bridge br0，endpoint 还是由 veth pair 实现，一端连接到容器中（即 eth0），另一端连接到 namespace 的 br0 上。</p>\n<p>br0 除了连接所有的 endpoint，还会连接一个 vxlan 设备，用于与其他 host 建立 vxlan tunnel。容器之间的数据就是通过这个 tunnel 通信的。逻辑网络拓扑结构如图所示：</p>\n<p><img src=\"./overlay.jpg\" alt=\"\"></p>\n<h2 id=\"2-3-Ingress-Routing-Mesh\"><a href=\"#2-3-Ingress-Routing-Mesh\" class=\"headerlink\" title=\"2.3 Ingress Routing Mesh\"></a>2.3 Ingress Routing Mesh</h2><p>上文暴露服务端口中有讲到routing mesh，现在来详细介绍这一网络功能。</p>\n<p>默认情况下，发布端口的swarm服务使用routing mesh来实现。当客户端连接到任何swarm节点上的已发布端口（无论它是否正在运行给定服务）时，客户端请求将被透明地重定向到正在运行该服务的worker。实际上，Docker充当集群服务的负载均衡器。使用routing mesh的服务以<em>虚拟IP（VIP）模式运行</em>。即使在每个节点上运行的服务（通过<code>--global</code>标志）也使用routing mesh。使用routing mesh时，无法保证哪个Docker节点服务客户端请求。</p>\n<p>要在群集中使用 ingress 网络，您需要在启用群集模式之前在群集节点之间打开以下端口：</p>\n<ul>\n<li>端口<code>7946</code>  TCP / UDP用于容器网络发现。</li>\n<li>端口<code>4789</code>  UDP 用于容器 ingress 网络。</li>\n</ul>\n<p>例如，以下命令将nginx容器中的端口80发布到群集中任何节点的端口8080：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker service create \\</span><br><span class=\"line\">  --name my-web \\</span><br><span class=\"line\">  --publish published=8080,target=80 \\</span><br><span class=\"line\">  --replicas 2 \\</span><br><span class=\"line\">  nginx</span><br></pre></td></tr></table></figure>\n<p>当您在任何节点上访问端口8080时，Docker会将您的请求路由到活动容器。在群集节点本身上，端口8080实际上可能不受约束，但routing mesh 知道如何路由流量并防止发生任何端口冲突。</p>\n<p>routing mesh在已发布的端口上侦听分配给该节点的任何IP地址。对于可外部路由的IP地址，该端口可从主机外部获得。对于所有其他IP地址，只能从主机内访问。</p>\n<p><img src=\"\\img\\ingress-routing-mesh.png\" alt=\"\"></p>\n<p>如果想要绕过routing mesh，可以使用<em>DNS循环（DNSRR）模式</em>启动服务，方法是将<code>--endpoint-mode</code>标志设置为<code>dnsrr</code>。这样就必须在服务前运行自己的负载均衡器。Docker主机上的服务名称的DNS查询返回运行该服务的节点的IP地址列表。配置负载均衡器以使用此列表并平衡节点之间的流量。</p>\n<p>对于我们云桌面应用场景来说，直接利用routing mesh 功能即可，不需要额外搭建负载均衡器。</p>\n<h2 id=\"2-4-服务发现\"><a href=\"#2-4-服务发现\" class=\"headerlink\" title=\"2.4 服务发现\"></a>2.4 服务发现</h2><p>Docker Swarm 原生就提供了服务发现的功能。要使用服务发现，需要相互通信的 services 必须属于同一个 overlay 网络，所以我们先得创建一个新的 overlay 网络。 直接使用 <code>ingress</code> 行不行？很遗憾，目前 <code>ingress</code> 没有提供服务发现，必须创建自己的 overlay 网络。</p>\n<p>Docker Swarm mode下会为每个节点的Docker engine内置一个DNS server，各个节点间的DNS server通过control plane的gossip协议互相交互信息。此处DNS server用于容器间的服务发现。swarm mode会为每个 –net=自定义网络  的service分配一个DNS entry。目前必须是自定义网络，比如overaly。而bridge和routing mesh 的service，是不会分配DNS的。</p>\n<p>那么，下面就来详细介绍服务发现的原理。</p>\n<p>每个Docker容器都有一个DNS解析器，它将DNS查询转发到docker engine，该引擎充当DNS服务器。docker 引擎收到请求后就会在发出请求的容器所在的所有网络中，检查域名对应的是不是一个容器或者是服务，如果是，docker引擎就会从存储的key-value建值对中查找这个容器名、任务名、或者服务名对应的IP地址，并把这个IP地址或者是服务的虚拟IP地址返回给发起请求的域名解析器。  </p>\n<p>由上可知，docker的服务发现的作用范围是网络级别，也就意味着只有在同一个网络上的容器或任务才能利用内嵌的DNS服务来相互发现，不在同一个网络里面的服务是不能解析名称的，另外，为了安全和性能只有当一个节点上有容器或任务在某个网络里面时，这个节点才会存储那个网络里面的DNS记录。</p>\n<p>如果目的容器或服务和源容器不在同一个网络里面，Docker引擎会把这个DNS查询转发到配置的默认DNS服务  。</p>\n<p><img src=\"\\img\\service-dns.png\" alt=\"\"></p>\n<p>在上面的例子中，总共有两个服务myservice和client，其中myservice有两个容器，这两个服务在同一个网里面。在client里针对docker.com和myservice各执行了一个curl操作，下面时执行的流程： </p>\n<ul>\n<li>为了client解析docker.com和myservice，DNS查询进行初始化</li>\n<li>容器内建的解析器在127.0.0.11:53拦截到这个DNS查询请求，并把请求转发到docker引擎的DNS服务</li>\n<li>myservice 被解析成服务对应的虚拟IP（10.0.0.3）。在接下来的内部负载均衡阶段再被解析成一个具体任务容器的IP地址。如果是容器名称这一步直接解析成容器对应的IP地址（10.0.0.4或者10.0.0.5）。</li>\n<li>docker.com 在docker引擎或者mynet网络上不能被解析成服务，所以这个请求被转发到外部DNS，例如配置好的默认DNS服务器（8.8.8.8）上。</li>\n</ul>\n<p>Docker1.12+的服务发现和负载均衡是结合到一起的， 实现办法有两种，DNS轮询和IPVS。 配置参数 –endpoint-mode  (vip or dnsrr) ，默认使用VIP方式。DNS轮询有一些缺点，比如一些应用可能缓存DNS请求， 导致容器变化之后DNS不能实时更新；DNS生效时间也导致不能实时反映服务变化情况。VIP和IPVS原理上比较容易理解， 就是Docker为每个服务分配了一个VIP，DNS解析服务名称或者自定义的别名到这个VIP上。由于VIP本身没有容器提供服务，Docker把到VIP的请求通过IPVS技术负载到后面的容器上。</p>\n<p>因此正常情况下，使用默认的VIP方式即可。</p>\n<h2 id=\"3-5-负载均衡\"><a href=\"#3-5-负载均衡\" class=\"headerlink\" title=\"3.5 负载均衡\"></a>3.5 负载均衡</h2><p>负载均衡分为两种：</p>\n<p>Swarm集群内的service之间的相互访问需要做负载均衡，称为内部负载均衡（Internal LB）；</p>\n<p>从Swarm集群外部访问服务的公开端口，也需要做负载均衡，称外部负载均衡(Exteral LB or Ingress LB)。</p>\n<h3 id=\"3-5-1-Internal-LB\"><a href=\"#3-5-1-Internal-LB\" class=\"headerlink\" title=\"3.5.1 Internal LB\"></a>3.5.1 Internal LB</h3><p>内部负载均衡就是我们在上一段提到的服务发现，集群内部通过DNS访问service时，Swarm默认通过VIP（virtual IP）、iptables、IPVS转发到某个容器。 </p>\n<p><img src=\"\\img\\intelnal-lb.jpg\" alt=\"\"></p>\n<p>当在docker swarm集群模式下创建一个服务时，会自动在服务所属的网络上给服务额外的分配一个虚拟IP，当解析服务名字时就会返回这个虚拟IP。对虚拟IP的请求会通过overlay网络自动的负载到这个服务所有的健康任务上。这个方式也避免了客户端的负载均衡，因为只有单独的一个IP会返回到客户端，docker会处理虚拟IP到具体任务的路由，并把请求平均的分配给所有的健康任务。  </p>\n<p><img src=\".\\internal-lb-2.png\" alt=\"\"></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># 创建overlay网络：mynet </span><br><span class=\"line\">$ docker network create -d overlay mynet  </span><br><span class=\"line\">a59umzkdj2r0ua7x8jxd84dhr </span><br><span class=\"line\"># 利用mynet网络创建myservice服务，并复制两份  </span><br><span class=\"line\">$ docker service create --network mynet --name myservice --replicas 2 busybox ping localhost  </span><br><span class=\"line\">78t5r8cr0f0h6k2c3k7ih4l6f5</span><br><span class=\"line\"># 通过下面的命令查看myservice对应的虚拟IP </span><br><span class=\"line\">$ docker service inspect myservice  </span><br><span class=\"line\">...</span><br><span class=\"line\">&quot;VirtualIPs&quot;: [ </span><br><span class=\"line\">\t&#123;  </span><br><span class=\"line\">     &quot;NetworkID&quot;: &quot;a59umzkdj2r0ua7x8jxd84dhr&quot;,  </span><br><span class=\"line\">     \t\t\t&quot;Addr&quot;: &quot;10.0.0.3/24&quot;  </span><br><span class=\"line\">      &#125;,  </span><br><span class=\"line\">]</span><br></pre></td></tr></table></figure>\n<p>注：swarm中服务还有另外一种负载均衡技术可选DNS round robin (DNS RR) （在创建服务时通过–endpoint-mode配置项指定），在DNSRR模式下，docker不再为服务创建VIP，docker DNS服务直接利用轮询的策略把服务名称直接解析成一个容器的IP地址。 </p>\n<h3 id=\"3-5-2-Exteral-LB\"><a href=\"#3-5-2-Exteral-LB\" class=\"headerlink\" title=\"3.5.2 Exteral LB\"></a>3.5.2 Exteral LB</h3><p>Exteral LB(Ingress LB 或者 Swarm Mode Routing Mesh) 看名字就知道，这个负载均衡方式和前面提到的Ingress网络有关。Swarm网络要提供对外访问的服务就需要打开公开端口，并映射到宿主机。Exteral LB就是外部通过公开端口访问集群时做的负载均衡。</p>\n<p>当创建或更新一个服务时，你可以利用–publish选项把一个服务暴露到外部，在docker swarm模式下发布一个端口意味着在集群中的所有节点都会监听这个端口，这时当访问一个监听了端口但是并没有对应服务运行在其上的节点会发生什么呢？接下来就该我们的路由网（routing mesh）出场了，路由网时docker1.12引入的一个新特性，它结合了IPVS和iptables创建了一个强大的集群范围的L4层负载均衡，它使所有节点接收服务暴露端口的请求成为可能。当任意节点接收到针对某个服务暴露的TCP/UDP端口的请求时，这个节点会利用预先定义过的Ingress overlay网络，把请求转发给服务对应的虚拟IP。ingress网络和其他的overlay网络一样，只是它的目的是为了转换来自客户端到集群的请求，它也是利用我们前一小节介绍过的基于VIP的负载均衡技术。 </p>\n<p>启动服务后，你可以为应用程序创建外部DNS记录，并将其映射到任何或所有Docker swarm节点。你无需担心你的容器具体运行在那个节点上，因为有了路由网这个特性后，你的集群看起来就像是单独的一个节点一样。  </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#在集群中创建一个复制两份的服务，并暴露在8000端口  </span><br><span class=\"line\">$ docker service create --name app --replicas 2 --network appnet --publish 8000:80 nginx</span><br></pre></td></tr></table></figure>\n<p><img src=\"\\img\\external-routing-mesh.png\" alt=\"\"></p>\n<p>上面这个图表明了路由网是怎么工作的： </p>\n<ul>\n<li>服务（app）拥有两份复制，并把端口映射到外部端口的8000</li>\n<li>路由网在集群中的所有节点上都暴露出8000</li>\n<li>外部对服务app的请求可以是任意节点，在本例子中外部的负载均衡器将请求转发到了没有app服务的主机上</li>\n<li>docker swarm的IPVS利用ingress overlay网路将请求重新转发到运行着app服务的节点的容器中</li>\n</ul>\n<p>注：以上服务发现和负载均衡参考文档 <a href=\"https://success.docker.com/article/ucp-service-discovery\" target=\"_blank\" rel=\"noopener\">https://success.docker.com/article/ucp-service-discovery</a></p>\n<h1 id=\"3-Docker-Swarm-存储架构\"><a href=\"#3-Docker-Swarm-存储架构\" class=\"headerlink\" title=\"3.Docker Swarm 存储架构\"></a>3.Docker Swarm 存储架构</h1><p>从业务数据的角度看，容器可以分为两类：无状态（stateless）容器和有状态（stateful）容器。</p>\n<p>无状态是指容器在运行过程中不需要保存数据，每次访问的结果不依赖上一次访问，比如提供静态页面的 web 服务器。有状态是指容器需要保存数据，而且数据会发生变化，访问的结果依赖之前请求的处理结果，最典型的就是数据库服务器。</p>\n<p>简单来讲，状态（state）就是数据，如果容器需要处理并存储数据，它就是有状态的，反之则无状态。</p>\n<p>对于有状态的容器，如何保存数据呢？</p>\n<h2 id=\"3-1-Storage-Driver\"><a href=\"#3-1-Storage-Driver\" class=\"headerlink\" title=\"3.1 Storage Driver\"></a>3.1 Storage Driver</h2><p>Docker在启动容器的时候，需要创建文件系统，为rootfs提供挂载点。最底层的引导文件系统bootfs主要包含 bootloader和kernel，bootloader主要是引导加载kernel，当kernel被加载到内存中后 bootfs就被umount了。 rootfs包含的就是典型 Linux 系统中的/dev，/proc，/bin，/etc等标准目录和文件。</p>\n<p>Docker 模型的核心部分是有效利用分层镜像机制，镜像可以通过分层来进行继承，基于基础镜像（没有父镜像），可以制作各种具体的应用镜像。Docker1.10引入新的可寻址存储模型，使用安全内容哈希代替随机的UUID管理镜像。同时，Docker提供了迁移工具，将已经存在的镜像迁移到新模型上。不同 Docker 容器就可以共享一些基础的文件系统层，同时再加上自己独有的可读写层，大大提高了存储的效率。其中主要的机制就是分层模型和将不同目录挂载到同一个虚拟文件系统。</p>\n<p>Docker存储方式提供管理分层镜像和容器的可读写层的具体实现。最初Docker仅能在支持AUFS文件系统的ubuntu发行版上运行，但是由于AUFS未能加入Linux内核，为了寻求兼容性、扩展性，Docker在内部通过graphdriver机制这种可扩展的方式来实现对不同文件系统的支持。</p>\n<p><img src=\".\\storage-driver.png\" alt=\"\"></p>\n<p>如上图所示，容器由最上面一个可写的容器层，以及若干只读的镜像层组成，容器的数据就存放在这些层中。这样的分层结构最大的特性是 Copy-on-Write：</p>\n<ol>\n<li>新数据会直接存放在最上面的容器层。</li>\n<li>修改现有数据会先从镜像层将数据复制到容器层，修改后的数据直接保存在容器层中，镜像层保持不变。</li>\n<li>如果多个层中有命名相同的文件，用户只能看到最上面那层中的文件。</li>\n</ol>\n<p>分层结构使镜像和容器的创建、共享以及分发变得非常高效，而这些都要归功于 Docker storage driver。正是 storage driver 实现了多层数据的堆叠并为用户提供一个单一的合并之后的统一视图。</p>\n<p>Docker 支持多种 storage driver，有 AUFS、Device Mapper、Btrfs、OverlayFS、VFS 和 ZFS。它们都能实现分层的架构，同时又有各自的特性。关于这些方案的对比可以查阅相关文档（<a href=\"http://dockone.io/article/1513）。对于\" target=\"_blank\" rel=\"noopener\">http://dockone.io/article/1513）。对于</a> Docker 用户来说，具体选择使用哪个 storage driver 是一个难题，不过 Docker 官方给出了一个简单的答案： <strong>优先使用 Linux 发行版默认的 storage driver</strong>。 使用<code>docker info</code>查看系统默认的storage driver。</p>\n<p>使用的storage driver是与主机上的Backing Filesystem有关的。storage driver 是专门用来存放docker容器和镜像的，Backing Filesystem指的是主机的文件系统。默认的centos7 driver 是overlay2，对应的Backing Filesystem 是ext4。这个应该是没有问题的。storage driver 和Backing Filesystem 对应关系见下表。 </p>\n<p><img src=\".\\storage_backfing.png\" alt=\"1534987414895\"></p>\n<p>对于某些容器，直接将数据放在由 storage driver 维护的层中是很好的选择，比如那些无状态的应用。无状态意味着容器没有需要持久化的数据，随时可以从镜像直接创建。</p>\n<p>比如 busybox，它是一个工具箱，我们启动 busybox 是为了执行诸如 wget，ping 之类的命令，不需要保存数据供以后使用，使用完直接退出，容器删除时存放在容器层中的工作数据也一起被删除，这没问题，下次再启动新容器即可。</p>\n<p>但对于另一类应用这种方式就不合适了，它们有持久化数据的需求，容器启动时需要加载已有的数据，容器销毁时希望保留产生的新数据，也就是说，这类容器是有状态的。</p>\n<h2 id=\"3-2-数据持久化方案\"><a href=\"#3-2-数据持久化方案\" class=\"headerlink\" title=\"3.2 数据持久化方案\"></a>3.2 数据持久化方案</h2><p>那么对于有状态的容器，如何保存数据呢？</p>\n<p>选项一：打包在容器里。</p>\n<p>显然不行。除非数据不会发生变化，否则，如何在多个副本直接保持同步呢？</p>\n<p>选项二：数据放在 Docker 主机的本地目录中，通过 volume 映射到容器里。</p>\n<p>位于同一个主机的副本倒是能够共享这个 volume，但不同主机中的副本如何同步呢？</p>\n<p>选项三：利用 Docker 的 volume driver，由外部 storage provider 管理和提供 volume，所有 Docker 主机 volume 将挂载到各个副本。</p>\n<p>这是目前最佳的方案。volume 不依赖 Docker 主机和容器，生命周期由 storage provider 管理，volume 的高可用和数据有效性也全权由 provider 负责，Docker 只管使用。</p>\n<p>我们知道Docker 单机的存储方案里面，通过 data volume 可以存储容器的状态。不过其本质是 Docker 主机本地的目录。 本地目录就存在一个隐患：如果 Docker Host 宕机了，如何恢复容器？ 一个办法就是定期备份数据，但这种方案还是会丢失从上次备份到宕机这段时间的数据。更好的方案是由专门的 storage provider 提供 volume，Docker 从 provider 那里获取 volume 并挂载到容器。这样即使 Host 挂了，也可以立刻在其他可用 Host 上启动相同镜像的容器，挂载之前使用的 volume，这样就不会有数据丢失。</p>\n<p>还有一个容易混淆的存储方案，是通过在Docker主机上挂载共享文件系统（例如Ceph，GlusterFS，NFS）。如下图所示：</p>\n<p><img src=\".\\Chart_Multi-Host-Persistence-Shared-Among-Containers.png\" alt=\"\"></p>\n<p>在运行Docker容器的每个主机上配置分布式文件系统。通过创建一致的命名约定和统一命名空间，所有正在运行的容器都可以访问底层的共享存储后端。共享文件系统存储方案利用分布式文件系统与显式存储技术相结合。由于挂载点在所有节点上都可用，因此可以利用它在容器之间创建共享挂载点。这种方案其实是属于选项二的一种变种。</p>\n<p>尽管该方案对于特定用例而言，是Docker 存储方案的一个有价值的补充，但它具有限制容器到特定主机的可移植性的显着缺点。它也没有利用针对数据密集型工作负载优化的专用存储后端。为了解决这些限制，volume plugins 被添加到Docker中，将容器的功能扩展到各种存储后端，并且而无需强制更改应用程序设计或部署体系结构。 这就是我们下节将详细介绍的 Volume Driver。</p>\n<h2 id=\"3-3-Volume-Driver\"><a href=\"#3-3-Volume-Driver\" class=\"headerlink\" title=\"3.3 Volume Driver\"></a>3.3 Volume Driver</h2><p>假设有两个 Dokcer 主机，Host1 运行了一个 MySQL 容器，为了保护数据，data volume 由 storage provider 提供，如下图所示。 </p>\n<p><img src=\".\\storage-provider-1.png\" alt=\"\"></p>\n<p>当 Host1 发生故障，我们会在 Host2 上启动相同的 MySQL 镜像，并挂载 data volume。 </p>\n<p><img src=\".\\storage-provider-2.png\" alt=\"\"></p>\n<p>Docker 是如何实现这个跨主机管理 data volume 方案的呢？</p>\n<p><strong>答案是 volume driver。</strong></p>\n<p>利用 Docker 的 volume driver，由外部 storage provider 管理和提供 volume，所有 Docker 主机 volume 将挂载到各个副本。任何一个 data volume 都是由 driver 管理的，创建 volume 时如果不特别指定，将使用 <code>local</code> 类型的 driver，即从 Docker Host 的本地目录中分配存储空间。如果要支持跨主机的 volume，则需要使用第三方 driver。</p>\n<p>目前已经有很多可用的 driver，比如使用 Azure File Storage 的 driver，使用 GlusterFS 的 driver，完整的列表可参考 <a href=\"https://docs.docker.com/engine/extend/legacy_plugins/#volume-plugins\" target=\"_blank\" rel=\"noopener\">https://docs.docker.com/engine/extend/legacy_plugins/#volume-plugins</a> </p>\n<p><img src=\".\\Docker-Volume-Plugin-Architecture.png\" alt=\"\"></p>\n<p>Volume drivers 让我们将应用程序与底层存储系统隔离。例如，如果原先服务使用一个 NFS驱动器，现在可以在不改变应用程序的逻辑下，直接更新服务使用不同的驱动程序，例如将数据存储在云端,。 </p>\n<p>通过<code>docker volume create</code> 命令可以直接创建volume，同时在参数中指定driver 类型。    下面的例子使用vieux/sshfs  这种类型的Volume drivers。首先创建一个单独的 volume，然后创建容器引用该volume。</p>\n<h3 id=\"3-3-1-安装driver插件\"><a href=\"#3-3-1-安装driver插件\" class=\"headerlink\" title=\"3.3.1 安装driver插件\"></a>3.3.1 安装driver插件</h3><p>这个示例假设有两个节点，两个节点直接可以通过ssh互相连接。在Docker 主机上，安装<code>vieux/sshfs</code> 插件。</p>\n<figure class=\"highlight docker\"><figcaption><span>plugin install --grant-all-permissions vieux/sshfs</span></figcaption><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">docker plugin install --grant-all-permissions vieux/sshfs</span><br></pre></td></tr></table></figure>\n<h3 id=\"3-3-2-创建volume\"><a href=\"#3-3-2-创建volume\" class=\"headerlink\" title=\"3.3.2 创建volume\"></a>3.3.2 创建volume</h3><p>本例中指定一个SSH密码，但如果两个主机配置了共享密钥，可以省略的密码。每个volume driver 可以有零个或多个可配置选项，每一个都指定使用 -o 标志。 </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">docker volume create --driver vieux/sshfs \\</span><br><span class=\"line\">  -o sshcmd=test@node2:/home/test \\</span><br><span class=\"line\">  -o password=testpassword \\</span><br><span class=\"line\">  sshvolume</span><br></pre></td></tr></table></figure>\n<h3 id=\"3-3-3-使用volume\"><a href=\"#3-3-3-使用volume\" class=\"headerlink\" title=\"3.3.3 使用volume\"></a>3.3.3 使用volume</h3><p>本例中指定一个SSH密码,但如果两个主机配置了共享密钥,您可以省略的密码。每个卷司机可以有零个或多个可配置的选项。如果 <strong>volume driver</strong>  需要传递参数选项,你必须使用 <strong>–mount</strong>  挂载卷,而不是使用 -v。 </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker run -d \\</span><br><span class=\"line\">  --name sshfs-container \\</span><br><span class=\"line\">  --volume-driver vieux/sshfs \\</span><br><span class=\"line\">  --mount src=sshvolume,target=/app,volume-opt=sshcmd=test@node2:/home/test,volume-opt=password=testpassword \\</span><br><span class=\"line\">  nginx:latest</span><br></pre></td></tr></table></figure>\n<p>关于volume driver更多的介绍，可以查看官方文档：<a href=\"https://docs.docker.com/storage/volumes/#use-a-volume-driver\" target=\"_blank\" rel=\"noopener\">https://docs.docker.com/storage/volumes/#use-a-volume-driver</a></p>\n<p><a href=\"https://docs.docker.com/storage/volumes/\" target=\"_blank\" rel=\"noopener\">https://docs.docker.com/storage/volumes/</a></p>\n<h1 id=\"4-Docker-Swarm-服务架构\"><a href=\"#4-Docker-Swarm-服务架构\" class=\"headerlink\" title=\"4.Docker Swarm 服务架构\"></a>4.Docker Swarm 服务架构</h1><p>我们知道，Docker 场景下，应用是以容器的形式运行并提供服务的。而在Swarm场景下，任务是一个个具体的容器，通过抽象出服务的概念，来管理具有一定关联关系的任务容器。 </p>\n<p>为了将某一个服务封装为 Service 在Swarm 中部署执行，我们需要通过指定容器 image 以及需要在容器中执行的 Commands 来创建你的 Service，除此之外，还需要配置如下选项，</p>\n<ul>\n<li>指定可以在 Swarm 之外可以被访问的服务端口号 port</li>\n<li>指定加入某个 Overlay 网络以便 Service 与 Service 之间可以建立连接并通讯，</li>\n<li>指定该 Service 所要使用的 CPU 和 内存的大小，</li>\n<li>指定一个滚动更新的策略 (Rolling Update Policy)</li>\n<li>指定多少个 Task 的副本 (replicas) 在 Swarm 集群中同时存在</li>\n</ul>\n<p>如下所示，<code>docker-swarm.yml</code>文件是一个YAML文件，它定义了如何Docker容器在生产中应表现。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">version: &quot;3&quot;</span><br><span class=\"line\">services:</span><br><span class=\"line\">  web:</span><br><span class=\"line\">    # replace username/repo:tag with your name and image details</span><br><span class=\"line\">    image: username/repo:tag</span><br><span class=\"line\">    deploy:</span><br><span class=\"line\">      replicas: 5</span><br><span class=\"line\">      resources:</span><br><span class=\"line\">        limits:</span><br><span class=\"line\">          cpus: &quot;0.1&quot;</span><br><span class=\"line\">          memory: 50M</span><br><span class=\"line\">      restart_policy:</span><br><span class=\"line\">        condition: on-failure</span><br><span class=\"line\">    ports:</span><br><span class=\"line\">      - &quot;4000:80&quot;</span><br><span class=\"line\">    networks:</span><br><span class=\"line\">      - webnet</span><br><span class=\"line\">networks:</span><br><span class=\"line\">  webnet:</span><br></pre></td></tr></table></figure>\n<p>该<code>docker-swarm.yml</code>文件告诉Docker执行以下操作：</p>\n<ul>\n<li>拉username/repo 仓库中拉取镜像。</li>\n<li>将该映像的5个实例作为一个被调用的服务运行<code>web</code>，限制每个实例使用，最多10％的CPU（跨所有内核）和50MB的RAM。</li>\n<li>如果一个失败，立即重启容器。</li>\n<li>将主机上的端口4000映射到<code>web</code>端口80。</li>\n<li>指示<code>web</code>容器通过称为负载平衡的网络共享端口80 <code>webnet</code>。（在内部，容器本身<code>web</code>在短暂的端口发布到 80端口。）</li>\n<li><code>webnet</code>使用默认设置（负载平衡的覆盖网络）定义网络。</li>\n</ul>\n<h2 id=\"4-1-服务和任务\"><a href=\"#4-1-服务和任务\" class=\"headerlink\" title=\"4.1 服务和任务\"></a>4.1 服务和任务</h2><p>我们一般使用如下命令来部署一个服务到swarm中：</p>\n<p><code>docker stack deploy -c docker-swarm.yml myservice。</code></p>\n<p>docker-swarm.yml中定义了服务所需要达到的状态。我们不需要告诉swarm如何做，我们只需要告诉swarm 我们想要的服务状态是什么，swarm最终会帮我们达到这个状态。</p>\n<p>swarm管理器将服务定义为服务的所需状态。swarm将群中节点上的服务调度为一个或多个副本任务。任务在群集中的节点上彼此独立地运行。</p>\n<p>例如，想要在HTTP侦听器的三个实例之间进行负载平衡。下图显示了具有三个副本的HTTP侦听器服务。监听器的三个实例中的每一个都是集群中的任务。容器是一个孤立的过程。在swarm模式模型中，每个任务只调用一个容器。任务类似于调度程序放置容器的“槽”。容器生效后，调度程序会识别该任务处于运行状态。如果容器未通过运行状况检查或终止，则任务将终止。</p>\n<p><img src=\"./services-diagram.png\" alt=\"\"></p>\n<p>Services，Tasks 和 Containers 之间的关系可以用上面这张图来描述。来看这张图的逻辑，表示用户想要通过 Manager 节点部署一个有 3 个副本的 Nginx 的 Service，Manager 节点接收到用户的 Service definition 后，便开始对该 Service 进行调度，将会在当前可用的Worker（或者Manager ）节点中启动相应的 Tasks 以及相关的副本；所以可以看到，Service 实际上是 Task 的定义，而 Task 则是执行在节点上的程序。</p>\n<p>Task 是什么呢？其实就是一个 Container，只是，在 Swarm 中，每个 Task 都有自己的名字和编号，如图，比如 nginx.1、nginx.2 和 nginx.3，这些 Container 各自运行在各自的 node 上，当然，一个 node 上可以运行多个 Container；</p>\n<h2 id=\"4-2-任务调度\"><a href=\"#4-2-任务调度\" class=\"headerlink\" title=\"4.2 任务调度\"></a>4.2 任务调度</h2><p>任务是swarm集群内调度的原子单位。在创建或更新服务时，声明定义服务状态，协调器通过调度任务来实现所需的状态。例如上例的服务，该服务指示协调器始终保持三个HTTP侦听器实例的运行。协调器通过创建三个任务来响应。每个任务都是调度程序通过生成容器来填充的插槽。容器是任务的实例化。如果HTTP侦听器任务随后未通过其运行状况检查或崩溃，则协调器会创建一个新的副本任务，该任务会生成一个新容器。</p>\n<p>任务是单向机制。它通过一系列状态单调进行：已分配，准备，运行等。如果任务失败，则协调器将删除任务及其容器，然后根据服务指定的所需状态创建新任务以替换它。</p>\n<p>Docker swarm mode 模式的底层技术实际上就是指的是调度器( scheduler )和编排器( orchestrator )；下面这张图展示了 Swarm mode 如何从一个 Service 的创建请求并且成功将该 Service 分发到 worker 节点上执行的过程。</p>\n<p><img src=\".\\swarm-service-lifecycle.png\" alt=\"\"></p>\n<ul>\n<li>首先，看上半部分Swarm manager <ol>\n<li>用户通过 Docker Engine Client 使用命令 docker service create 提交 Service definition，</li>\n<li>根据 Service definition 创建相应的 Task，</li>\n<li>为 Task 分配 IP 地址，<br>注意，这是分配运行在 Swarm 集群中 Container 的 IP 地址，该 IP 地址最佳的分配地点是在这里，因为 Manager 节点上保存得有最新最全的 Tasks 的状态信息，为了保证不与其他的 Task 分配到相同的 IP，所以在这里就将 IP 地址给初始化好；</li>\n<li>将 Task 分发到 Node 上，可以是 Manager 节点也可以使 Worker 节点，</li>\n<li>对 Worker 节点进行相应的初始化使得它可以执行 Task</li>\n</ol>\n</li>\n<li>接着，看下半部分Swarm  Work<ol>\n<li>首先连接 manager 的分配器( scheduler)检查该 task</li>\n<li>验证通过以后，便开始通过 Worker 节点上的执行器( executor )执行；</li>\n</ol>\n</li>\n</ul>\n<p>注意，上述 task 的执行过程是一种单向机制，比如它会按顺序的依次经历 assigned, prepared 和 running 等执行状态，不过在某些特殊情况下，在执行过程中，某个 task 失败了( fails )，编排器( orchestrator )直接将该 task 以及它的 container 给删除掉，然后在其它节点上另外创建并执行该 task；</p>\n<h2 id=\"4-3-任务状态\"><a href=\"#4-3-任务状态\" class=\"headerlink\" title=\"4.3 任务状态\"></a>4.3 任务状态</h2><p>通过运行<code>docker service ps &lt;service-name&gt;</code>以获取任务的状态。该 <code>CURRENT STATE</code>字段显示任务的状态以及任务的持续时间 。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[root@swarm01 ~]# docker service ps myweb_web</span><br><span class=\"line\">ID                  NAME                IMAGE               NODE                DESIRED STATE       CURRENT STATE            ERROR                         PORTS</span><br><span class=\"line\">52lolsfykj4t        myweb_web.1         httpd:v2            swarm03             Running             Running 16 minutes ago</span><br><span class=\"line\">zwpufyfacagy         \\_ myweb_web.1     httpd:v2            swarm03             Shutdown            Failed 16 minutes ago    &quot;task: non-zero exit (255)&quot;</span><br><span class=\"line\">00fhp9k1byf6        myweb_web.2         httpd:v2            swarm02             Running             Running 16 minutes ago</span><br><span class=\"line\">kgtiyw39xpcc         \\_ myweb_web.2     httpd:v2            swarm02             Shutdown            Failed 17 minutes ago    &quot;task: non-zero exit (255)&quot;</span><br><span class=\"line\">vefk4yn0b00m        myweb_web.3         httpd:v2            swarm01             Running             Running 16 minutes ago</span><br><span class=\"line\">pnj4z0j281ng         \\_ myweb_web.3     httpd:v2            swarm01             Shutdown            Failed 17 minutes ago    &quot;task: non-zero exit (255)&quot;</span><br></pre></td></tr></table></figure>\n<p>任务具有如下几种状态：</p>\n<table>\n<thead>\n<tr>\n<th>任务状态</th>\n<th>描述</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><code>NEW</code></td>\n<td>任务已初始化。</td>\n</tr>\n<tr>\n<td><code>PENDING</code></td>\n<td>分配了任务的资源。</td>\n</tr>\n<tr>\n<td><code>ASSIGNED</code></td>\n<td>Docker将任务分配给节点。</td>\n</tr>\n<tr>\n<td><code>ACCEPTED</code></td>\n<td>该任务被工作节点接受。如果工作节点拒绝该任务，则状态将更改为<code>REJECTED</code>。</td>\n</tr>\n<tr>\n<td><code>PREPARING</code></td>\n<td>Docker正在准备任务。</td>\n</tr>\n<tr>\n<td><code>STARTING</code></td>\n<td>Docker正在开始这项任务。</td>\n</tr>\n<tr>\n<td><code>RUNNING</code></td>\n<td>任务正在执行。</td>\n</tr>\n<tr>\n<td><code>COMPLETE</code></td>\n<td>任务退出时没有错误代码。</td>\n</tr>\n<tr>\n<td><code>FAILED</code></td>\n<td>任务退出并显示错误代码。</td>\n</tr>\n<tr>\n<td><code>SHUTDOWN</code></td>\n<td>Docker请求关闭任务。</td>\n</tr>\n<tr>\n<td><code>REJECTED</code></td>\n<td>工作节点拒绝了该任务。</td>\n</tr>\n<tr>\n<td><code>ORPHANED</code></td>\n<td>该节点停机时间过长。</td>\n</tr>\n<tr>\n<td><code>REMOVE</code></td>\n<td>该任务不是终端，但关联的服务已被删除或缩小。</td>\n</tr>\n</tbody>\n</table>\n<p>如果在某个 service 的<a href=\"http://www.shangyang.me/2018/02/01/docker-swarm-03-architect/#%E8%B0%83%E5%BA%A6%E8%BF%87%E7%A8%8B\" target=\"_blank\" rel=\"noopener\">调度过程</a>中，发现当前没有可用的 node 资源可以执行该 service，这个时候，该 service 的状态将会保持为 pending 的状态；下面，我们来看一些例子可能使得 service 维持在 pending 状态，</p>\n<ul>\n<li>如果当所有的节点都被停止或者进入了 drained 状态，这个时候，你试图创建一个 service，该 service 将会一直保持 pending 状态直到当前某个节点可用为止；不过要注意的是，第一个恢复的 node 将会得到所有的 task 调度请求，接收并执行，因此，这种情况在 production 环境上要尽量避免；</li>\n<li>你可以为你的 service 设置执行所需的内存大小，在调度的过程当中，发现没有一个 node 能够满足你的内存请求，那么你当前的 service 将会一直处于 pending 状态直到有满足需求的 node 出现；</li>\n<li>可以对服务施加放置约束，并且可能无法在给定时间遵守约束。 </li>\n</ul>\n<p>上面的例子都表明一个事实，就是你期望的执行条件与 Swarm 现有的可用资源的情况不匹配，不吻合，因此，作为 Swarm 的管理者，应该考虑对 Swarm 集群整体进行扩容；</p>\n<h2 id=\"4-4-服务副本与全局服务\"><a href=\"#4-4-服务副本与全局服务\" class=\"headerlink\" title=\"4.4 服务副本与全局服务\"></a>4.4 服务副本与全局服务</h2><p>Docker Swarm 有两种类型的服务部署模式：复制和全局。</p>\n<p>Swarm通过<code>--mode</code>选项设置服务类型，提供了两种模式：一种是replicated，我们可以指定服务Task的个数（也就是需要创建几个冗余副本），这也是Swarm默认使用的服务类型；另一种是global，这样会在Swarm集群的每个Node上都创建一个服务。如下图所示（出自Docker官网），是一个包含replicated和global模式的Swarm集群： </p>\n<p><img src=\".\\docker-swarm-replicated-vs-global.png\" alt=\"\"></p>\n<p>上图中，黄色表示的replicated模式下的Service Replicas，灰色表示global模式下Service的分布。 </p>\n<p>在Swarm mode下使用Docker，可以实现部署运行服务、服务扩容缩容、删除服务、滚动更新等功能。</p>\n<h2 id=\"4-5-调度策略\"><a href=\"#4-5-调度策略\" class=\"headerlink\" title=\"4.5 调度策略\"></a>4.5 调度策略</h2><p>Swarm提供了几种不同的方法来控制服务任务可以在哪些节点上运行。</p>\n<ul>\n<li><p>可以指定服务是需要运行特定数量的副本还是应该在每个工作节点上全局运行。请参阅2.4服务副本与全局服务。</p>\n</li>\n<li><p>您可以配置服务的 CPU或内存要求，该服务仅在满足这些要求的节点上运行。</p>\n</li>\n<li><p>通过设置约束标签 –constraint ，您可以将服务配置为仅在具有特定（任意）元数据集的节点上运行，并且如果不存在适当的节点，则会导致部署失败。例如，您可以指定您的服务只应在任意标签<code>pci_compliant</code>设置为的节点上运行 <code>true</code>。</p>\n</li>\n<li><p>通过设置首选项 –preferences，您可以将具有一系列值的任意标签应用于每个节点，并使用算法将服务的任务分布到这些节点上。目前，唯一支持的算法是<code>spread</code>，该算法将尝试均匀放置它们。例如，如果<code>rack</code>使用值为1-10 的标签标记每个节点，然后指定键入的放置首选项<code>rack</code>，服务任务将尽可能均匀地放置在具有标签的所有节点上。如果具有<code>rack</code>标签的节点个数，大于任务个数，那么则会根据rack 值的大小排序，然后优先选择排名靠前的节点。</p>\n<p>与约束不同，放置首选项是尽力而为，如果没有节点可以满足首选项，则服务不会失败。如果为服务指定放置首选项，则当集群管理器决定哪些节点应运行服务任务时，与该首选项匹配的节点的排名会更高。其他因素，例如服务的高可用性，也是计划节点运行服务任务的因素。例如，如果您有N个节点带有机架标签（然后是其他一些节点），并且您的服务配置为运行N + 1个副本，那么+1将在一个尚未拥有该服务的节点上进行调度，如果无论该节点是否具有<code>rack</code>标签。</p>\n</li>\n</ul>\n<h2 id=\"4-6-服务管理\"><a href=\"#4-6-服务管理\" class=\"headerlink\" title=\"4.6 服务管理\"></a>4.6 服务管理</h2><p>docker swarm 还可以使用 stack 这一模型，用来部署管理和关联不同的服务。stack 功能比service 还要强大，因为它具备多服务编排部署的能力。<strong>stack file</strong> 是一种 yaml 格式的文件，类似于 docker-compose.yml 文件，它定义了一个或多个服务，并定义了服务的环境变量、部署标签、容器数量以及相关的环境特定配置等。 </p>\n<p>以下实验仅仅创建利用stack 创建单一服务。</p>\n<h3 id=\"4-6-1-创建服务\"><a href=\"#4-6-1-创建服务\" class=\"headerlink\" title=\"4.6.1 创建服务\"></a>4.6.1 创建服务</h3><p>服务配置文件docker-swarm.yml ，内容如下</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">version: &apos;3&apos;</span><br><span class=\"line\">services:</span><br><span class=\"line\">  web:</span><br><span class=\"line\">    image: &quot;httpd:v1&quot;</span><br><span class=\"line\">    deploy:</span><br><span class=\"line\">      replicas: 3</span><br><span class=\"line\">      resources:</span><br><span class=\"line\">        limits:</span><br><span class=\"line\">          cpus: &quot;0.5&quot;</span><br><span class=\"line\">          memory: 256M</span><br><span class=\"line\">      restart_policy:</span><br><span class=\"line\">        condition: on-failure</span><br><span class=\"line\">    ports:</span><br><span class=\"line\">     - &quot;7080:80&quot;</span><br><span class=\"line\">    volumes:</span><br><span class=\"line\">     - /var/www/html:/usr/local/apache2/htdocs</span><br></pre></td></tr></table></figure>\n<p>image: “httpd” ：httpd是docker hub 下载的镜像。提供简单的httpd服务，此处用来做服务都高可用验证。</p>\n<p>replicas: 3 表示该服务副本有3个，可对服务副本进行扩缩容，后续会讲到。</p>\n<p>/var/www/html:/usr/local/apache2/htdocs ：表示httpd 首页的内容。各个swarm节点上展示的内容分别是其主机名信息。</p>\n<p>在docker-swarm.yml 同级目录下，执行命令</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[root@swarm01 swarm]# docker stack deploy -c docker-swarm.yml myservice</span><br><span class=\"line\">Creating network myservice_default</span><br><span class=\"line\">Creating service myservice_web</span><br></pre></td></tr></table></figure>\n<h3 id=\"4-6-2-查看服务\"><a href=\"#4-6-2-查看服务\" class=\"headerlink\" title=\"4.6.2 查看服务\"></a>4.6.2 查看服务</h3><p>查看服务创建是否成功。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[root@swarm01 swarm]# docker stack ls</span><br><span class=\"line\">NAME                SERVICES</span><br><span class=\"line\">myservice           1</span><br><span class=\"line\">[root@swarm01 swarm]# docker stack services myservice</span><br><span class=\"line\">ID                  NAME                MODE                REPLICAS            IMAGE               PORTS</span><br><span class=\"line\">kom8x49f7qv2        myservice_web       replicated          3/3                 httpd:latest        *:7080-&gt;80/tcp</span><br><span class=\"line\">[root@swarm01 swarm]# docker stack ps myservice</span><br><span class=\"line\">ID                  NAME                IMAGE               NODE                DESIRED STATE       CURRENT STATE            ERROR               PORTS</span><br><span class=\"line\">sybn1dur1wmw        myservice_web.1     httpd:latest        swarm03             Running             Running 45 seconds ago</span><br><span class=\"line\">mlcb5r9vzv6z        myservice_web.2     httpd:latest        swarm03             Running             Running 45 seconds ago</span><br><span class=\"line\">fw21lcu3zp83        myservice_web.3     httpd:latest        swarm01             Running             Running 45 seconds ago</span><br></pre></td></tr></table></figure>\n<p>可以看到 swarm01 上面运行了1个容器，swarm03上面运行了2个，而swarm02上面一个容器都没有，这是咋回事呢？哦，原来我们在创建集群过程中，对swarm02 做了状态变更的操作，将其设置为不运行任务。<code>docker node update  --availability drain  swarm02</code>. 查看集群状态。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[root@swarm01 swarm]# docker node ls</span><br><span class=\"line\">ID                            HOSTNAME            STATUS              AVAILABILITY        MANAGER STATUS      ENGINE VERSION</span><br><span class=\"line\">epoic1y0vv830vnwbc6nnacjc *   swarm01             Ready               Active              Leader              18.03.1-ce</span><br><span class=\"line\">nsqqv181e0r7mu77azixtqhzl     swarm02             Ready               Drain               Reachable           18.03.1-ce</span><br><span class=\"line\">mkkxaeqergz8xw21bbkd47q1c     swarm03             Ready               Active              Reachable           18.03.1-ce</span><br></pre></td></tr></table></figure>\n<p>此时打开浏览器，输入HTTP URL。任意swarm集群节点都可以访问httpd服务。</p>\n<p><img src=\".\\swarm-web.png\" alt=\"\"></p>\n<p>需要说明的是，由于3个节点的/var/www/html 路径未做共享存储。因此swarm会随机显示一个swarm节点的网页内容。</p>\n<h3 id=\"4-6-3-服务扩缩容\"><a href=\"#4-6-3-服务扩缩容\" class=\"headerlink\" title=\"4.6.3 服务扩缩容\"></a>4.6.3 服务扩缩容</h3><p>修改docker-swarm.yml 配置字段 <code>replicas: 5</code> ，将容器个数提升到5个。由于实验环境资源有限，因此这里将swarm02状态变更为可以状态，用以分配容器。</p>\n<p><code>docker node update  --availability active swarm02</code>.</p>\n<p>重新执行部署命令<code>docker stack deploy -c docker-swarm.yml myservice</code></p>\n<p>也可以通过命令直接对服务进行操作，但是不推荐。 <code>docker service scale 服务ID=服务个数</code></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[root@swarm01 swarm]# docker stack ps myservice</span><br><span class=\"line\">ID                  NAME                IMAGE               NODE                DESIRED STATE       CURRENT STATE                    ERROR               PORTS</span><br><span class=\"line\">unwepav5z9nz        myservice_web.1     httpd:v1            swarm02             Running             Running less than a second ago</span><br><span class=\"line\">3j5o1tktcg1l        myservice_web.2     httpd:v1            swarm01             Running             Preparing 3 seconds ago</span><br><span class=\"line\">wl1unt6lynft        myservice_web.3     httpd:v1            swarm03             Running             Running less than a second ago</span><br><span class=\"line\">[root@swarm01 swarm]# vi docker-swarm.yml</span><br><span class=\"line\">[root@swarm01 swarm]# docker stack deploy -c docker-swarm.yml myservice</span><br><span class=\"line\">Updating service myservice_web (id: 85quzwi5k0btkn5wlht4jbco9)</span><br><span class=\"line\">image httpd:v1 could not be accessed on a registry to record</span><br><span class=\"line\">its digest. Each node will access httpd:v1 independently,</span><br><span class=\"line\">possibly leading to different nodes running different</span><br><span class=\"line\">versions of the image.</span><br><span class=\"line\"></span><br><span class=\"line\">[root@swarm01 swarm]# docker stack ps myservice</span><br><span class=\"line\">ID                  NAME                IMAGE               NODE                DESIRED STATE       CURRENT STATE                    ERROR               PORTS</span><br><span class=\"line\">unwepav5z9nz        myservice_web.1     httpd:v1            swarm02             Running             Running 24 seconds ago</span><br><span class=\"line\">3j5o1tktcg1l        myservice_web.2     httpd:v1            swarm01             Running             Running 31 seconds ago</span><br><span class=\"line\">wl1unt6lynft        myservice_web.3     httpd:v1            swarm03             Running             Running 24 seconds ago</span><br><span class=\"line\">ujedjxbuffxp        myservice_web.4     httpd:v1            swarm01             Running             Running less than a second ago</span><br><span class=\"line\">ml9qvjh2add7        myservice_web.5     httpd:v1            swarm02             Running             Running less than a second ago</span><br></pre></td></tr></table></figure>\n<p>同样，对于服务缩容，也仅需要修改<code>replicas:</code> 配置即可。这里不再演示。</p>\n<h3 id=\"4-6-4-删除服务\"><a href=\"#4-6-4-删除服务\" class=\"headerlink\" title=\"4.6.4 删除服务\"></a>4.6.4 删除服务</h3><p>例如，删除myredis应用服务，执行docker service rm myredis，则应用服务myredis的全部副本都会被删除。 </p>\n<p>本例使用的是stack ，删除stack ，即可删除其下的服务。没错，docker语法非常类似,<code>docker stack rm myservice</code>。</p>\n<p>需要注意的是，对swarm上面服务的操作，都必须在manager节点上运行。</p>\n<h3 id=\"4-6-5-服务升降级\"><a href=\"#4-6-5-服务升降级\" class=\"headerlink\" title=\"4.6.5 服务升降级\"></a>4.6.5 服务升降级</h3><p>服务升降级，对应到docker中，则利用了容器镜像的更新，升级很好理解，直接把tag 标签版本往上提。那么降级，其实也可以理解为另外一种“升级“，只是镜像内容是上一个版本的而已。对于镜像标签的命令应该遵循一套严格的规范，这里不再赘述。</p>\n<p>先介绍利用service 命令行工具的用法。</p>\n<p>服务的滚动更新，这里我参考官网文档的例子说明。在Manager Node上执行如下命令：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">docker service create --replicas 3  --name redis --update-delay 10s redis:3.0.6</span><br></pre></td></tr></table></figure>\n<p>上面通过指定 –update-delay 选项，表示需要进行更新的服务，每次成功部署一个，延迟10秒钟，然后再更新下一个服务。如果某个服务更新失败，则Swarm的调度器就会暂停本次服务的部署更新。</p>\n<p>另外，也可以更新已经部署的服务所在容器中使用的Image的版本，例如执行如下命令：</p>\n<p>将Redis服务对应的Image版本有3.0.6更新为3.0.7，同样，如果更新失败，则暂停本次更新。</p>\n<p>那么，stack file 如何定义滚动更新呢？</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">update_config:</span><br><span class=\"line\">  parallelism: 2</span><br><span class=\"line\">  delay: 10s</span><br></pre></td></tr></table></figure>\n<p>修改docker-swarm.yml 如下</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">version: &apos;3&apos;</span><br><span class=\"line\">services:</span><br><span class=\"line\">  web:</span><br><span class=\"line\">    image: &quot;httpd:v2&quot;</span><br><span class=\"line\">    deploy:</span><br><span class=\"line\">      replicas: 3</span><br><span class=\"line\">      update_config:</span><br><span class=\"line\">        parallelism: 2</span><br><span class=\"line\">        delay: 30s</span><br><span class=\"line\">      resources:</span><br><span class=\"line\">        limits:</span><br><span class=\"line\">          cpus: &quot;0.5&quot;</span><br><span class=\"line\">          memory: 256M</span><br><span class=\"line\">      restart_policy:</span><br><span class=\"line\">        condition: on-failure</span><br><span class=\"line\">    ports:</span><br><span class=\"line\">     - &quot;7080:80&quot;</span><br><span class=\"line\">    volumes:</span><br><span class=\"line\">     - /var/www/html:/usr/local/apache2/htdocs</span><br></pre></td></tr></table></figure>\n<p>parallelism 表示同时升级的个数，delay 则表示间隔多长时间升级。</p>\n<p>同时将httpd 由v1 升级到v2，这里我们使用同一个镜像，只是tag 修改了一下。查看服务状态</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[root@swarm01 swarm]# docker stack ps myservice</span><br><span class=\"line\">ID                  NAME                  IMAGE               NODE                DESIRED STATE       CURRENT STATE                 ERROR               PORTS</span><br><span class=\"line\">qk8pvvc9nmv0        myservice_web.1       httpd:v2            swarm01             Running             Running 56 seconds ago</span><br><span class=\"line\">unwepav5z9nz         \\_ myservice_web.1   httpd:v1            swarm02             Shutdown            Shutdown 48 seconds ago</span><br><span class=\"line\">uyj0pm1df9hl        myservice_web.2       httpd:v2            swarm02             Running             Running 46 seconds ago</span><br><span class=\"line\">3j5o1tktcg1l         \\_ myservice_web.2   httpd:v1            swarm01             Shutdown            Shutdown about a minute ago</span><br><span class=\"line\">qdjov5zf5alb        myservice_web.3       httpd:v2            swarm03             Running             Running 10 seconds ago</span><br><span class=\"line\">wl1unt6lynft         \\_ myservice_web.3   httpd:v1            swarm03             Shutdown            Shutdown 12 seconds ago</span><br></pre></td></tr></table></figure>\n<p>可以看到同时只有2个容器在升级，同时第3个容器也是在间隔了30s 之后，才开始升级。</p>\n<p> –filter 过滤条件<code>docker stack ps  --filter  &quot;desired-state=running&quot;  myservice</code></p>\n<h3 id=\"4-6-6-服务健康检查\"><a href=\"#4-6-6-服务健康检查\" class=\"headerlink\" title=\"4.6.6 服务健康检查\"></a>4.6.6 服务健康检查</h3><p>对于容器而言，最简单的健康检查是进程级的健康检查，即检验进程是否存活。Docker Daemon会自动监控容器中的PID1进程，如果docker run命令中指明了restart policy，可以根据策略自动重启已结束的容器。在很多实际场景下，仅使用进程级健康检查机制还远远不够。比如，容器进程虽然依旧运行却由于应用死锁无法继续响应用户请求，这样的问题是无法通过进程监控发现的 。</p>\n<p>下面先介绍Docker容器健康检查机制，之后再介绍Docker Swarm mode的新特性。</p>\n<p><strong>Docker 原生健康检查能力</strong></p>\n<p>而自 1.12 版本之后，Docker 引入了原生的健康检查实现，可以在Dockerfile中声明应用自身的健康检测配置。HEALTHCHECK 指令声明了健康检测命令，用这个命令来判断容器主进程的服务状态是否正常，从而比较真实的反应容器实际状态。</p>\n<p>HEALTHCHECK 指令格式：</p>\n<ul>\n<li>HEALTHCHECK [选项] CMD &lt;命令&gt;：设置检查容器健康状况的命令</li>\n<li>HEALTHCHECK NONE：如果基础镜像有健康检查指令，使用这行可以屏蔽掉</li>\n</ul>\n<p>注：在Dockerfile中 HEALTHCHECK 只可以出现一次，如果写了多个，只有最后一个生效。</p>\n<p>使用包含 HEALTHCHECK 指令的dockerfile构建出来的镜像，在实例化Docker容器的时候，就具备了健康状态检查的功能。启动容器后会自动进行健康检查。</p>\n<blockquote>\n<p>HEALTHCHECK 支持下列选项：</p>\n<ul>\n<li>interval=&lt;间隔&gt;：两次健康检查的间隔，默认为 30 秒;</li>\n<li>timeout=&lt;间隔&gt;：健康检查命令运行超时时间，如果超过这个时间，本次健康检查就被视为失败，默认 30 秒;</li>\n<li>retries=&lt;次数&gt;：当连续失败指定次数后，则将容器状态视为 unhealthy，默认 3 次。</li>\n<li>start-period=&lt;间隔&gt;: 应用的启动的初始化时间，在启动过程中的健康检查失效不会计入，默认 0 秒; (从17.05)引入</li>\n</ul>\n</blockquote>\n<p>在 HEALTHCHECK [选项] CMD 后面的命令，格式和 ENTRYPOINT 一样，分为 shell 格式，和 exec 格式。命令的返回值决定了该次健康检查的成功与否：</p>\n<ul>\n<li>0：成功;</li>\n<li>1：失败;</li>\n<li>2：保留值</li>\n</ul>\n<p>容器启动之后，初始状态会为 starting (启动中)。Docker Engine会等待 interval 时间，开始执行健康检查命令，并周期性执行。如果单次检查返回值非0或者运行需要比指定 timeout 时间还长，则本次检查被认为失败。如果健康检查连续失败超过了 retries 重试次数，状态就会变为 unhealthy (不健康)。</p>\n<p>注：</p>\n<ul>\n<li>一旦有一次健康检查成功，Docker会将容器置回 healthy (健康)状态</li>\n<li>当容器的健康状态发生变化时，Docker Engine会发出一个 health_status 事件。</li>\n</ul>\n<p>假设我们有个镜像是个最简单的 Web 服务，我们希望增加健康检查来判断其 Web 服务是否在正常工作，我们可以用 curl来帮助判断，其 Dockerfile 的 HEALTHCHECK 可以这么写：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">FROM elasticsearch:5.5 </span><br><span class=\"line\"> </span><br><span class=\"line\">HEALTHCHECK --interval=5s --timeout=2s --retries=12 \\ </span><br><span class=\"line\">  CMD curl --silent --fail localhost:9200/_cluster/health || exit 1</span><br></pre></td></tr></table></figure>\n<p>然后编译镜像，并运行</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">docker build -t test/elasticsearch:5.5 . </span><br><span class=\"line\">docker run --rm -d \\ </span><br><span class=\"line\">    --name=elasticsearch \\ </span><br><span class=\"line\">    test/elasticsearch:5.5</span><br></pre></td></tr></table></figure>\n<p>执行 docker ps容器检查命令，发现过了几秒之后，Elasticsearch容器从 starting 状态进入了 healthy 状态</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker ps </span><br><span class=\"line\">CONTAINER ID        IMAGE                    COMMAND                  CREATED             STATUS                            PORTS                NAMES </span><br><span class=\"line\">c9a6e68d4a7f        test/elasticsearch:5.5   &quot;/docker-entrypoin...&quot;   2 seconds ago       Up 2 seconds (health: starting)   9200/tcp, 9300/tcp   elasticsearch </span><br><span class=\"line\">$ docker ps </span><br><span class=\"line\">CONTAINER ID        IMAGE                    COMMAND                  CREATED             STATUS                    PORTS                NAMES </span><br><span class=\"line\">c9a6e68d4a7f        test/elasticsearch:5.5   &quot;/docker-entrypoin...&quot;   14 seconds ago      Up 13 seconds (healthy)   9200/tcp, 9300/tcp   elasticsearch</span><br></pre></td></tr></table></figure>\n<p><strong>Docker Swarm健康检查能力</strong></p>\n<p>在Docker 1.13之后，在Docker Swarm mode中提供了对健康检查策略的支持。</p>\n<p>可以在 <code>docker service create</code> 命令中指明健康检查策略</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker service create -d \\</span><br><span class=\"line\">    --name=elasticsearch \\</span><br><span class=\"line\">    --health-cmd=&quot;curl --silent --fail localhost:9200/_cluster/health || exit 1&quot; \\</span><br><span class=\"line\">    --health-interval=5s \\</span><br><span class=\"line\">    --health-retries=12 \\</span><br><span class=\"line\">    --health-timeout=2s \\</span><br><span class=\"line\">    elasticsearch</span><br></pre></td></tr></table></figure>\n<p>在Swarm模式下，Swarm manager会监控服务task的健康状态，如果容器进入 <code>unhealthy</code> 状态，它会停止容器并且重新启动一个新容器来取代它。这个过程中会自动更新服务的 load balancer (routing mesh) 后端或者 DNS记录，可以保障服务的可用性。</p>\n<p>在1.13版本之后，在服务更新阶段也增加了对健康检查的支持，这样在新容器完全启动成功并进入健康状态之前，load balancer/DNS解析不会将请求发送给它。这样可以保证应用在更新过程中请求不会中断。</p>\n<p><img src=\".\\healthcheck.png\" alt=\"\"></p>\n<p>docker 官方网站有些常用容器的健康检查的例子：<a href=\"https://github.com/docker-library/healthcheck\" target=\"_blank\" rel=\"noopener\">https://github.com/docker-library/healthcheck</a></p>\n<h1 id=\"5-附录\"><a href=\"#5-附录\" class=\"headerlink\" title=\"5.附录\"></a>5.附录</h1><h2 id=\"5-1-参考文档\"><a href=\"#5-1-参考文档\" class=\"headerlink\" title=\"5.1 参考文档\"></a>5.1 参考文档</h2><p>官网文档：</p>\n<p><a href=\"https://docs.docker.com/engine/swarm/\" target=\"_blank\" rel=\"noopener\">https://docs.docker.com/engine/swarm/</a></p>\n<p><a href=\"https://docs.docker.com/get-started/part4/\" target=\"_blank\" rel=\"noopener\">https://docs.docker.com/get-started/part4/</a></p>\n<p><a href=\"https://docs.docker.com/network/overlay/#publish-ports-on-an-overlay-network\" target=\"_blank\" rel=\"noopener\">https://docs.docker.com/network/overlay/#publish-ports-on-an-overlay-network</a></p>\n<p>技术博客：</p>\n<p><a href=\"http://www.dockerinfo.net/2552.html\" target=\"_blank\" rel=\"noopener\">http://www.dockerinfo.net/2552.html</a></p>\n<p><a href=\"https://www.cnblogs.com/bigberg/p/8761047.html\" target=\"_blank\" rel=\"noopener\">https://www.cnblogs.com/bigberg/p/8761047.html</a></p>\n<p><a href=\"http://blog.51cto.com/cloudman/1968287\" target=\"_blank\" rel=\"noopener\">http://blog.51cto.com/cloudman/1968287</a> </p>\n<p><a href=\"https://neuvector.com/network-security/docker-swarm-container-networking/\" target=\"_blank\" rel=\"noopener\">https://neuvector.com/network-security/docker-swarm-container-networking/</a></p>\n<p><a href=\"http://www.uml.org.cn/yunjisuan/201708282.asp\" target=\"_blank\" rel=\"noopener\">http://www.uml.org.cn/yunjisuan/201708282.asp</a></p>\n","site":{"data":{}},"excerpt":"<p>本文主要介绍docker swarm 对于容器应用的调度管理分析。Docker swarm 1.12.0  是一个非常重要的版本，1.12.0之前的版本 docker swarm 是一个独立的项目，需要额外下载swarm镜像进行安装。1.12.0  之后的版本，swarm功能直接继承到docker engine 当中。直接使用docker CLI 可创建一个swarm 集群、可在swarm 集群中部署应用程序，同时具备管理能力。 </p>\n<p>关于swarm的详细介绍，可参考官方文档：<a href=\"https://docs.docker.com/engine/swarm/\" target=\"_blank\" rel=\"noopener\">https://docs.docker.com/engine/swarm/</a></p>\n<h1 id=\"1-Docker-Swarm-集群架构\"><a href=\"#1-Docker-Swarm-集群架构\" class=\"headerlink\" title=\"1.Docker Swarm 集群架构\"></a>1.Docker Swarm 集群架构</h1><p>Swarm 集群中，有两种类型的节点：Manager和Worker。</p>\n<p>要部署服务到swarm，需要将服务定义提交给Manager节点。Manager节点将称为work的工作单元分派 给Worker节点。</p>\n<p><img src=\"\\img\\swarm-node.png\" alt=\"\"></p>","more":"<h2 id=\"1-1-Manager-Node\"><a href=\"#1-1-Manager-Node\" class=\"headerlink\" title=\"1.1 Manager Node\"></a>1.1 Manager Node</h2><p>Manager节点处理集群管理任务：</p>\n<ul>\n<li>维护集群状态</li>\n<li>调度服务</li>\n<li>为 Swarm 提供外部可调用的 API 接口</li>\n</ul>\n<p>Manager 节点需要时刻维护和保存当前 Swarm 集群中各个节点的一致性状态，这里主要是指各个 Tasks 的执行的状态和其它节点的状态。因为 Swarm 集群是一个典型的分布式集群，在保证一致性上，Manager 节点采用 <a href=\"https://raft.github.io/raft.pdf\" target=\"_blank\" rel=\"noopener\">Raft</a> 协议来保证分布式场景下的数据一致性。</p>\n<p>通常为了保证 Manager 节点的高可用，Docker 建议采用奇数个 Manager 节点，这样的话，你可以在 Manager 失败的时候不用关机维护，Docker 官方给出如下的建议：</p>\n<ul>\n<li>3 个 Manager 节点最多可以同时容忍 1 个 Manager 节点失效的情况下保证高可用；</li>\n<li>5 个 Manager 节点最多可以同时容忍 2 个 Manager 节点失效的情况下保证高可用；</li>\n<li>N 个 Manager 节点最多可以同时容忍 (N−1)/2个 Manager 节点失效的情况下保证高可用；</li>\n<li>最多最多的情况下，使用 7 个 Manager 节点就够了，否则反而会降低集群的性能了。</li>\n</ul>\n<blockquote>\n<p><strong>重要说明</strong>：添加更多管理器并不意味着可扩展性更高或性能更高。一般而言，情况正好相反。</p>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>群体大小</th>\n<th>多数</th>\n<th>容错</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>1</td>\n<td>1</td>\n<td>0</td>\n</tr>\n<tr>\n<td>2</td>\n<td>2</td>\n<td>0</td>\n</tr>\n<tr>\n<td><strong>3</strong></td>\n<td>2</td>\n<td><strong>1</strong></td>\n</tr>\n<tr>\n<td>4</td>\n<td>3</td>\n<td>1</td>\n</tr>\n<tr>\n<td><strong>5</strong></td>\n<td>3</td>\n<td><strong>2</strong></td>\n</tr>\n<tr>\n<td>6</td>\n<td>4</td>\n<td>2</td>\n</tr>\n<tr>\n<td><strong>7</strong></td>\n<td>4</td>\n<td><strong>3</strong></td>\n</tr>\n<tr>\n<td>8</td>\n<td>5</td>\n<td>3</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"1-2-Worker-Node\"><a href=\"#1-2-Worker-Node\" class=\"headerlink\" title=\"1.2 Worker Node\"></a>1.2 Worker Node</h2><p>Worker节点接收并执行从Manager节点分派的任务。默认情况下，Manager节点还将服务作为Worker节点运行，但也可以将它们配置为仅运行Manage任务。 可将Manager可用性设置为<code>Drain</code> ，这样服务就不会被分配到Manager节点上运行了。</p>\n<p>Worker节点不参与Raft分布状态，做出调度决策或提供群模式HTTP API。</p>\n<p>通过 docker node promote 命令将一个 Worker 节点提升为 Manager 节点。通常情况下，该命令使用在维护的过程中，需要将 Manager 节点占时下线进行维护操作。同样可以使用 docker node demote 将某个 manager 节点降级为 worker 节点。 </p>\n<h2 id=\"1-3-小结\"><a href=\"#1-3-小结\" class=\"headerlink\" title=\"1.3 小结\"></a>1.3 小结</h2><p>综上可知， Swarm 集群的管理工作是由manager节点实现。如上图所示，manager节点实现的功能主要包括：node discovery，scheduler,cluster管理等。同时，为了保证Manager 节点的高可用，Manager 节点需要时刻维护和保存当前 Swarm 集群中各个节点的一致性状态。在保证一致性上，Manager 节点采用 <a href=\"https://raft.github.io/raft.pdf\" target=\"_blank\" rel=\"noopener\">Raft</a> 协议来保证分布式场景下的数据一致性；</p>\n<p>Docker Swarm内置了Raft一致性算法，可以保证分布式系统的数据保持一致性同步。Etcd, Consul等高可用键值存储系统也是采用了这种算法。这个算法的作用简单点说就是随时保证集群中有一个Leader，由Leader接收数据更新，再同步到其他各个Follower节点。在Swarm中的作用表现为当一个Leader 节点 down掉时，系统会立即选取出另一个Leader节点，由于这个节点同步了之前节点的所有数据，所以可以无缝地管理集群。</p>\n<p>Raft的详细解释可以参考<a href=\"http://thesecretlivesofdata.com/raft/\" target=\"_blank\" rel=\"noopener\">《The Secret Lives of Data–Raft: Understandable Distributed Consensus》</a>。</p>\n<p>为保证Magnger高可用，一般部署3-7个Manager 节点。而在云桌面的场景下，可能一个集群就只有两台服务器。显然此时是无法满足Manager 3个节点的高可用要求。因此我们面对的挑战就是如何修改swarm Manager Raft相关部分代码，使其在只有2个节点的情况下，也可以保证一个节点宕机的高可用。</p>\n<h1 id=\"2-Docker-Swarm-网络架构\"><a href=\"#2-Docker-Swarm-网络架构\" class=\"headerlink\" title=\"2.Docker Swarm 网络架构\"></a>2.Docker Swarm 网络架构</h1><h2 id=\"2-1-网络概述\"><a href=\"#2-1-网络概述\" class=\"headerlink\" title=\"2.1 网络概述\"></a>2.1 网络概述</h2><p>我们知道，Docker 的几种网络方案：none、host、bridge 和 joined 容器，它们解决了单个 Docker Host 内容器通信的问题。那么跨主机容器间通信的方案又有哪些呢？</p>\n<p>跨主机网络方案包括：</p>\n<ol>\n<li>docker 原生的 overlay 和 macvlan。</li>\n<li>第三方方案：常用的包括 flannel、weave 和 calico。</li>\n</ol>\n<p>Docker 网络是一个非常活跃的技术领域，不断有新的方案开发出来，那么要问个非常重要的问题了： 如此众多的方案是如何与 docker 集成在一起的？答案是：libnetwork 以及 CNM。</p>\n<p>libnetwork 是 docker 容器网络库，最核心的内容是其定义的 Container Network Model (CNM)，这个模型对容器网络进行了抽象，由以下三类组件组成：</p>\n<ol>\n<li>Sandbox：Sandbox 是容器的网络栈，包含容器的 interface、路由表和 DNS 设置。 Linux Network Namespace 是 Sandbox 的标准实现。Sandbox 可以包含来自不同 Network 的 Endpoint。</li>\n<li>Endpoint：Endpoint 的作用是将 Sandbox 接入 Network。Endpoint 的典型实现是 veth pair，后面我们会举例。一个 Endpoint 只能属于一个网络，也只能属于一个 Sandbox。</li>\n<li>Network：Network 包含一组 Endpoint，同一 Network 的 Endpoint 可以直接通信。Network 的实现可以是 Linux Bridge、VLAN 等。</li>\n</ol>\n<p>如图所示两个容器，一个容器一个 Sandbox，每个 Sandbox 都有一个 Endpoint 连接到 Network 1，第二个 Sandbox 还有一个 Endpoint 将其接入 Network 2.</p>\n<p>libnetwork CNM 定义了 docker 容器的网络模型，按照该模型开发出的 driver 就能与 docker daemon 协同工作，实现容器网络。docker 原生的 driver 包括 none、bridge、overlay 和 macvlan，第三方 driver 包括 flannel、weave、calico 等。</p>\n<h2 id=\"2-2-Overlay网络\"><a href=\"#2-2-Overlay网络\" class=\"headerlink\" title=\"2.2 Overlay网络\"></a>2.2 Overlay网络</h2><p>Docker Swarm 内置的跨主机容器通信方案是overlay网络，这是一个基于VxLAN 协议的网络实现。VxLAN 可将二层数据封装到 UDP 进行传输，VxLAN 提供与 VLAN 相同的以太网二层服务，但是拥有更强的扩展性和灵活性。 overlay 通过虚拟出一个子网，让处于不同主机的容器能透明地使用这个子网。所以跨主机的容器通信就变成了在同一个子网下的容器通信，看上去就像是同一主机下的bridge网络通信。</p>\n<p><img src=\"\\img\\overlay-vxlan.png\" alt=\"1534939506173\"></p>\n<p>根据vxlan的作用知道，它是要在三层网络中虚拟出二层网络，即跨网段建立虚拟子网。简单的理解就是把发送到虚拟子网地址<code>10.0.0.3</code>的报文封装为发送到真实IP<code>192.168.1.3</code>的报文。这必然会有更大的数据开销，但却简化了集群的网络连接，让分布在不同主机的容器好像都在同一个主机上一样 。</p>\n<p><code>overlay</code>网络会创建多个Docker主机之间的分布式网络。该网络位于（覆盖）特定于主机的网络之上，允许连接到它的容器（包括群集服务容器）安全地进行通信。Docker透明地处理每个数据包与正确的Docker守护程序主机和正确的目标容器的路由。</p>\n<p>初始化swarm或将Docker主机加入现有swarm时，会在该Docker主机上创建两个新网络：</p>\n<ul>\n<li>ingress overlay 网络，处理与swarm集群服务相关的控制和数据流量。创建群组服务并且不将其连接到用户定义的覆盖网络时，服务将默认连接到ingress overlay网络。集群中只能有一个ingress overlay 网络。</li>\n<li>docker_gwbridge 桥接网络，它将各个Docker守护程序连接到参与该群集的其他Docker守护进程。同时该docker_gwbridge 网络将为主机上的容器提供访问外网的能力。</li>\n</ul>\n<p>服务或容器一次可以连接到多个网络。服务或容器只能通过它们各自连接的网络进行通信。</p>\n<h3 id=\"2-2-1-创建overlay网络\"><a href=\"#2-2-1-创建overlay网络\" class=\"headerlink\" title=\"2.2.1 创建overlay网络\"></a>2.2.1 创建overlay网络</h3><blockquote>\n<p><strong>先决条件</strong>：</p>\n<ul>\n<li><p>使用overlay 网络的Docker守护程序的防火墙规则</p>\n<p>您需要以下端口打开来往于overlay 网络上的每个Docker主机的流量：</p>\n<ul>\n<li>用于集群管理通信的TCP端口2377</li>\n<li>TCP和UDP端口7946用于节点之间的通信</li>\n<li>UDP端口4789用于覆盖网络流量</li>\n</ul>\n</li>\n<li><p>在创建overlay 网络之前，您需要将Docker守护程序初始化为swarm管理器，<code>docker swarm init</code>或者使用它将其连接到现有的swarm <code>docker swarm join</code>。这些中的任何一个都会创建默认ingress overlay 网络，默认情况下 由群服务使用。即使您从未计划使用群组服务，也需要执行此操作。之后，您可以创建其他用户定义的overlay 网络。</p>\n</li>\n</ul>\n</blockquote>\n<p>要创建用于swarm服务的覆盖网络，请使用如下命令：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker network create -d overlay my-overlay</span><br></pre></td></tr></table></figure>\n<p>要创建可由群集服务或独立容器用于与在其他Docker守护程序上运行的其他独立容器通信的覆盖网络，请添加<code>--attachable</code>标志：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker network create -d overlay --attachable my-attachable-overlay</span><br></pre></td></tr></table></figure>\n<p>同时可以指定IP地址范围，子网，网关和其他选项。详情<code>docker network create --help</code>请见</p>\n<h3 id=\"2-2-2-加密overlay网络\"><a href=\"#2-2-2-加密overlay网络\" class=\"headerlink\" title=\"2.2.2 加密overlay网络\"></a>2.2.2 加密overlay网络</h3><p>默认情况下，使用GCM模式下的AES算法加密所有swarm群集服务管理流量 。集群中的管理器节点每隔12小时轮换用于加密数据的密钥。</p>\n<p>要加密应用程序数据，请<code>--opt encrypted</code>在创建覆盖网络时添加。这样可以在vxlan级别启用IPSEC加密。此加密会产生不可忽视的性能损失，因此您应该在生产中使用此选项之前对其进行测试。</p>\n<p>启用覆盖加密后，Docker会在所有节点之间创建IPSEC隧道，在这些节点上为连接到覆盖网络的服务安排任务。这些隧道还在GCM模式下使用AES算法，管理器节点每12小时自动旋转密钥。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">docker network create --opt encrypted --driver overlay my-encrypted-network</span><br></pre></td></tr></table></figure>\n<h3 id=\"2-2-3-暴露服务端口\"><a href=\"#2-2-3-暴露服务端口\" class=\"headerlink\" title=\"2.2.3 暴露服务端口\"></a>2.2.3 暴露服务端口</h3><p>对于连接到同一个swarm集群的服务来说，它们之间所有的端口都是互相暴露的。对于可在服务外部访问的端口，必须使用 -p ( 或者 –publish) 发布该端口。</p>\n<table>\n<thead>\n<tr>\n<th>Flag value</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><code>-p 8080:80</code> or <code>-p published=8080,target=80</code></td>\n<td>将服务上的TCP端口80映射到routing mesh上的端口8080。</td>\n</tr>\n<tr>\n<td><code>-p 8080:80/udp</code> or <code>-p published=8080,target=80,protocol=udp</code></td>\n<td>将服务上的UDP端口80映射到routing mesh上的端口8080。</td>\n</tr>\n</tbody>\n</table>\n<h3 id=\"2-2-4-实现原理\"><a href=\"#2-2-4-实现原理\" class=\"headerlink\" title=\"2.2.4 实现原理\"></a>2.2.4 实现原理</h3><p>下面我们讨论下overlay 网络的具体实现：</p>\n<p>docker 会为每个 overlay 网络创建一个独立的 network namespace，其中会有一个 linux bridge br0，endpoint 还是由 veth pair 实现，一端连接到容器中（即 eth0），另一端连接到 namespace 的 br0 上。</p>\n<p>br0 除了连接所有的 endpoint，还会连接一个 vxlan 设备，用于与其他 host 建立 vxlan tunnel。容器之间的数据就是通过这个 tunnel 通信的。逻辑网络拓扑结构如图所示：</p>\n<p><img src=\"./overlay.jpg\" alt=\"\"></p>\n<h2 id=\"2-3-Ingress-Routing-Mesh\"><a href=\"#2-3-Ingress-Routing-Mesh\" class=\"headerlink\" title=\"2.3 Ingress Routing Mesh\"></a>2.3 Ingress Routing Mesh</h2><p>上文暴露服务端口中有讲到routing mesh，现在来详细介绍这一网络功能。</p>\n<p>默认情况下，发布端口的swarm服务使用routing mesh来实现。当客户端连接到任何swarm节点上的已发布端口（无论它是否正在运行给定服务）时，客户端请求将被透明地重定向到正在运行该服务的worker。实际上，Docker充当集群服务的负载均衡器。使用routing mesh的服务以<em>虚拟IP（VIP）模式运行</em>。即使在每个节点上运行的服务（通过<code>--global</code>标志）也使用routing mesh。使用routing mesh时，无法保证哪个Docker节点服务客户端请求。</p>\n<p>要在群集中使用 ingress 网络，您需要在启用群集模式之前在群集节点之间打开以下端口：</p>\n<ul>\n<li>端口<code>7946</code>  TCP / UDP用于容器网络发现。</li>\n<li>端口<code>4789</code>  UDP 用于容器 ingress 网络。</li>\n</ul>\n<p>例如，以下命令将nginx容器中的端口80发布到群集中任何节点的端口8080：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker service create \\</span><br><span class=\"line\">  --name my-web \\</span><br><span class=\"line\">  --publish published=8080,target=80 \\</span><br><span class=\"line\">  --replicas 2 \\</span><br><span class=\"line\">  nginx</span><br></pre></td></tr></table></figure>\n<p>当您在任何节点上访问端口8080时，Docker会将您的请求路由到活动容器。在群集节点本身上，端口8080实际上可能不受约束，但routing mesh 知道如何路由流量并防止发生任何端口冲突。</p>\n<p>routing mesh在已发布的端口上侦听分配给该节点的任何IP地址。对于可外部路由的IP地址，该端口可从主机外部获得。对于所有其他IP地址，只能从主机内访问。</p>\n<p><img src=\"\\img\\ingress-routing-mesh.png\" alt=\"\"></p>\n<p>如果想要绕过routing mesh，可以使用<em>DNS循环（DNSRR）模式</em>启动服务，方法是将<code>--endpoint-mode</code>标志设置为<code>dnsrr</code>。这样就必须在服务前运行自己的负载均衡器。Docker主机上的服务名称的DNS查询返回运行该服务的节点的IP地址列表。配置负载均衡器以使用此列表并平衡节点之间的流量。</p>\n<p>对于我们云桌面应用场景来说，直接利用routing mesh 功能即可，不需要额外搭建负载均衡器。</p>\n<h2 id=\"2-4-服务发现\"><a href=\"#2-4-服务发现\" class=\"headerlink\" title=\"2.4 服务发现\"></a>2.4 服务发现</h2><p>Docker Swarm 原生就提供了服务发现的功能。要使用服务发现，需要相互通信的 services 必须属于同一个 overlay 网络，所以我们先得创建一个新的 overlay 网络。 直接使用 <code>ingress</code> 行不行？很遗憾，目前 <code>ingress</code> 没有提供服务发现，必须创建自己的 overlay 网络。</p>\n<p>Docker Swarm mode下会为每个节点的Docker engine内置一个DNS server，各个节点间的DNS server通过control plane的gossip协议互相交互信息。此处DNS server用于容器间的服务发现。swarm mode会为每个 –net=自定义网络  的service分配一个DNS entry。目前必须是自定义网络，比如overaly。而bridge和routing mesh 的service，是不会分配DNS的。</p>\n<p>那么，下面就来详细介绍服务发现的原理。</p>\n<p>每个Docker容器都有一个DNS解析器，它将DNS查询转发到docker engine，该引擎充当DNS服务器。docker 引擎收到请求后就会在发出请求的容器所在的所有网络中，检查域名对应的是不是一个容器或者是服务，如果是，docker引擎就会从存储的key-value建值对中查找这个容器名、任务名、或者服务名对应的IP地址，并把这个IP地址或者是服务的虚拟IP地址返回给发起请求的域名解析器。  </p>\n<p>由上可知，docker的服务发现的作用范围是网络级别，也就意味着只有在同一个网络上的容器或任务才能利用内嵌的DNS服务来相互发现，不在同一个网络里面的服务是不能解析名称的，另外，为了安全和性能只有当一个节点上有容器或任务在某个网络里面时，这个节点才会存储那个网络里面的DNS记录。</p>\n<p>如果目的容器或服务和源容器不在同一个网络里面，Docker引擎会把这个DNS查询转发到配置的默认DNS服务  。</p>\n<p><img src=\"\\img\\service-dns.png\" alt=\"\"></p>\n<p>在上面的例子中，总共有两个服务myservice和client，其中myservice有两个容器，这两个服务在同一个网里面。在client里针对docker.com和myservice各执行了一个curl操作，下面时执行的流程： </p>\n<ul>\n<li>为了client解析docker.com和myservice，DNS查询进行初始化</li>\n<li>容器内建的解析器在127.0.0.11:53拦截到这个DNS查询请求，并把请求转发到docker引擎的DNS服务</li>\n<li>myservice 被解析成服务对应的虚拟IP（10.0.0.3）。在接下来的内部负载均衡阶段再被解析成一个具体任务容器的IP地址。如果是容器名称这一步直接解析成容器对应的IP地址（10.0.0.4或者10.0.0.5）。</li>\n<li>docker.com 在docker引擎或者mynet网络上不能被解析成服务，所以这个请求被转发到外部DNS，例如配置好的默认DNS服务器（8.8.8.8）上。</li>\n</ul>\n<p>Docker1.12+的服务发现和负载均衡是结合到一起的， 实现办法有两种，DNS轮询和IPVS。 配置参数 –endpoint-mode  (vip or dnsrr) ，默认使用VIP方式。DNS轮询有一些缺点，比如一些应用可能缓存DNS请求， 导致容器变化之后DNS不能实时更新；DNS生效时间也导致不能实时反映服务变化情况。VIP和IPVS原理上比较容易理解， 就是Docker为每个服务分配了一个VIP，DNS解析服务名称或者自定义的别名到这个VIP上。由于VIP本身没有容器提供服务，Docker把到VIP的请求通过IPVS技术负载到后面的容器上。</p>\n<p>因此正常情况下，使用默认的VIP方式即可。</p>\n<h2 id=\"3-5-负载均衡\"><a href=\"#3-5-负载均衡\" class=\"headerlink\" title=\"3.5 负载均衡\"></a>3.5 负载均衡</h2><p>负载均衡分为两种：</p>\n<p>Swarm集群内的service之间的相互访问需要做负载均衡，称为内部负载均衡（Internal LB）；</p>\n<p>从Swarm集群外部访问服务的公开端口，也需要做负载均衡，称外部负载均衡(Exteral LB or Ingress LB)。</p>\n<h3 id=\"3-5-1-Internal-LB\"><a href=\"#3-5-1-Internal-LB\" class=\"headerlink\" title=\"3.5.1 Internal LB\"></a>3.5.1 Internal LB</h3><p>内部负载均衡就是我们在上一段提到的服务发现，集群内部通过DNS访问service时，Swarm默认通过VIP（virtual IP）、iptables、IPVS转发到某个容器。 </p>\n<p><img src=\"\\img\\intelnal-lb.jpg\" alt=\"\"></p>\n<p>当在docker swarm集群模式下创建一个服务时，会自动在服务所属的网络上给服务额外的分配一个虚拟IP，当解析服务名字时就会返回这个虚拟IP。对虚拟IP的请求会通过overlay网络自动的负载到这个服务所有的健康任务上。这个方式也避免了客户端的负载均衡，因为只有单独的一个IP会返回到客户端，docker会处理虚拟IP到具体任务的路由，并把请求平均的分配给所有的健康任务。  </p>\n<p><img src=\".\\internal-lb-2.png\" alt=\"\"></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># 创建overlay网络：mynet </span><br><span class=\"line\">$ docker network create -d overlay mynet  </span><br><span class=\"line\">a59umzkdj2r0ua7x8jxd84dhr </span><br><span class=\"line\"># 利用mynet网络创建myservice服务，并复制两份  </span><br><span class=\"line\">$ docker service create --network mynet --name myservice --replicas 2 busybox ping localhost  </span><br><span class=\"line\">78t5r8cr0f0h6k2c3k7ih4l6f5</span><br><span class=\"line\"># 通过下面的命令查看myservice对应的虚拟IP </span><br><span class=\"line\">$ docker service inspect myservice  </span><br><span class=\"line\">...</span><br><span class=\"line\">&quot;VirtualIPs&quot;: [ </span><br><span class=\"line\">\t&#123;  </span><br><span class=\"line\">     &quot;NetworkID&quot;: &quot;a59umzkdj2r0ua7x8jxd84dhr&quot;,  </span><br><span class=\"line\">     \t\t\t&quot;Addr&quot;: &quot;10.0.0.3/24&quot;  </span><br><span class=\"line\">      &#125;,  </span><br><span class=\"line\">]</span><br></pre></td></tr></table></figure>\n<p>注：swarm中服务还有另外一种负载均衡技术可选DNS round robin (DNS RR) （在创建服务时通过–endpoint-mode配置项指定），在DNSRR模式下，docker不再为服务创建VIP，docker DNS服务直接利用轮询的策略把服务名称直接解析成一个容器的IP地址。 </p>\n<h3 id=\"3-5-2-Exteral-LB\"><a href=\"#3-5-2-Exteral-LB\" class=\"headerlink\" title=\"3.5.2 Exteral LB\"></a>3.5.2 Exteral LB</h3><p>Exteral LB(Ingress LB 或者 Swarm Mode Routing Mesh) 看名字就知道，这个负载均衡方式和前面提到的Ingress网络有关。Swarm网络要提供对外访问的服务就需要打开公开端口，并映射到宿主机。Exteral LB就是外部通过公开端口访问集群时做的负载均衡。</p>\n<p>当创建或更新一个服务时，你可以利用–publish选项把一个服务暴露到外部，在docker swarm模式下发布一个端口意味着在集群中的所有节点都会监听这个端口，这时当访问一个监听了端口但是并没有对应服务运行在其上的节点会发生什么呢？接下来就该我们的路由网（routing mesh）出场了，路由网时docker1.12引入的一个新特性，它结合了IPVS和iptables创建了一个强大的集群范围的L4层负载均衡，它使所有节点接收服务暴露端口的请求成为可能。当任意节点接收到针对某个服务暴露的TCP/UDP端口的请求时，这个节点会利用预先定义过的Ingress overlay网络，把请求转发给服务对应的虚拟IP。ingress网络和其他的overlay网络一样，只是它的目的是为了转换来自客户端到集群的请求，它也是利用我们前一小节介绍过的基于VIP的负载均衡技术。 </p>\n<p>启动服务后，你可以为应用程序创建外部DNS记录，并将其映射到任何或所有Docker swarm节点。你无需担心你的容器具体运行在那个节点上，因为有了路由网这个特性后，你的集群看起来就像是单独的一个节点一样。  </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#在集群中创建一个复制两份的服务，并暴露在8000端口  </span><br><span class=\"line\">$ docker service create --name app --replicas 2 --network appnet --publish 8000:80 nginx</span><br></pre></td></tr></table></figure>\n<p><img src=\"\\img\\external-routing-mesh.png\" alt=\"\"></p>\n<p>上面这个图表明了路由网是怎么工作的： </p>\n<ul>\n<li>服务（app）拥有两份复制，并把端口映射到外部端口的8000</li>\n<li>路由网在集群中的所有节点上都暴露出8000</li>\n<li>外部对服务app的请求可以是任意节点，在本例子中外部的负载均衡器将请求转发到了没有app服务的主机上</li>\n<li>docker swarm的IPVS利用ingress overlay网路将请求重新转发到运行着app服务的节点的容器中</li>\n</ul>\n<p>注：以上服务发现和负载均衡参考文档 <a href=\"https://success.docker.com/article/ucp-service-discovery\" target=\"_blank\" rel=\"noopener\">https://success.docker.com/article/ucp-service-discovery</a></p>\n<h1 id=\"3-Docker-Swarm-存储架构\"><a href=\"#3-Docker-Swarm-存储架构\" class=\"headerlink\" title=\"3.Docker Swarm 存储架构\"></a>3.Docker Swarm 存储架构</h1><p>从业务数据的角度看，容器可以分为两类：无状态（stateless）容器和有状态（stateful）容器。</p>\n<p>无状态是指容器在运行过程中不需要保存数据，每次访问的结果不依赖上一次访问，比如提供静态页面的 web 服务器。有状态是指容器需要保存数据，而且数据会发生变化，访问的结果依赖之前请求的处理结果，最典型的就是数据库服务器。</p>\n<p>简单来讲，状态（state）就是数据，如果容器需要处理并存储数据，它就是有状态的，反之则无状态。</p>\n<p>对于有状态的容器，如何保存数据呢？</p>\n<h2 id=\"3-1-Storage-Driver\"><a href=\"#3-1-Storage-Driver\" class=\"headerlink\" title=\"3.1 Storage Driver\"></a>3.1 Storage Driver</h2><p>Docker在启动容器的时候，需要创建文件系统，为rootfs提供挂载点。最底层的引导文件系统bootfs主要包含 bootloader和kernel，bootloader主要是引导加载kernel，当kernel被加载到内存中后 bootfs就被umount了。 rootfs包含的就是典型 Linux 系统中的/dev，/proc，/bin，/etc等标准目录和文件。</p>\n<p>Docker 模型的核心部分是有效利用分层镜像机制，镜像可以通过分层来进行继承，基于基础镜像（没有父镜像），可以制作各种具体的应用镜像。Docker1.10引入新的可寻址存储模型，使用安全内容哈希代替随机的UUID管理镜像。同时，Docker提供了迁移工具，将已经存在的镜像迁移到新模型上。不同 Docker 容器就可以共享一些基础的文件系统层，同时再加上自己独有的可读写层，大大提高了存储的效率。其中主要的机制就是分层模型和将不同目录挂载到同一个虚拟文件系统。</p>\n<p>Docker存储方式提供管理分层镜像和容器的可读写层的具体实现。最初Docker仅能在支持AUFS文件系统的ubuntu发行版上运行，但是由于AUFS未能加入Linux内核，为了寻求兼容性、扩展性，Docker在内部通过graphdriver机制这种可扩展的方式来实现对不同文件系统的支持。</p>\n<p><img src=\".\\storage-driver.png\" alt=\"\"></p>\n<p>如上图所示，容器由最上面一个可写的容器层，以及若干只读的镜像层组成，容器的数据就存放在这些层中。这样的分层结构最大的特性是 Copy-on-Write：</p>\n<ol>\n<li>新数据会直接存放在最上面的容器层。</li>\n<li>修改现有数据会先从镜像层将数据复制到容器层，修改后的数据直接保存在容器层中，镜像层保持不变。</li>\n<li>如果多个层中有命名相同的文件，用户只能看到最上面那层中的文件。</li>\n</ol>\n<p>分层结构使镜像和容器的创建、共享以及分发变得非常高效，而这些都要归功于 Docker storage driver。正是 storage driver 实现了多层数据的堆叠并为用户提供一个单一的合并之后的统一视图。</p>\n<p>Docker 支持多种 storage driver，有 AUFS、Device Mapper、Btrfs、OverlayFS、VFS 和 ZFS。它们都能实现分层的架构，同时又有各自的特性。关于这些方案的对比可以查阅相关文档（<a href=\"http://dockone.io/article/1513）。对于\" target=\"_blank\" rel=\"noopener\">http://dockone.io/article/1513）。对于</a> Docker 用户来说，具体选择使用哪个 storage driver 是一个难题，不过 Docker 官方给出了一个简单的答案： <strong>优先使用 Linux 发行版默认的 storage driver</strong>。 使用<code>docker info</code>查看系统默认的storage driver。</p>\n<p>使用的storage driver是与主机上的Backing Filesystem有关的。storage driver 是专门用来存放docker容器和镜像的，Backing Filesystem指的是主机的文件系统。默认的centos7 driver 是overlay2，对应的Backing Filesystem 是ext4。这个应该是没有问题的。storage driver 和Backing Filesystem 对应关系见下表。 </p>\n<p><img src=\".\\storage_backfing.png\" alt=\"1534987414895\"></p>\n<p>对于某些容器，直接将数据放在由 storage driver 维护的层中是很好的选择，比如那些无状态的应用。无状态意味着容器没有需要持久化的数据，随时可以从镜像直接创建。</p>\n<p>比如 busybox，它是一个工具箱，我们启动 busybox 是为了执行诸如 wget，ping 之类的命令，不需要保存数据供以后使用，使用完直接退出，容器删除时存放在容器层中的工作数据也一起被删除，这没问题，下次再启动新容器即可。</p>\n<p>但对于另一类应用这种方式就不合适了，它们有持久化数据的需求，容器启动时需要加载已有的数据，容器销毁时希望保留产生的新数据，也就是说，这类容器是有状态的。</p>\n<h2 id=\"3-2-数据持久化方案\"><a href=\"#3-2-数据持久化方案\" class=\"headerlink\" title=\"3.2 数据持久化方案\"></a>3.2 数据持久化方案</h2><p>那么对于有状态的容器，如何保存数据呢？</p>\n<p>选项一：打包在容器里。</p>\n<p>显然不行。除非数据不会发生变化，否则，如何在多个副本直接保持同步呢？</p>\n<p>选项二：数据放在 Docker 主机的本地目录中，通过 volume 映射到容器里。</p>\n<p>位于同一个主机的副本倒是能够共享这个 volume，但不同主机中的副本如何同步呢？</p>\n<p>选项三：利用 Docker 的 volume driver，由外部 storage provider 管理和提供 volume，所有 Docker 主机 volume 将挂载到各个副本。</p>\n<p>这是目前最佳的方案。volume 不依赖 Docker 主机和容器，生命周期由 storage provider 管理，volume 的高可用和数据有效性也全权由 provider 负责，Docker 只管使用。</p>\n<p>我们知道Docker 单机的存储方案里面，通过 data volume 可以存储容器的状态。不过其本质是 Docker 主机本地的目录。 本地目录就存在一个隐患：如果 Docker Host 宕机了，如何恢复容器？ 一个办法就是定期备份数据，但这种方案还是会丢失从上次备份到宕机这段时间的数据。更好的方案是由专门的 storage provider 提供 volume，Docker 从 provider 那里获取 volume 并挂载到容器。这样即使 Host 挂了，也可以立刻在其他可用 Host 上启动相同镜像的容器，挂载之前使用的 volume，这样就不会有数据丢失。</p>\n<p>还有一个容易混淆的存储方案，是通过在Docker主机上挂载共享文件系统（例如Ceph，GlusterFS，NFS）。如下图所示：</p>\n<p><img src=\".\\Chart_Multi-Host-Persistence-Shared-Among-Containers.png\" alt=\"\"></p>\n<p>在运行Docker容器的每个主机上配置分布式文件系统。通过创建一致的命名约定和统一命名空间，所有正在运行的容器都可以访问底层的共享存储后端。共享文件系统存储方案利用分布式文件系统与显式存储技术相结合。由于挂载点在所有节点上都可用，因此可以利用它在容器之间创建共享挂载点。这种方案其实是属于选项二的一种变种。</p>\n<p>尽管该方案对于特定用例而言，是Docker 存储方案的一个有价值的补充，但它具有限制容器到特定主机的可移植性的显着缺点。它也没有利用针对数据密集型工作负载优化的专用存储后端。为了解决这些限制，volume plugins 被添加到Docker中，将容器的功能扩展到各种存储后端，并且而无需强制更改应用程序设计或部署体系结构。 这就是我们下节将详细介绍的 Volume Driver。</p>\n<h2 id=\"3-3-Volume-Driver\"><a href=\"#3-3-Volume-Driver\" class=\"headerlink\" title=\"3.3 Volume Driver\"></a>3.3 Volume Driver</h2><p>假设有两个 Dokcer 主机，Host1 运行了一个 MySQL 容器，为了保护数据，data volume 由 storage provider 提供，如下图所示。 </p>\n<p><img src=\".\\storage-provider-1.png\" alt=\"\"></p>\n<p>当 Host1 发生故障，我们会在 Host2 上启动相同的 MySQL 镜像，并挂载 data volume。 </p>\n<p><img src=\".\\storage-provider-2.png\" alt=\"\"></p>\n<p>Docker 是如何实现这个跨主机管理 data volume 方案的呢？</p>\n<p><strong>答案是 volume driver。</strong></p>\n<p>利用 Docker 的 volume driver，由外部 storage provider 管理和提供 volume，所有 Docker 主机 volume 将挂载到各个副本。任何一个 data volume 都是由 driver 管理的，创建 volume 时如果不特别指定，将使用 <code>local</code> 类型的 driver，即从 Docker Host 的本地目录中分配存储空间。如果要支持跨主机的 volume，则需要使用第三方 driver。</p>\n<p>目前已经有很多可用的 driver，比如使用 Azure File Storage 的 driver，使用 GlusterFS 的 driver，完整的列表可参考 <a href=\"https://docs.docker.com/engine/extend/legacy_plugins/#volume-plugins\" target=\"_blank\" rel=\"noopener\">https://docs.docker.com/engine/extend/legacy_plugins/#volume-plugins</a> </p>\n<p><img src=\".\\Docker-Volume-Plugin-Architecture.png\" alt=\"\"></p>\n<p>Volume drivers 让我们将应用程序与底层存储系统隔离。例如，如果原先服务使用一个 NFS驱动器，现在可以在不改变应用程序的逻辑下，直接更新服务使用不同的驱动程序，例如将数据存储在云端,。 </p>\n<p>通过<code>docker volume create</code> 命令可以直接创建volume，同时在参数中指定driver 类型。    下面的例子使用vieux/sshfs  这种类型的Volume drivers。首先创建一个单独的 volume，然后创建容器引用该volume。</p>\n<h3 id=\"3-3-1-安装driver插件\"><a href=\"#3-3-1-安装driver插件\" class=\"headerlink\" title=\"3.3.1 安装driver插件\"></a>3.3.1 安装driver插件</h3><p>这个示例假设有两个节点，两个节点直接可以通过ssh互相连接。在Docker 主机上，安装<code>vieux/sshfs</code> 插件。</p>\n<figure class=\"highlight docker\"><figcaption><span>plugin install --grant-all-permissions vieux/sshfs</span></figcaption><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">docker plugin install --grant-all-permissions vieux/sshfs</span><br></pre></td></tr></table></figure>\n<h3 id=\"3-3-2-创建volume\"><a href=\"#3-3-2-创建volume\" class=\"headerlink\" title=\"3.3.2 创建volume\"></a>3.3.2 创建volume</h3><p>本例中指定一个SSH密码，但如果两个主机配置了共享密钥，可以省略的密码。每个volume driver 可以有零个或多个可配置选项，每一个都指定使用 -o 标志。 </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">docker volume create --driver vieux/sshfs \\</span><br><span class=\"line\">  -o sshcmd=test@node2:/home/test \\</span><br><span class=\"line\">  -o password=testpassword \\</span><br><span class=\"line\">  sshvolume</span><br></pre></td></tr></table></figure>\n<h3 id=\"3-3-3-使用volume\"><a href=\"#3-3-3-使用volume\" class=\"headerlink\" title=\"3.3.3 使用volume\"></a>3.3.3 使用volume</h3><p>本例中指定一个SSH密码,但如果两个主机配置了共享密钥,您可以省略的密码。每个卷司机可以有零个或多个可配置的选项。如果 <strong>volume driver</strong>  需要传递参数选项,你必须使用 <strong>–mount</strong>  挂载卷,而不是使用 -v。 </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker run -d \\</span><br><span class=\"line\">  --name sshfs-container \\</span><br><span class=\"line\">  --volume-driver vieux/sshfs \\</span><br><span class=\"line\">  --mount src=sshvolume,target=/app,volume-opt=sshcmd=test@node2:/home/test,volume-opt=password=testpassword \\</span><br><span class=\"line\">  nginx:latest</span><br></pre></td></tr></table></figure>\n<p>关于volume driver更多的介绍，可以查看官方文档：<a href=\"https://docs.docker.com/storage/volumes/#use-a-volume-driver\" target=\"_blank\" rel=\"noopener\">https://docs.docker.com/storage/volumes/#use-a-volume-driver</a></p>\n<p><a href=\"https://docs.docker.com/storage/volumes/\" target=\"_blank\" rel=\"noopener\">https://docs.docker.com/storage/volumes/</a></p>\n<h1 id=\"4-Docker-Swarm-服务架构\"><a href=\"#4-Docker-Swarm-服务架构\" class=\"headerlink\" title=\"4.Docker Swarm 服务架构\"></a>4.Docker Swarm 服务架构</h1><p>我们知道，Docker 场景下，应用是以容器的形式运行并提供服务的。而在Swarm场景下，任务是一个个具体的容器，通过抽象出服务的概念，来管理具有一定关联关系的任务容器。 </p>\n<p>为了将某一个服务封装为 Service 在Swarm 中部署执行，我们需要通过指定容器 image 以及需要在容器中执行的 Commands 来创建你的 Service，除此之外，还需要配置如下选项，</p>\n<ul>\n<li>指定可以在 Swarm 之外可以被访问的服务端口号 port</li>\n<li>指定加入某个 Overlay 网络以便 Service 与 Service 之间可以建立连接并通讯，</li>\n<li>指定该 Service 所要使用的 CPU 和 内存的大小，</li>\n<li>指定一个滚动更新的策略 (Rolling Update Policy)</li>\n<li>指定多少个 Task 的副本 (replicas) 在 Swarm 集群中同时存在</li>\n</ul>\n<p>如下所示，<code>docker-swarm.yml</code>文件是一个YAML文件，它定义了如何Docker容器在生产中应表现。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">version: &quot;3&quot;</span><br><span class=\"line\">services:</span><br><span class=\"line\">  web:</span><br><span class=\"line\">    # replace username/repo:tag with your name and image details</span><br><span class=\"line\">    image: username/repo:tag</span><br><span class=\"line\">    deploy:</span><br><span class=\"line\">      replicas: 5</span><br><span class=\"line\">      resources:</span><br><span class=\"line\">        limits:</span><br><span class=\"line\">          cpus: &quot;0.1&quot;</span><br><span class=\"line\">          memory: 50M</span><br><span class=\"line\">      restart_policy:</span><br><span class=\"line\">        condition: on-failure</span><br><span class=\"line\">    ports:</span><br><span class=\"line\">      - &quot;4000:80&quot;</span><br><span class=\"line\">    networks:</span><br><span class=\"line\">      - webnet</span><br><span class=\"line\">networks:</span><br><span class=\"line\">  webnet:</span><br></pre></td></tr></table></figure>\n<p>该<code>docker-swarm.yml</code>文件告诉Docker执行以下操作：</p>\n<ul>\n<li>拉username/repo 仓库中拉取镜像。</li>\n<li>将该映像的5个实例作为一个被调用的服务运行<code>web</code>，限制每个实例使用，最多10％的CPU（跨所有内核）和50MB的RAM。</li>\n<li>如果一个失败，立即重启容器。</li>\n<li>将主机上的端口4000映射到<code>web</code>端口80。</li>\n<li>指示<code>web</code>容器通过称为负载平衡的网络共享端口80 <code>webnet</code>。（在内部，容器本身<code>web</code>在短暂的端口发布到 80端口。）</li>\n<li><code>webnet</code>使用默认设置（负载平衡的覆盖网络）定义网络。</li>\n</ul>\n<h2 id=\"4-1-服务和任务\"><a href=\"#4-1-服务和任务\" class=\"headerlink\" title=\"4.1 服务和任务\"></a>4.1 服务和任务</h2><p>我们一般使用如下命令来部署一个服务到swarm中：</p>\n<p><code>docker stack deploy -c docker-swarm.yml myservice。</code></p>\n<p>docker-swarm.yml中定义了服务所需要达到的状态。我们不需要告诉swarm如何做，我们只需要告诉swarm 我们想要的服务状态是什么，swarm最终会帮我们达到这个状态。</p>\n<p>swarm管理器将服务定义为服务的所需状态。swarm将群中节点上的服务调度为一个或多个副本任务。任务在群集中的节点上彼此独立地运行。</p>\n<p>例如，想要在HTTP侦听器的三个实例之间进行负载平衡。下图显示了具有三个副本的HTTP侦听器服务。监听器的三个实例中的每一个都是集群中的任务。容器是一个孤立的过程。在swarm模式模型中，每个任务只调用一个容器。任务类似于调度程序放置容器的“槽”。容器生效后，调度程序会识别该任务处于运行状态。如果容器未通过运行状况检查或终止，则任务将终止。</p>\n<p><img src=\"./services-diagram.png\" alt=\"\"></p>\n<p>Services，Tasks 和 Containers 之间的关系可以用上面这张图来描述。来看这张图的逻辑，表示用户想要通过 Manager 节点部署一个有 3 个副本的 Nginx 的 Service，Manager 节点接收到用户的 Service definition 后，便开始对该 Service 进行调度，将会在当前可用的Worker（或者Manager ）节点中启动相应的 Tasks 以及相关的副本；所以可以看到，Service 实际上是 Task 的定义，而 Task 则是执行在节点上的程序。</p>\n<p>Task 是什么呢？其实就是一个 Container，只是，在 Swarm 中，每个 Task 都有自己的名字和编号，如图，比如 nginx.1、nginx.2 和 nginx.3，这些 Container 各自运行在各自的 node 上，当然，一个 node 上可以运行多个 Container；</p>\n<h2 id=\"4-2-任务调度\"><a href=\"#4-2-任务调度\" class=\"headerlink\" title=\"4.2 任务调度\"></a>4.2 任务调度</h2><p>任务是swarm集群内调度的原子单位。在创建或更新服务时，声明定义服务状态，协调器通过调度任务来实现所需的状态。例如上例的服务，该服务指示协调器始终保持三个HTTP侦听器实例的运行。协调器通过创建三个任务来响应。每个任务都是调度程序通过生成容器来填充的插槽。容器是任务的实例化。如果HTTP侦听器任务随后未通过其运行状况检查或崩溃，则协调器会创建一个新的副本任务，该任务会生成一个新容器。</p>\n<p>任务是单向机制。它通过一系列状态单调进行：已分配，准备，运行等。如果任务失败，则协调器将删除任务及其容器，然后根据服务指定的所需状态创建新任务以替换它。</p>\n<p>Docker swarm mode 模式的底层技术实际上就是指的是调度器( scheduler )和编排器( orchestrator )；下面这张图展示了 Swarm mode 如何从一个 Service 的创建请求并且成功将该 Service 分发到 worker 节点上执行的过程。</p>\n<p><img src=\".\\swarm-service-lifecycle.png\" alt=\"\"></p>\n<ul>\n<li>首先，看上半部分Swarm manager <ol>\n<li>用户通过 Docker Engine Client 使用命令 docker service create 提交 Service definition，</li>\n<li>根据 Service definition 创建相应的 Task，</li>\n<li>为 Task 分配 IP 地址，<br>注意，这是分配运行在 Swarm 集群中 Container 的 IP 地址，该 IP 地址最佳的分配地点是在这里，因为 Manager 节点上保存得有最新最全的 Tasks 的状态信息，为了保证不与其他的 Task 分配到相同的 IP，所以在这里就将 IP 地址给初始化好；</li>\n<li>将 Task 分发到 Node 上，可以是 Manager 节点也可以使 Worker 节点，</li>\n<li>对 Worker 节点进行相应的初始化使得它可以执行 Task</li>\n</ol>\n</li>\n<li>接着，看下半部分Swarm  Work<ol>\n<li>首先连接 manager 的分配器( scheduler)检查该 task</li>\n<li>验证通过以后，便开始通过 Worker 节点上的执行器( executor )执行；</li>\n</ol>\n</li>\n</ul>\n<p>注意，上述 task 的执行过程是一种单向机制，比如它会按顺序的依次经历 assigned, prepared 和 running 等执行状态，不过在某些特殊情况下，在执行过程中，某个 task 失败了( fails )，编排器( orchestrator )直接将该 task 以及它的 container 给删除掉，然后在其它节点上另外创建并执行该 task；</p>\n<h2 id=\"4-3-任务状态\"><a href=\"#4-3-任务状态\" class=\"headerlink\" title=\"4.3 任务状态\"></a>4.3 任务状态</h2><p>通过运行<code>docker service ps &lt;service-name&gt;</code>以获取任务的状态。该 <code>CURRENT STATE</code>字段显示任务的状态以及任务的持续时间 。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[root@swarm01 ~]# docker service ps myweb_web</span><br><span class=\"line\">ID                  NAME                IMAGE               NODE                DESIRED STATE       CURRENT STATE            ERROR                         PORTS</span><br><span class=\"line\">52lolsfykj4t        myweb_web.1         httpd:v2            swarm03             Running             Running 16 minutes ago</span><br><span class=\"line\">zwpufyfacagy         \\_ myweb_web.1     httpd:v2            swarm03             Shutdown            Failed 16 minutes ago    &quot;task: non-zero exit (255)&quot;</span><br><span class=\"line\">00fhp9k1byf6        myweb_web.2         httpd:v2            swarm02             Running             Running 16 minutes ago</span><br><span class=\"line\">kgtiyw39xpcc         \\_ myweb_web.2     httpd:v2            swarm02             Shutdown            Failed 17 minutes ago    &quot;task: non-zero exit (255)&quot;</span><br><span class=\"line\">vefk4yn0b00m        myweb_web.3         httpd:v2            swarm01             Running             Running 16 minutes ago</span><br><span class=\"line\">pnj4z0j281ng         \\_ myweb_web.3     httpd:v2            swarm01             Shutdown            Failed 17 minutes ago    &quot;task: non-zero exit (255)&quot;</span><br></pre></td></tr></table></figure>\n<p>任务具有如下几种状态：</p>\n<table>\n<thead>\n<tr>\n<th>任务状态</th>\n<th>描述</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><code>NEW</code></td>\n<td>任务已初始化。</td>\n</tr>\n<tr>\n<td><code>PENDING</code></td>\n<td>分配了任务的资源。</td>\n</tr>\n<tr>\n<td><code>ASSIGNED</code></td>\n<td>Docker将任务分配给节点。</td>\n</tr>\n<tr>\n<td><code>ACCEPTED</code></td>\n<td>该任务被工作节点接受。如果工作节点拒绝该任务，则状态将更改为<code>REJECTED</code>。</td>\n</tr>\n<tr>\n<td><code>PREPARING</code></td>\n<td>Docker正在准备任务。</td>\n</tr>\n<tr>\n<td><code>STARTING</code></td>\n<td>Docker正在开始这项任务。</td>\n</tr>\n<tr>\n<td><code>RUNNING</code></td>\n<td>任务正在执行。</td>\n</tr>\n<tr>\n<td><code>COMPLETE</code></td>\n<td>任务退出时没有错误代码。</td>\n</tr>\n<tr>\n<td><code>FAILED</code></td>\n<td>任务退出并显示错误代码。</td>\n</tr>\n<tr>\n<td><code>SHUTDOWN</code></td>\n<td>Docker请求关闭任务。</td>\n</tr>\n<tr>\n<td><code>REJECTED</code></td>\n<td>工作节点拒绝了该任务。</td>\n</tr>\n<tr>\n<td><code>ORPHANED</code></td>\n<td>该节点停机时间过长。</td>\n</tr>\n<tr>\n<td><code>REMOVE</code></td>\n<td>该任务不是终端，但关联的服务已被删除或缩小。</td>\n</tr>\n</tbody>\n</table>\n<p>如果在某个 service 的<a href=\"http://www.shangyang.me/2018/02/01/docker-swarm-03-architect/#%E8%B0%83%E5%BA%A6%E8%BF%87%E7%A8%8B\" target=\"_blank\" rel=\"noopener\">调度过程</a>中，发现当前没有可用的 node 资源可以执行该 service，这个时候，该 service 的状态将会保持为 pending 的状态；下面，我们来看一些例子可能使得 service 维持在 pending 状态，</p>\n<ul>\n<li>如果当所有的节点都被停止或者进入了 drained 状态，这个时候，你试图创建一个 service，该 service 将会一直保持 pending 状态直到当前某个节点可用为止；不过要注意的是，第一个恢复的 node 将会得到所有的 task 调度请求，接收并执行，因此，这种情况在 production 环境上要尽量避免；</li>\n<li>你可以为你的 service 设置执行所需的内存大小，在调度的过程当中，发现没有一个 node 能够满足你的内存请求，那么你当前的 service 将会一直处于 pending 状态直到有满足需求的 node 出现；</li>\n<li>可以对服务施加放置约束，并且可能无法在给定时间遵守约束。 </li>\n</ul>\n<p>上面的例子都表明一个事实，就是你期望的执行条件与 Swarm 现有的可用资源的情况不匹配，不吻合，因此，作为 Swarm 的管理者，应该考虑对 Swarm 集群整体进行扩容；</p>\n<h2 id=\"4-4-服务副本与全局服务\"><a href=\"#4-4-服务副本与全局服务\" class=\"headerlink\" title=\"4.4 服务副本与全局服务\"></a>4.4 服务副本与全局服务</h2><p>Docker Swarm 有两种类型的服务部署模式：复制和全局。</p>\n<p>Swarm通过<code>--mode</code>选项设置服务类型，提供了两种模式：一种是replicated，我们可以指定服务Task的个数（也就是需要创建几个冗余副本），这也是Swarm默认使用的服务类型；另一种是global，这样会在Swarm集群的每个Node上都创建一个服务。如下图所示（出自Docker官网），是一个包含replicated和global模式的Swarm集群： </p>\n<p><img src=\".\\docker-swarm-replicated-vs-global.png\" alt=\"\"></p>\n<p>上图中，黄色表示的replicated模式下的Service Replicas，灰色表示global模式下Service的分布。 </p>\n<p>在Swarm mode下使用Docker，可以实现部署运行服务、服务扩容缩容、删除服务、滚动更新等功能。</p>\n<h2 id=\"4-5-调度策略\"><a href=\"#4-5-调度策略\" class=\"headerlink\" title=\"4.5 调度策略\"></a>4.5 调度策略</h2><p>Swarm提供了几种不同的方法来控制服务任务可以在哪些节点上运行。</p>\n<ul>\n<li><p>可以指定服务是需要运行特定数量的副本还是应该在每个工作节点上全局运行。请参阅2.4服务副本与全局服务。</p>\n</li>\n<li><p>您可以配置服务的 CPU或内存要求，该服务仅在满足这些要求的节点上运行。</p>\n</li>\n<li><p>通过设置约束标签 –constraint ，您可以将服务配置为仅在具有特定（任意）元数据集的节点上运行，并且如果不存在适当的节点，则会导致部署失败。例如，您可以指定您的服务只应在任意标签<code>pci_compliant</code>设置为的节点上运行 <code>true</code>。</p>\n</li>\n<li><p>通过设置首选项 –preferences，您可以将具有一系列值的任意标签应用于每个节点，并使用算法将服务的任务分布到这些节点上。目前，唯一支持的算法是<code>spread</code>，该算法将尝试均匀放置它们。例如，如果<code>rack</code>使用值为1-10 的标签标记每个节点，然后指定键入的放置首选项<code>rack</code>，服务任务将尽可能均匀地放置在具有标签的所有节点上。如果具有<code>rack</code>标签的节点个数，大于任务个数，那么则会根据rack 值的大小排序，然后优先选择排名靠前的节点。</p>\n<p>与约束不同，放置首选项是尽力而为，如果没有节点可以满足首选项，则服务不会失败。如果为服务指定放置首选项，则当集群管理器决定哪些节点应运行服务任务时，与该首选项匹配的节点的排名会更高。其他因素，例如服务的高可用性，也是计划节点运行服务任务的因素。例如，如果您有N个节点带有机架标签（然后是其他一些节点），并且您的服务配置为运行N + 1个副本，那么+1将在一个尚未拥有该服务的节点上进行调度，如果无论该节点是否具有<code>rack</code>标签。</p>\n</li>\n</ul>\n<h2 id=\"4-6-服务管理\"><a href=\"#4-6-服务管理\" class=\"headerlink\" title=\"4.6 服务管理\"></a>4.6 服务管理</h2><p>docker swarm 还可以使用 stack 这一模型，用来部署管理和关联不同的服务。stack 功能比service 还要强大，因为它具备多服务编排部署的能力。<strong>stack file</strong> 是一种 yaml 格式的文件，类似于 docker-compose.yml 文件，它定义了一个或多个服务，并定义了服务的环境变量、部署标签、容器数量以及相关的环境特定配置等。 </p>\n<p>以下实验仅仅创建利用stack 创建单一服务。</p>\n<h3 id=\"4-6-1-创建服务\"><a href=\"#4-6-1-创建服务\" class=\"headerlink\" title=\"4.6.1 创建服务\"></a>4.6.1 创建服务</h3><p>服务配置文件docker-swarm.yml ，内容如下</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">version: &apos;3&apos;</span><br><span class=\"line\">services:</span><br><span class=\"line\">  web:</span><br><span class=\"line\">    image: &quot;httpd:v1&quot;</span><br><span class=\"line\">    deploy:</span><br><span class=\"line\">      replicas: 3</span><br><span class=\"line\">      resources:</span><br><span class=\"line\">        limits:</span><br><span class=\"line\">          cpus: &quot;0.5&quot;</span><br><span class=\"line\">          memory: 256M</span><br><span class=\"line\">      restart_policy:</span><br><span class=\"line\">        condition: on-failure</span><br><span class=\"line\">    ports:</span><br><span class=\"line\">     - &quot;7080:80&quot;</span><br><span class=\"line\">    volumes:</span><br><span class=\"line\">     - /var/www/html:/usr/local/apache2/htdocs</span><br></pre></td></tr></table></figure>\n<p>image: “httpd” ：httpd是docker hub 下载的镜像。提供简单的httpd服务，此处用来做服务都高可用验证。</p>\n<p>replicas: 3 表示该服务副本有3个，可对服务副本进行扩缩容，后续会讲到。</p>\n<p>/var/www/html:/usr/local/apache2/htdocs ：表示httpd 首页的内容。各个swarm节点上展示的内容分别是其主机名信息。</p>\n<p>在docker-swarm.yml 同级目录下，执行命令</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[root@swarm01 swarm]# docker stack deploy -c docker-swarm.yml myservice</span><br><span class=\"line\">Creating network myservice_default</span><br><span class=\"line\">Creating service myservice_web</span><br></pre></td></tr></table></figure>\n<h3 id=\"4-6-2-查看服务\"><a href=\"#4-6-2-查看服务\" class=\"headerlink\" title=\"4.6.2 查看服务\"></a>4.6.2 查看服务</h3><p>查看服务创建是否成功。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[root@swarm01 swarm]# docker stack ls</span><br><span class=\"line\">NAME                SERVICES</span><br><span class=\"line\">myservice           1</span><br><span class=\"line\">[root@swarm01 swarm]# docker stack services myservice</span><br><span class=\"line\">ID                  NAME                MODE                REPLICAS            IMAGE               PORTS</span><br><span class=\"line\">kom8x49f7qv2        myservice_web       replicated          3/3                 httpd:latest        *:7080-&gt;80/tcp</span><br><span class=\"line\">[root@swarm01 swarm]# docker stack ps myservice</span><br><span class=\"line\">ID                  NAME                IMAGE               NODE                DESIRED STATE       CURRENT STATE            ERROR               PORTS</span><br><span class=\"line\">sybn1dur1wmw        myservice_web.1     httpd:latest        swarm03             Running             Running 45 seconds ago</span><br><span class=\"line\">mlcb5r9vzv6z        myservice_web.2     httpd:latest        swarm03             Running             Running 45 seconds ago</span><br><span class=\"line\">fw21lcu3zp83        myservice_web.3     httpd:latest        swarm01             Running             Running 45 seconds ago</span><br></pre></td></tr></table></figure>\n<p>可以看到 swarm01 上面运行了1个容器，swarm03上面运行了2个，而swarm02上面一个容器都没有，这是咋回事呢？哦，原来我们在创建集群过程中，对swarm02 做了状态变更的操作，将其设置为不运行任务。<code>docker node update  --availability drain  swarm02</code>. 查看集群状态。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[root@swarm01 swarm]# docker node ls</span><br><span class=\"line\">ID                            HOSTNAME            STATUS              AVAILABILITY        MANAGER STATUS      ENGINE VERSION</span><br><span class=\"line\">epoic1y0vv830vnwbc6nnacjc *   swarm01             Ready               Active              Leader              18.03.1-ce</span><br><span class=\"line\">nsqqv181e0r7mu77azixtqhzl     swarm02             Ready               Drain               Reachable           18.03.1-ce</span><br><span class=\"line\">mkkxaeqergz8xw21bbkd47q1c     swarm03             Ready               Active              Reachable           18.03.1-ce</span><br></pre></td></tr></table></figure>\n<p>此时打开浏览器，输入HTTP URL。任意swarm集群节点都可以访问httpd服务。</p>\n<p><img src=\".\\swarm-web.png\" alt=\"\"></p>\n<p>需要说明的是，由于3个节点的/var/www/html 路径未做共享存储。因此swarm会随机显示一个swarm节点的网页内容。</p>\n<h3 id=\"4-6-3-服务扩缩容\"><a href=\"#4-6-3-服务扩缩容\" class=\"headerlink\" title=\"4.6.3 服务扩缩容\"></a>4.6.3 服务扩缩容</h3><p>修改docker-swarm.yml 配置字段 <code>replicas: 5</code> ，将容器个数提升到5个。由于实验环境资源有限，因此这里将swarm02状态变更为可以状态，用以分配容器。</p>\n<p><code>docker node update  --availability active swarm02</code>.</p>\n<p>重新执行部署命令<code>docker stack deploy -c docker-swarm.yml myservice</code></p>\n<p>也可以通过命令直接对服务进行操作，但是不推荐。 <code>docker service scale 服务ID=服务个数</code></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[root@swarm01 swarm]# docker stack ps myservice</span><br><span class=\"line\">ID                  NAME                IMAGE               NODE                DESIRED STATE       CURRENT STATE                    ERROR               PORTS</span><br><span class=\"line\">unwepav5z9nz        myservice_web.1     httpd:v1            swarm02             Running             Running less than a second ago</span><br><span class=\"line\">3j5o1tktcg1l        myservice_web.2     httpd:v1            swarm01             Running             Preparing 3 seconds ago</span><br><span class=\"line\">wl1unt6lynft        myservice_web.3     httpd:v1            swarm03             Running             Running less than a second ago</span><br><span class=\"line\">[root@swarm01 swarm]# vi docker-swarm.yml</span><br><span class=\"line\">[root@swarm01 swarm]# docker stack deploy -c docker-swarm.yml myservice</span><br><span class=\"line\">Updating service myservice_web (id: 85quzwi5k0btkn5wlht4jbco9)</span><br><span class=\"line\">image httpd:v1 could not be accessed on a registry to record</span><br><span class=\"line\">its digest. Each node will access httpd:v1 independently,</span><br><span class=\"line\">possibly leading to different nodes running different</span><br><span class=\"line\">versions of the image.</span><br><span class=\"line\"></span><br><span class=\"line\">[root@swarm01 swarm]# docker stack ps myservice</span><br><span class=\"line\">ID                  NAME                IMAGE               NODE                DESIRED STATE       CURRENT STATE                    ERROR               PORTS</span><br><span class=\"line\">unwepav5z9nz        myservice_web.1     httpd:v1            swarm02             Running             Running 24 seconds ago</span><br><span class=\"line\">3j5o1tktcg1l        myservice_web.2     httpd:v1            swarm01             Running             Running 31 seconds ago</span><br><span class=\"line\">wl1unt6lynft        myservice_web.3     httpd:v1            swarm03             Running             Running 24 seconds ago</span><br><span class=\"line\">ujedjxbuffxp        myservice_web.4     httpd:v1            swarm01             Running             Running less than a second ago</span><br><span class=\"line\">ml9qvjh2add7        myservice_web.5     httpd:v1            swarm02             Running             Running less than a second ago</span><br></pre></td></tr></table></figure>\n<p>同样，对于服务缩容，也仅需要修改<code>replicas:</code> 配置即可。这里不再演示。</p>\n<h3 id=\"4-6-4-删除服务\"><a href=\"#4-6-4-删除服务\" class=\"headerlink\" title=\"4.6.4 删除服务\"></a>4.6.4 删除服务</h3><p>例如，删除myredis应用服务，执行docker service rm myredis，则应用服务myredis的全部副本都会被删除。 </p>\n<p>本例使用的是stack ，删除stack ，即可删除其下的服务。没错，docker语法非常类似,<code>docker stack rm myservice</code>。</p>\n<p>需要注意的是，对swarm上面服务的操作，都必须在manager节点上运行。</p>\n<h3 id=\"4-6-5-服务升降级\"><a href=\"#4-6-5-服务升降级\" class=\"headerlink\" title=\"4.6.5 服务升降级\"></a>4.6.5 服务升降级</h3><p>服务升降级，对应到docker中，则利用了容器镜像的更新，升级很好理解，直接把tag 标签版本往上提。那么降级，其实也可以理解为另外一种“升级“，只是镜像内容是上一个版本的而已。对于镜像标签的命令应该遵循一套严格的规范，这里不再赘述。</p>\n<p>先介绍利用service 命令行工具的用法。</p>\n<p>服务的滚动更新，这里我参考官网文档的例子说明。在Manager Node上执行如下命令：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">docker service create --replicas 3  --name redis --update-delay 10s redis:3.0.6</span><br></pre></td></tr></table></figure>\n<p>上面通过指定 –update-delay 选项，表示需要进行更新的服务，每次成功部署一个，延迟10秒钟，然后再更新下一个服务。如果某个服务更新失败，则Swarm的调度器就会暂停本次服务的部署更新。</p>\n<p>另外，也可以更新已经部署的服务所在容器中使用的Image的版本，例如执行如下命令：</p>\n<p>将Redis服务对应的Image版本有3.0.6更新为3.0.7，同样，如果更新失败，则暂停本次更新。</p>\n<p>那么，stack file 如何定义滚动更新呢？</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">update_config:</span><br><span class=\"line\">  parallelism: 2</span><br><span class=\"line\">  delay: 10s</span><br></pre></td></tr></table></figure>\n<p>修改docker-swarm.yml 如下</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">version: &apos;3&apos;</span><br><span class=\"line\">services:</span><br><span class=\"line\">  web:</span><br><span class=\"line\">    image: &quot;httpd:v2&quot;</span><br><span class=\"line\">    deploy:</span><br><span class=\"line\">      replicas: 3</span><br><span class=\"line\">      update_config:</span><br><span class=\"line\">        parallelism: 2</span><br><span class=\"line\">        delay: 30s</span><br><span class=\"line\">      resources:</span><br><span class=\"line\">        limits:</span><br><span class=\"line\">          cpus: &quot;0.5&quot;</span><br><span class=\"line\">          memory: 256M</span><br><span class=\"line\">      restart_policy:</span><br><span class=\"line\">        condition: on-failure</span><br><span class=\"line\">    ports:</span><br><span class=\"line\">     - &quot;7080:80&quot;</span><br><span class=\"line\">    volumes:</span><br><span class=\"line\">     - /var/www/html:/usr/local/apache2/htdocs</span><br></pre></td></tr></table></figure>\n<p>parallelism 表示同时升级的个数，delay 则表示间隔多长时间升级。</p>\n<p>同时将httpd 由v1 升级到v2，这里我们使用同一个镜像，只是tag 修改了一下。查看服务状态</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[root@swarm01 swarm]# docker stack ps myservice</span><br><span class=\"line\">ID                  NAME                  IMAGE               NODE                DESIRED STATE       CURRENT STATE                 ERROR               PORTS</span><br><span class=\"line\">qk8pvvc9nmv0        myservice_web.1       httpd:v2            swarm01             Running             Running 56 seconds ago</span><br><span class=\"line\">unwepav5z9nz         \\_ myservice_web.1   httpd:v1            swarm02             Shutdown            Shutdown 48 seconds ago</span><br><span class=\"line\">uyj0pm1df9hl        myservice_web.2       httpd:v2            swarm02             Running             Running 46 seconds ago</span><br><span class=\"line\">3j5o1tktcg1l         \\_ myservice_web.2   httpd:v1            swarm01             Shutdown            Shutdown about a minute ago</span><br><span class=\"line\">qdjov5zf5alb        myservice_web.3       httpd:v2            swarm03             Running             Running 10 seconds ago</span><br><span class=\"line\">wl1unt6lynft         \\_ myservice_web.3   httpd:v1            swarm03             Shutdown            Shutdown 12 seconds ago</span><br></pre></td></tr></table></figure>\n<p>可以看到同时只有2个容器在升级，同时第3个容器也是在间隔了30s 之后，才开始升级。</p>\n<p> –filter 过滤条件<code>docker stack ps  --filter  &quot;desired-state=running&quot;  myservice</code></p>\n<h3 id=\"4-6-6-服务健康检查\"><a href=\"#4-6-6-服务健康检查\" class=\"headerlink\" title=\"4.6.6 服务健康检查\"></a>4.6.6 服务健康检查</h3><p>对于容器而言，最简单的健康检查是进程级的健康检查，即检验进程是否存活。Docker Daemon会自动监控容器中的PID1进程，如果docker run命令中指明了restart policy，可以根据策略自动重启已结束的容器。在很多实际场景下，仅使用进程级健康检查机制还远远不够。比如，容器进程虽然依旧运行却由于应用死锁无法继续响应用户请求，这样的问题是无法通过进程监控发现的 。</p>\n<p>下面先介绍Docker容器健康检查机制，之后再介绍Docker Swarm mode的新特性。</p>\n<p><strong>Docker 原生健康检查能力</strong></p>\n<p>而自 1.12 版本之后，Docker 引入了原生的健康检查实现，可以在Dockerfile中声明应用自身的健康检测配置。HEALTHCHECK 指令声明了健康检测命令，用这个命令来判断容器主进程的服务状态是否正常，从而比较真实的反应容器实际状态。</p>\n<p>HEALTHCHECK 指令格式：</p>\n<ul>\n<li>HEALTHCHECK [选项] CMD &lt;命令&gt;：设置检查容器健康状况的命令</li>\n<li>HEALTHCHECK NONE：如果基础镜像有健康检查指令，使用这行可以屏蔽掉</li>\n</ul>\n<p>注：在Dockerfile中 HEALTHCHECK 只可以出现一次，如果写了多个，只有最后一个生效。</p>\n<p>使用包含 HEALTHCHECK 指令的dockerfile构建出来的镜像，在实例化Docker容器的时候，就具备了健康状态检查的功能。启动容器后会自动进行健康检查。</p>\n<blockquote>\n<p>HEALTHCHECK 支持下列选项：</p>\n<ul>\n<li>interval=&lt;间隔&gt;：两次健康检查的间隔，默认为 30 秒;</li>\n<li>timeout=&lt;间隔&gt;：健康检查命令运行超时时间，如果超过这个时间，本次健康检查就被视为失败，默认 30 秒;</li>\n<li>retries=&lt;次数&gt;：当连续失败指定次数后，则将容器状态视为 unhealthy，默认 3 次。</li>\n<li>start-period=&lt;间隔&gt;: 应用的启动的初始化时间，在启动过程中的健康检查失效不会计入，默认 0 秒; (从17.05)引入</li>\n</ul>\n</blockquote>\n<p>在 HEALTHCHECK [选项] CMD 后面的命令，格式和 ENTRYPOINT 一样，分为 shell 格式，和 exec 格式。命令的返回值决定了该次健康检查的成功与否：</p>\n<ul>\n<li>0：成功;</li>\n<li>1：失败;</li>\n<li>2：保留值</li>\n</ul>\n<p>容器启动之后，初始状态会为 starting (启动中)。Docker Engine会等待 interval 时间，开始执行健康检查命令，并周期性执行。如果单次检查返回值非0或者运行需要比指定 timeout 时间还长，则本次检查被认为失败。如果健康检查连续失败超过了 retries 重试次数，状态就会变为 unhealthy (不健康)。</p>\n<p>注：</p>\n<ul>\n<li>一旦有一次健康检查成功，Docker会将容器置回 healthy (健康)状态</li>\n<li>当容器的健康状态发生变化时，Docker Engine会发出一个 health_status 事件。</li>\n</ul>\n<p>假设我们有个镜像是个最简单的 Web 服务，我们希望增加健康检查来判断其 Web 服务是否在正常工作，我们可以用 curl来帮助判断，其 Dockerfile 的 HEALTHCHECK 可以这么写：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">FROM elasticsearch:5.5 </span><br><span class=\"line\"> </span><br><span class=\"line\">HEALTHCHECK --interval=5s --timeout=2s --retries=12 \\ </span><br><span class=\"line\">  CMD curl --silent --fail localhost:9200/_cluster/health || exit 1</span><br></pre></td></tr></table></figure>\n<p>然后编译镜像，并运行</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">docker build -t test/elasticsearch:5.5 . </span><br><span class=\"line\">docker run --rm -d \\ </span><br><span class=\"line\">    --name=elasticsearch \\ </span><br><span class=\"line\">    test/elasticsearch:5.5</span><br></pre></td></tr></table></figure>\n<p>执行 docker ps容器检查命令，发现过了几秒之后，Elasticsearch容器从 starting 状态进入了 healthy 状态</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker ps </span><br><span class=\"line\">CONTAINER ID        IMAGE                    COMMAND                  CREATED             STATUS                            PORTS                NAMES </span><br><span class=\"line\">c9a6e68d4a7f        test/elasticsearch:5.5   &quot;/docker-entrypoin...&quot;   2 seconds ago       Up 2 seconds (health: starting)   9200/tcp, 9300/tcp   elasticsearch </span><br><span class=\"line\">$ docker ps </span><br><span class=\"line\">CONTAINER ID        IMAGE                    COMMAND                  CREATED             STATUS                    PORTS                NAMES </span><br><span class=\"line\">c9a6e68d4a7f        test/elasticsearch:5.5   &quot;/docker-entrypoin...&quot;   14 seconds ago      Up 13 seconds (healthy)   9200/tcp, 9300/tcp   elasticsearch</span><br></pre></td></tr></table></figure>\n<p><strong>Docker Swarm健康检查能力</strong></p>\n<p>在Docker 1.13之后，在Docker Swarm mode中提供了对健康检查策略的支持。</p>\n<p>可以在 <code>docker service create</code> 命令中指明健康检查策略</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker service create -d \\</span><br><span class=\"line\">    --name=elasticsearch \\</span><br><span class=\"line\">    --health-cmd=&quot;curl --silent --fail localhost:9200/_cluster/health || exit 1&quot; \\</span><br><span class=\"line\">    --health-interval=5s \\</span><br><span class=\"line\">    --health-retries=12 \\</span><br><span class=\"line\">    --health-timeout=2s \\</span><br><span class=\"line\">    elasticsearch</span><br></pre></td></tr></table></figure>\n<p>在Swarm模式下，Swarm manager会监控服务task的健康状态，如果容器进入 <code>unhealthy</code> 状态，它会停止容器并且重新启动一个新容器来取代它。这个过程中会自动更新服务的 load balancer (routing mesh) 后端或者 DNS记录，可以保障服务的可用性。</p>\n<p>在1.13版本之后，在服务更新阶段也增加了对健康检查的支持，这样在新容器完全启动成功并进入健康状态之前，load balancer/DNS解析不会将请求发送给它。这样可以保证应用在更新过程中请求不会中断。</p>\n<p><img src=\".\\healthcheck.png\" alt=\"\"></p>\n<p>docker 官方网站有些常用容器的健康检查的例子：<a href=\"https://github.com/docker-library/healthcheck\" target=\"_blank\" rel=\"noopener\">https://github.com/docker-library/healthcheck</a></p>\n<h1 id=\"5-附录\"><a href=\"#5-附录\" class=\"headerlink\" title=\"5.附录\"></a>5.附录</h1><h2 id=\"5-1-参考文档\"><a href=\"#5-1-参考文档\" class=\"headerlink\" title=\"5.1 参考文档\"></a>5.1 参考文档</h2><p>官网文档：</p>\n<p><a href=\"https://docs.docker.com/engine/swarm/\" target=\"_blank\" rel=\"noopener\">https://docs.docker.com/engine/swarm/</a></p>\n<p><a href=\"https://docs.docker.com/get-started/part4/\" target=\"_blank\" rel=\"noopener\">https://docs.docker.com/get-started/part4/</a></p>\n<p><a href=\"https://docs.docker.com/network/overlay/#publish-ports-on-an-overlay-network\" target=\"_blank\" rel=\"noopener\">https://docs.docker.com/network/overlay/#publish-ports-on-an-overlay-network</a></p>\n<p>技术博客：</p>\n<p><a href=\"http://www.dockerinfo.net/2552.html\" target=\"_blank\" rel=\"noopener\">http://www.dockerinfo.net/2552.html</a></p>\n<p><a href=\"https://www.cnblogs.com/bigberg/p/8761047.html\" target=\"_blank\" rel=\"noopener\">https://www.cnblogs.com/bigberg/p/8761047.html</a></p>\n<p><a href=\"http://blog.51cto.com/cloudman/1968287\" target=\"_blank\" rel=\"noopener\">http://blog.51cto.com/cloudman/1968287</a> </p>\n<p><a href=\"https://neuvector.com/network-security/docker-swarm-container-networking/\" target=\"_blank\" rel=\"noopener\">https://neuvector.com/network-security/docker-swarm-container-networking/</a></p>\n<p><a href=\"http://www.uml.org.cn/yunjisuan/201708282.asp\" target=\"_blank\" rel=\"noopener\">http://www.uml.org.cn/yunjisuan/201708282.asp</a></p>"}],"PostAsset":[],"PostCategory":[{"post_id":"cjhajczof000be4rww9peqwrt","category_id":"cjhajczoc0007e4rw4adiezct","_id":"cjhajczop000ie4rwtu7k89jk"},{"post_id":"cjhajczo60004e4rwozbl2dwd","category_id":"cjhajczoc0007e4rw4adiezct","_id":"cjhajczor000le4rwqyh3p0nz"},{"post_id":"cjhajczod0009e4rwm9mn90tf","category_id":"cjhajczoc0007e4rw4adiezct","_id":"cjhajczou000pe4rwc0wkr6ce"},{"post_id":"cjhajczos000me4rwfi9ljy1r","category_id":"cjhajczoc0007e4rw4adiezct","_id":"cjhajczp0000ue4rwnp3lxac1"},{"post_id":"cjhajczoy000se4rwaasmefg4","category_id":"cjhajczoc0007e4rw4adiezct","_id":"cjhajczp4000ze4rwdgx9by9b"},{"post_id":"cjhajczp50011e4rwtcresxyd","category_id":"cjhajczoc0007e4rw4adiezct","_id":"cjhajczpb0018e4rwak0up7xm"},{"post_id":"cjhajczp70014e4rwpju9ryqs","category_id":"cjhajczoc0007e4rw4adiezct","_id":"cjhajczpf001be4rw56qym44c"},{"post_id":"cjhajczp90016e4rwbgwkji48","category_id":"cjhajczoc0007e4rw4adiezct","_id":"cjhajczpo001fe4rwif4sk2uc"},{"post_id":"cjhajczpc0019e4rwe273vqqw","category_id":"cjhajczoc0007e4rw4adiezct","_id":"cjhajczpu001ie4rwdnlv1u3e"},{"post_id":"cjhajczpp001ge4rw72egfcwg","category_id":"cjhajczoc0007e4rw4adiezct","_id":"cjhajczpz001le4rwpxw49tb6"},{"post_id":"cjnh6x610000064b2ft51a525","category_id":"cjhajczoc0007e4rw4adiezct","_id":"cjnh6x61h000364b252vvva2x"},{"post_id":"cjnh6x618000164b2knb5mzjw","category_id":"cjhajczoc0007e4rw4adiezct","_id":"cjnh6x61k000464b26c7jxzqe"}],"PostTag":[{"post_id":"cjhajcznm0000e4rwcj6fy8zu","tag_id":"cjhajczo20002e4rw2n47g46z","_id":"cjhajczoc0008e4rwb9oxiltv"},{"post_id":"cjhajczo80005e4rw9po13kw9","tag_id":"cjhajczo20002e4rw2n47g46z","_id":"cjhajczoe000ae4rwtwlq1k7t"},{"post_id":"cjhajcznv0001e4rwxsxtyr3d","tag_id":"cjhajczo20002e4rw2n47g46z","_id":"cjhajczoi000de4rw8tpakbvr"},{"post_id":"cjhajczo30003e4rw5oibrebo","tag_id":"cjhajczof000ce4rwn2l1g5yc","_id":"cjhajczor000ke4rwgz8nbvpw"},{"post_id":"cjhajczon000ge4rw29ef8d1k","tag_id":"cjhajczo20002e4rw2n47g46z","_id":"cjhajczot000ne4rwek86cain"},{"post_id":"cjhajczoq000je4rw5qye8mqf","tag_id":"cjhajczo20002e4rw2n47g46z","_id":"cjhajczox000re4rwwm07hpou"},{"post_id":"cjhajczov000qe4rws0y5ozjl","tag_id":"cjhajczo20002e4rw2n47g46z","_id":"cjhajczoz000te4rw66g580mv"},{"post_id":"cjhajczod0009e4rwm9mn90tf","tag_id":"cjhajczon000he4rwepopz7ng","_id":"cjhajczp2000xe4rws2bhy6na"},{"post_id":"cjhajczod0009e4rwm9mn90tf","tag_id":"cjhajczou000oe4rwqqx1geo3","_id":"cjhajczp40010e4rwbfy2k6hy"},{"post_id":"cjhajczp0000we4rwkfbth5ah","tag_id":"cjhajczo20002e4rw2n47g46z","_id":"cjhajczp70013e4rw9x0ae913"},{"post_id":"cjhajczp2000ye4rw357hwar7","tag_id":"cjhajczo20002e4rw2n47g46z","_id":"cjhajczp80015e4rwiuqw5das"},{"post_id":"cjhajczof000be4rww9peqwrt","tag_id":"cjhajczp0000ve4rwglzurwvf","_id":"cjhajczpe001ae4rwpgu193x4"},{"post_id":"cjhajczof000be4rww9peqwrt","tag_id":"cjhajczp60012e4rwtx9wiucm","_id":"cjhajczpm001de4rwtapvgvk6"},{"post_id":"cjhajczoj000ee4rwob7rym5n","tag_id":"cjhajczpa0017e4rw2en13wmp","_id":"cjhajczpt001he4rwadjs8yd6"},{"post_id":"cjhajczos000me4rwfi9ljy1r","tag_id":"cjhajczpn001ee4rwz83649iv","_id":"cjhajczpx001ke4rwnny85hho"},{"post_id":"cjhajczoy000se4rwaasmefg4","tag_id":"cjhajczpw001je4rwytcxdcy7","_id":"cjhajczq3001oe4rw6tr48c0i"},{"post_id":"cjhajczoy000se4rwaasmefg4","tag_id":"cjhajczq0001me4rw705w7u7q","_id":"cjhajczq4001pe4rwdzv6kzsz"},{"post_id":"cjhajczp50011e4rwtcresxyd","tag_id":"cjhajczq1001ne4rwygrxg4ce","_id":"cjhajczq9001se4rw62qrfrqo"},{"post_id":"cjhajczp50011e4rwtcresxyd","tag_id":"cjhajczq0001me4rw705w7u7q","_id":"cjhajczqa001te4rwo0rq7aja"},{"post_id":"cjhajczp70014e4rwpju9ryqs","tag_id":"cjhajczq7001re4rwnop7ijvo","_id":"cjhajczqb001ve4rwcln454v3"},{"post_id":"cjhajczp90016e4rwbgwkji48","tag_id":"cjhajczqa001ue4rw5nk2ouo8","_id":"cjhajczqe0020e4rwb1tcugfw"},{"post_id":"cjhajczp90016e4rwbgwkji48","tag_id":"cjhajczqb001we4rwaloy9umt","_id":"cjhajczqe0021e4rwvjkmmsap"},{"post_id":"cjhajczp90016e4rwbgwkji48","tag_id":"cjhajczqb001xe4rwqpg8ts4z","_id":"cjhajczqf0023e4rw29e9v87n"},{"post_id":"cjhajczp90016e4rwbgwkji48","tag_id":"cjhajczq0001me4rw705w7u7q","_id":"cjhajczqf0024e4rwwzy7w79y"},{"post_id":"cjhajczpc0019e4rwe273vqqw","tag_id":"cjhajczqd001ze4rwemq0y6sh","_id":"cjhajczqj0026e4rwe8td5u5q"},{"post_id":"cjhajczpc0019e4rwe273vqqw","tag_id":"cjhajczqe0022e4rwko3973u6","_id":"cjhajczqj0027e4rw66c6xwsv"},{"post_id":"cjhajczpp001ge4rw72egfcwg","tag_id":"cjhajczqh0025e4rwy17gbqrf","_id":"cjhajczqk0028e4rwxb2es7tc"},{"post_id":"cjhajczpp001ge4rw72egfcwg","tag_id":"cjhajczou000oe4rwqqx1geo3","_id":"cjhajczqk0029e4rwrtpq1cef"},{"post_id":"cjnh6x610000064b2ft51a525","tag_id":"cjnh6x61b000264b2hyfmvp4y","_id":"cjnh6x61n000764b29z03ppay"},{"post_id":"cjnh6x610000064b2ft51a525","tag_id":"cjnh6x61k000564b21wqeofl2","_id":"cjnh6x61n000864b2cae8xvc3"},{"post_id":"cjnh6x618000164b2knb5mzjw","tag_id":"cjnh6x61b000264b2hyfmvp4y","_id":"cjnh6x61p000a64b2cdjxw6tk"},{"post_id":"cjnh6x618000164b2knb5mzjw","tag_id":"cjnh6x61n000964b2ck74x4ca","_id":"cjnh6x61p000b64b2z4hzzumr"}],"Tag":[{"name":"随笔","_id":"cjhajczo20002e4rw2n47g46z"},{"name":"Google","_id":"cjhajczof000ce4rwn2l1g5yc"},{"name":"Elasticsearch","_id":"cjhajczon000he4rwepopz7ng"},{"name":"Monitor","_id":"cjhajczou000oe4rwqqx1geo3"},{"name":"Git","_id":"cjhajczp0000ve4rwglzurwvf"},{"name":"GitHub","_id":"cjhajczp60012e4rwtx9wiucm"},{"name":"Hexo","_id":"cjhajczpa0017e4rw2en13wmp"},{"name":"LAMP","_id":"cjhajczpn001ee4rwz83649iv"},{"name":"Open-falcon","_id":"cjhajczpw001je4rwytcxdcy7"},{"name":"Go","_id":"cjhajczq0001me4rw705w7u7q"},{"name":"Prometheus","_id":"cjhajczq1001ne4rwygrxg4ce"},{"name":"Python","_id":"cjhajczq7001re4rwnop7ijvo"},{"name":"TICK","_id":"cjhajczqa001ue4rw5nk2ouo8"},{"name":"InfluxDB","_id":"cjhajczqb001we4rwaloy9umt"},{"name":"Telegraf","_id":"cjhajczqb001xe4rwqpg8ts4z"},{"name":"Sensu","_id":"cjhajczqd001ze4rwemq0y6sh"},{"name":"Ruby","_id":"cjhajczqe0022e4rwko3973u6"},{"name":"Zookeeper","_id":"cjhajczqh0025e4rwy17gbqrf"},{"name":"Docker Swarm","_id":"cjnh6x61b000264b2hyfmvp4y"},{"name":"HA","_id":"cjnh6x61k000564b21wqeofl2"},{"name":"Service","_id":"cjnh6x61n000964b2ck74x4ca"}]}}